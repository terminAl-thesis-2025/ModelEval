{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluieren der Embedding Modelle\n",
    "Extrahieren der Textchunks für die Kontextdatenbank\n"
   ],
   "id": "81589414c852fa1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:33:04.466080Z",
     "start_time": "2025-04-24T07:33:02.612640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import pymupdf4llm\n",
    "import chromadb\n",
    "import time\n",
    "from semantic_text_splitter import MarkdownSplitter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "import torch\n",
    "from types import MethodType\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n"
   ],
   "id": "3a3a4cbec7d132a8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:05:49.016420Z",
     "start_time": "2025-04-23T14:01:45.464325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf_files = glob.glob(os.path.join(\"/home/m/PycharmProjects/ModelEval/data\", \"*.pdf\"))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_path in pdf_files:\n",
    "    try:\n",
    "        # Parse the PDF using pymupdf4llm\n",
    "        parsed_content = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            \"filename\": os.path.basename(pdf_path),\n",
    "            \"content\": parsed_content\n",
    "        })\n",
    "        print(f\"Successfully processed: {pdf_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")"
   ],
   "id": "f605ab2b2a6da09f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: /home/m/PycharmProjects/ModelEval/data/ubuntu-server-guide-2024-01-22.pdf\n",
      "Successfully processed: /home/m/PycharmProjects/ModelEval/data/postgresql-17-A4.pdf\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:05:50.521598Z",
     "start_time": "2025-04-23T14:05:49.070290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-small\", use_fast=True)\n",
    "\n",
    "test_text = results[0][\"content\"]\n",
    "tokens = tokenizer.encode(test_text, add_special_tokens=False)\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "total_characters = len(test_text)\n",
    "\n",
    "average_chars_per_512_tokens = (total_characters / total_tokens) * 512\n",
    "print(f\"Average characters per 512 tokens: {average_chars_per_512_tokens:.2f}\")\n"
   ],
   "id": "f19eaf67c2aa6a80",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (392480 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average characters per 512 tokens: 1743.23\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:05:55.083612Z",
     "start_time": "2025-04-23T14:05:50.528077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split each document into chunks\n",
    "splitter = MarkdownSplitter(1700)\n",
    "for result in results:\n",
    "    result[\"chunks\"] = splitter.chunks(result[\"content\"])\n",
    "\n",
    "# Flatten into lists of texts & ids\n",
    "all_texts = []\n",
    "all_ids   = []\n",
    "for res in results:\n",
    "    fname = res[\"filename\"]\n",
    "    for idx, chunk in enumerate(res[\"chunks\"]):\n",
    "        all_ids.append(f\"{fname}__chunk_{idx}\")\n",
    "        all_texts.append(chunk)\n",
    "\n",
    "print(f\"Total chunks to index: {len(all_texts)}\")"
   ],
   "id": "b32161171e9ae3fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to index: 6838\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:05:55.181990Z",
     "start_time": "2025-04-23T14:05:55.161650Z"
    }
   },
   "cell_type": "code",
   "source": "results[0][\"chunks\"]",
   "id": "ea194c8c235eda1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Ubuntu Server** is a version of the Ubuntu operating system designed and engineered as a backbone for the internet.\\n\\nUbuntu Server brings economic and technical scalability to your datacentre, public or private. Whether you want to\\ndeploy an OpenStack cloud, a Kubernetes cluster or a 50,000-node render farm, Ubuntu Server delivers the best value\\nscale-out performance available.\\n## **In this documentation**\\n\\n\\nTutorials\\n\\nGet started - a hands-on introduction to Ubuntu Server for new users\\n\\n\\nTutorials How-to guides\\nGet started - a hands-on introduction to Ubuntu Server for new users Step-by-step guides covering key operations and comm\\n\\nExplanation Reference\\nConcepts - discussion and clarification of key topics Technical information - package specifications, APIs, a\\n\\n\\nReference\\nTechnical information - package specifications, APIs, a',\n",
       " '## **Project and community**\\n\\nUbuntu Server is a member of the Ubuntu family. It’s an open source project that welcomes community projects,\\ncontributions, suggestions, fixes and constructive feedback.\\n\\nIf you find any errors or have suggestions for improvements to pages, please use the link at the bottom of each topic\\ntitled: “Help improve this document in the forum.” This link will take you to the Server Discourse forum for the\\nspecific page you are viewing. There you can share your comments or let us know about bugs with any page.\\n\\n[• Read our Code of Conduct](https://ubuntu.com/community/code-of-conduct)\\n\\n[• Get support](https://ubuntu.com/support/community-support)\\n\\n[• Join the Discourse forum](https://discourse.ubuntu.com/c/server/17)\\n\\n[• Download](https://ubuntu.com/server)\\n\\n  - Find out how to contribute to the Server Guide, or let us know your feedback and suggestions.\\n\\n[Thinking about using Ubuntu Server for your next project? Get in touch!](https://ubuntu.com/server/contact-us?product=server)\\n## **PDFs and previous releases**\\n\\nBelow are links to the previous Ubuntu Server release server guides as well as an offline copy of the current version of\\nthis site:\\n\\n[Ubuntu 20.04 LTS (Focal Fossa) and later: PDF](https://assets.ubuntu.com/ubuntu-server-guide)\\n[Ubuntu 18.04 LTS (Bionic Beaver): Web and PDF](https://help.ubuntu.com/18.04/serverguide/index.html)',\n",
       " '## **Navigation**\\n\\nNavigation\\n\\nLevel Path Navlink\\n\\n0 Introduction\\n\\n0\\n\\n1 tutorials **Tutorials**\\n\\n2 Core tutorial\\n\\n3 installation Basic installation\\n3 install/general How to operate the server installer\\n3 install/step-by-step Screen-by-screen installer guide\\n3 install/storage Configuring storage\\n2 Next steps\\n3 install/subscription Attach your Ubuntu Pro subscription\\n3 install/reporting-problems Report a problem with the installer\\n0\\n\\n1 how-to **How-to guides**\\n2 Advanced installation\\n3 install/netboot-amd64 amd64 netboot install\\n3 install/netboot-arm64 arm64 netboot install\\n3 install/netboot-ppc64el ppc64el netboot install\\n3 install/ppc64el Virtual CDROM and Petitboot on ppc64el\\n\\n1\\n\\n\\n-----',\n",
       " 'Level Path Navlink\\n\\n3 install/s390x-zvm s390x install via z/VM\\n3 install/s390x-lpar s390x install via LPAR\\n2 Automatic installation\\n3 install/autoinstall Introduction\\n3 install/autoinstall-quickstart Autoinstall quickstart\\n3 install/autoinstall-quickstart-s390x Autoinstall quickstart on s390x\\n3 install/autoinstall-reference Autoinstall reference\\n3 install/autoinstall-schema Autoinstall schema\\n3 install/vm-autoinstall-on-s390x z/VM autoinstall on s390x\\n3 install/lpar-autoinstall-on-s390x LPAR autoinstall on s390x\\n2 ROCK Images\\n3 rock-images/introduction Introduction\\n3 rock-images/container-customization-with-docker Container customization with Docker\\n3 rock-images/multi-node-configuration-with-docker-compose Multi-node configuration with Docker-Compose\\n2 Software\\n\\n3 package-management Package management\\n3 upgrade-introduction Upgrade\\n3 third-party-apt-repositories Third party APT repositories\\n3 reporting-bugs Reporting bugs\\n3 kernel-crash-dump Kernel crash dump\\n2 OpenLDAP\\n3 service-ldap-introduction Introduction\\n3 service-ldap Installation\\n3 service-ldap-access-control Access control\\n3 service-ldap-replication Replication\\n3 service-ldap-usage Simple LDAP user and group management\\n3 service-ldap-with-tls SSL/TLS\\n3 service-ldap-backup-restore Backup and restore\\n2 Kerberos\\n\\n3 kerberos-introduction Introduction\\n\\n3 service-kerberos Kerberos server\\n\\n3 service-kerberos-principals Service principals\\n3 kerberos-encryption-types Kerberos encryption types\\n3 service-kerberos-secondary-kdc Secondary KDC\\n3 service-kerberos-workstation-auth Basic workstation authentication\\n\\n3 service-kerberos-with-openldap-backend Kerberos with OpenLDAP backend\\n2 Network user authentication with SSSD',\n",
       " '3 service-sssd Introduction\\n\\n3 service-sssd-ad Active Directory\\n3 service-sssd-ldap LDAP\\n3 service-sssd-ldap-krb LDAP and Kerberos\\n3 service-sssd-troubleshooting Troubleshooting\\n2 WireGuard VPN\\n\\n3 wireguard-vpn-introduction Introduction\\n3 Peer-to-site\\n\\n4 wireguard-vpn-peer2site-introduction Introduction\\n4 wireguard-vpn-peer2site-router On router\\n4 wireguard-vpn-peer2site-inside Inside device\\n3 wireguard-vpn-site2site Site-to-site\\n3 wireguard-vpn-defaultgw Default gateway\\n3 wireguard-vpn-other-tasks Other tasks\\n3 wireguard-vpn-security Security tips\\n3 wireguard-vpn-troubleshooting Troubleshooting\\n2 Virtualisation and containers\\n\\n3 Virtual machines\\n\\n4 virtualization-qemu QEMU\\n4 virtualization-multipass Multipass\\n4 virtualization-uvt UVtool\\n\\n4 virtualization-virt-tools Virt-manager\\n4 virtualization-libvirt libvirt and virsh\\n\\n3 Containers\\n\\n2\\n\\n\\n-----',\n",
       " 'Level Path Navlink\\n\\n4 containers-lxc LXC\\n\\n4 containers-lxd LXD\\n\\n4 docker-for-system-admins Docker for sysadmins\\n2 Network\\n3 how-to-install-and-configure-isc-kea Install isc-kea\\n3 how-to-install-and-configure-isc-dhcp-server Install isc-dhcp-server\\n3 use-timedatectl-and-timesyncd Synchronise time\\n3 how-to-serve-the-network-time-protocol-with-chrony Serve the NTP\\n3 openvswitch-dpdk Use Open vSwitch with DPDK\\n2 Samba\\n\\n3 samba-active-directory Join Active Directory\\n3 samba-file-server Set up a file server\\n3 samba-print-server Set up a print server\\n3 samba-share-access-control Share access controls\\n3 samba-apparmor-profile Create AppArmor profile\\n3 Legacy pages\\n4 samba-domain-controller NT4 domain controller\\n\\n4 samba-openldap-backend OpenLDAP backend\\n2 Mail services\\n3 mail-postfix Install Postfix\\n3 mail-dovecot Install Dovecot\\n\\n3 mail-exim4 Install Exim4\\n\\n2 Backups\\n3 backups-bacula Install Bacula\\n3 tools-rsnapshot Install rsnapshot\\n3 how-to-back-up-using-shell-scripts Backup with shell scripts\\n2 Web services\\n\\n3 how-to-install-a-squid-server Install a Squid server\\n3 lamp-applications Get started with LAMP\\n3 how-to-install-apache2 Install Apache2\\n3 how-to-configure-apache2-settings Apache2 settings\\n3 how-to-use-apache2-modules Apache2 modules\\n3 how-to-install-nginx Install nginx\\n3 how-to-configure-nginx nginx settings\\n3 how-to-use-nginx-modules nginx modules\\n3 programming-php Install PHP\\n3 programming-ruby-on-rails Install Ruby on Rails\\n3 how-to-install-and-configure-phpmyadmin Install phpMyAdmin\\n3 how-to-install-and-configure-wordpress Install WordPress\\n2 Graphics\\n3 nvidia-drivers-installation Install NVIDIA drivers\\n3 gpu-virtualization-with-qemu-kvm vGPU with QEMU/KVM\\n0',\n",
       " '1 explanation **Explanation**\\n2 Software\\n\\n3 about-apt-upgrade-and-phased-updates About apt upgrade and phased updates\\n3 changing-package-files Changing package files\\n2 Network\\n\\n3 network-introduction Networking key concepts\\n3 network-configuration Configuring networks\\n3 network-dhcp About DHCP\\n3 network-ntp Time synchronisation\\n3 network-dpdk The DPDK library\\n2 Cryptography\\n3 introduction-to-crypto-libraries Introduction to crypto libraries\\n3 openssl OpenSSL\\n3 gnutls GnuTLS\\n3 network-security-services-nss Network Security Services (NSS)\\n3 java-cryptography-configuration Java cryptography configuration\\n3 bind-9-dnssec-cryptography-selection BIND 9 DNSSEC cryptography selection\\n3 openssh-crypto-configuration OpenSSH crypto configuration\\n\\n3\\n\\n\\n-----',\n",
       " 'Level Path Navlink\\n\\n3 troubleshooting-tls-ssl Troubleshooting TLS/SSL\\n2 Virtualisation and containers\\n\\n3 Virtual machines\\n\\n4 vm-tools-in-the-ubuntu-space VM tools overview\\n4 using-qemu-for-microvms QEMU for microVMs\\n4 upgrading-the-machine-type-of-your-vm Upgrading machine type\\n3 Containers\\n\\n4 container-tools-in-the-ubuntu-space Container tools overview\\n3 about-openstack About OpenStack\\n2 Web servers\\n\\n3 about-web-servers About web servers\\n\\n3 proxy-servers-squid About Squid proxy servers\\n2 Introduction to…\\n\\n3 introduction-to-virtualization Virtualization\\n\\n3 introduction-to-networking Networking\\n3 samba-introduction Samba\\n\\n3 introduction-to-web-services Web services\\n\\n3 mail-introduction Mail services\\n\\n3 backups-introduction Backups\\n0\\n\\n1 reference **Reference**\\n\\n2 Cloud Images\\n3 cloud-images/introduction Introduction\\n3 cloud-images/amazon-ec2 Amazon EC2\\n3 cloud-images/google-cloud-engine Google Compute Engine (GCE)\\n3 find-ubuntu-images-on-azure Microsoft Azure\\n2 Multipath\\n3 device-mapper-multipathing-introduction Introduction\\n3 device-mapper-multipathing-configuration Configuration\\n3 device-mapper-multipathing-setup Setup\\n3 device-mapper-multipathing-usage-debug Usage and debug\\n2 Security\\n3 security-introduction Introduction\\n3 security-users Users\\n3 security-smart-cards Smart cards\\n4 security-smart-cards-ssh SSH\\n3 security-apparmor AppArmor\\n3 security-firewall Firewall\\n3 security-certificates Certificates\\n3 security-trust-store CA trust store\\n3 security-console Console\\n2 High Availability\\n3 ubuntu-ha-introduction Introduction',\n",
       " '3 ubuntu-ha-pacemaker-resource-agents Pacemaker - resource agents\\n3 ubuntu-ha-pacemaker-fence-agents Pacemaker - fence agents\\n3 ubuntu-ha-drbd Distributed Replicated Block Device (DRBD)\\n3 ubuntu-ha-migrate-from-crmsh-to-pcs Ubuntu HA - Migrate from crmsh to pcs\\n2 Databases\\n\\n3 databases-introduction Introduction\\n\\n3 databases-mysql MySQL\\n3 databases-postgresql PostgreSQL\\n2 Monitoring\\n3 logging-monitoring-alerting Logging, Monitoring and Alerting (LMA)\\n3 logwatch Install Logwatch\\n3 tools-munin Install Munin\\n\\n3 tools-nagios Install Nagios Core 3\\n2 Backups\\n3 basic-backup-shell-script Basic backup shell script\\n3 archive-rotation-shell-script Archive rotation shell script\\n2 Other Services\\n\\n3 service-cups CUPS\\n\\n4\\n\\n\\n-----\\n\\nLevel Path Navlink\\n\\n3 service-debuginfod Debuginfod\\n4 service-debuginfod-faq Debuginfod FAQ\\n3 service-domain-name-service-dns Domain Name Service (DNS)\\n3 service-ftp FTP\\n3 service-iscsi iSCSI\\n\\n3 service-nfs NFS\\n\\n3 service-openssh OpenSSH\\n3 service-openvpn OpenVPN\\n3 service-gitolite gitolite\\n3 vpn-clients VPN clients\\n2 Tools\\n\\n3 tools-byobu byobu\\n3 tools-etckeeper etckeeper\\n3 pam-motd pam_motd\\n3 tools-puppet Puppet',\n",
       " '# **Redirects**\\n\\nMapping table\\n\\nPath Location\\n\\n/server/docs/introduction Ubuntu Server documentation\\n/server/docs/installation-advanced [Advanced Installation](https://discourse.ubuntu.com/t/installation-advanced/11577)\\n/server/docs/installation-iscsi [Installation - iSCSI](https://discourse.ubuntu.com/t/installation-iscsi/11321)\\n/server/docs/security-ecryptfs [eCryptfs is deprecated](https://discourse.ubuntu.com/t/security-ecryptfs/11886)\\n\\nThis section of our documentation contains step-by-step tutorials to help outline what Ubuntu Server is capable of\\nwhile helping you achieve specific aims.\\n\\nWe hope our tutorials make as few assumptions as possible and are broadly accessible to anyone with an interest in\\nUbuntu Server. They should also be a good place to start learning about Ubuntu Server in general, how it works, and\\nwhat it’s capable of.\\n## **Core tutorial**\\n\\nIn our **core tutorial**, you will learn how to set up an Ubuntu Server; from installing using a bootable USB device, to\\nnavigating the Server installer menu.\\n\\nGetting started\\n\\nBasic installation\\n\\nThe Server installer\\n\\nHow to operate the Server installer\\nScreen-by-screen installer guide\\nConfiguring storage',\n",
       " '## **Next steps**\\n\\nOnce your Ubuntu Server is up and running, you may be interested in this collection of related tutorials and topics\\nthat will help you learn more about how it works and what’s available to you. These pages can be viewed in any order.\\n\\nUbuntu Pro\\n\\nAttach your Ubuntu Pro subscription\\nThe Server installer\\n\\nReport a problem with the installer\\n\\n5\\n\\n\\n-----\\n\\nIf you have a specific goal, but are already familiar with Ubuntu Server, take a look at our *How-to* guides. These have\\nmore in-depth detail and can be applied to a broader set of applications.\\n\\nTake a look at our *Reference* section when you need to determine what commands are available, and how to interact\\nwith various tools.\\n\\nFinally, for a better understanding of how Ubuntu Server works and how it can be used and configured, our *Explanation*\\nsection enables you to expand your knowledge of the operating system and additional software.\\n\\nThis chapter provides an overview of how to install Ubuntu Server Edition. You can also refer to this guide on how to\\noperate the installer for more information on using the installer, and to this screen-by-screen reference guide for more\\ninformation about each of the installer screens.\\n## **Preparing to install**\\n\\nThis section explains various aspects to consider before starting the installation.',\n",
       " '## **System requirements**\\n\\nUbuntu Server Edition provides a common, minimalist base for a variety of server applications, such as file/print\\nservices, web hosting, email hosting, etc. This version supports four 64-bit architectures:\\n\\n  - amd64 (Intel/AMD 64-bit)\\n\\n - arm64 (64-bit ARM)\\n\\n - ppc64el (POWER8 and POWER9)\\n\\n - s390x (IBM Z and LinuxONE)\\n\\nThe recommended system requirements are:\\n\\n  - CPU: 1 gigahertz or better\\n\\n  - RAM: 1 gigabyte or more\\n\\n  - Disk: a minimum of 2.5 gigabytes\\n## **Perform a system back up**\\n\\nBefore installing Ubuntu Server Edition you should make sure all data on the system is backed up.\\n\\nIf this is not the first time an operating system has been installed on your computer, it is likely you will need to\\nre-partition your disk to make room for Ubuntu.\\n\\nAny time you partition your disk, you should be prepared to lose everything on the disk should you make a mistake or\\nsomething goes wrong during partitioning. The programs used in installation are quite reliable, most have seen years\\nof use, but they also perform destructive actions.\\n## **Download the server ISO**\\n\\n[You can obtain the amd64 server download from https://releases.ubuntu.com/. Select the version you wish to install](https://releases.ubuntu.com/)\\nand select the “server install image” download. Note that the server download includes the installer.\\n\\nThere are platform specific how-to guides for installations on:\\n\\n - s390x LPAR\\n\\n z/VM\\n\\n - ppc64el',\n",
       " '## **Create a bootable USB**\\n\\n[There are many ways to boot the installer but the simplest and most common way is to create a bootable USB stick](https://ubuntu.com/tutorials/tutorial-create-a-usb-stick-on-ubuntu)\\n[to boot the system to be installed with (tutorials for other operating systems are also available).](https://ubuntu.com/search?q=%22create+a+bootable+USB+stick%22)\\n## **Boot the installer**\\n\\nPlug the USB stick into the system to be installed and start it.\\n\\nMost computers will automatically boot from USB or DVD, though in some cases this is disabled to improve boot\\ntimes. If you don’t see the boot message and the “Welcome” screen which should appear after it, you will need to set\\nyour computer to boot from the install media.\\n\\n6\\n\\n\\n-----\\n\\nThere should be an on-screen message when the computer starts telling you what key to press for settings or a boot\\nmenu. Depending on the manufacturer, this could be Escape, F2, F10 or F12. Simply restart your computer and hold\\ndown this key until the boot menu appears, then select the drive with the Ubuntu install media.\\n\\n[If you are still having problems, check out the Ubuntu Community documentation on booting from](https://help.ubuntu.com/community/BootFromCD)\\n[CD/DVD.](https://help.ubuntu.com/community/BootFromCD)\\n\\nAfter a few moments, the installer will start in its language selection screen.',\n",
       " '## **Using the installer**\\n\\nThe installer is designed to be easy to use and have sensible defaults so for a first install you can mostly just accept\\nthe defaults for the most straightforward install:\\n\\n  - Choose your language\\n\\n  - Update the installer (if offered)\\n\\n  - Select your keyboard layout\\n\\n  - Do not configure networking (the installer attempts to configure wired network interfaces via DHCP, but you\\ncan continue without networking if this fails)\\n\\n  - Do not configure a proxy or custom mirror unless you have to in your network\\n\\n  - For storage, leave “use an entire disk” checked, and choose a disk to install to, then select “Done” on the\\nconfiguration screen and confirm the install\\n\\n  - Enter a username, hostname and password\\n\\n  - On the SSH and snap screens, select “Done”\\n\\n  - You will now see log messages as the install is completed\\n\\n  - Select restart when this is complete, and log in using the username and password provided\\n\\nThis document explains how to use the installer in general terms. For a step-by-step guide through the screens of the\\ninstaller, you can use our screen-by-screen reference guide.\\n\\n7\\n\\n\\n-----',\n",
       " '## **Get the installer**\\n\\n[Installer images are made (approximately) daily and are available from https://cdimage.ubuntu.com/ubuntu-server/](https://cdimage.ubuntu.com/ubuntu-server/daily-live/current/)\\n[daily-live/current/. These are not tested as extensively as the images from release day, but they contain the latest](https://cdimage.ubuntu.com/ubuntu-server/daily-live/current/)\\npackages and installer, so fewer updates will be required during or after installation.\\n\\n[You can download the server installer for amd64 from https://ubuntu.com/download/server and other architectures](https://ubuntu.com/download/server)\\n[from http://cdimage.ubuntu.com/releases/20.04/release/.](http://cdimage.ubuntu.com/releases/20.04/release/)\\n## **Installer UI navigation**\\n\\nIn general, the installer can be used with the up and down arrows and space or Enter keys and a little typing.\\n\\nTab and Shift + Tab move the focus down and up respectively. Home / End / Page Up / Page Down can be used to\\nnavigate through long lists more quickly in the usual way.',\n",
       " '## **Running the installer over serial**\\n\\nBy default, the installer runs on the first virtual terminal, tty1 . This is what is displayed on any connected monitor\\nby default. However, servers do not always have a monitor. Some out-of-band management systems provide a remote\\nvirtual terminal, but some times it is necessary to run the installer on the serial port. To do this, the kernel command\\n[line needs to have an appropriate console specified on it – a common value is](https://www.kernel.org/doc/html/latest/admin-guide/serial-console.html) console=ttyS0 but this is not something\\nthat can be generically documented.\\n\\nWhen running on serial, the installer starts in a basic mode that does using only the ASCII character set and black\\nand white colours. If you are connecting from a terminal emulator such as gnome-terminal that supports Unicode and\\nrich colours you can switch to “rich mode” which uses Unicode, colours and supports many languages.\\n## **Connecting to the installer over SSH**\\n\\nIf the only available terminal is very basic, an alternative is to connect via SSH. If the network is up by the time the\\ninstaller starts, instructions are offered on the initial screen in basic mode. Otherwise, instructions are available from\\nthe help menu once networking is configured.\\n\\nIn addition, connecting via SSH is assumed to be capable of displaying all Unicode characters, enabling more translations to be used than can be displayed on a virtual terminal.\\n## **Help menu**\\n\\nThe help menu is always in the top right of the screen. It contains help – both general and for the currently displayed\\nscreen – and some general actions.',\n",
       " '## **Switching to a shell prompt**\\n\\nYou can switch to a shell at any time by selecting “Enter shell” from the help menu, or pressing Control + Z or F2.\\n\\nIf you are accessing the installer via tty1, you can also access a shell by switching to a different virtual terminal\\n(Control + Alt + arrow, or Control + Alt + number keys, move between virtual terminals).\\n## **Global keys**\\n\\nThere are some global keys you can press at any time:\\n\\nKey Action\\n\\nESC Go back\\n\\nF1 Open help menu\\nControl + Z, F2 Switch to shell\\nControl + L, F3 Redraw screen\\nControl + T, F4 Toggle rich mode (colour, Unicode) on and off\\n\\nThe installer is designed to be easy to use without the need to refer to documentation. However, this reference guide\\nprovides more information for each of the screens of the installer.\\n\\n8\\n\\n\\n-----\\n\\n## **Language selection**\\n\\nThis screen selects the language for the installer and the default language for the installed system.\\n\\nMore languages can be displayed if you connect via SSH.\\n\\n9\\n\\n\\n-----\\n\\n## **Refresh**\\n\\nThis screen is shown if there is an update for the installer available. This allows you to get any improvements and\\nbug fixes made since release.\\n\\nIf you choose to update, the new version will be downloaded and the installer will restart at the same point of the\\ninstallation.\\n\\n10\\n\\n\\n-----\\n\\n## **Keyboard**\\n\\nChoose the layout and variant of keyboard attached to the system, if any. When running in a virtual terminal, it is\\npossible to guess the layout and variant by answering questions about the keyboard.',\n",
       " '## **Zdev (s390x only)**\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ngeneric-ccw │\\n\\n0.0.0009  - │\\n\\n0.0.000c  - │\\n\\n0.0.000d  - │\\n\\n0.0.000e  - │\\n\\n│\\n\\ndasd-eckd │\\n\\n0.0.0190  - │\\n\\n0.0.0191  - │\\n\\n0.0.019d  - │\\n\\n0.0.019e >┌────────────┐\\n\\n0.0.0200 >│< (close) │\\n\\n0.0.0300 >│ Enable │\\n\\n0.0.0400 >│ Disable │\\n\\n0.0.0592 >└────────────┘ v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n11\\n\\n\\n-----\\n\\nThis screen is only shown on s390x and allows z-specific configuration of devices.\\n\\nThe list of devices can be long. Home / End / Page Up / Page Down can be used to navigate through the list more\\nquickly.\\n## **Network**\\n\\nnetwork800×600 2.99 KB\\n\\nThis screen allows the configuration of the network. Ubuntu Server uses NetPlan to configure networking and the\\nUI of the installer can configure a subset of NetPlan’s capabilities. In particular it can configure DHCP or static\\naddressing, VLANs and bonds.\\n\\nIf networking is present (defined as “at least one interface has a default route”) then the installer will install updates\\nfrom the archive at the end of installation.\\n\\n12\\n\\n\\n-----\\n\\n## **Proxy**\\n\\nproxy800×600 2.69 KB\\n\\nThe proxy configured on this screen is used for accessing the package repository and the snap store both in the installer\\nenvironment and in the installed system.\\n\\n13\\n\\n\\n-----\\n\\n## **Mirror**\\n\\nmirror800×600 1.99 KB\\n\\nThe installer will attempt to use geoip to look up an appropriate default package mirror for your location. If you want\\nor need to use a different mirror, enter its URL here.\\n\\n14\\n\\n\\n-----',\n",
       " '## **Storage**\\n\\nstorage_config800×600 3.89 KB\\n\\nStorage configuration is a complicated topic and has its own page for documentation.\\n\\n15\\n\\n\\n-----\\n\\nstorage_confirm800×600 3.71 KB\\n\\nOnce the storage configuration is confirmed, the install begins in the background.\\n\\n16\\n\\n\\n-----\\n\\n## **Identity**\\n\\nidentity800×600 2.43 KB\\n\\nThe default user will be an administrator, able to use sudo (this is why a password is needed, even if SSH public key\\naccess is enabled on the next screen).\\n\\n17\\n\\n\\n-----\\n\\n## **SSH**\\n\\nssh800×600 2.27 KB\\n\\nA default Ubuntu install has no open ports. It is very common to administer servers via SSH so the installer allows\\nit to be installed with the click of a button.\\n\\nYou can import keys for the default user from GitHub or Launchpad.\\n\\nIf you import a key, then password authentication is disabled by default but it can be re-enabled again if you wish.\\n\\n18\\n\\n\\n-----\\n\\n## **Snaps**\\n\\nsnaps800×600 9.09 KB\\n\\nIf a network connection is enabled, a selection of snaps that are useful in a server environment are presented and can\\nbe selected for installation.\\n\\n19\\n\\n\\n-----\\n\\n## **Installation logs**\\n\\ninstall_progress800×600 4.62 KB\\n\\nThe final screen of the installer shows the progress of the installer and allows viewing of the full log file. Once the\\ninstall has completed and security updates installed, the installer waits for confirmation before restarting.\\n\\n20\\n\\n\\n-----\\n\\ninstall_done800×600 6.64 KB\\n\\n\\n21\\n\\n\\n-----',\n",
       " '## **Guided options**\\n\\nSelecting “Use an entire disk” on the Guided storage configuration screen will install Ubuntu onto the selected disk,\\nreplacing any partitions or data already there.\\n\\nYou can choose whether or not to set up LVM, and if you do, whether or not to encrypt the volume with LUKS. If\\nyou encrypt the volume, you need to choose a passphrase that will need to be entered each time the system boots.\\n\\nIf you select “Custom storage layout”, no configuration will be applied to the disks.\\n\\nIn either case, the installer moves onto the main storage customisation screen.\\n\\n22\\n\\n\\n-----\\n\\n## **The main storage screen**\\n\\nThis screen presents a summary of the current storage configuration. Each device or partition of a device corresponds\\nto a different row (which can be selected), and pressing Enter or space while a device is selected opens a menu of\\nactions that apply to that device.\\n## **Partitions**\\n\\nTo add a partition to a device, select “Add GPT Partition” for that device.\\n\\n23\\n\\n\\n-----\\n\\nYou can leave “Size” blank to use all the remaining space on the device.\\n\\n24\\n\\n\\n-----',\n",
       " '## **RAID**\\n\\n[Linux software RAID (RAID stands for “Redundant Array of Inexpensive Disks”) can be used to combine several](https://raid.wiki.kernel.org/index.php/Linux_Raid)\\ndisks into a single device that is (usually) tolerant to any one disk failure.\\n\\nA software RAID device can be created out of entire disks or unformatted partitions. Select the “Create software\\nRAID (“MD”)” button to open the creation dialog.\\n\\nThe server installer supports creating devices with RAID level 0, 1, 5, 6 or 10. It does not allow customising other\\n[options such as metadata format or RAID10 layout at this time. See the Linux RAID documentation for more details.](https://raid.wiki.kernel.org/index.php/Linux_Raid)\\n\\nA software RAID device can be formatted and mounted directly, can be partitioned into several partitions, or even be\\nused as part of another RAID device or LVM volume group.\\n\\n25\\n\\n\\n-----',\n",
       " '## **Logical Volume Manager (LVM)**\\n\\nThe LVM is a system of managing logical volumes, or filesystems, that is much more advanced and flexible than the\\ntraditional method of partitioning a disk into one or more segments and formatting that partition with a filesystem.\\nIt can be used to combine several disks into one larger pool of storage but it offers advantages even in a single disk\\nsystem, such as snapshots and easy resizing of logical volumes.\\n\\nAs with RAID, a LVM volume group can be created out of entire disks or unformatted partitions. Select the “Create\\nLVM volume group” button to open the creation dialog.\\n\\nOnce a volume group has been created, it can be divided into named logical volumes which can then be formatted\\nand mounted. It generally makes sense to leave some space in the volume group for storage of snapshots and creation\\nof more logical volumes as needed.\\n\\nThe server installer does not supported configuring any of the many, many options that LVM supports when creating\\nvolume groups and logical volumes.',\n",
       " '## **Selecting boot devices**\\n\\nOn all architectures other than s390x, the bootloader needs to be installed to a disk in such a way that the system\\nfirmware can find it on boot. By default, the first device to have a partition created on it is selected as a boot device\\n\\n26\\n\\n\\n-----',\n",
       " 'but this can be changed later.\\n\\nOn amd64 and arm64 systems, multiple disks can be selected as boot devices, which means a system can be configured\\nso that it will continue to boot after a failure of any one drive (assuming the root filesystem is placed on a RAID).\\nThe bootloader will be installed to each of these drives, and the operating system configured to install new versions\\nof GRUB to each drive as it is updated.\\n\\namd64 systems use GRUB as the bootloader. amd64 systems can boot in either UEFI or legacy (sometimes called\\n“BIOS”) mode (many systems can be configured to boot in either mode) and the bootloader is located completely\\ndifferently in the two modes.\\n\\nIn legacy mode, the bootloader is read from the first “sector” of a hard drive (exactly which hard drive is up to the\\nsystem firmware, which can usually be configured in a vendor-specific way). The installer will write GRUB to the start\\nof all disks selected as a boot devices. As GRUB does not entirely fit in one sector, a small unformatted partition is\\nneeded at the start of the disk, which will automatically be created when a disk is selected as a boot device (a disk\\nwith an existing GPT partition table can only be used as a boot device if it has this partition).',\n",
       " 'In UEFI mode, the bootloader loaded from a “EFI System Partition” (ESP), which is a partition with a particular\\ntype GUID. The installer automatically creates a 512MiB ESP on a disk when it is selected as a boot device and\\nwill install GRUB there (a disk with an existing partition table can only be used as a boot device if it has an ESP\\n– bootloaders for multiple operating systems can be installed into a single ESP). UEFI defines a standard way to\\nconfigure the way in which the operating system is chosen on boot, and the installer uses this to configure the system\\nto boot the just-installed operating system. One of the ESPs must be mounted at /boot/efi .\\n\\nSupported arm64 servers boot using UEFI, and are configured the same way as an UEFI-booting amd64 system.\\n\\nppc64el systems also load their bootloader (Petitboot, a small linux kernel) from a “PReP” partition with a special\\nflag, so in most ways they are similar to a UEFI system. The installer only supports one PReP partition at this time.',\n",
       " '## **Limitations and workarounds**\\n\\nCurrently, the installer cannot *edit* partition tables. You can use existing partitions or reformat a drive entirely but\\nyou cannot, for example, remove a large partition and replace it with two smaller ones.\\n\\nThe installer allows the creation of LVM volume groups and logical volumes and MD raid devices, but does not allow\\ntweaking of the parameters – for example, all logical volumes are linear and all MD raid devices use the default\\nmetadata format (1.2).\\n\\nThese limits can both be worked around in the same way: drop to a shell and use the usual shell commands to edit\\nthe partition table or create the LV or RAID with desired parameters, and then select these partitions or devices as\\nmount points in the installer. Any changes you make while the installer is running but before altering the storage\\nconfiguration will reflected in the installer.\\n\\nThe installer cannot yet configure iSCSI mounts, ZFS at all, or btrfs subvolumes.\\n\\n[Attaching the Ubuntu Pro subscription to Ubuntu brings you the enterprise lifecycle, including Linux kernel livepatch-](https://ubuntu.com/pro)\\n[ing, access to FIPS-validated packages, and compliance with security profiles such as CIS. This is not required for](https://ubuntu.com/security/livepatch)\\n[Ubuntu Pro instances through public clouds such as AWS, Azure or GCP, since these are automatically attached from](https://ubuntu.com/public-cloud)\\nlaunch.\\n\\n**Νote** :\\n\\n[Subscriptions are not just for enterprise customers. Anyone can get a personal subscription for free on up](https://ubuntu.com/pro)\\n[to 5 machines, or 50 if you are an official Ubuntu Community member.](https://wiki.ubuntu.com/Membership)',\n",
       " 'The following instructions explain how to attach your subscription to your Ubuntu systems.\\n## **Step 1: Install the Ubuntu Pro Client**\\n\\nThis step is necessary for Ubuntu Pro users or holders of personal subscriptions. If you are an Ubuntu Pro user\\nthrough a public cloud offering, your subscription is already attached and you may skip these instructions.\\n\\nWe first need to make sure that we have the latest version of the Ubuntu Pro Client running. The package used to\\naccess the Pro Client ( pro ) is ubuntu-advantage-tools :\\n\\nsudo apt update\\n\\nsudo apt install ubuntu-advantage-tools\\n\\nIf you already have ubuntu-advantage-tools installed, this install command will upgrade the package to the latest\\n\\nversion.\\n\\n27\\n\\n\\n-----',\n",
       " '## **Step 2: Attach your subscription**\\n\\nOnce you have the latest version of the Pro Client installed, you need to attach the Ubuntu Pro token to your Pro\\nClient to gain access to the services provided under Ubuntu Pro.\\n\\n[First you need to retrieve your Ubuntu Pro token from the Ubuntu Pro dashboard. To access your dashboard, you](https://ubuntu.com/pro)\\n[need an Ubuntu One account. If you still need to create one, be sure to sign up using the email address used to create](https://login.ubuntu.com/)\\nyour subscription.\\n\\nThe Ubuntu One account functions as a single-sign-on (SSO), so once logged in we can go straight to the Ubuntu Pro\\n[dashboard at ubuntu.com/pro. Then click on the ‘Machines’ column in the ‘Your Paid Subscriptions’ table to reveal](http://ubuntu.com/pro)\\nyour token.\\n\\nNow we’re ready to attach our Ubuntu Pro token to the Pro Client:\\n\\nsudo pro attach <your_pro_token>\\n\\nYou will know that the token has been successfully attached when you see the list of services, descriptions and their\\nenabled/disabled status in a table similar to this:\\n\\nSERVICE ENTITLED STATUS DESCRIPTION\\n\\nesm-apps yes enabled Expanded Security Maintenance for Applications\\n\\nesm-infra yes enabled Expanded Security Maintenance for Infrastructure\\n\\nlivepatch yes enabled Canonical Livepatch service\\n\\nrealtime-kernel yes disabled Ubuntu kernel with PREEMPT_RT patches integrated\\n\\nNote that Extended Security Maintenance (ESM) and Livepatch will auto-enable once your token has been attached\\nto your machine.\\n\\nAfter attaching the Pro Client with your token you can also use the Pro Client to activate most of the Ubuntu Pro\\nservices, including Livepatch, FIPS, and the CIS Benchmark tool.',\n",
       " '## **Further reading**\\n\\n[• For more information about the Ubuntu Pro Client, you can read our documentation.](https://canonical-ubuntu-pro-client.readthedocs-hosted.com/en/latest/)\\n\\n[• For a guided tour through the most commonly-used commands available through the Ubuntu Pro Client, check](https://canonical-ubuntu-pro-client.readthedocs-hosted.com/en/latest/tutorials/basic_commands.html)\\n[out this tutorial.](https://canonical-ubuntu-pro-client.readthedocs-hosted.com/en/latest/tutorials/basic_commands.html)\\n\\nWe always hope, of course, that every install with the server installer succeeds. But reality doesn’t always work that\\nway and there will sometimes be failures of various kinds. This section explains the most useful way to report any\\nfailures so that we can fix the bugs causing them, and we’ll keep the topic up to date as the installer changes.\\n\\nThe first thing to do is to update your Subiquity snap. Not only because we fix issues that cause failures over time\\nbut also because we’ve been working on features to make failure reporting easier.\\n\\nA failure will result in a crash report being generated which bundles up all the information we need to fully diagnose\\na failure. These live in /var/crash in the installer environment, and for Ubuntu 19.10 and newer this is persisted to\\nthe install media by default (if there is space).',\n",
       " 'When an error occurs you are presented with a dialog that allows you to upload the report to the error tracker and\\noffers options for continuing. Uploads to the error tracker are non-interactive and anonymous, so they are useful for\\ntracking which kinds of errors are affecting most users, but they do not give us a way to ask you to help diagnose the\\nfailure.\\n\\nYou can create a Launchpad bug report, which does let us establish this kind of two way communication, based on\\nthe contents of a crash report by using the standard apport-cli tool that is part of Ubuntu. Copy the crash report to\\nanother system, run:\\n\\napport-cli /path/to/report.crash\\n\\nand follow the prompts.\\n\\nYou can also run apport-cli in the installer environment by switching to a shell but apport won’t be able to open a\\nbrowser to allow you to complete the report so you’ll have to type the URL by hand on another machine.\\n\\nIf you have a specific goal, but are already familiar with Ubuntu Server, our *how-to* guides have more in-depth detail\\nthan our tutorials and can be applied to a broader set of applications. They’ll help you achieve an end result but may\\nrequire you to understand and adapt the steps to fit your specific requirements.\\n\\nAdvanced installation\\n\\namd64 netboot install\\n\\n28\\n\\n\\n-----',\n",
       " 'arm64 netboot install\\n\\nppc64el netboot install\\nVirtual CDROM and Petitboot on ppc64el\\ns390x install via z/VM\\ns390x install via LPAR\\n\\nAutomatic installation\\n\\nIntroduction to Automated Server installer\\n\\nAutoinstall quickstart\\nAutoinstall quickstart on s390x\\nAutoinstall config file reference\\nAutoinstall JSON schema\\nIBM z/VM autoinstall on s390x\\nIBM LPAR autoinstall on s390x\\n\\nROCK Images\\n\\nIntroduction\\n\\nContainer customization with Docker\\nMulti-node configuration with Docker-Compose\\nSoftware\\n\\nPackage management\\nUpgrade\\nThird party APT repositories\\nReporting bugs\\nKernel crash dump\\nOpenLDAP\\n\\nIntroduction\\n\\nInstallation\\n\\nAccess control\\n\\nReplication\\nSimple LDAP user and group management\\nSSL/TLS\\nBackup and restore\\nKerberos\\n\\nIntroduction\\n\\nKerberos server\\n\\nService principals\\nKerberos encryption types\\nSecondary KDC\\nBasic workstation authentication\\n\\nKerberos with OpenLDAP backend\\nNetwork user authentication with SSSD\\n\\nIntroduction\\n\\nActive directory\\nLDAP\\n\\nLDAP and Kerberos\\n\\nTroubleshooting\\nWireGuard VPN\\n\\nIntroduction\\n\\nPeer to site\\n\\nIntroduction\\n\\nOn router\\n\\nInside device\\n\\nSite to site\\n\\nDefault gateway\\nOther tasks\\n\\nSecurity tips\\nTroubleshooting\\n## **Virtualization**\\n\\nVirtual machines (VMs)\\n\\nQEMU\\nCreate VMs with Multipass\\n\\n29\\n\\n\\n-----',\n",
       " '## **Networking** **Mail services**\\n\\n\\nCreate cloud image VMs with UVtool\\nVM tooling\\n\\nHow to use the libvirt library with virsh\\nHow to use virt-manager and other virt* tools\\nContainers\\n\\nLXC\\n\\nLXD\\n\\nDocker for system admins\\n\\nNetworking tools\\n\\nDHCP: Install isc-kea\\n\\nDHCP: Install isc-dhcp-server\\nTime sync: Using timedatectl and timesyncd\\nTime sync: Serve the Network Time Protocol\\nInstall Open vSwitch with DPDK\\nSamba\\n\\nJoin Active Directory\\nSet up a file server\\nSet up a print server\\nSet up share access control\\nSet up an AppArmor profile\\nNT4 domain controller (legacy)\\nOpenLDAP backend (legacy)\\n\\n\\nThese guides will help you get started with the most common and popular mail services. If you aren’t sure which\\nservice to use, check out our overview of these options.\\n\\nHow to install and configure…\\n\\nPostfix\\nDovecot\\n\\nExim4\\n## **Backups**\\n\\nThese guides focus on helping you set up backup systems. If you need guidance on these options, see our introduction\\nto system backups.\\n\\nHow to install and configure…\\n\\nBacula\\n\\nRsnapshot\\nShell scripts\\n\\nBackup with shell scripts\\n## **Web**\\n\\nProxy servers\\n\\nInstall a Squid server\\nApache\\n\\nInstall Apache2\\nConfigure Apache2\\nExtend Apache2 with modules\\nNginx\\n\\nInstall nginx\\nConfigure nginx\\n\\n30\\n\\n\\n-----',\n",
       " '## **Graphics**\\n\\n\\nExtend nginx with modules\\nWeb Programming\\n\\nInstall PHP\\n\\nInstall Ruby on Rails\\nLAMP applications\\n\\nGet started with LAMP applications\\nInstall phpMyAdmin\\nInstall WordPress\\n\\nOn-system GPU\\n\\nNvidia driver installation\\n\\nVirtual GPU\\n\\nVirtualized GPU with QEMU/KVM\\n\\n\\namd64 systems boot in either UEFI or legacy (“BIOS”) mode (many systems can be configured to boot in either\\nmode). The precise details depend on the system firmware, but both modes usually support the “Preboot eXecution\\nEnvironment” (PXE) specification, which allows the provisioning of a bootloader over the network.\\n\\nThe process for network booting the live server installer is similar for both modes and goes like this:\\n\\n1. The to-be-installed machine boots, and is directed to network boot.\\n2. The DHCP/BOOTP server tells the machine its network configuration and where to get the bootloader.\\n3. The machine’s firmware downloads the bootloader over TFTP and executes it.\\n4. The bootloader downloads configuration, also over TFTP, telling it where to download the kernel, RAM Disk\\nand kernel command line to use.\\n5. The RAM Disk looks at the kernel command line to learn how to configure the network and where to download\\nthe server ISO from.\\n\\n6. The RAM Disk downloads the ISO and mounts it as a loop device.\\n7. From this point on the install follows the same path as if the ISO was on a local block device.',\n",
       " 'The difference between UEFI and legacy modes is that in UEFI mode the bootloader is an EFI executable, signed so\\n[that is accepted by Secure Boot, and in legacy mode it is PXELINUX. Most DHCP/BOOTP servers can be configured](https://wiki.syslinux.org/wiki/index.php?title=PXELINUX)\\nto serve the right bootloader to a particular machine.\\n## **Configuring DHCP/BOOTP and TFTP**\\n\\nThere are several implementations of the DHCP/BOOTP and TFTP protocols available. This document will briefly\\ndescribe how to configure dnsmasq to perform both of these roles.\\n\\n1. Install dnsmasq with:\\n\\nsudo apt install dnsmasq\\n\\n2. Put something like this in /etc/dnsmasq.conf.d/pxe.conf :\\n\\ninterface=<your interface>,lo\\n\\nbind-interfaces\\n\\ndhcp-range=<your interface>,192.168.0.100,192.168.0.200\\n\\ndhcp-boot=pxelinux.0\\n\\ndhcp-match=set:efi-x86_64,option:client-arch,7\\n\\ndhcp-boot=tag:efi-x86_64,bootx64.efi\\n\\nenable-tftp\\n\\ntftp-root=/srv/tftp\\n\\n**Note**\\n\\nThis assumes several things about your network; read man dnsmasq or the default /etc/dnsmasq.conf for\\nlots more options.\\n\\n3. Restart dnsmasq with:\\n\\nsudo systemctl restart dnsmasq.service\\n\\n31\\n\\n\\n-----',\n",
       " '## **Serving the bootloaders and configuration.**\\n\\n**We need to make this section possible to write sanely**\\n\\nIdeally this would be something like:\\n\\napt install cd-boot-images-amd64\\n\\nln -s /usr/share/cd-boot-images-amd64 /srv/tftp/boot-amd64\\n\\n**Mode-independent set up**\\n\\n1. Download the latest live server ISO for the release you want to install:\\n\\nwget http://cdimage.ubuntu.com/ubuntu-server/daily-live/current/focal-live-server-amd64.iso\\n\\n2. Mount it.\\n\\nmount ubuntu-19.10-live-server-amd64.iso /mnt\\n\\n3. Copy the kernel and initrd from it to where the dnsmasq serves TFTP from:\\n\\ncp /mnt/casper/{vmlinuz,initrd} /srv/tftp/\\n\\n**Set up the files for UEFI booting**\\n\\n1. Copy the signed shim binary into place:\\n\\napt download shim-signed\\n\\ndpkg-deb --fsys-tarfile shim-signed*deb | tar x ./usr/lib/shim/shimx64.efi.signed -O > /srv/tftp/bootx64.efi\\n\\n2. Copy the signed GRUB binary into place:\\n\\napt download grub-efi-amd64-signed\\n\\ndpkg-deb --fsys-tarfile grub-efi-amd64-signed*deb | tar x ./usr/lib/grub/x86_64-efi-signed/grubnetx64.efi.signed \\nO > /srv/tftp/grubx64.efi\\n\\n3. GRUB also needs a font to be available over TFTP:\\n\\napt download grub-common\\n\\ndpkg-deb --fsys-tarfile grub-common*deb | tar x ./usr/share/grub/unicode.pf2 -O > /srv/tftp/unicode.pf2\\n\\n4. Create /srv/tftp/grub/grub.cfg that contains:\\n\\nset default=\"0\"\\n\\nset timeout=-1\\n\\nif loadfont unicode ; then\\n\\nset gfxmode=auto\\n\\nset locale_dir=$prefix/locale\\n\\nset lang=en_US\\n\\nfi\\n\\nterminal_output gfxterm\\n\\nset menu_color_normal=white/black\\n\\nset menu_color_highlight=black/light-gray\\n\\nif background_color 44,0,30; then\\n\\nclear\\n\\nfi\\n\\nfunction gfxmode {\\n\\nset gfxpayload=\"${1}\"\\n\\nif [ \"${1}\" = \"keep\" ]; then\\n\\nset vt_handoff=vt.handoff=7\\n\\nelse',\n",
       " 'set vt_handoff=\\n\\nfi\\n\\n}\\n\\nset linux_gfx_mode=keep\\n\\nexport linux_gfx_mode\\n\\n32\\n\\n\\n-----',\n",
       " \"menuentry 'Ubuntu 20.04' {\\n\\ngfxmode $linux_gfx_mode\\n\\nlinux /vmlinux $vt_handoff quiet splash\\n\\ninitrd /initrd\\n\\n}\\n\\n**Set up the files for legacy boot**\\n\\n1. Download pxelinux.0 and put it into place:\\n\\nwget http://archive.ubuntu.com/ubuntu/dists/eoan/main/installer-amd64/current/images/netboot/ubuntu\\ninstaller/amd64/pxelinux.0\\n\\nmkdir -p /srv/tftp\\n\\nmv pxelinux.0 /srv/tftp/\\n\\n5. Make sure to have installed package syslinux-common and then:\\n\\ncp /usr/lib/syslinux/modules/bios/ldlinux.c32 /srv/tftp/\\n\\n6. Create /srv/tftp/pxelinux.cfg/default containing:\\n\\nDEFAULT install\\n\\nLABEL install\\n\\nKERNEL vmlinuz\\n\\nINITRD initrd\\n\\nAPPEND root=/dev/ram0 ramdisk_size=1500000 ip=dhcp url=http://cdimage.ubuntu.com/ubuntu-server/daily\\nlive/current/focal-live-server-amd64.iso\\n\\nAs you can see, this downloads the ISO from Ubuntu’s servers. You may well want to host it somewhere on your\\ninfrastructure and change the URL to match.\\n\\nThis configuration is obviously very simple. PXELINUX has many, many options, and you can consult its documen[tation at https://wiki.syslinux.org/wiki/index.php?title=PXELINUX for more.](https://wiki.syslinux.org/wiki/index.php?title=PXELINUX)\\n\\nThis document provides the steps needed to install an system via netbooting and subiquity in UEFI mode with Ubuntu\\n20.04 (or later). The process is applicable to both of the architectures, arm64 and amd64. This process is inpired by\\n[this Ubuntu Discourse post for](https://discourse.ubuntu.com/t/netbooting-the-live-server-installer/14510) *legacy mode*, which is UEFI’s predecessor. Focal (20.04, 20.04.5) and Groovy (20.10)\\nhave been tested with the following method.\",\n",
       " '## **Configuring TFTP**\\n\\nThis article assumes that you have setup your tftp (and/or DHCP/bootp if necessary, depending on your LAN\\n[configuration) by following this Ubuntu Discourse post, or you could also consider build your own tftp in this way if](https://discourse.ubuntu.com/t/netbooting-the-live-server-installer/14510)\\nyour DNS and DHCP is already well configured:\\n\\n$ sudo apt install tftpd-hpa\\n\\nIf the installation is successful, check the corresponding TFTP service is active by this command:\\n\\n$ systemctl status tftpd-hpa.service\\n\\nIt is expected to show *active (running)* from the output messages. We will also assume your tftp root path is\\n\\n/var/lib/tftpboot in the remaining of this article.',\n",
       " '## **Serving Files**\\n\\n**You can skip the whole section of the following manual setup instruction by using** [this non-official tool.](https://github.com/dannf/ubuntu-server-netboot)\\nThe tool will setup your TFTP server to serve necessary files for netbooting.\\n\\n**Necessary Files**\\n\\nThere are several files needed for this process. The following files are needed:\\n\\n  - Ubuntu live server image\\n\\n**–**\\nFor arm64 architecture, its image name has a *-arm64* suffix. For example, *ubuntu-20.04.5-live-server-*\\n*arm64.iso* .\\n\\n**–**\\nFor amd64 architecture, its image name has a *-amd64* suffix. For example, *ubuntu-20.04.5-live-server-*\\n*amd64.iso* .\\n\\n  - grub efi binary (and the corresponding grub.cfg, which is a txt file)\\n\\n33\\n\\n\\n-----',\n",
       " '**–** For arm64 architecture, it is grubnetaa64.efi.signed .\\n**–** For amd64 architecture, it is grubnetx64.efi.signed .\\n\\n - initrd extracted from your target Ubuntu live server image (use hwe-initrd instread if you want to boot with\\nHWE kernel)\\n\\n - vmlinuz extracted from your target Ubuntu live server image (use hwe-vmlinuz instead if you want to boot with\\nHWE kernel)\\n\\n**Examples**\\n\\nIn the following sections, we will take arm64 image as an example. This means the following files are used:\\n\\n  - Ubuntu 20.04.5 live server image *ubuntu-20.04.5-live-server-arm64.iso* [from https://cdimage.ubuntu.com/](https://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-arm64.iso)\\n[ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-arm64.iso](https://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-arm64.iso)\\n\\n  - grub efi binary grubnetaa64.efi.signed [from http://ports.ubuntu.com/ubuntu-ports/dists/focal/main/uefi/](http://ports.ubuntu.com/ubuntu-ports/dists/focal/main/uefi/grub2-arm64/current/grubnetaa64.efi.signed)\\n[grub2-arm64/current/grubnetaa64.efi.signed](http://ports.ubuntu.com/ubuntu-ports/dists/focal/main/uefi/grub2-arm64/current/grubnetaa64.efi.signed)\\n\\n - initrd extracted from *ubuntu-20.04.5-live-server-arm64.iso*\\n\\n - vmlinuz extracted from *ubuntu-20.04.5-live-server-arm64.iso*\\n\\nPlease replace the corresponding files when you want to work on amd64 image. For example, your files may be:',\n",
       " '  - Ubuntu 20.04.5 live server image *ubuntu-20.04.5-live-server-amd64.iso* [from https://releases.ubuntu.com/20.04.](https://releases.ubuntu.com/20.04.5/ubuntu-20.04.5-live-server-amd64.iso)\\n[5/ubuntu-20.04.5-live-server-amd64.iso](https://releases.ubuntu.com/20.04.5/ubuntu-20.04.5-live-server-amd64.iso)\\n\\n  - grub efi binary grubnetx64.efi.signed [from http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-](http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed)\\n[amd64/current/grubnetx64.efi.signed](http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed)\\n\\n - initrd extracted from *ubuntu-20.04.5-live-server-amd64.iso*\\n\\n - vmlinuz extracted from *ubuntu-20.04.5-live-server-amd64.iso*\\n\\n**Download and Serve Grub EFI Binary**\\n\\nThe grub binary helps us redirect the downloading path to the target files via grub.cfg [. You may refer to this discourse](https://discourse.ubuntu.com/t/netbooting-the-live-server-installer/14510)\\n[post to get more information about the PXE process and why we need this binary.](https://discourse.ubuntu.com/t/netbooting-the-live-server-installer/14510)\\n\\n$ sudo wget http://ports.ubuntu.com/ubuntu-ports/dists/focal/main/uefi/grub2-arm64/current/grubnetaa64.efi.signed \\nO /var/lib/tftpboot/grubnetaa64.efi.signed\\n\\nPlease note you may need to change **the archive dists name** from focal to your target distribution name.\\n\\n**Download and Serve More Files**',\n",
       " '[Fetch the installer by downloading a Ubuntu arm server iso, e.g. 20.04.5 live server arm64 iso. Please note the prefix](http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-arm64.iso)\\n*live* is significant. We will need the files available only in the live version.\\n\\nMount the iso and copy the target files we need to the TFTP folder\\n\\n$ sudo mount ./ubuntu-20.04.5-live-server-arm64.iso /mnt\\n\\n$ sudo mkdir /var/lib/tftpboot/grub /var/lib/tftpboot/casper\\n\\n$ sudo cp /mnt/boot/grub/grub.cfg /var/lib/tftpboot/grub/\\n\\n$ sudo cp /mnt/casper/initrd /var/lib/tftpboot/casper/\\n\\n$ sudo cp /mnt/casper/vmlinuz /var/lib/tftpboot/casper/\\n\\nSo, the TFTP root folder should look like this now:\\n\\n$ find /var/lib/tftpboot/\\n\\n/var/lib/tftpboot/\\n\\n/var/lib/tftpboot/grub\\n\\n/var/lib/tftpboot/grub/grub.cfg\\n\\n/var/lib/tftpboot/grubnetaa64.efi.signed\\n\\n/var/lib/tftpboot/casper\\n\\n/var/lib/tftpboot/casper/initrd\\n\\n/var/lib/tftpboot/casper/vmlinuz\\n\\nFinally, let’s customize the grub menu so we could install our target image by fetching it directly over the internet.\\n\\n$ sudo chmod +w /var/lib/tftpboot/grub/grub.cfg\\n\\n$ sudo vi /var/lib/tftpboot/grub/grub.cfg\\n\\nAdd an new entry\\n\\nmenuentry \"Install Ubuntu Server (Focal 20.04.5) (Pull the iso from web)\" {\\n\\nset gfxpayload=keep\\n\\n34\\n\\n\\n-----',\n",
       " 'linux /casper/vmlinuz url=http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5\\nlive-server-arm64.iso only-ubiquity ip=dhcp --\\ninitrd /casper/initrd\\n\\n}\\n\\n*ip=dhcp* is for the dhcp management setup in the lab. *url* is used to point to your target image download url. Remember\\nto change them according to your scenario.\\n\\nIf everything goes well, you should get into the expected grub menu of the ephemeral live prompt. Select the entry\\nyou just put in grub.cfg, which is Install Ubuntu Server (Focal 20.04.5) (Pull the iso from web) in our example.\\nWaiting a bit for downloading the iso and then you will see the subiquity welcome message. Enjoy the installation!',\n",
       " '## **Appendix**\\n\\n**Always Make Sure of the Serving File Names**\\n\\nFor example, please make sure the target file name for *linux* and *initrd* is correct. For example, the default initrd\\nbinary file name of 20.04.5 is *initrd*, and it is *initrd.lz* for 20.10. Always make sure you serve the right file names. This\\nis a frequent troubleshooting issue. Pay attention on this detail could save a lot of your time.\\n\\n**Booting Screenshots**\\n\\nIf your setup is correct, your grub.cfg should redirect the process to an ephemeral environment to download your\\ntarget image assigned in the grub entry of grub.cfg . You will see a screen like this if you are able to access console or\\nmonitor device of your target machine:\\n\\nWait a bit to complete downloading. If you see this subiquity welcome page, the installer is successfully launched via\\nyour UEFI PXE setup. Configurations!!\\n\\n35\\n\\n\\n-----',\n",
       " '- Open a terminal window on your workstation and make sure the ‘ipmitool’ package is installed.\\n\\n- Verify if you can reach the BMC of the IBM Power system via ipmitool with a simple ipmitool call like:\\n\\n$ ipmitool -I lanplus -H Power9Box -U <user> -P <password> power status\\n\\nChassis Power is off\\n\\nor:\\n\\n$ ipmitool -I lanplus -H Power9Box -U <user> -P <password> fru print 47\\n\\nProduct Name : OpenPOWER Firmware\\n\\nProduct Version : open-power-SUPERMICRO-P9DSU-V2.12-20190404-prod\\n\\nProduct Extra : op-build-1b9269e\\n\\nProduct Extra : buildroot-2018.11.3-12-g222837a\\n\\nProduct Extra : skiboot-v6.0.19\\n\\nProduct Extra : hostboot-c00d44a-pb1307d7\\n\\nProduct Extra : occ-8fa3854\\n\\nProduct Extra : linux-4.19.30-openpower1-p22d1df8\\n\\nProduct Extra : petitboot-v1.7.5-p8f5fc86\\n\\nor:\\n\\n$ ipmitool -I lanplus -H Power9Box -U <user> -P <password> sol info\\n\\nSet in progress : set-complete\\n\\nEnabled : true\\n\\nForce Encryption : false\\n\\nForce Authentication : false\\n\\nPrivilege Level : OPERATOR\\n\\nCharacter Accumulate Level (ms) : 0\\n\\nCharacter Send Threshold : 0\\n\\nRetry Count : 0\\n\\nRetry Interval (ms) : 0\\n\\nVolatile Bit Rate (kbps) : 115.2\\n\\nNon-Volatile Bit Rate (kbps) : 115.2\\n\\nPayload Channel : 1 (0x01)\\n\\nPayload Port : 623\\n\\n- Open a second terminal and activate serial-over-LAN (sol), so that you have two terminal windows open:\\n\\n1. to control the BMC via IPMI\\n\\n2. for the serial-over-LAN console\\n\\n- Activate serial-over-LAN:\\n\\n36\\n\\n\\n-----',\n",
       " '$ ipmitool -I lanplus -H Power9Box -U <user> -P <password> sol activate\\n\\n...\\n\\n- And power the system on in the ‘control terminal’ and watch the sol console:\\n\\n$ ipmitool -I lanplus -H Power9Box -U <user> -P <password> power on\\n\\n...\\n\\nIt takes some time to see the first lines in the sol console:\\n\\n[SOL Session operational. Use ~? for help]\\n\\n--== Welcome to Hostboot ==-\\n2.77131|secure|SecureROM valid - enabling functionality\\n\\n3.15860|secure|Booting in secure mode.\\n\\n5.59684|Booting from SBE side 0 on master proc=00050000\\n\\n5.60502|ISTEP 6. 5 - host_init_fsi\\n\\n5.87228|ISTEP 6. 6 - host_set_ipl_parms\\n\\n6.11032|ISTEP 6. 7 - host_discover_targets\\n\\n6.67868|HWAS|PRESENT> DIMM[03]=A0A0000000000000\\n\\n6.67870|HWAS|PRESENT> Proc[05]=8800000000000000\\n\\n6.67871|HWAS|PRESENT> Core[07]=3FFF0C33FFC30000\\n\\n6.98988|ISTEP 6. 8 - host_update_master_tpm\\n\\n7.22711|SECURE|Security Access Bit> 0xC000000000000000\\n\\n7.22711|SECURE|Secure Mode Disable (via Jumper)> 0x0000000000000000\\n\\n7.22731|ISTEP 6. 9 - host_gard\\n\\n7.43353|HWAS|FUNCTIONAL> DIMM[03]=A0A0000000000000\\n\\n7.43354|HWAS|FUNCTIONAL> Proc[05]=8800000000000000\\n\\n7.43356|HWAS|FUNCTIONAL> Core[07]=3FFF0C33FFC30000\\n\\n7.44509|ISTEP 6.10 - host_revert_sbe_mcs_setup\\n\\n…\\n\\n- After a moment the system reaches the Petitboot screen:\\n\\nPetitboot (v1.7.5-p8f5fc86) 9006-12P 1302NXA\\n\\n─────────────────────────────────────────────────\\n\\n[Network: enP2p1s0f0 / 0c:c4:7a:87:04:d8]\\n\\nExecute\\n\\nnetboot enP2p1s0f0 (pxelinux.0)\\n\\n[CD/DVD: sr0 / 2019-10-17-13-35-12-00]\\n\\nInstall Ubuntu Server\\n\\n[Disk: sda2 / 295f571b-b731-4ebb-b752-60aadc80fc1b]\\n\\nUbuntu, with Linux 5.4.0-14-generic (recovery mode)\\n\\nUbuntu, with Linux 5.4.0-14-generic\\n\\nUbuntu\\n\\nSystem information\\n\\nSystem configuration',\n",
       " 'System status log\\n\\nLanguage\\n\\nRescan devices\\n\\nRetrieve config from URL\\n\\nPlugins (0)\\n\\n*Exit to shell\\n\\n─────────────────────────────────────────────────\\n\\nEnter=accept, e=edit, n=new, x=exit, l=language, g=log, h=help\\n\\nSelect ‘*Exit to shell’\\n\\n**Notice:**\\nMake sure you really watch the sol, since the petitboot screen (above) has a time out (usually 10 or 30 seconds)\\nand afterwards it automatically proceeds and it tries to boot from the configured devices (usually disk). This\\ncan be prevented by just navigating in petitboot.\\nThe petitboot shell is small Linux based OS:\\n\\n...\\n\\n37\\n\\n\\n-----',\n",
       " 'Exiting petitboot. Type \\'exit\\' to return.\\n\\nYou may run \\'pb-sos\\' to gather diagnostic data\\n\\n**Notice:**\\n\\nIn case one needs to gather system details and diagnostic data for IBM support, this can be done here by running\\n‘pb-sos’ (see msg).\\n\\n  - Now download the ‘live-server’ ISO image (notice that ‘focal-live-server-ppc64el.iso’ uses subiquity, ‘focal-servers390x.iso’ uses d-i):\\nAgain for certain web locations a proxy needs to be used:\\n\\n/ # export http_proxy=http://squid.proxy:3128 # in case a proxy is required\\n\\n/ #\\n\\n/ # wget http://cdimage.ubuntu.com/ubuntu-server/daily-live/current/focal-live-server-ppc64el.iso\\n\\nConnecting to <proxy-ip>:3128 (<proxy-ip>:3128)\\n\\nfocal-live-server-pp 100% |....................| 922M 0:00:00 ETA\\n\\n  - Next is to loop-back mount the ISO:\\n\\n/ # mkdir iso\\n\\n/ # mount -o loop focal-live-server-ppc64el.iso iso\\n\\nOr in case autodetect of type iso9660 is not supported or not working, explicitly specify the ‘iso9660’ type:\\n\\n/ # mount -t iso9660 -o loop focal-live-server-ppc64el.iso iso\\n\\n  - Now load kernel and initrd from the loop-back mount, specify any needed kernel parameters and get it going:\\n\\n/ # kexec -l ./iso/casper/vmlinux --initrd=./iso/casper/initrd.gz --append=\"ip=dhcp url=http://cdimage.ubuntu.com/\\n\\nserver/daily-live/current/focal-live-server-ppc64el.iso http_proxy=http://squid.proxy:3128 --- quiet\"\\n\\n/ # kexec -e\\n\\nThe system is going down NOW!\\n\\nSent SIGTERM to all processes\\n\\nSent SIGKILL to all processes\\n\\n...\\n\\n**Note** that in order to boot with and install the hwe kernel (if available), just substitute vmlinux with vmlinux-hwe in\\nthe first kexec line.\\n\\n  - The system now performs the initial boot of the installer:',\n",
       " '[ 1200.687004] kexec_core: Starting new kernel\\n\\n[ 1277.493883374,5] OPAL: Switch to big-endian OS\\n\\n[ 1280.465061219,5] OPAL: Switch to little-endian OS\\n\\nln: /tmp/mountroot-fail-hooks.d//scripts/init-premount/lvm2: No such file or directory\\n\\nInternet Systems Consortium DHCP Client 4.4.1\\n\\nCopyright 2004-2018 Internet Systems Consortium.\\n\\nAll rights reserved.\\n\\nFor info, please visit https://www.isc.org/software/dhcp/\\n\\nListening on LPF/enP2p1s0f3/0c:c4:7a:87:04:db\\n\\nSending on LPF/enP2p1s0f3/0c:c4:7a:87:04:db\\n\\nListening on LPF/enP2p1s0f2/0c:c4:7a:87:04:da\\n\\nSending on LPF/enP2p1s0f2/0c:c4:7a:87:04:da\\n\\nListening on LPF/enP2p1s0f1/0c:c4:7a:87:04:d9\\n\\nSending on LPF/enP2p1s0f1/0c:c4:7a:87:04:d9\\n\\nListening on LPF/enP2p1s0f0/0c:c4:7a:87:04:d8\\n\\nSending on LPF/enP2p1s0f0/0c:c4:7a:87:04:d8\\n\\nSending on Socket/fallback\\n\\nDHCPDISCOVER on enP2p1s0f3 to 255.255.255.255 port 67 interval 3\\n\\n(xid=0x8d5704c)\\n\\nDHCPDISCOVER on enP2p1s0f2 to 255.255.255.255 port 67 interval 3\\n\\n(xid=0x94b25b28)\\n\\nDHCPDISCOVER on enP2p1s0f1 to 255.255.255.255 port 67 interval 3\\n\\n(xid=0x4edd0558)\\n\\nDHCPDISCOVER on enP2p1s0f0 to 255.255.255.255 port 67 interval 3\\n\\n(xid=0x61c90d28)\\n\\nDHCPOFFER of 10.245.71.102 from 10.245.71.3\\n\\nDHCPREQUEST for 10.245.71.102 on enP2p1s0f0 to 255.255.255.255 port 67\\n\\n(xid=0x280dc961)\\n\\n38\\n\\n\\n-----',\n",
       " 'DHCPACK of 10.245.71.102 from 10.245.71.3 (xid=0x61c90d28)\\n\\nbound to 10.245.71.102 -- renewal in 236 seconds.\\n\\nConnecting to 91.189.89.11:3128 (91.189.89.11:3128)\\n\\nfocal-live-server-pp 1% | | 14.0M 0:01:04 ETA\\n\\nfocal-live-server-pp 4% |* | 45.1M 0:00:38 ETA\\n\\nfocal-live-server-pp 8% |** | 76.7M 0:00:33 ETA\\n\\nfocal-live-server-pp 11% |*** | 105M 0:00:31 ETA\\n\\nfocal-live-server-pp 14% |**** | 133M 0:00:29 ETA\\n\\nfocal-live-server-pp 17% |***** | 163M 0:00:27 ETA\\n\\nfocal-live-server-pp 20% |****** | 190M 0:00:26 ETA\\n\\nfocal-live-server-pp 24% |******* | 222M 0:00:25 ETA\\n\\nfocal-live-server-pp 27% |******** | 253M 0:00:23 ETA\\n\\nfocal-live-server-pp 30% |********* | 283M 0:00:22 ETA\\n\\nfocal-live-server-pp 34% |********** | 315M 0:00:21 ETA\\n\\nfocal-live-server-pp 37% |*********** | 343M 0:00:20 ETA\\n\\nfocal-live-server-pp 39% |************ | 367M 0:00:19 ETA\\n\\nfocal-live-server-pp 42% |************* | 392M 0:00:18 ETA\\n\\nfocal-live-server-pp 45% |************** | 420M 0:00:17 ETA\\n\\nfocal-live-server-pp 48% |*************** | 451M 0:00:16 ETA\\n\\nfocal-live-server-pp 52% |**************** | 482M 0:00:15 ETA\\n\\nfocal-live-server-pp 55% |***************** | 514M 0:00:14 ETA\\n\\nfocal-live-server-pp 59% |****************** | 546M 0:00:13 ETA\\n\\nfocal-live-server-pp 62% |******************** | 578M 0:00:11 ETA\\n\\nfocal-live-server-pp 65% |********************* | 607M 0:00:10 ETA\\n\\nfocal-live-server-pp 69% |********************** | 637M 0:00:09 ETA\\n\\nfocal-live-server-pp 72% |*********************** | 669M 0:00:08 ETA\\n\\nfocal-live-server-pp 75% |************************ | 700M 0:00:07 ETA\\n\\nfocal-live-server-pp 79% |************************* | 729M 0:00:06 ETA',\n",
       " \"focal-live-server-pp 82% |************************** | 758M 0:00:05 ETA\\n\\nfocal-live-server-pp 85% |*************************** | 789M 0:00:04 ETA\\n\\nfocal-live-server-pp 88% |**************************** | 817M 0:00:03 ETA\\n\\nfocal-live-server-pp 91% |***************************** | 842M 0:00:02 ETA\\n\\nfocal-live-server-pp 93% |****************************** | 867M 0:00:01 ETA\\n\\nfocal-live-server-pp 97% |******************************* | 897M 0:00:00 ETA\\n\\nfocal-live-server-pp 100% |********************************| 922M 0:00:00 ETA\\n\\nmount: mounting /cow on /root/cow failed: No such file or directory\\n\\nConnecting to plymouth: Connection refused\\n\\npasswd: password expiry information changed.\\n\\n[ 47.202736] /dev/loop3: Can't open blockdev\\n\\n[ 52.672550] cloud-init[3759]: Cloud-init v. 20.1-10-g71af48df-0ubuntu1 running\\n\\n'init-local' at Wed, 18 Mar 2020 15:18:07 +0000. Up 51.87 seconds.\\n\\n...\\n\\n  - And you will eventually reach the initial subiquity installer screen:\\n\\n▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\\n\\nWillkommen! Bienvenue! Welcome! Добро пожаловать! Welkom\\n\\n▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀\\n\\nUse UP, DOWN and ENTER keys to select your language.\\n\\n[ English         - ]\\n\\n[ Asturianu         - ]\\n\\n[ Català         - ]\\n\\n[ Hrvatski         - ]\\n\\n[ Nederlands         - ]\\n\\n[ Suomi         - ]\\n\\n[ Français         - ]\\n\\n[ Deutsch         - ]\\n\\n[ Ελληνικά         - ]\\n\\n[ Magyar         - ]\\n\\n[ Latviešu         - ]\\n\\n[ Norsk bokmål         - ]\\n\\n[ Polski         - ]\\n\\n[ Русский         - ]\\n\\n39\\n\\n\\n-----\",\n",
       " '[ Español         - ]\\n\\n[ Українська         - ]',\n",
       " '```',\n",
       " '- The rest is (subiquity-) installation as usual...\\n\\n[There is also documentation on booting the installer over the network.](https://ubuntu.com/server/docs/install/netboot-ppc64el)\\n\\n - *Notice:*\\n\\nNot all IBM Power machines come with the capability to install via a virtual CDROM !\\n\\n  - A separate system (ideally in the same network, because of ipmitool) is needed to host the ppc64el ISO Image\\nfile, that is later used as virtual CDROM.\\n\\n  - Login to this separate host and make sure that the ipmitool package is installed:\\n\\n$ sudo apt install ipmitool\\n\\nas well as Samba:\\n\\n$ sudo apt install samba\\n\\n  - Next is to setup and configure Samba:\\n\\n$ sudo touch /etc/samba/smb.conf && sudo tee -a /etc/samba/smb.conf <<EOF\\n\\n[winshare]\\n\\npath=/var/winshare\\n\\nbrowseable = yes\\n\\nread only = no\\n\\nguest ok = yes\\n\\nEOF\\n\\nAnd do a quick verification that the required lines are in:\\n\\n$ tail -n 5 /etc/samba/smb.conf\\n\\n[winshare]\\n\\npath=/var/winshare\\n\\nbrowseable = yes\\n\\nread only = no\\n\\nguest ok = yes\\n\\n  - (Optional)\\nFor downloading the image you may have to use a proxy server:\\n\\n$ sudo touch ~/.wgetrc && sudo tee -a ~/.wgetrc <<EOF\\n\\nuse_proxy=yes\\n\\nhttp_proxy=squid.proxy:3128\\n\\nhttps_proxy=squid.proxy:3128\\n\\nEOF\\n\\n  - The ISO image needs to be downloaded now:\\n\\n$ wget http://cdimage.ubuntu.com/ubuntu/releases/focal/release/ubuntu-20.04-live-server-ppc64el.iso -\\ndirectory-prefix=/var/winshare\\n\\nThe proxy can also be passed over as wget argument, like this:\\n\\n$ wget -e use_proxy=yes -e http_proxy=squid.proxy:3128 http://cdimage.ubuntu.com/ubuntu/releases/focal/release/ubu\\n\\n20.04-live-server-ppc64el.iso --directory-prefix=/var/winshare\\n\\n  - Change file mode of the ISO image file:\\n\\n$ sudo chmod -R 755 /var/winshare/',\n",
       " '$ ls -l /var/winshare/\\n\\n-rwxr-xr-x 1 ubuntu ubuntu 972500992 Mar 23 08:02 focal-live-server-ppc64el.iso\\n\\n  - Restart and check the Samba service:\\n\\n$ sudo service smbd restart\\n\\n$ sudo service smbd status\\n\\n   - smbd.service - Samba SMB Daemon\\n\\nLoaded: loaded (/lib/systemd/system/smbd.service; enabled; vendor\\n\\npreset: ena\\n\\nActive: active (running) since Tue 2020-02-04 15:17:12 UTC; 4s ago\\n\\n40\\n\\n\\n-----\\n\\nDocs: man:smbd(8)\\n\\nman:samba(7)\\n\\nman:smb.conf(5)\\n\\nMain PID: 6198 (smbd)\\n\\nStatus: \"smbd: ready to serve connections...\"\\n\\nTasks: 4 (limit: 19660)\\n\\nCGroup: /system.slice/smbd.service\\n\\n├─6198 /usr/sbin/smbd --foreground --no-process-group\\n\\n├─6214 /usr/sbin/smbd --foreground --no-process-group\\n\\n├─6215 /usr/sbin/smbd --foreground --no-process-group\\n\\n└─6220 /usr/sbin/smbd --foreground --no-process-group\\n\\nFeb 04 15:17:12 host systemd[1]: Starting Samba SMB Daemon…\\n\\nFeb 04 15:17:12 host systemd[1]: Started Samba SMB Daemon.\\n\\n- Test Samba share:\\n\\nubuntu@host:~$ smbclient -L localhost\\n\\nWARNING: The \"syslog\" option is deprecated\\n\\nEnter WORKGROUP\\\\ubuntu\\'s password:\\n\\nSharename Type Comment\\n\\n--------- ---- ------\\nprint$ Disk Printer Drivers\\n\\nwinshare Disk\\n\\nIPC$ IPC IPC Service (host server (Samba, Ubuntu))\\n\\nReconnecting with SMB1 for workgroup listing.\\n\\nServer Comment\\n\\n--------- ------\\nWorkgroup Master\\n\\n--------- ------\\nWORKGROUP host\\n\\n- Get the IP address of the Samba host:\\n\\n$ ip -4 -brief address show\\n\\nlo UNKNOWN 127.0.0.1/8\\n\\nibmveth2 UNKNOWN 10.245.246.42/24\\n\\n- (Optional)\\nEven more testing if the Samba share is accessible from remote:\\n\\nuser@workstation:~$ mkdir -p /tmp/test\\n\\nuser@workstation:~$ sudo mount -t cifs -o\\n\\nusername=guest,password=guest //10.245.246.42/winshare /tmp/test/',\n",
       " 'user@workstation:~$ ls -la /tmp/test/\\n\\ntotal 1014784\\n\\ndrwxr-xr-x 2 root root 0 May 4 15:46 .\\n\\ndrwxrwxrwt 18 root root 420 May 4 19:25 ..\\n\\n-rwxr-xr-x 1 root root 1038249984 May 3 19:37 ubuntu-20.04-live-server-ppc64el.iso\\n\\n- Now use a browser and navigate to the BMC of the Power system that should be installed (let’s assume the\\nBMC’s IP address is 10.245.246.247):\\n\\nfirefox http://10.245.246.247/\\n\\n- Login to the BMC and find and select:\\nVirtual Media --> CDROM\\n\\n- Enter the IP address of the Samba share:\\n\\n10.245.246.42\\n\\nand the path to the Samba share:\\n\\n\\\\winshare\\\\focal-live-server-ppc64el.iso\\n\\n- Click Save and Mount\\n(make sure that the virtual CDROM is really properly mounted !)\\n\\nCD-ROM Image:\\n\\nThis option allows you to share a CD-ROM image over a Windows Share with a\\n\\n41\\n\\n\\n-----\\n\\nmaximum size of 4.7GB. This image will be emulated to the host as USB device.\\n\\nDevice 1 There is an iso file mounted.\\n\\nDevice 2 No disk emulation set.\\n\\nDevice 3 No disk emulation set.\\n\\n<Refresh Status>\\n\\nShare host: 10.245.246.42\\n\\nPath to image: \\\\winshare\\\\focal-live-server-ppc64el.iso\\n\\nUser (optional):\\n\\nPassword (optional):\\n\\n<Save> <Mount> <Unmount>\\n\\n- *Notice:*\\n\\nIt’s important that you see a status like:\\n\\nDevice 1 There is an iso file mounted\\n\\nOnly in this case the virtual CDROM is properly mounted and you will see the boot / install from CDROM\\nentry in petitboot:\\n\\n[CD/DVD: sr0 / 2020-03-23-08-02-42-00]\\n\\nInstall Ubuntu Server\\n\\n- Now use the ipmitool to boot the system into the petitboot loader:\\n\\n$ ipmitool -I lanplus -H 10.245.246.247 -U ADMIN -P <password> power status\\n\\n$ ipmitool -I lanplus -H 10.245.246.247 -U ADMIN -P <password> sol activate',\n",
       " '$ ipmitool -I lanplus -H 10.245.246.247 -U ADMIN -P <password> power on\\n\\nChassis Power Control: Up/On\\n\\n- And reach the Petitboot screen:\\n\\nPetitboot (v1.7.5-p8f5fc86) 9006-12C BOS0026\\n\\n─────────────────────────────────────────────\\n\\n[Network: enP2p1s0f0 / ac:1f:6b:09:c0:52]\\n\\nexecute\\n\\nnetboot enP2p1s0f0 (pxelinux.0)\\n\\nSystem information\\n\\nSystem configuration\\n\\nSystem status log\\n\\nLanguage\\n\\nRescan devices\\n\\nRetrieve config from URL\\n\\n*Plugins (0)\\n\\nExit to shell\\n\\n─────────────────────────────────────────────\\n\\nEnter=accept, e=edit, n=new, x=exit, l=language, g=log, h=help\\n\\nDefault boot cancelled\\n\\n- And make sure that booting from CDROM is enabled:\\n\\nPetitboot (v1.7.5-p8f5fc86) 9006-12C BOS0026\\n\\n─────────────────────────────────────────────\\n\\n[Network: enP2p1s0f0 / ac:1f:6b:09:c0:52]\\n\\nExecute\\n\\nnetboot enP2p1s0f0 (pxelinux.0)\\n\\n[Disk: sda2 / ebdb022b-96b2-4f4f-ae63-69300ded13f4]\\n\\nUbuntu, with Linux 5.4.0-12-generic (recovery mode)\\n\\nUbuntu, with Linux 5.4.0-12-generic\\n\\nUbuntu\\n\\nSystem information\\n\\nSystem configuration\\n\\nSystem status log\\n\\nLanguage\\n\\nRescan devices\\n\\n42\\n\\n\\n-----\\n\\nRetrieve config from URL\\n\\n*Plugins (0)\\n\\nExit to shell\\n\\n─────────────────────────────────────────────\\n\\nEnter=accept, e=edit, n=new, x=exit, l=language, g=log, h=help\\n\\n[sda3] Processing new Disk device\\n\\nPetitboot System Configuration\\n\\n──────────────────────────────────────────────\\n\\nAutoboot: ( ) Disabled\\n\\n(*) Enabled\\n\\nBoot Order: (0) Any CD/DVD device\\n\\n(1) disk: sda2 [uuid: ebdb022b-96b2-4f4f-ae63-69300ded13f4]\\n\\n(2) net: enP2p1s0f0 [mac: ac:1f:6b:09:c0:52]\\n\\n[ Add Device ]\\n\\n[ Clear & Boot Any ]\\n\\n[ Clear ]\\n\\nTimeout: 30 seconds\\n\\nNetwork: (*) DHCP on all active interfaces\\n\\n( ) DHCP on a specific interface',\n",
       " '( ) Static IP configuration\\n\\n─────────────────────────────────────────────\\n\\ntab=next, shift+tab=previous, x=exit, h=help\\n\\nPetitboot System Configuration\\n\\n─────────────────────────────────────────────\\n\\nNetwork: (*) DHCP on all active interfaces\\n\\n( ) DHCP on a specific interface\\n\\n( ) Static IP configuration\\n\\nDNS Server(s): (eg. 192.168.0.2)\\n\\n(if not provided by DHCP server)\\n\\nHTTP Proxy:\\n\\nHTTPS Proxy:\\n\\nDisk R/W: ( ) Prevent all writes to disk\\n\\n(*) Allow bootloader scripts to modify disks\\n\\nBoot console: (*) /dev/hvc0 [IPMI / Serial]\\n\\n( ) /dev/tty1 [VGA]\\n\\nCurrent interface: /dev/hvc0\\n\\n[ OK ] [ Help ] [ Cancel ]\\n\\n───────────────────────────────────────────\\n\\ntab=next, shift+tab=previous, x=exit, h=help\\n\\n- Now select the ‘Install Ubuntu Server’ entry below the CD/DVD entry:\\n\\n[CD/DVD: sr0 / 2020-03-23-08-02-42-00]\\n\\n   - Install Ubuntu Server\\n\\n- And let Petitboot boot from the (virtual) CDROM image:\\n\\nSent SIGKILL to all processes\\n\\n[ 119.355371] kexec_core: Starting new kernel\\n\\n[ 194.483947394,5] OPAL: Switch to big-endian OS\\n\\n43\\n\\n\\n-----\\n\\n[ 197.454615202,5] OPAL: Switch to little-endian OS\\n\\n  - Finally the initial subiquity installer screen will show up in the console:\\n\\n▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\\n\\nWillkommen! Bienvenue! Welcome! Добро пожаловать! Welkom\\n\\n▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀\\n\\nUse UP, DOWN and ENTER keys to select your language.\\n\\n[ English           - ]\\n\\n[ Asturianu           - ]\\n\\n[ Català           - ]\\n\\n[ Hrvatski           - ]\\n\\n[ Nederlands           - ]\\n\\n[ Suomi           - ]\\n\\n[ Français           - ]\\n\\n[ Deutsch           - ]\\n\\n[ Ελληνικά           - ]\\n\\n[ Magyar           - ]\\n\\n[ Latviešu           - ]\\n\\n[ Norsk bokmål           - ]',\n",
       " '[ Polski           - ]\\n\\n[ Русский           - ]\\n\\n[ Español           - ]\\n\\n[ Українська           - ]\\n\\n  - The rest of the installation is business as usual …\\n\\nDoing a manual live installation as described here - meaning without specifying a parmfile - is supported since Ubuntu\\nServer LTS 20.04.5 (‘Focal’) and any newer release, like 20.10 (‘Groovy’).\\n\\nThe following guide assumes that a z/VM guest has been defined, and that it is able to either reach the public\\n\\ncdimage.ubuntu.com server or an internal FTP or HTTP server that hosts an Ubuntu Server 20.04 installer image, like\\n[this 20.04 (a.k.a. Focal) daily live image here: http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-](http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso)\\n[20.04.5-live-server-s390x.iso](http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso)\\n\\n  - Find a place to download the installer image:\\n\\nuser@workstation:~$ wget\\n\\nhttp://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso\\n\\n--2020-08-08 16:01:52-\\nhttp://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso\\n\\nResolving cdimage.ubuntu.com (cdimage.ubuntu.com)... 2001:67c:1560:8001::1d, 2001:67c:1360:8001::27, 2001:67c:1360\\n\\nConnecting to cdimage.ubuntu.com\\n\\n(cdimage.ubuntu.com)|2001:67c:1560:8001::1d|:80... connected.\\n\\nHTTP request sent, awaiting response... 200 OK\\n\\nLength: 705628160 (673M) [application/x-iso9660-image]\\n\\nSaving to: ‘ubuntu-20.04.5-live-server-s390x.iso’\\n\\nubuntu-20.04.5-live 100%[===================>] 672.94M 37.1MB/s in\\n\\n17s',\n",
       " '2020-08-08 16:02:10 (38.8 MB/s) - ‘ubuntu-20.04.5-live-server-s390x.iso’ saved\\n\\n[705628160/705628160]\\n\\n  - Now loop-back mount the ISO to extract four files that are needed for a z/VM guest installation:\\n\\nuser@workstation:~$ mkdir iso\\n\\nuser@workstation:~$ sudo mount -o loop ubuntu-20.04.5-live-server-s390x.iso iso\\n\\nuser@workstation:~$\\n\\nuser@workstation:~$ ls -1 ./iso/boot/{ubuntu.exec,parmfile.*,kernel.u*,initrd.u*}\\n\\n./iso/boot/initrd.ubuntu\\n\\n./iso/boot/kernel.ubuntu\\n\\n./iso/boot/parmfile.ubuntu\\n\\n./iso/boot/ubuntu.exec\\n\\n44\\n\\n\\n-----\\n\\n  - Now transfer these four files to your z/VM guest (for example to its ‘A’ file mode), using either the 3270 terminal\\nemulator or ftp.\\n\\n  - Then log on to your z/VM guest that you want to use for the installation. In this example it will be guest\\n‘10.222.111.24’.\\n\\n - Execute the ubuntu REXX script to kick-off the installation:\\n\\nubuntu\\n\\n00: 0000004 FILES PURGED\\n\\n00: RDR FILE 0125 SENT FROM 10.222.111.24 PUN WAS 0125 RECS 101K CPY 001 A NOHOLD NO\\n\\nKEEP\\n\\n00: RDR FILE 0129 SENT FROM 10.222.111.24 PUN WAS 0129 RECS 0001 CPY 001 A NOHOLD NO\\n\\nKEEP\\n\\n00: RDR FILE 0133 SENT FROM 10.222.111.24 PUN WAS 0133 RECS 334K CPY 001 A NOHOLD NO\\n\\nKEEP\\n\\n00: 0000003 FILES CHANGED\\n\\n00: 0000003 FILES CHANGED\\n\\n01: HCPGSP2627I The virtual machine is placed in CP mode due to a SIGP initial CPU reset from CPU 00.\\n\\n02: HCPGSP2627I The virtual machine is placed in CP mode due to a SIGP initial CPU reset from CPU 00.\\n\\n03: HCPGSP2627I The virtual machine is placed in CP mode due to a SIGP initial CPU reset from CPU 00.\\n\\n¬ 0.390935| Initramfs unpacking failed: Decoding failed\\n\\nUnable to find a medium container a live file system',\n",
       " \"  - In the usual case that no parmfile was configured, the installation system now offers to interactively configure\\nthe basic network:\\n\\nAttempt interactive netboot from a URL?\\n\\nyes no (default yes): yes\\n\\nAvailable qeth devices:\\n\\n0.0.0600 0.0.0603\\n\\nzdev to activate (comma separated, optional): 0600\\n\\nQETH device 0.0.0600:0.0.0601:0.0.0602 configured\\n\\nTwo methods available for IP configuration:\\n\\n    - static: for static IP configuration\\n\\n    - dhcp: for automatic IP configuration\\n\\nstatic dhcp (default 'dhcp'): static\\n\\nip: 10.222.111.24\\n\\ngateway (default 10.222.111.1): .\\n\\ndns (default .):\\n\\nvlan id (optional):\\n\\nhttp://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso (default)\\n\\nurl: ftp://10.11.12.2:21/ubuntu-live-server-20.04.5/ubuntu-20.04.5-live-server-s390x.iso\\n\\nhttp_proxy (optional):\\n\\n  - Make sure that the same version of the ISO image that was used to extract the installer files – kernel and\\ninitrd – is referenced at the ‘url:’ setting. It can be at a different location, for example directly referencing\\nthe public cdimage.ubuntu.com [server: http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.](http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso)\\n[04.5-live-server-s390x.iso](http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso)\\n\\n  - The boot-up of the live-server installation now completes:\\n\\nConfiguring networking...\\n\\nQETH device 0.0.0600:0.0.0601:0.0.0602 already configured\\n\\nIP-Config: enc600 hardware address 02:28:0a:00:00:39 mtu 1500\\n\\nIP-Config: enc600 guessed broadcast address 10.222.111255\\n\\nIP-Config: enc600 complete:\",\n",
       " \"address: 10.222.111.24 broadcast: 10.222.111255 netmask: 255.255.255.0\\n\\ngateway: 10.222.111.1 dns0 : 10.222.111.1 dns1 : 0.0.0.0\\n\\nrootserver: 0.0.0.0 rootpath:\\n\\nfilename :\\n\\nConnecting to 10.11.12.2:21 (10.11.12.2:21)\\n\\nfocal-live-server-s 5% !* ! 35.9M 0:00:17 ETA\\n\\nfocal-live-server-s 19% !****** ! 129M 0:00:08 ETA\\n\\nfocal-live-server-s 33% !********** ! 225M 0:00:05 ETA\\n\\n45\\n\\n\\n-----\\n\\nfocal-live-server-s 49% !*************** ! 330M 0:00:04 ETA\\n\\nfocal-live-server-s 60% !******************* ! 403M 0:00:03 ETA\\n\\nfocal-live-server-s 76% !************************ ! 506M 0:00:01 ETA\\n\\nfocal-live-server-s 89% !**************************** ! 594M 0:00:00 ETA\\n\\nfocal-live-server-s 100% !********************************! 663M 0:00:00 ETA\\n\\npasswd: password expiry information changed.\\n\\nQETH device 0.0.0600:0.0.0601:0.0.0602 already configured\\n\\nno search or nameservers found in /run/net-enc600.conf /run/net-*.conf /run/net6\\n\\n-*.conf\\n\\n¬ 594.766372| /dev/loop3: Can't open blockdev\\n\\n¬ 595.610434| systemd¬1|: multi-user.target: Job getty.target/start deleted to\\n\\nbreak ordering cycle starting with multi-user.target/start\\n\\n¬ ¬0;1;31m SKIP ¬0m| Ordering cycle found, skipping ¬0;1;39mLogin Prompts ¬0m\\n\\n¬ 595.623027| systemd¬1|: Failed unmounting /cdrom.\\n\\n¬ ¬0;1;31mFAILED ¬0m| Failed unmounting ¬0;1;39m/cdrom ¬0m.\\n\\n¬ 598.973538| cloud-init¬1256|: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 runnin\\n\\ng 'init-local' at Thu, 04 Jun 2020 12:06:46 +0000. Up 598.72 seconds.\\n\\n¬ 599.829069| cloud-init¬1288|: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 runnin\\n\\ng 'init' at Thu, 04 Jun 2020 12:06:47 +0000. Up 599.64 seconds.\\n\\n¬ 599.829182| cloud-init¬1288|: ci-info: ++++++++++++++++++++++++++++++++++++Ne\",\n",
       " 't device info+++++++++++++++++++++++++++++++++++++\\n\\n¬ 599.829218| cloud-init¬1288|: ci-info: +--------+------+--------------------\\n----+---------------+--------+-------------------+\\n\\n¬ 599.829255| cloud-init¬1288|: ci-info: ! Device ! Up ! Address\\n\\n! Mask ! Scope ! Hw-Address !\\n\\n¬ 599.829292| cloud-init¬1288|: ci-info: +--------+------+--------------------\\n----+---------------+--------+-------------------+\\n\\n¬ 599.829333| cloud-init¬1288|: ci-info: ! enc600 ! True ! 10.222.111.24\\n\\n! 255.255.255.0 ! global ! 02:28:0a:00:00:39 !\\n\\n¬ 599.829376| cloud-init¬1288|: ci-info: ! enc600 ! True ! fe80::28:aff:fe00:3\\n\\n/64 ! . ! link ! 02:28:0a:00:00:39 !\\n\\n¬ 599.829416| cloud-init¬1288|: ci-info: ! lo ! True ! 127.0.0.1\\n\\n! 255.0.0.0 ! host ! . !\\n\\n¬ 599.829606| cloud-init¬1288|: ci-info: ! lo ! True ! ::1/128\\n\\n! . ! host ! . !\\n\\n¬ 599.829684| cloud-init¬1288|: ci-info: +--------+------+--------------------\\n----+---------------+--------+-------------------+\\n\\n¬ 599.829721| cloud-init¬1288|: ci-info: ++++++++++++++++++++++++++++++Route IP\\n\\nv4 info++++++++++++++++++++++++++++++\\n\\n¬ 599.829754| cloud-init¬1288|: ci-info: +-------+--------------+-------------\\n+---------------+-----------+-------+\\n\\n¬ 599.829789| cloud-init¬1288|: ci-info: ! Route ! Destination ! Gateway\\n\\n! Genmask ! Interface ! Flags !\\n\\n¬ 599.829822| cloud-init¬1288|: ci-info: +-------+--------------+-------------\\n+---------------+-----------+-------+\\n\\n¬ 599.829858| cloud-init¬1288|: ci-info: ! 0 ! 0.0.0.0 ! 10.222.111.1\\n\\n! 0.0.0.0 ! enc600 ! UG !\\n\\n¬ 599.829896| cloud-init¬1288|: ci-info: ! 1 ! 10.222.1110 ! 0.0.0.0\\n\\n! 255.255.255.0 ! enc600 ! U !\\n\\n¬ 599.829930| cloud-init¬1288|: ci-info: +-------+--------------+-------------',\n",
       " \"+---------------+-----------+-------+\\n\\n¬ 599.829962| cloud-init¬1288|: ci-info: +++++++++++++++++++Route IPv6 info++++\\n\\n+++++++++++++++\\n\\n¬ 599.829998| cloud-init¬1288|: ci-info: +-------+-------------+---------+----\\n------+-------+\\n\\n¬ 599.830031| cloud-init¬1288|: ci-info: ! Route ! Destination ! Gateway ! Inte\\n\\nrface ! Flags !\\n\\n¬ 599.830064| cloud-init¬1288|: ci-info: +-------+-------------+---------+----\\n------+-------+\\n\\n¬ 599.830096| cloud-init¬1288|: ci-info: ! 1 ! fe80::/64 ! :: ! en\\n\\nc600 ! U !\\n\\n¬ 599.830131| cloud-init¬1288|: ci-info: ! 3 ! local ! :: ! en\\n\\n46\\n\\n\\n-----\\n\\nc600 ! U !\\n\\n¬ 599.830164| cloud-init¬1288|: ci-info: ! 4 ! ff00::/8 ! :: ! en\\n\\nc600 ! U !\\n\\n¬ 599.830212| cloud-init¬1288|: ci-info: +-------+-------------+---------+----\\n------+-------+\\n\\n¬ 601.077953| cloud-init¬1288|: Generating public/private rsa key pair.\\n\\n¬ 601.078101| cloud-init¬1288|: Your identification has been saved in /etc/ssh/\\n\\nssh_host_rsa_key\\n\\n¬ 601.078136| cloud-init¬1288|: Your public key has been saved in /etc/ssh/ssh\\n\\nhost_rsa_key.pub\\n\\n¬ 601.078170| cloud-init¬1288|: The key fingerprint is:\\n\\n¬ 601.078203| cloud-init¬1288|: SHA256:kHtkABZwk8AE80fy0KPzTRcYpht4iXdZmJ37Cgi3\\n\\nfJ0 root§ubuntu-server\\n\\n¬ 601.078236| cloud-init¬1288|: The key's randomart image is:\\n\\n¬ 601.078274| cloud-init¬1288|: +---¬RSA 3072|----+\\n\\n¬ 601.078307| cloud-init¬1288|: !o+*+B++*.. !\\n\\n¬ 601.078340| cloud-init¬1288|: ! o.X+=+=+ !\\n\\n¬ 601.078373| cloud-init¬1288|: ! +.O.= oo !\\n\\n¬ 601.078406| cloud-init¬1288|: ! ++.+.=o !\\n\\n¬ 601.078439| cloud-init¬1288|: ! *.=.oSo !\\n\\n¬ 601.078471| cloud-init¬1288|: ! = +.E . !\\n\\n¬ 601.078503| cloud-init¬1288|: ! . . . !\\n\\n¬ 601.078537| cloud-init¬1288|: ! . !\",\n",
       " \"¬ 601.078570| cloud-init¬1288|: ! !\\n\\n¬ 601.078602| cloud-init¬1288|: +----¬SHA256|-----+\\n\\n¬ 601.078635| cloud-init¬1288|: Generating public/private dsa key pair.\\n\\n¬ 601.078671| cloud-init¬1288|: Your identification has been saved in /etc/ssh/\\n\\nssh_host_dsa_key\\n\\n¬ 601.078704| cloud-init¬1288|: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_dsa_key.pub\\n\\n¬ 601.078736| cloud-init¬1288|: The key fingerprint is:\\n\\n¬ 601.078767| cloud-init¬1288|: SHA256:ZBNyksVVYZVhKJeL+PWKpsdUcn21yiceX/DboXQd\\n\\nPq0 root§ubuntu-server\\n\\n¬ 601.078800| cloud-init¬1288|: The key's randomart image is:\\n\\n¬ 601.078835| cloud-init¬1288|: +---¬DSA 1024|----+\\n\\n¬ 601.078867| cloud-init¬1288|: ! o++...+=+o !\\n\\n¬ 601.078899| cloud-init¬1288|: ! .+....+.. .!\\n\\n¬ 601.078932| cloud-init¬1288|: ! +. + o o!\\n\\n¬ 601.078964| cloud-init¬1288|: ! o..o = oo.!\\n\\n¬ 601.078996| cloud-init¬1288|: ! S. =..o++!\\n\\n¬ 601.079029| cloud-init¬1288|: ! o *.*=!\\n\\n¬ 601.079061| cloud-init¬1288|: ! o .o.B.*!\\n\\n¬ 601.079094| cloud-init¬1288|: ! = .oEo.!\\n\\n¬ 601.079135| cloud-init¬1288|: ! .+ !\\n\\n¬ 601.079167| cloud-init¬1288|: +----¬SHA256|-----+\\n\\n¬ 601.079199| cloud-init¬1288|: Generating public/private ecdsa key pair.\\n\\n¬ 601.079231| cloud-init¬1288|: Your identification has been saved in /etc/ssh/\\n\\nssh_host_ecdsa_key\\n\\n¬ 601.079263| cloud-init¬1288|: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_ecdsa_key.pub\\n\\n¬ 601.079295| cloud-init¬1288|: The key fingerprint is:\\n\\n¬ 601.079327| cloud-init¬1288|: SHA256:Bitar9fVHUH2FnYVSJJnldprdAcM5Est0dmRWFTU\\n\\ni8k root§ubuntu-server\\n\\n¬ 601.079362| cloud-init¬1288|: The key's randomart image is:\\n\\n¬ 601.079394| cloud-init¬1288|: +---¬ECDSA 256|---+\\n\\n¬ 601.079426| cloud-init¬1288|: ! o**O%&!\",\n",
       " \"¬ 601.079458| cloud-init¬1288|: ! o.OB+=!\\n\\n¬ 601.079491| cloud-init¬1288|: ! . B *o+!\\n\\n¬ 601.079525| cloud-init¬1288|: ! o . E.=o!\\n\\n¬ 601.079557| cloud-init¬1288|: ! o . S .....+!\\n\\n¬ 601.079589| cloud-init¬1288|: ! o o . . . .o !\\n\\n¬ 601.079621| cloud-init¬1288|: ! . .. . . !\\n\\n¬ 601.079653| cloud-init¬1288|: ! .. . !\\n\\n47\\n\\n\\n-----\\n\\n¬ 601.079685| cloud-init¬1288|: ! .. !\\n\\n¬ 601.079717| cloud-init¬1288|: +----¬SHA256|-----+\\n\\n¬ 601.079748| cloud-init¬1288|: Generating public/private ed25519 key pair.\\n\\n¬ 601.079782| cloud-init¬1288|: Your identification has been saved in /etc/ssh/\\n\\nssh_host_ed25519_key\\n\\n¬ 601.079814| cloud-init¬1288|: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_ed25519_key.pub\\n\\n¬ 601.079847| cloud-init¬1288|: The key fingerprint is:\\n\\n¬ 601.079879| cloud-init¬1288|: SHA256:yWsZ/5+7u7D3SIcd7HYnyajXyeWnt5nQ+ZI3So3b\\n\\neN8 root§ubuntu-server\\n\\n¬ 601.079911| cloud-init¬1288|: The key's randomart image is:\\n\\n¬ 601.079942| cloud-init¬1288|: +--¬ED25519 256|--+\\n\\n¬ 601.079974| cloud-init¬1288|: ! !\\n\\n¬ 601.080010| cloud-init¬1288|: ! !\\n\\n¬ 601.080042| cloud-init¬1288|: ! !\\n\\n¬ 601.080076| cloud-init¬1288|: ! . . . !\\n\\n¬ 601.080107| cloud-init¬1288|: ! S o !\\n\\n¬ 601.080139| cloud-init¬1288|: ! = o=++!\\n\\n¬ 601.080179| cloud-init¬1288|: ! + . o**§=!\\n\\n¬ 601.080210| cloud-init¬1288|: ! . oo+&B%!\\n\\n¬ 601.080244| cloud-init¬1288|: ! ..o*%/E!\\n\\n¬ 601.080289| cloud-init¬1288|: +----¬SHA256|-----+\\n\\n¬ 612.293731| cloud-init¬2027|: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 runnin\\n\\ng 'modules:config' at Thu, 04 Jun 2020 12:06:59 +0000. Up 612.11 seconds.\\n\\n¬ 612.293866| cloud-init¬2027|: Set the following 'random' passwords\\n\\n¬ 612.293940| cloud-init¬2027|: installer:wgYsAPzYQbFYqU2X2hYm\",\n",
       " 'ci-info: no authorized SSH keys fingerprints found for user installer.\\n\\n<14>Jun 4 12:07:00 ec2:\\n\\n<14>Jun 4 12:07:00 ec2: #######################################################\\n\\n######\\n\\n<14>Jun 4 12:07:00 ec2: -----BEGIN SSH HOST KEY FINGERPRINTS----\\n<14>Jun 4 12:07:00 ec2: 1024 SHA256:ZBNyksVVYZVhKJeL+PWKpsdUcn21yiceX/DboXQdPq0\\n\\nroot§ubuntu-server (DSA)\\n\\n<14>Jun 4 12:07:00 ec2: 256 SHA256:Bitar9fVHUH2FnYVSJJnldprdAcM5Est0dmRWFTUi8k\\n\\nroot§ubuntu-server (ECDSA)\\n\\n<14>Jun 4 12:07:00 ec2: 256 SHA256:yWsZ/5+7u7D3SIcd7HYnyajXyeWnt5nQ+ZI3So3beN8\\n\\nroot§ubuntu-server (ED25519)\\n\\n<14>Jun 4 12:07:00 ec2: 3072 SHA256:kHtkABZwk8AE80fy0KPzTRcYpht4iXdZmJ37Cgi3fJ0\\n\\nroot§ubuntu-server (RSA)\\n\\n<14>Jun 4 12:07:00 ec2: -----END SSH HOST KEY FINGERPRINTS----\\n<14>Jun 4 12:07:00 ec2: #######################################################\\n\\n######\\n\\n-----BEGIN SSH HOST KEY KEYS----\\necdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBIXM6t1/\\n\\n35ot/aPI59ThIJBzg+qGJJ17+1ZVHfzMEDbsTwpM7e9pstPZUM7W1IHWqDvLQDBm/hGg4u8ZGEqmIMI=\\n\\nroot§ubuntu-server\\n\\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIN7QtU+en+RGruj2zuxWgkMqLmh+35/GR/OEOD16k4nA\\n\\nroot§ubuntu-server\\n\\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDJdKT7iUAvSjkUqI1l3fHysE+Gj7ulwGgGjYh639px\\n\\nkcHEbbS3V48eROY9BmDISEHfjYXGY2wEH0tGJjNRROGJhZJVNR+qAqJBioj9d/TwXEgwLP8eAy9aVtJB\\n\\nK1rIylnMQltx/SIhgiymjHLCtKlVoIS4l0frT9FiF54Qi/JeJlwGJIW3W2XgcY9ODT0Q5g3PSmlZ8KTR\\n\\nimTf9Fy7WJEPA08b3fimYWsz9enuS/gECEUGV3M1MvrzpAQju27NUEOpSMZHR62IMxGvIjYIu3dUkAzm\\n\\nMBdwxHdLMQ8rI8PehyHDiFr6g2Ifxoy5QLmb3hISKlq/R6pLLeXbb748gN2i8WCvK0AEGfa/kJDW3RNU\\n\\nVYd+ACBBzyhVbiw7W1CQW/ohik3wyosUyi9nJq2IqOA7kkGH+1XoYq/e4/MoqxhIK/oaiudYAkaCWmP1',\n",
       " 'r/fBa3hlf0f7mVHvxA3tWZc2wYUxFPTmePvpydP2PSctHMhgboaHrGIY2CdSqg8SUdPKrOE= root§ub\\n\\nuntu-server\\n\\n-----END SSH HOST KEY KEYS----\\n¬ 612.877357| cloud-init¬2045|: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 runnin\\n\\ng \\'modules:final\\' at Thu, 04 Jun 2020 12:07:00 +0000. Up 612.79 seconds.\\n\\n¬ 612.877426| cloud-init¬2045|: ci-info: no authorized SSH keys fingerprints fo\\n\\nund for user installer.\\n\\n¬ 612.877468| cloud-init¬2045|: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 finish\\n\\ned at Thu, 04 Jun 2020 12:07:00 +0000. Datasource DataSourceNoCloud ¬seed=/var/l\\n\\n48\\n\\n\\n-----\\n\\nib/cloud/seed/nocloud|¬dsmode=net|. Up 612.87 seconds\\n\\n¬\\n612.877509| cloud-init¬2045|: Welcome to Ubuntu Server InstallerÜ\\n\\n¬ 612.877551| cloud-init¬2045|: Above you will find SSH host keys and a random\\n\\npassword set for the `installer` user. You can use these credentials to ssh-in a\\n\\nnd complete the installation. If you provided SSH keys in the cloud-init datasou\\n\\nrce, they were also provisioned to the installer user.\\n\\n¬ 612.877634| cloud-init¬2045|: If you have access to the graphical console, li\\n\\nke TTY1 or HMC ASCII terminal you can complete the installation there too.\\n\\nIt is possible to connect to the installer over the network, which\\n\\nmight allow the use of a more capable terminal.\\n\\nTo connect, SSH to installer§10.222.111.24.\\n\\nThe password you should use is \"KRuXtz5dURAyPkcjcUvA\".\\n\\nThe host key fingerprints are:\\n\\nRSA SHA256:3IvYMkU05lQSKBxOVZUJMzdtXpz3RJl3dEQsg3UWc54\\n\\nECDSA SHA256:xd1xnkBpn49DUbuP8uWro2mu1GM4MtnqR2WEWg1fS3o\\n\\nED25519 SHA256:Hk3+/4+X7NJBHl6/e/6xFhNXsbHBsOvt6i8YEFUepko\\n\\nUbuntu Focal Fossa (development branch) ubuntu-server ttyS0',\n",
       " '  - The next step is to remotely connect to the install system and to proceed with the Subiquity installer.\\n\\n  - Please notice that at the end of the installer boot-up process, all necessary data is provided to proceed with\\nrunning the installer in a remote SSH shell. The command to execute locally is:\\n\\nuser@workstation:~$ ssh installer@10.222.111.24\\n\\n  - A temporary random password for the installation was created and shared as well, which should be used without\\nthe leading and trailing double quotes:\\n\\n\"KRuXtz5dURAyPkcjcUvA\"\\n\\nuser@workstation:~$ ssh installer@10.222.111.24\\n\\nThe authenticity of host \\'10.222.111.24 (10.222.111.24)\\' can\\'t be established.\\n\\nECDSA key fingerprint is\\n\\nSHA256:xd1xnkBpn49DUbuP8uWro2mu1GM4MtnqR2WEWg1fS3o.\\n\\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\\n\\nWarning: Permanently added \\'10.222.111.24\\' (ECDSA) to the list of known hosts.\\n\\ninstaller@10.222.111.24\\'s password: KRuXtz5dURAyPkcjcUvA\\n\\n  - One may now temporarily see some login messages like these:\\n\\nWelcome to Ubuntu Focal Fossa (development branch) (GNU/Linux 5.4.0-42-generic s390x)\\n\\n    - Documentation: https://help.ubuntu.com\\n\\n    - Management: https://landscape.canonical.com\\n\\n    - Support: https://ubuntu.com/pro\\n\\nSystem information as of Wed Jun 3 17:32:10 UTC 2020\\n\\nSystem load: 0.0 Memory usage: 2% Processes: 146\\n\\nUsage of /home: unknown Swap usage: 0% Users logged in: 0\\n\\n0 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\nindividual files in /usr/share/doc/*/copyright.',\n",
       " 'Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.\\n\\n  - Eventually, the initial Subiquity installer screen appears:\\n\\n49\\n\\n\\n-----\\n\\n====================================================================\\n\\nWillkommen! Bienvenue! Welcome! ????? ??????????! Welkom!\\n\\n====================================================================\\n\\nUse UP, DOWN and ENTER keys to select your language.\\n\\n[ English         - ]\\n\\n[ Asturianu         - ]\\n\\n[ Cataln         - ]\\n\\n[ Hrvatski         - ]\\n\\n[ Nederlands         - ]\\n\\n[ Suomi         - ]\\n\\n[ Francais         - ]\\n\\n[ Deutsch         - ]\\n\\n[ Magyar         - ]\\n\\n[ Latvie?u         - ]\\n\\n[ Norsk bokm?l         - ]\\n\\n[ Polski         - ]\\n\\n[ Espanol         - ]\\n\\n  - From here just proceed with the installation as usual …\\n(I’m leaving some pretty standard screenshots here just to give an example for a basic installation …)\\n\\n====================================================================\\n\\nKeyboard configuration ==================================================================\\n\\nPlease select your keyboard layout below, or select \"Identify keyboard\" to\\n\\ndetect your layout automatically.\\n\\nLayout: [ English (US) v ]\\n\\nVariant: [ English (US) v ]\\n\\n[ Identify keyboard ]\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ngeneric-ccw │\\n\\n0.0.0009  - │\\n\\n0.0.000c  - │\\n\\n0.0.000d  - │\\n\\n0.0.000e  - │\\n\\n│\\n\\ndasd-eckd │\\n\\n0.0.0190  - │\\n\\n0.0.0191  - │\\n\\n0.0.019d  - │\\n\\n0.0.019e  \\n0.0.0200  \\n0.0.0300  \\n0.0.0400  \\n50\\n\\n\\n-----\\n\\n0.0.0592  - v\\n\\n[ Continue ]\\n\\n[ Back ]',\n",
       " '  - If the list is long, hit the End key that will automatically scroll you down to the bottom of the Z devices list and\\n\\nscreen.\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ngeneric-ccw │\\n\\n0.0.0009  - │\\n\\n0.0.000c  - │\\n\\n0.0.000d  - │\\n\\n0.0.000e  - │\\n\\n│\\n\\ndasd-eckd │\\n\\n0.0.0190  - │\\n\\n0.0.0191  - │\\n\\n0.0.019d  - │\\n\\n0.0.019e >┌────────────┐\\n\\n0.0.0200 >│< (close) │\\n\\n0.0.0300 >│ Enable │\\n\\n0.0.0400 >│ Disable │\\n\\n0.0.0592 >└────────────┘ v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ngeneric-ccw │\\n\\n0.0.0009  - │\\n\\n0.0.000c  - │\\n\\n0.0.000d  - │\\n\\n0.0.000e  - │\\n\\n│\\n\\ndasd-eckd │\\n\\n0.0.0190  - │\\n\\n0.0.0191  - │\\n\\n0.0.019d  - │\\n\\n0.0.019e  \\n0.0.0200 online dasda  \\n0.0.0300  \\n0.0.0400  \\n0.0.0592  - v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\n^\\n\\ndasd-eckd\\n\\n0.0.0190  \\n0.0.0191  \\n0.0.019d  \\n0.0.019e  - │\\n\\n51\\n\\n\\n-----\\n\\n0.0.0200 online dasda  - │\\n\\n0.0.0300  - │\\n\\n0.0.0400  - │\\n\\n0.0.0592  - │\\n\\n│\\n\\nqeth │\\n\\n0.0.0600:0.0.0601:0.0.0602 enc600  - │\\n\\n0.0.0603:0.0.0604:0.0.0605  - │\\n\\n│\\n\\ndasd-eckd │\\n\\n0.0.1607  - v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nNetwork connections\\n\\n====================================================================\\n\\nConfigure at least one interface this server can use to talk to other',\n",
       " 'machines, and which preferably provides sufficient access for updates.\\n\\nNAME TYPE NOTES\\n\\n[ enc600 eth  -  - ]\\n\\nstatic 10.222.111.24/24\\n\\n02:28:0a:00:00:39 / Unknown Vendor / Unknown Model\\n\\n[ Create bond > ]\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nConfigure proxy\\n\\n====================================================================\\n\\nIf this system requires a proxy to connect to the internet, enter its\\n\\ndetails here.\\n\\nProxy address:\\n\\nIf you need to use a HTTP proxy to access the outside world,\\n\\nenter the proxy information here. Otherwise, leave this\\n\\nblank.\\n\\nThe proxy information should be given in the standard form\\n\\nof \"http://[[user][:pass]@]host[:port]/\".\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\n52\\n\\n\\n-----\\n\\nConfigure Ubuntu archive mirror\\n\\n====================================================================\\n\\nIf you use an alternative mirror for Ubuntu, enter its details here.\\n\\nMirror address: http://ports.ubuntu.com/ubuntu-ports\\n\\nYou may provide an archive mirror that will be used instead\\n\\nof the default.\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nGuided storage configuration\\n\\n====================================================================\\n\\nConfigure a guided storage layout, or create a custom one:\\n\\n(X) Use an entire disk\\n\\n[ 0X0200 local disk 6.876G v ]\\n\\n[ ] Set up this disk as an LVM group\\n\\n[ ] Encrypt the LVM group with LUKS\\n\\nPassphrase:\\n\\nConfirm passphrase:\\n\\n( ) Custom storage layout\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nStorage configuration',\n",
       " \"====================================================================\\n\\nFILE SYSTEM SUMMARY ^\\n\\n│\\n\\nMOUNT POINT SIZE TYPE DEVICE TYPE │\\n\\n[ / 6.875G new ext4 new partition of local disk > ] │\\n\\n│\\n\\n│\\n\\nAVAILABLE DEVICES │\\n\\n│\\n\\nNo available devices │\\n\\n│\\n\\n[ Create software RAID (md) > ] │\\n\\n[ Create volume group (LVM) > ] │\\n\\n53\\n\\n\\n-----\\n\\nUSED DEVICES\\n\\nv\\n\\n[ Done ]\\n\\n[ Reset ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nStorage configuration\\n\\n====================================================================\\n\\nFILE SYSTEM SUMMARY ^\\n\\n│\\n\\n┌──────────────────────Confirm destructive action ──────────────────────┐\\n\\n│ │\\n\\n│ Selecting Continue below will begin the installation process and │\\n\\n│ result in the loss of data on the disks selected to be formatted. │\\n\\n│ │\\n\\n│ You will not be able to return to this or a previous screen once the │\\n\\n│ installation has started. │\\n\\n│ │\\n\\n│ Are you sure you want to continue? │\\n\\n│ │\\n\\n│ [ No ] │\\n\\n│ [ Continue ] │\\n\\n│ │\\n\\n└────────────────────────────────────────────────────────────────────────┘\\n\\n[ Reset ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nProfile setup\\n\\n====================================================================\\n\\nEnter the username and password you will use to log in to the system. You\\n\\ncan configure SSH access on the next screen but a password is still needed\\n\\nfor sudo.\\n\\nYour name: Ed Example\\n\\nYour server's name: 10.222.111.24\\n\\nThe name it uses when it talks to other computers.\\n\\nPick a username: ubuntu\\n\\nChoose a password: ********\\n\\nConfirm your password: ********\\n\\n[ Done ]\\n\\n====================================================================\\n\\nSSH Setup\",\n",
       " '====================================================================\\n\\nYou can choose to install the OpenSSH server package to enable secure remote\\n\\naccess to your server.\\n\\n[ ] Install OpenSSH server\\n\\n54\\n\\n\\n-----\\n\\nImport SSH identity: [ No v ]\\n\\nYou can import your SSH keys from Github or Launchpad.\\n\\nImport Username:\\n\\n[X] Allow password authentication over SSH\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n  - It’s a nice and convenient new feature to add the user’s SSH keys during the installation to the system, since\\nthat makes the system login password-less on the initial login!\\n\\n====================================================================\\n\\nSSH Setup\\n\\n====================================================================\\n\\nYou can choose to install the OpenSSH server package to enable secure remote\\n\\naccess to your server.\\n\\n[X] Install OpenSSH server\\n\\nImport SSH identity: [ from Launchpad v ]\\n\\nYou can import your SSH keys from Github or Launchpad.\\n\\nLaunchpad Username: user\\n\\nEnter your Launchpad username.\\n\\n[X] Allow password authentication over SSH\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nSSH Setup\\n\\n====================================================================\\n\\nYou can choose to install the OpenSSH server package to enable secure remote\\n\\naccess to your server.\\n\\n┌───────────────────────────Confirm SSH keys ───────────────────────────┐\\n\\n│ │\\n\\n│ Keys with the following fingerprints were fetched. Do you want to │\\n\\n│ use them? │\\n\\n│ │\\n\\n│ 2048 SHA256:joGsdfW7NbJRkg17sRyXaegoR0iZEdDWdR9Hpbc2KIw user@W520 │\\n\\n│ (RSA) │\\n\\n│ 521 SHA256:T3JzxvB6K1GzXJpP5NFgX4yXvk0jhhgvbw01F7/fZ2c │\\n\\n│ frank.heimes@canonical.com (ECDSA) │\\n\\n│ │\\n\\n│ [ Yes ] │\\n\\n│ [ No ] │',\n",
       " \"│ │\\n\\n└────────────────────────────────────────────────────────────────────────┘\\n\\n[ Done ]\\n\\n55\\n\\n\\n-----\\n\\n[ Back ]\\n\\n====================================================================\\n\\nFeatured Server Snaps\\n\\n====================================================================\\n\\nThese are popular snaps in server environments. Select or deselect with\\n\\nSPACE, press ENTER to see more details of the package, publisher and\\n\\nversions available.\\n\\n[ ] kata-containers Lightweight virtual machines that seamlessly plug into >\\n\\n[ ] docker Docker container runtime  \\n[ ] mosquitto Eclipse Mosquitto MQTT broker  \\n[ ] etcd Resilient key-value store by CoreOS  \\n[ ] stress-ng A tool to load, stress test and benchmark a computer s >\\n\\n[ ] sabnzbd SABnzbd  \\n[ ] wormhole get things from one computer to another, safely  \\n[ ] slcli Python based SoftLayer API Tool.  \\n[ ] doctl DigitalOcean command line tool  \\n[ ] keepalived High availability VRRP/BFD and load-balancing for Linu >\\n\\n[ ] juju Simple, secure and stable devops. Juju keeps complexit >\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nInstall complete!\\n\\n====================================================================\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n\\n│ configuring raid (mdadm) service ^│\\n\\n│ installing kernel │\\n\\n│ setting up swap │\\n\\n│ apply networking config │\\n\\n│ writing etc/fstab │\\n\\n│ configuring multipath │\\n\\n│ updating packages on target system │\\n\\n│ configuring pollinate user-agent on target │\\n\\n│ updating initramfs configuration │\\n\\n│ finalizing installation │\\n\\n│ running 'curtin hook' │\\n\\n│ curtin command hook │\\n\\n│ executing late commands │\",\n",
       " \"│final system configuration │\\n\\n│ configuring cloud-init ││\\n\\n│ installing openssh-server | v│\\n\\n└──────────────────────────────────────────────────────────────────────────┘\\n\\n[ View full log ]\\n\\n====================================================================\\n\\nInstallation complete!\\n\\n====================================================================\\n\\n┌────────────────────────────Finished install! ───────────────────────────┐\\n\\n│ apply networking config ^│\\n\\n│ writing etc/fstab │\\n\\n│ configuring multipath │\\n\\n│ updating packages on target system │\\n\\n│ configuring pollinate user-agent on target │\\n\\n│ updating initramfs configuration │\\n\\n│ finalizing installation │\\n\\n│ running 'curtin hook' │\\n\\n│ curtin command hook │\\n\\n│ executing late commands │\\n\\n56\\n\\n\\n-----\\n\\n│final system configuration │\\n\\n│ configuring cloud-init │\\n\\n│ installing openssh-server │\\n\\n│ restoring apt configuration ││\\n\\n│downloading and installing security updates v│\\n\\n└──────────────────────────────────────────────────────────────────────────┘\\n\\n[ View full log ]\\n\\n[ Reboot ]\\n\\nInstallation complete!\\n\\n====================================================================\\n\\n┌────────────────────────────Finished install! ───────────────────────────┐\\n\\n│ apply networking config ^│\\n\\n│ writing etc/fstab │\\n\\n│ configuring multipath │\\n\\n│ updating packages on target system │\\n\\n│ configuring pollinate user-agent on target │\\n\\n│ updating initramfs configuration │\\n\\n│ finalizing installation │\\n\\n│ running 'curtin hook' │\\n\\n│ curtin command hook │\\n\\n│ executing late commands │\\n\\n│final system configuration │\\n\\n│ configuring cloud-init │\\n\\n│ installing openssh-server │\\n\\n│ restoring apt configuration ││\\n\\n│downloading and installing security updates v│\",\n",
       " '└──────────────────────────────────────────────────────────────────────────┘\\n\\n[ Connection to 10.222.111.24 closed by remote host. [ Rebooting... ]\\n\\nConnection to 10.222.111.24 closed.\\n\\nuser@workstation:~$\\n\\n - Type reset to clear the screen and to revert it back to the defaults.\\n\\n  - Now remove the old host key, since the system got a new one during the installation:\\n\\nuser@workstation:~$ ssh-keygen -f \"/home/user/.ssh/known_hosts\" -R \"10.222.111.24\"\\n\\n# Host 10.222.111.24 found: line 159\\n\\n/home/user/.ssh/known_hosts updated.\\n\\nOriginal contents retained as /home/user/.ssh/known_hosts.old\\n\\nuser@workstation:~$\\n\\n  - And finally login to the newly installed z/VM guest:\\n\\nuser@workstation:~$ ssh ubuntu@10.222.111.24\\n\\nWarning: Permanently added the ECDSA host key for IP address\\n\\n\\'10.222.111.24\\' to the list of known hosts.\\n\\nWelcome to Ubuntu 20.04.5 LTS (GNU/Linux 5.4.0-42-generic s390x)\\n\\n    - Documentation: https://help.ubuntu.com\\n\\n    - Management: https://landscape.canonical.com\\n\\n    - Support: https://ubuntu.com/pro\\n\\nSystem information as of Wed 03 Jun 2020 05:50:05 PM UTC\\n\\nSystem load: 0.08 Memory usage: 2% Processes: 157\\n\\nUsage of /: 18.7% of 6.70GB Swap usage: 0% Users logged in: 0\\n\\n0 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright.\\n\\n57\\n\\n\\n-----\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.\\n\\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\\n\\nSee \"man sudo_root\" for details.\\n\\nubuntu@10.222.111.24:~$ uptime',\n",
       " '17:50:09 up 1 min, 1 user, load average: 0.08, 0.11, 0.05\\n\\nubuntu@10.222.111.24:~$ lsb_release -a\\n\\nNo LSB modules are available.\\n\\nDistributor ID: Ubuntu\\n\\nDescription: Ubuntu 20.04.5 LTS\\n\\nRelease: 20.04\\n\\nCodename: focal\\n\\nubuntu@10.222.111.24:~$ uname -a\\n\\nLinux 10.222.111.24 5.4.0-42-generic #30-Ubuntu SMP Wed Aug 05 16:57:22 UTC 2020 s390x s390x s390x GNU/Linux\\n\\nubuntu@10.222.111.24:~$ exit\\n\\nlogout\\n\\nConnection to 10.222.111.24 closed.\\n\\nuser@workstation:~$\\n\\nDone !\\n\\nDoing a manual live installation like described here - meaning without specifying a parmfile - is supported since Ubuntu\\nServer LTS 20.04.5 (‘Focal’) and any newer release, like 22.04 (‘Jammy’).\\n\\nThe following guide assumes that an FTP server to host the installation files is in place, which can be used by the\\n‘Load from Removable Media and Server’ task of the Hardware Management Console (HMC).\\n\\n  - Download the ‘focal daily live image’ from here (later 20.04.5 image):\\n\\n[http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso](http://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso)\\n\\n  - Now loop-back mount the ISO to extract four files that are needed for the installation:\\n\\nuser@workstation:~$ mkdir iso\\n\\nuser@workstation:~$ sudo mount -o loop ubuntu-20.04.5-live-server-s390x.iso iso\\n\\nuser@workstation:~$\\n\\nuser@workstation:~$ ls -1 ./iso/boot/{ubuntu.exec,parmfile.*,kernel.u*,initrd.u*}\\n\\n./iso/boot/initrd.ubuntu\\n\\n./iso/boot/kernel.ubuntu\\n\\n./iso/boot/parmfile.ubuntu\\n\\n./iso/boot/ubuntu.exec\\n\\n  - Now make the files available via your FTP server.\\n\\n  - Open the IBM Z HMC and navigate to ‘Systems Management’ on your machine.',\n",
       " '  - Select the LPAR that you are going to install Ubuntu Server on. In this example we use LPAR s1lp11 .\\n\\n  - Now select menu: ‘Recovery’ --> ‘Load from Removable Media or Server’ task.\\n\\n  - Fill out the ‘Load from Removable Media or Server’ form as follows (adapt the settings to your particular\\ninstallation environment):\\n\\nLoad from Removable Media, or Server - <machine>:s1lp11\\n\\nUse this task to load operating system software or utility programs\\n\\nfrom a CD / DVD-ROM or a server that can be accessed using FTP.\\n\\nSelect the source of the software:\\n\\no Hardware Management Console CD / DVD-ROM\\n\\no Hardware Management Console CD / DVD-ROM and assign for operating system use\\n\\no Hardware Management Console USB flash memory drive\\n\\no Hardware Management Console USB flash memory drive and assign for operating system use\\n\\n   - FTP Source\\n\\nHost computer: install-server\\n\\nUser ID: ftpuser\\n\\nPassword: ********\\n\\nAccount (optional):\\n\\nFile location (optional): ubuntu-live-server-20.04.5/boot\\n\\n58\\n\\n\\n-----\\n\\nYou may need to adjust the file’s location according to your install server environment.\\n\\n- Confirm the entered data:\\n\\nLoad from Removable Media or Server - Select Software to Install \\n<machine>:s1lp11\\n\\nSelect the software to install.\\n\\nSelect Name Description\\n\\n  - ubuntu-live-server-20.04.5/boot/ubuntu.ins Ubuntu for IBM Z (default kernel)\\n\\n- Confirm again that jobs might be cancelled if proceeding:\\n\\nLoad from Removable Media or Server Task Confirmation \\n<machine>:s1lp11\\n\\nLoad will cause jobs to be cancelled.\\n\\nDo you want to continue with this task?\\n\\nACT33501\\n\\n- And confirm a last time that it’s understood that the task is disruptive:',\n",
       " 'Disruptive Task Confirmation : Load from Removable Media or Server \\n<machine>:s1lp11\\n\\nAttention: The Load from Removable Media or Server task is disruptive.\\n\\nExecuting the Load from Removable Media or Server task may\\n\\nadversely affect the objects listed below. Review the confirmation text\\n\\nfor each object before continuing with the Load from Removable Media\\n\\nor Server task.\\n\\nObjects that will be affected by the Load from Removable Media or\\n\\nServer task\\n\\nSystem Name Type OS Name Status Confirmation Text\\n\\n<machine>:s1lp11 Image Operating Load from Removable Media\\n\\nor Server causes operations to be disrupted, since the target is\\n\\ncurrently in use and operating normally.\\n\\nDo you want to execute the Load from Removable Media or Server task?\\n\\n- The ‘Load from Removable media or Server’ task is now executed:\\n\\nLoad from Removable media or Server Progress - P00B8F67:S1LPB\\n\\nTurn on context sensitive help.\\n\\nFunction duration time: 00:55:00\\n\\nElapsed time: 00:00:04\\n\\nSelect Object Name Status\\n\\n  - <machine> s1lp11 Please wait while the image is being loaded.\\n\\n- This may take a moment, but you will soon see:\\n\\nLoad from Removable media or Server Progress - <machine>:s1lp11\\n\\nFunction duration time: 00:55:00\\n\\nElapsed time: 00:00:21\\n\\nSelect Object Name Status\\n\\n  - <machine> s1lp11 Success\\n\\n- Close the ‘Load from Removable media or Server’ task and open the console a.k.a. ‘Operating System Messages’\\ninstead.\\nIf no parmfile was configured or provided, one will find the following lines in the ‘Operating System Messages’\\ntask:\\n\\nOperating System Messages - <machine>:s1lp11\\n\\nMessage\\n\\nUnable to find a medium container a live file system\\n\\nAttempt interactive netboot from a URL?',\n",
       " \"yes no (default yes):\\n\\n59\\n\\n\\n-----\\n\\n  - By default, one will now see the interactive network configuration menu (again, only if no parmfile was prepared\\nwith sufficient network configuration information).\\n\\n  - Proceed with the interactive network configuration – in this case in a VLAN environment:\\n\\nUnable to find a medium container a live file system\\n\\nAttempt interactive netboot from a URL?\\n\\nyes no (default yes):\\n\\nyes\\n\\nAvailable qeth devices:\\n\\n0.0.c000 0.0.c003 0.0.c006 0.0.c009 0.0.c00c 0.0.c00f\\n\\nzdev to activate (comma separated, optional):\\n\\n0.0.c000\\n\\nQETH device 0.0.c000:0.0.c001:0.0.c002 configured\\n\\nTwo methods available for IP configuration:\\n\\n   - static: for static IP configuration\\n\\n   - dhcp: for automatic IP configuration\\n\\nstatic dhcp (default 'dhcp'):\\n\\nstatic\\n\\nip:\\n\\n10.222.111.11\\n\\ngateway (default 10.222.111.1):\\n\\n10.222.111.1\\n\\ndns (default 10.222.111.1):\\n\\n10.222.111.1\\n\\nvlan id (optional):\\n\\n1234\\n\\nhttp://cdimage.ubuntu.com/ubuntu/releases/20.04.5/release/ubuntu-20.04.5-live-server-s390x.iso (default)\\n\\nurl:\\n\\nftp://10.11.12.2:21/ubuntu-live-server-20.04.5/ubuntu-20.04.5-live-server-s390x.iso\\n\\nhttp_proxy (optional):\\n\\n  - After the last interactive step here (that this is about an optional proxy configuration), the installer will complete\\nits boot-up process:\\n\\nConfiguring networking...\\n\\nIP-Config: encc000.1234 hardware address 3e:00:10:55:00:ff mtu 1500\\n\\nIP-Config: encc000.1234 guessed broadcast address 10.222.111.255\\n\\nIP-Config: encc000.1234 complete:\\n\\naddress: 10.222.111.11 broadcast: 10.222.111.255 netmask: 255.255.255.0\\n\\ngateway: 10.222.111.1 dns0 : 10.222.111.1 dns1 : 0.0.0.0\\n\\nrootserver: 0.0.0.0 rootpath:\\n\\nfilename :\\n\\nConnecting to 10.11.12.2:21 (10.11.12.2:21)\",\n",
       " \"focal-live-server-s 10% |*** | 72.9M 0:00:08 ETA\\n\\nfocal-live-server-s 25% |******** | 168M 0:00:05 ETA\\n\\nfocal-live-server-s 42% |************* | 279M 0:00:04 ETA\\n\\nfocal-live-server-s 58% |****************** | 390M 0:00:02 ETA\\n\\nfocal-live-server-s 75% |************************ | 501M 0:00:01 ETA\\n\\nfocal-live-server-s 89% |**************************** | 595M 0:00:00 ETA\\n\\nfocal-live-server-s 99% |******************************* | 662M 0:00:00 ETA\\n\\nfocal-live-server-s 100% |********************************| 663M 0:00:00 ETA\\n\\nip: RTNETLINK answers: File exists\\n\\nno search or nameservers found in /run/net-encc000.1234.conf / run/net-*.conf /run/net6-*.conf\\n\\n[ 399.808930] /dev/loop3: Can't open blockdev\\n\\n[[0;1;31m SKIP [0m] Ordering cycle found, skipping [0;1;39mLogin Prompts[0m\\n\\n[ 401.547705] systemd[1]: multi-user.target: Job getty.target/start deleted to\\n\\nbreak ordering cycle starting with multi-user.target/start\\n\\n[ 406.241972] cloud-init[1321]: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 running\\n\\n'init-local' at Wed, 03 Jun 2020 17:07:39 +0000. Up 406.00 seconds.\\n\\n[ 407.025557] cloud-init[1348]: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 running\\n\\n'init' at Wed, 03 Jun 2020 17:07:40 +0000. Up 406.87 seconds.\\n\\n[ 407.025618] cloud-init[1348]: ci-info: ++++Net device info++++\\n\\n60\\n\\n\\n-----\\n\\n[ 407.025658] cloud-init[1348]: ci-info: +--------------+------+--------------\\n---------------+---------------+--------+-------------------+\\n\\n[ 407.025696] cloud-init[1348]: ci-info: | Device | Up | Addr\\n\\ness | Mask | Scope | Hw-Address |\\n\\n[ 407.025731] cloud-init[1348]: ci-info: +--------------+------+--------------\\n---------------+---------------+--------+-------------------+\",\n",
       " '[ 407.025766] cloud-init[1348]: ci-info: | encc000 | True | fe80::3ca7:10f\\n\\nf:fea5:c69e/64 | . | link | 72:5d:0d:09:ea:76 |\\n\\n[ 407.025802] cloud-init[1348]: ci-info: | encc000.1234 | True | 10.245.\\n\\n236.11 | 255.255.255.0 | global | 72:5d:0d:09:ea:76 |\\n\\n[ 407.025837] cloud-init[1348]: ci-info: | encc000.1234 | True | fe80::3ca7:10f\\n\\nf:fea5:c69e/64 | . | link | 72:5d:0d:09:ea:76 |\\n\\n[ 407.025874] cloud-init[1348]: ci-info: | lo | True | 127.0\\n\\n.0.1 | 255.0.0.0 | host | . |\\n\\n[ 407.025909] cloud-init[1348]: ci-info: | lo | True | ::1/\\n\\n128 | . | host | . |\\n\\n[ 407.025944] cloud-init[1348]: ci-info: +--------------+------+--------------\\n---------------+---------------+--------+-------------------+\\n\\n[ 407.025982] cloud-init[1348]: ci-info: +++++++++++++Route I\\n\\nPv4 info++++++++++++++\\n\\n[ 407.026017] cloud-init[1348]: ci-info: +-------+--------------+-------------\\n+---------------+--------------+-------+\\n\\n[ 407.026072] cloud-init[1348]: ci-info: | Route | Destination | Gateway\\n\\n| Genmask | Interface | Flags |\\n\\n[ 407.026107] cloud-init[1348]: ci-info: +-------+--------------+-------------\\n+---------------+--------------+-------+\\n\\n[ 407.026141] cloud-init[1348]: ci-info: | 0 | 0.0.0.0 | 10.222.111.1\\n\\n| 0.0.0.0 | encc000.1234 | UG |\\n\\n[ 407.026176] cloud-init[1348]: ci-info: | 1 | 10.222.111.0 | 0.0.0.0\\n\\n| 255.255.255.0 | encc000.1234 | U |\\n\\n[ 407.026212] cloud-init[1348]: ci-info: +-------+--------------+-------------\\n+---------------+--------------+-------+\\n\\n[ 407.026246] cloud-init[1348]: ci-info: ++++++++++++++++++++Route IPv6 info+++\\n\\n++++++++++++++++++\\n\\n[ 407.026280] cloud-init[1348]: ci-info: +-------+-------------+---------+----\\n---------+-------+',\n",
       " \"[ 407.026315] cloud-init[1348]: ci-info: | Route | Destination | Gateway | Int\\n\\nerface | Flags |\\n\\n[ 407.026355] cloud-init[1348]: ci-info: +-------+-------------+---------+----\\n---------+-------+\\n\\n[ 407.026390] cloud-init[1348]: ci-info: | 1 | fe80::/64 | :: | en\\n\\ncc000 | U |\\n\\n[ 407.026424] cloud-init[1348]: ci-info: | 2 | fe80::/64 | :: | encc\\n\\n000.1234 | U |\\n\\n[ 407.026458] cloud-init[1348]: ci-info: | 4 | local | :: | en\\n\\ncc000 | U |\\n\\n[ 407.026495] cloud-init[1348]: ci-info: | 5 | local | :: | encc\\n\\n000.1234 | U |\\n\\n[ 407.026531] cloud-init[1348]: ci-info: | 6 | ff00::/8 | :: | en\\n\\ncc000 | U |\\n\\n[ 407.026566] cloud-init[1348]: ci-info: | 7 | ff00::/8 | :: | encc\\n\\n000.1234 | U |\\n\\n[ 407.026600] cloud-init[1348]: ci-info: +-------+-------------+---------+----\\n---------+-------+\\n\\n[ 407.883058] cloud-init[1348]: Generating public/private rsa key pair.\\n\\n[ 407.883117] cloud-init[1348]: Your identification has been saved in /etc/ssh/\\n\\nssh_host_rsa_key\\n\\n[ 407.883154] cloud-init[1348]: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_rsa_key.pub\\n\\n[ 407.883190] cloud-init[1348]: The key fingerprint is:\\n\\n[ 407.883232] cloud-init[1348]: SHA256:KX5cHC4YL9dXpvhnP6eSfS+J/zmKgg9zdlEzaEb+\\n\\nRTA root@ubuntu-server\\n\\n[ 407.883267] cloud-init[1348]: The key's randomart image is:\\n\\n61\\n\\n\\n-----\\n\\n[ 407.883302] cloud-init[1348]: +---[RSA 3072]----+\\n\\n[ 407.883338] cloud-init[1348]: | . E.. |\\n\\n[ 407.883374] cloud-init[1348]: | o . o |\\n\\n[ 407.883408] cloud-init[1348]: | . .= +o. |\\n\\n[ 407.883443] cloud-init[1348]: | + =ooo++ |\\n\\n[ 407.883478] cloud-init[1348]: | + S *.o. |\\n\\n[ 407.883512] cloud-init[1348]: | . = o o. |\\n\\n[ 407.883546] cloud-init[1348]: | .o+o ..+o. |\",\n",
       " \"[ 407.883579] cloud-init[1348]: | o=.. =o+++|\\n\\n[ 407.883613] cloud-init[1348]: | .... ++*O|\\n\\n[ 407.883648] cloud-init[1348]: +----[SHA256]-----+\\n\\n[ 407.883682] cloud-init[1348]: Generating public/private dsa key pair.\\n\\n[ 407.883716] cloud-init[1348]: Your identification has been saved in /etc/ssh/\\n\\nssh_host_dsa_key\\n\\n[ 407.883750] cloud-init[1348]: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_dsa_key.pub\\n\\n[ 407.883784] cloud-init[1348]: The key fingerprint is:\\n\\n[ 407.883817] cloud-init[1348]: SHA256:xu3vlG1BReKDy3DsuMZc/lg5y/+nhzlEmLDk/qFZ\\n\\nAm0 root@ubuntu-server\\n\\n[ 407.883851] cloud-init[1348]: The key's randomart image is:\\n\\n[ 407.883905] cloud-init[1348]: +---[DSA 1024]----+\\n\\n[ 407.883941] cloud-init[1348]: | ..o|\\n\\n[ 407.883975] cloud-init[1348]: | o. o o |\\n\\n[ 407.884008] cloud-init[1348]: | +.o+o+ |\\n\\n[ 407.884042] cloud-init[1348]: | ...E*oo.. |\\n\\n[ 407.884076] cloud-init[1348]: | S+o =.. |\\n\\n[ 407.884112] cloud-init[1348]: | . +o+oo.o |\\n\\n[ 407.884145] cloud-init[1348]: | **+o*o |\\n\\n[ 407.884179] cloud-init[1348]: | .oo.*+oo|\\n\\n[ 407.884212] cloud-init[1348]: | .+ ===|\\n\\n[ 407.884246] cloud-init[1348]: +----[SHA256]-----+\\n\\n[ 407.884280] cloud-init[1348]: Generating public/private ecdsa key pair.\\n\\n[ 407.884315] cloud-init[1348]: Your identification has been saved in /etc/ssh/\\n\\nssh_host_ecdsa_key\\n\\n[ 407.884352] cloud-init[1348]: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_ecdsa_key.pub\\n\\n[ 407.884388] cloud-init[1348]: The key fingerprint is:\\n\\n[ 407.884422] cloud-init[1348]: SHA256:P+hBF3fj/pu6+0KaywUYii3Lyuc09Za9/a2elCDO\\n\\ngdE root@ubuntu-server\\n\\n[ 407.884456] cloud-init[1348]: The key's randomart image is:\\n\\n[ 407.884490] cloud-init[1348]: +---[ECDSA 256]---+\",\n",
       " \"[ 407.884524] cloud-init[1348]: | |\\n\\n[ 407.884558] cloud-init[1348]: | . |\\n\\n[ 407.884591] cloud-init[1348]: | ..E . o |\\n\\n[ 407.884625] cloud-init[1348]: | o .ooo o . |\\n\\n[ 407.884660] cloud-init[1348]: | o +S.+.. . |\\n\\n[ 407.884694] cloud-init[1348]: | . +..*oo.+ . |\\n\\n[ 407.884728] cloud-init[1348]: | = o+=.+.+ |\\n\\n[ 407.884762] cloud-init[1348]: | . o......++o oo|\\n\\n[ 407.884795] cloud-init[1348]: | oo. . +.*@*+|\\n\\n[ 407.884829] cloud-init[1348]: +----[SHA256]-----+\\n\\n[ 407.884862] cloud-init[1348]: Generating public/private ed25519 key pair.\\n\\n[ 407.884896] cloud-init[1348]: Your identification has been saved in /etc/ssh/\\n\\nssh_host_ed25519_key\\n\\n[ 407.884930] cloud-init[1348]: Your public key has been saved in /etc/ssh/ssh_\\n\\nhost_ed25519_key.pub\\n\\n[ 407.884966] cloud-init[1348]: The key fingerprint is:\\n\\n[ 407.884999] cloud-init[1348]: SHA256:CbZpkR9eFHuB1sCDZwSdSdwJzy9FpsIWRIyc9ers\\n\\nhZ0 root@ubuntu-server\\n\\n[ 407.885033] cloud-init[1348]: The key's randomart image is:\\n\\n[ 407.885066] cloud-init[1348]: +--[ED25519 256]--+\\n\\n[ 407.885100] cloud-init[1348]: | ../%X..o |\\n\\n[ 407.885133] cloud-init[1348]: | .=o&*+= |\\n\\n62\\n\\n\\n-----\\n\\n[ 407.885167] cloud-init[1348]: | = .+*.* . |\\n\\n[ 407.885200] cloud-init[1348]: | . B = + o |\\n\\n[ 407.885238] cloud-init[1348]: | + S . . . |\\n\\n[ 407.885274] cloud-init[1348]: | . o o o |\\n\\n[ 407.885308] cloud-init[1348]: | + E |\\n\\n[ 407.885345] cloud-init[1348]: | . . |\\n\\n[ 407.885378] cloud-init[1348]: | . |\\n\\n[ 407.885420] cloud-init[1348]: +----[SHA256]-----+\\n\\n[ 418.521933] cloud-init[2185]: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 runnin\\n\\ng 'modules:config' at Wed, 03 Jun 2020 17:07:52 +0000. Up 418.40 seconds.\",\n",
       " \"[ 418.522012] cloud-init[2185]: Set the following 'random' passwords\\n\\n[ 418.522053] cloud-init[2185]: installer:C7BZrW76s4mJzmpf4eUy\\n\\nci-info: no authorized SSH keys fingerprints found for user installer.\\n\\n<14>Jun 3 17:07:52 ec2:\\n\\n<14>Jun 3 17:07:52 ec2: #######################################################\\n\\n######\\n\\n<14>Jun 3 17:07:52 ec2: -----BEGIN SSH HOST KEY FINGERPRINTS----\\n<14>Jun 3 17:07:52 ec2: 1024 SHA256:xu3vlG1BReKDy3DsuMZc/lg5y/+nhzlEmLDk/qFZAm0\\n\\nroot@ubuntu-server (DSA)\\n\\n<14>Jun 3 17:07:52 ec2: 256 SHA256:P+hBF3fj/pu6+0KaywUYii3Lyuc09Za9/a2elCDOgdE\\n\\nroot@ubuntu-server (ECDSA)\\n\\n<14>Jun 3 17:07:52 ec2: 256 SHA256:CbZpkR9eFHuB1sCDZwSdSdwJzy9FpsIWRIyc9ershZ0\\n\\nroot@ubuntu-server (ED25519)\\n\\n<14>Jun 3 17:07:52 ec2: 3072 SHA256:KX5cHC4YL9dXpvhnP6eSfS+J/zmKgg9zdlEzaEb+RTA\\n\\nroot@ubuntu-server (RSA)\\n\\n<14>Jun 3 17:07:52 ec2: -----END SSH HOST KEY FINGERPRINTS----\\n<14>Jun 3 17:07:52 ec2: #######################################################\\n\\n######\\n\\n-----BEGIN SSH HOST KEY KEYS----\\necdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBC2zp4Fq\\n\\nr1+NJOIEQIISbX+EzeJ6ucXSLi2xEvurgwq8iMYT6yYOXBOPc/XzeFa6vBCDZk3SSSW6Lq83y7VmdRQ=\\n\\nroot@ubuntu-server\\n\\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJFzgips94nJNoR4QumiyqlJoSlZ48P+NVrd7zgD5k4T\\n\\nroot@ubuntu-server\\n\\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQChKo06O715FAjd6ImK7qZbWnL/cpgQ2A2gQEqFNO+1\\n\\njoF/41ygxuw5aG0IQObWFpV9jDsMF5z4qHKzX8tFCpKC0s4uR8QBxh1dDm4wcwcgtAfVLqh7S4/R9Sqa\\n\\nIFnkCzxThhNeMarcRrutY0mIzspmCg/QvfE1wrXJzl+RtOJ7GiuHHqpm76fX+6ZF1BYhkA87dXQiID2R\\n\\nyUubSXKGg0NtzlgSzPqD3GB+HxRHHHLT5/Xq+njPq8jIUpqSoHtkBupsyVmcD9gDbz6vng2PuBHwZP9X\\n\\n17QtyOwxddxk4xIXaTup4g8bH1oF/czsWqVxNdfB7XqzROFUOD9rMIB+DwBihsmH1kRik4wwLi6IH4hu\",\n",
       " 'xrykKvfb1xcZe65kR42oDI7JbBwxvxGrOKx8DrEXnBpOWozS0IDm2ZPh3ci/0uCJ4LTItByyCfAe/gyR\\n\\n5si4SkmXrIXf5BnErZRgyJnfxKXmsFaSh7wf15w6GmsgzyD9sI2jES9+4By32ZzYOlDpi0s= root@ub\\n\\nuntu-server\\n\\n-----END SSH HOST KEY KEYS----\\n[ 418.872320] cloud-init[2203]: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 runnin\\n\\ng \\'modules:final\\' at Wed, 03 Jun 2020 17:07:52 +0000. Up 418.79 seconds.\\n\\n[ 418.872385] cloud-init[2203]: ci-info: no authorized SSH keys fingerprints fo\\n\\nund for user installer.\\n\\n[ 418.872433] cloud-init[2203]: Cloud-init v. 20.2-45-g5f7825e2-0ubuntu1 finish\\n\\ned at Wed, 03 Jun 2020 17:07:52 +0000. Datasource DataSourceNoCloud [seed=/var/l\\n\\nib/cloud/seed/nocloud][dsmode=net]. Up 418.86 seconds\\n\\n[ 418.872484] cloud-init[2203]: Welcome to Ubuntu Server Installer!\\n\\n[ 418.872529] cloud-init[2203]: Above you will find SSH host keys and a random\\n\\npassword set for the `installer` user. You can use these credentials to ssh-in a\\n\\nnd complete the installation. If you provided SSH keys in the cloud-init datasou\\n\\nrce, they were also provisioned to the installer user.\\n\\n[ 418.872578] cloud-init[2203]: If you have access to the graphical console, li\\n\\nke TTY1 or HMC ASCII terminal you can complete the installation there too.\\n\\nIt is possible to connect to the installer over the network, which\\n\\nmight allow the use of a more capable terminal.\\n\\nTo connect, SSH to installer@10.222.111.11.\\n\\n63\\n\\n\\n-----\\n\\nThe password you should use is \"C7BZrW76s4mJzmpf4eUy\".\\n\\nThe host key fingerprints are:\\n\\nRSA SHA256:KX5cHC4YL9dXpvhnP6eSfS+J/zmKgg9zdlEzaEb+RTA\\n\\nECDSA SHA256:P+hBF3fj/pu6+0KaywUYii3Lyuc09Za9/a2elCDOgdE\\n\\nED25519 SHA256:CbZpkR9eFHuB1sCDZwSdSdwJzy9FpsIWRIyc9ershZ0',\n",
       " 'Ubuntu Focal Fossa (development branch) ubuntu-server sclp_line0\\n\\nubuntu-server login:\\n\\n  - At this point you can proceed with the regular installation either by using ‘Recovery’ --> ‘Integrated ASCII\\nConsole’ or with a remote SSH session.\\n\\n  - If the ‘Integrated ASCII Console’ was opened (you can hit F3to refresh the task), the initial Subiquity installation\\nscreen is presented, which looks like this:\\n\\n================================================================================\\n\\nWillkommen! Bienvenue! Welcome! ????? ??????????! Welkom! [ Help ]\\n\\n================================================================================\\n\\nUse UP, DOWN and ENTER keys to select your language.\\n\\n[ English         - ]\\n\\n[ Asturianu         - ]\\n\\n[ Cataln         - ]\\n\\n[ Hrvatski         - ]\\n\\n[ Nederlands         - ]\\n\\n[ Suomi         - ]\\n\\n[ Francais         - ]\\n\\n[ Deutsch         - ]\\n\\n[ Magyar         - ]\\n\\n[ Latvie?u         - ]\\n\\n[ Norsk bokm?l         - ]\\n\\n[ Polski         - ]\\n\\n[ Espanol         - ]\\n\\n  - Since the user experience is nicer in a remote SSH session, we recommend using that.\\nHowever, with certain network environments it’s just not possible to use a remote shell, and the ‘Integrated\\nASCII Console’ will be the only option.\\n\\n**Note** :\\n\\nAt the end of the installer boot-up process, **all** necessary information is provided to proceed with a remote\\nshell.\\n\\n  - The command to execute locally is:\\n\\nuser@workstation:~$ ssh installer@10.222.111.11\\n\\n  - A temporary random password for the installation was created and shared as well, which you should use without\\nthe leading and trailing double quotes:\\n\\n\"C7BZrW76s4mJzmpf4eUy\"',\n",
       " \"  - Hence the remote session for the installer can be opened by:\\n\\nuser@workstation:~$ ssh installer@10.222.111.11\\n\\nThe authenticity of host '10.222.111.11 (10.222.111.11)' can't be established.\\n\\nECDSA key fingerprint is SHA256:P+hBF3fj/pu6+0KaywUYii3Lyuc09Za9/a2elCDOgdE.\\n\\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\\n\\nWarning: Permanently added '10.222.111.11' (ECDSA) to the list of known hosts.\\n\\ninstaller@10.222.111.11's password: C7BZrW76s4mJzmpf4eUy\\n\\n  - One may swiftly see some login messages like the following ones:\\n\\nWelcome to Ubuntu Focal Fossa (development branch) (GNU/Linux 5.4.0-42-generic s390x)\\n\\n - Documentation: https://help.ubuntu.com\\n\\n - Management: https://landscape.canonical.com\\n\\n - Support: https://ubuntu.com/pro\\n\\n64\\n\\n\\n-----\\n\\nSystem information as of Wed Jun 3 17:32:10 UTC 2020\\n\\nSystem load: 0.0 Memory usage: 2% Processes: 146\\n\\nUsage of /home: unknown Swap usage: 0% Users logged in: 0\\n\\n0 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\nindividual files in /usr/share/doc/*/copyright.\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\\n\\napplicable law.\\n## **Eventually the initial Subiquity installer screen shows up:**\\n\\n====================================================================\\n\\nWillkommen! Bienvenue! Welcome! ????? ??????????! Welkom!\\n\\n====================================================================\\n\\nUse UP, DOWN and ENTER keys to select your language.\\n\\n[ English         - ]\\n\\n[ Asturianu         - ]\\n\\n[ Cataln         - ]\",\n",
       " '[ Hrvatski         - ]\\n\\n[ Nederlands         - ]\\n\\n[ Suomi         - ]\\n\\n[ Francais         - ]\\n\\n[ Deutsch         - ]\\n\\n[ Magyar         - ]\\n\\n[ Latvie?u         - ]\\n\\n[ Norsk bokm?l         - ]\\n\\n[ Polski         - ]\\n\\n[ Espanol         - ]\\n\\n  - From here just proceed with the installation as usual …\\n(I’m leaving some pretty standard screenshots here just to give an example for a basic installation …)\\n\\n====================================================================\\n\\nKeyboard configuration\\n\\n====================================================================\\n\\nPlease select your keyboard layout below, or select \"Identify keyboard\" to\\n\\ndetect your layout automatically.\\n\\nLayout: [ English (US) v ]\\n\\nVariant: [ English (US) v ]\\n\\n[ Identify keyboard ]\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nZdev setup\\n\\n65\\n\\n\\n-----\\n\\n====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ndasd-eckd\\n\\n0.0.1600  \\n0.0.1601  \\n0.0.1602  \\n0.0.1603  \\n0.0.1604  \\n0.0.1605  \\n0.0.1606  \\n0.0.1607  \\n0.0.1608  \\n0.0.1609  \\n0.0.160a  \\n0.0.160b  \\n0.0.160c  \\n0.0.160d  - v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ndasd-eckd\\n\\n0.0.1600 >┌────────────┐\\n\\n0.0.1601 >│< (close) │\\n\\n0.0.1602 >│ Enable │\\n\\n0.0.1603 >│ Disable │\\n\\n0.0.1604 >└────────────┘\\n\\n0.0.1605  \\n0.0.1606  \\n0.0.1607  \\n0.0.1608  \\n0.0.1609  \\n0.0.160a  \\n0.0.160b  \\n0.0.160c  \\n0.0.160d  - v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nZdev setup',\n",
       " '====================================================================\\n\\nID ONLINE NAMES ^\\n\\n│\\n\\ndasd-eckd\\n\\n0.0.1600  \\n0.0.1601 online dasda  \\n0.0.1602  \\n0.0.1603  \\n0.0.1604  \\n0.0.1605  \\n0.0.1606  \\n0.0.1607  \\n0.0.1608  \\n0.0.1609  \\n0.0.160a  \\n0.0.160b  \\n66\\n\\n\\n-----\\n\\n0.0.160c  \\n0.0.160d  - v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n  - One may hit the End key here – that will automatically scroll down to the bottom of the Z devices list and\\n\\nscreen:\\n\\n====================================================================\\n\\nZdev setup\\n\\n====================================================================\\n\\n0.0.f1de:0.0.f1df  - ^\\n\\n0.0.f1e0:0.0.f1e1  \\n0.0.f1e2:0.0.f1e3  \\n0.0.f1e4:0.0.f1e5  \\n0.0.f1e6:0.0.f1e7  \\n0.0.f1e8:0.0.f1e9  \\n0.0.f1ea:0.0.f1eb  \\n0.0.f1ec:0.0.f1ed  \\n0.0.f1ee:0.0.f1ef  \\n0.0.f1f0:0.0.f1f1  \\n0.0.f1f2:0.0.f1f3  \\n0.0.f1f4:0.0.f1f5  \\n0.0.f1f6:0.0.f1f7  \\n0.0.f1f8:0.0.f1f9  \\n0.0.f1fa:0.0.f1fb  \\n0.0.f1fc:0.0.f1fd  - │\\n\\n0.0.f1fe:0.0.f1ff  - v\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nNetwork connections\\n\\n====================================================================\\n\\nConfigure at least one interface this server can use to talk to other\\n\\nmachines, and which preferably provides sufficient access for updates.\\n\\nNAME TYPE NOTES\\n\\n[ encc000 eth  -  - ]\\n\\n72:00:bb:00:aa:11 / Unknown Vendor / Unknown Model\\n\\n[ encc000.1234 vlan  -  - ]\\n\\nstatic 10.222.111.11/24\\n\\nVLAN 1234 on interface encc000\\n\\n[ Create bond > ]\\n\\n[ Continue ]\\n\\n[ Back ]\\n\\n  - Depending on the installer version you are using you may face a little bug here.\\nIn that case the button will be named ‘Continue without network’, but the network is there. If you see that,',\n",
       " 'just ignore it and continue …\\n(If you wait long enough the label will be refreshed and corrected.)\\n\\n====================================================================\\n\\nConfigure proxy\\n\\n====================================================================\\n\\n67\\n\\n\\n-----\\n\\nIf this system requires a proxy to connect to the internet, enter its\\n\\ndetails here.\\n\\nProxy address:\\n\\nIf you need to use a HTTP proxy to access the outside world,\\n\\nenter the proxy information here. Otherwise, leave this\\n\\nblank.\\n\\nThe proxy information should be given in the standard form\\n\\nof \"http://[[user][:pass]@]host[:port]/\".\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nConfigure Ubuntu archive mirror\\n\\n====================================================================\\n\\nIf you use an alternative mirror for Ubuntu, enter its details here.\\n\\nMirror address: http://ports.ubuntu.com/ubuntu-ports\\n\\nYou may provide an archive mirror that will be used instead\\n\\nof the default.\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nGuided storage configuration\\n\\n====================================================================\\n\\nConfigure a guided storage layout, or create a custom one:\\n\\n(X) Use an entire disk\\n\\n[ 0X1601 local disk 6.877G v ]\\n\\n[ ] Set up this disk as an LVM group\\n\\n[ ] Encrypt the LVM group with LUKS\\n\\nPassphrase:\\n\\nConfirm passphrase:\\n\\n68\\n\\n\\n-----\\n\\n( ) Custom storage layout\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nStorage configuration\\n\\n====================================================================\\n\\nFILE SYSTEM SUMMARY ^\\n\\n│\\n\\nMOUNT POINT SIZE TYPE DEVICE TYPE │',\n",
       " \"[ / 6.875G new ext4 new partition of local disk > ] │\\n\\n│\\n\\n│\\n\\nAVAILABLE DEVICES │\\n\\n│\\n\\nNo available devices │\\n\\n│\\n\\n[ Create software RAID (md) > ] │\\n\\n[ Create volume group (LVM) > ] │\\n\\nUSED DEVICES\\n\\nv\\n\\n[ Done ]\\n\\n[ Reset ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nStorage configuration\\n\\n====================================================================\\n\\nFILE SYSTEM SUMMARY ^\\n\\n│\\n\\n┌──────────────────────Confirm destructive action ──────────────────────┐\\n\\n│ │\\n\\n│ Selecting Continue below will begin the installation process and │\\n\\n│ result in the loss of data on the disks selected to be formatted. │\\n\\n│ │\\n\\n│ You will not be able to return to this or a previous screen once the │\\n\\n│ installation has started. │\\n\\n│ │\\n\\n│ Are you sure you want to continue? │\\n\\n│ │\\n\\n│ [ No ] │\\n\\n│ [ Continue ] │\\n\\n│ │\\n\\n└────────────────────────────────────────────────────────────────────────┘\\n\\n[ Reset ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nProfile setup\\n\\n====================================================================\\n\\nEnter the username and password you will use to log in to the system. You\\n\\ncan configure SSH access on the next screen but a password is still needed\\n\\nfor sudo.\\n\\nYour name: Ed Example\\n\\nYour server's name: s1lp11\\n\\n69\\n\\n\\n-----\\n\\nThe name it uses when it talks to other computers.\\n\\nPick a username: ubuntu\\n\\nChoose a password: ********\\n\\nConfirm your password: ********\\n\\n[ Done ]\\n\\n====================================================================\\n\\nSSH Setup\\n\\n====================================================================\\n\\nYou can choose to install the OpenSSH server package to enable secure remote\",\n",
       " 'access to your server.\\n\\n[ ] Install OpenSSH server\\n\\nImport SSH identity: [ No v ]\\n\\nYou can import your SSH keys from Github or Launchpad.\\n\\nImport Username:\\n\\n[X] Allow password authentication over SSH\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n  - It’s a nice and convenient new feature to add the user’s SSH keys during the installation to the system, since\\nthat makes the system login password-less for the initial login!\\n\\n====================================================================\\n\\nSSH Setup\\n\\n====================================================================\\n\\nYou can choose to install the OpenSSH server package to enable secure remote\\n\\naccess to your server.\\n\\n[X] Install OpenSSH server\\n\\nImport SSH identity: [ from Launchpad v ]\\n\\nYou can import your SSH keys from Github or Launchpad.\\n\\nLaunchpad Username: user\\n\\nEnter your Launchpad username.\\n\\n[X] Allow password authentication over SSH\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\n70\\n\\n\\n-----\\n\\nSSH Setup\\n\\n====================================================================\\n\\nYou can choose to install the OpenSSH server package to enable secure remote\\n\\naccess to your server.\\n\\n┌───────────────────────────Confirm SSH keys ───────────────────────────┐\\n\\n│ │\\n\\n│ Keys with the following fingerprints were fetched. Do you want to │\\n\\n│ use them? │\\n\\n│ │\\n\\n│ 2048 SHA256:joGscmiamcaoincinaäonnväineorviZEdDWdR9Hpbc2KIw user@W520 │\\n\\n│ (RSA) │\\n\\n│ 521 SHA256:T3JzxvB6K1Gzidvoidhoidsaoicak0jhhgvbw01F7/fZ2c │\\n\\n│ ed.example@acme.com (ECDSA) │\\n\\n│ │\\n\\n│ [ Yes ] │\\n\\n│ [ No ] │\\n\\n│ │\\n\\n└────────────────────────────────────────────────────────────────────────┘\\n\\n[ Done ]\\n\\n[ Back ]',\n",
       " \"====================================================================\\n\\nFeatured Server Snaps\\n\\n====================================================================\\n\\nThese are popular snaps in server environments. Select or deselect with\\n\\nSPACE, press ENTER to see more details of the package, publisher and\\n\\nversions available.\\n\\n[ ] kata-containers Lightweight virtual machines that seamlessly plug into >\\n\\n[ ] docker Docker container runtime  \\n[ ] mosquitto Eclipse Mosquitto MQTT broker  \\n[ ] etcd Resilient key-value store by CoreOS  \\n[ ] stress-ng A tool to load, stress test and benchmark a computer s >\\n\\n[ ] sabnzbd SABnzbd  \\n[ ] wormhole get things from one computer to another, safely  \\n[ ] slcli Python based SoftLayer API Tool.  \\n[ ] doctl DigitalOcean command line tool  \\n[ ] keepalived High availability VRRP/BFD and load-balancing for Linu >\\n\\n[ ] juju Simple, secure and stable devops. Juju keeps complexit >\\n\\n[ Done ]\\n\\n[ Back ]\\n\\n====================================================================\\n\\nInstall complete!\\n\\n====================================================================\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n\\n│ configuring raid (mdadm) service ^│\\n\\n│ installing kernel │\\n\\n│ setting up swap │\\n\\n│ apply networking config │\\n\\n│ writing etc/fstab │\\n\\n│ configuring multipath │\\n\\n│ updating packages on target system │\\n\\n│ configuring pollinate user-agent on target │\\n\\n│ updating initramfs configuration │\\n\\n│ finalizing installation │\\n\\n│ running 'curtin hook' │\\n\\n│ curtin command hook │\\n\\n│ executing late commands │\\n\\n71\\n\\n\\n-----\\n\\n│final system configuration │\\n\\n│ configuring cloud-init ││\\n\\n│ installing openssh-server \\\\ v│\",\n",
       " \"└──────────────────────────────────────────────────────────────────────────┘\\n\\n[ View full log ]\\n\\n====================================================================\\n\\nInstallation complete!\\n\\n====================================================================\\n\\n┌────────────────────────────Finished install! ───────────────────────────┐\\n\\n│ apply networking config ^│\\n\\n│ writing etc/fstab │\\n\\n│ configuring multipath │\\n\\n│ updating packages on target system │\\n\\n│ configuring pollinate user-agent on target │\\n\\n│ updating initramfs configuration │\\n\\n│ finalizing installation │\\n\\n│ running 'curtin hook' │\\n\\n│ curtin command hook │\\n\\n│ executing late commands │\\n\\n│final system configuration │\\n\\n│ configuring cloud-init │\\n\\n│ installing openssh-server │\\n\\n│ restoring apt configuration ││\\n\\n│downloading and installing security updates v│\\n\\n└──────────────────────────────────────────────────────────────────────────┘\\n\\n[ View full log ]\\n\\n[ Reboot ]\\n\\nInstallation complete!\\n\\n====================================================================\\n\\n┌────────────────────────────Finished install! ───────────────────────────┐\\n\\n│ apply networking config ^│\\n\\n│ writing etc/fstab │\\n\\n│ configuring multipath │\\n\\n│ updating packages on target system │\\n\\n│ configuring pollinate user-agent on target │\\n\\n│ updating initramfs configuration │\\n\\n│ finalizing installation │\\n\\n│ running 'curtin hook' │\\n\\n│ curtin command hook │\\n\\n│ executing late commands │\\n\\n│final system configuration │\\n\\n│ configuring cloud-init │\\n\\n│ installing openssh-server │\\n\\n│ restoring apt configuration ││\\n\\n│downloading and installing security updates v│\\n\\n└──────────────────────────────────────────────────────────────────────────┘\",\n",
       " '[ Connection to 10.222.111.11 closed by remote host. [ Rebooting... ]\\n\\nConnection to 10.222.111.11 closed.\\n\\nuser@workstation:~$\\n\\n - Now type reset to clear the screen and reset it to its defaults.\\n\\n  - Before proceeding one needs to remove the old, temporary host key of the target system, since it was only for\\nuse during the installation:\\n\\nuser@workstation:~$ ssh-keygen -f \"/home/user/.ssh/known_hosts\" -R \"s1lp11\"\\n\\n# Host s1lp11 found: line 159\\n\\n/home/user/.ssh/known_hosts updated.\\n\\nOriginal contents retained as /home/user/.ssh/known_hosts.old\\n\\nuser@workstation:~$\\n\\n72\\n\\n\\n-----\\n\\n  - And assuming the post-installation reboot is done, one can now login:\\n\\nuser@workstation:~$ ssh ubuntu@s1lp11\\n\\nWarning: Permanently added the ECDSA host key for IP address\\n\\n\\'10.222.111.11\\' to the list of known hosts.\\n\\nWelcome to Ubuntu 20.04.5 LTS (GNU/Linux 5.4.0-42-generic s390x)\\n\\n   - Documentation: https://help.ubuntu.com\\n\\n   - Management: https://landscape.canonical.com\\n\\n   - Support: https://ubuntu.com/pro\\n\\nSystem information as of Wed 03 Jun 2020 05:50:05 PM UTC\\n\\nSystem load: 0.08 Memory usage: 2% Processes: 157\\n\\nUsage of /: 18.7% of 6.70GB Swap usage: 0% Users logged in: 0\\n\\n0 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\nindividual files in /usr/share/doc/*/copyright.\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.\\n\\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\\n\\nSee \"man sudo_root\" for details.\\n\\nubuntu@s1lp11:~$ uptime',\n",
       " '17:50:09 up 1 min, 1 user, load average: 0.08, 0.11, 0.05\\n\\nubuntu@s1lp11:~$ lsb_release -a\\n\\nNo LSB modules are available.\\n\\nDistributor ID: Ubuntu\\n\\nDescription: Ubuntu 20.04.5 LTS\\n\\nRelease: 20.04\\n\\nCodename: focal\\n\\nubuntu@s1lp11:~$ uname -a\\n\\nLinux s1lp11 5.4.0-42-generic #30-Ubuntu SMP Wed Aug 05 16:57:22 UTC 2020 s390x s390x s390x GNU/Linux\\n\\nubuntu@s1lp11:~$ exit\\n\\nlogout\\n\\nConnection to s1lp11 closed.\\n\\nuser@workstation:~$\\n\\nDone !\\n## **Introduction**\\n\\nSince version 20.04, the server installer supports automated installation mode (autoinstallation for short). You might\\nalso know this feature as *unattended*, *hands-off*, or *preseeded* installation.\\n\\nAutoinstallation lets you answer all those configuration questions ahead of time with an *autoinstall config*, and lets\\nthe installation process run without any interaction.\\n## **Differences from debian-installer preseeding**\\n\\n*Preseeds* are the way to automate an installer based on debian-installer (a.k.a. *d-i* ).\\n\\nAutoinstalls for the new server installer differ from preseeds in the following main ways:\\n\\n  - The format is completely different (cloud-init config, usually YAML, vs. debconf-set-selections format).\\n\\n  - When the answer to a question is not present in a preseed, d-i stops and asks the user for input. Autoinstalls are\\nnot like this: by default, if there is any autoinstall config at all, the installer takes the default for any unanswered\\nquestion (and fails if there is no default).\\n\\n**–**\\nYou can designate particular sections in the config as “interactive”, which means the installer will still stop\\nand ask about those.\\n\\n73\\n\\n\\n-----\\n\\n## **Provide the autoinstall config via cloud-init**',\n",
       " 'The autoinstall config is provided via cloud-init configuration, which is almost endlessly flexible. In most scenarios,\\n[the easiest way will be to provide user data via the NoCloud datasource.](https://cloudinit.readthedocs.io/en/latest/reference/datasources/nocloud.html)\\n\\nThe autoinstall config should be provided under the autoinstall key in the config. For example:\\n\\n#cloud-config\\n\\nautoinstall:\\n\\nversion: 1\\n\\n...\\n## **Run a truly automatic autoinstall**\\n\\nEven if a fully non-interactive autoinstall config is found, the server installer will ask for confirmation before writing\\nto the disks unless autoinstall is present on the kernel command line. This is to make it harder to accidentally create\\na USB stick that will reformat the machine it is plugged into at boot. Many autoinstalls will be done via netboot,\\nwhere the kernel command line is controlled by the netboot config – just remember to put autoinstall in there!\\n\\n**Quick start**\\n\\nSo you just want to try it out? Well we have the page for you.\\n\\n**Create an autoinstall config**\\n\\nWhen any system is installed using the server installer, an autoinstall file for repeating the install is created at\\n\\n/var/log/installer/autoinstall-user-data .\\n\\n**Translate a preseed file**\\n\\n[If you have a preseed file already, the autoinstall-generator snap can help translate that preseed data to an autoinstall](https://snapcraft.io/autoinstall-generator)\\n[file. See this discussion on the autoinstall generator tool for more details on how to set this up.](https://discourse.ubuntu.com/t/autoinstall-generator-tool-to-help-with-creation-of-autoinstall-files-based-on-preseed/21334)\\n## **The structure of an autoinstall config**',\n",
       " 'The autoinstall config has full documentation.\\n\\nTechnically speaking, the config is not defined as a textual format, but cloud-init config is usually provided as YAML\\nso that is the syntax the documentation uses. A minimal config consists of:\\n\\nversion: 1\\n\\nidentity:\\n\\nhostname: hostname\\n\\nusername: username\\n\\npassword: $crypted_pass\\n\\nHowever, here is a more complete example file that shows off most features:\\n\\nversion: 1\\n\\nreporting:\\n\\nhook:\\n\\ntype: webhook\\n\\nendpoint: http://example.com/endpoint/path\\n\\nearly-commands:\\n\\n  - ping -c1 198.162.1.1\\n\\nlocale: en_US\\n\\nkeyboard:\\n\\nlayout: gb\\n\\nvariant: dvorak\\n\\nnetwork:\\n\\nnetwork:\\n\\nversion: 2\\n\\nethernets:\\n\\nenp0s25:\\n\\ndhcp4: yes\\n\\nenp3s0: {}\\n\\nenp4s0: {}\\n\\nbonds:\\n\\n74\\n\\n\\n-----\\n\\nbond0:\\n\\ndhcp4: yes\\n\\ninterfaces:\\n\\n          - enp3s0\\n\\n          - enp4s0\\n\\nparameters:\\n\\nmode: active-backup\\n\\nprimary: enp3s0\\n\\nproxy: http://squid.internal:3128/\\n\\napt:\\n\\nprimary:\\n\\n    - arches: [default]\\n\\nuri: http://repo.internal/\\n\\nsources:\\n\\nmy-ppa.list:\\n\\nsource: \"deb http://ppa.launchpad.net/curtin-dev/test-archive/ubuntu $RELEASE main\"\\n\\nkeyid: B59D 5F15 97A5 04B7 E230 6DCA 0620 BBCF 0368 3F77\\n\\nstorage:\\n\\nlayout:\\n\\nname: lvm\\n\\nidentity:\\n\\nhostname: hostname\\n\\nusername: username\\n\\npassword: $crypted_pass\\n\\nssh:\\n\\ninstall-server: yes\\n\\nauthorized-keys:\\n\\n   - $key\\n\\nallow-pw: no\\n\\nsnaps:\\n\\n  - name: go\\n\\nchannel: 1.14/stable\\n\\nclassic: true\\n\\ndebconf-selections: |\\n\\nbind9 bind9/run-resolvconf boolean false\\n\\npackages:\\n\\n  - libreoffice\\n\\n  - dns-server^\\n\\nuser-data:\\n\\ndisable_root: false\\n\\nlate-commands:\\n\\n  - sed -ie \\'s/GRUB_TIMEOUT=.\\\\*/GRUB_TIMEOUT=30/\\' /target/etc/default/grub\\n\\nerror-commands:\\n\\n  - tar c /var/log/installer | nc 192.168.0.1 1000',\n",
       " 'Many keys and values correspond straightforwardly to questions the installer asks (e.g. keyboard selection). See the\\nreference for details of those that do not.\\n## **Error handling**\\n\\nProgress through the installer is reported via error-commands are executed and the traceback printed to the console.\\nThe server then just waits.\\n## **Interactions between autoinstall and cloud-init**\\n\\n**Delivery of autoinstall**\\n\\nCloud-config can be used to deliver the autoinstall data to the installation environment. The autoinstall quickstart\\nhas an example of writing the autoinstall config.\\n\\nNote that autoinstall is processed by Subiquity (not cloud-init), so please direct defects in autoinstall behavior and\\n[bug reports to Subiquity.](https://bugs.launchpad.net/subiquity/+filebug)\\n\\n75\\n\\n\\n-----\\n\\n**The installation environment**\\n\\nAt install time, the live-server environment is just that: a live but ephemeral copy of Ubuntu Server. This means that\\ncloud-init is present and running in that environment, and existing methods of interacting with cloud-init can be used\\nto configure the live-server ephemeral environment. For example, any #cloud-config user data keys are presented to\\nthe live-server containing [ssh_import_id](https://cloudinit.readthedocs.io/en/latest/reference/modules.html#ssh-import-id), then SSH keys will be added to the authorized_keys list for the ephemeral\\n\\nenvironment.\\n\\n**First boot configuration of the target system**\\n\\nAutoinstall data may optionally contain a user data sub-section, which is cloud-config data that is used to configure\\nthe target system on first boot.',\n",
       " 'Subiquity itself delegates some configuration items to cloud-init, and these items are processed on first boot.\\n\\nStarting with Ubuntu 22.10, once cloud-init has performed this first boot configuration, it will disable itself as cloud-init\\ncompletes configuration in the target system on first boot.\\n\\n**Possible future directions**\\n\\nWe might want to extend the ‘match specs’ for disks to cover other ways of selecting disks.\\n\\nThe intent of this page is to provide simple instructions to perform an autoinstall in a VM on your machine.\\n\\nThis page assumes that you are willing to install the latest Ubuntu release available (22.10 at the time of writing). For\\nother releases, you would need to substitute the name of the ISO image but the instructions should otherwise remain\\nthe same.\\n\\nThis page also assumes you are on the amd64 architecture. There is a version for s390x too.\\n## **Providing the autoinstall data over the network**\\n\\nThis method is the one that generalises most easily to doing an entirely network-based install, where a machine\\nnetboots and is then automatically installed.\\n\\n**Download the ISO**\\n\\n[Go to the 22.10 ISO download page and download the latest Ubuntu 22.10 live-server ISO.](https://releases.ubuntu.com/22.10/)\\n\\n**Mount the ISO**\\n\\nsudo mount -r ~/Downloads/ubuntu-22.10-live-server-amd64.iso /mnt\\n\\n**Write your autoinstall config**\\n\\nThis means creating cloud-init config as follows:\\n\\nmkdir -p ~/www\\n\\ncd ~/www\\n\\ncat > user-data << \\'EOF\\'\\n\\n#cloud-config\\n\\nautoinstall:\\n\\nversion: 1\\n\\nidentity:\\n\\nhostname: ubuntu-server\\n\\npassword: \"$6$exDY1mhS4KUYCE/2$zmn9ToZwTKLhCw.b4/b.ZRTIZM30JZ4QrOQ2aOXJ8yk96xpcCof0kxKwuX1kqLG/ygbJ1f8wxED22bTL4F46\\n\\nusername: ubuntu\\n\\nEOF\\n\\ntouch meta-data',\n",
       " 'The crypted password is just “ubuntu”.\\n\\n**Serve the cloud-init config over HTTP**\\n\\nLeave this running in a new terminal window:\\n\\ncd ~/www\\n\\npython3 -m http.server 3003\\n\\n76\\n\\n\\n-----\\n\\n**Create a target disk**\\n\\ntruncate -s 10G image.img\\n\\n**Run the install!**\\n\\nkvm -no-reboot -m 2048 \\\\\\n\\n-drive file=image.img,format=raw,cache=none,if=virtio \\\\\\n\\n-cdrom ~/Downloads/ubuntu-22.10-live-server-amd64.iso \\\\\\n\\n-kernel /mnt/casper/vmlinuz \\\\\\n\\n-initrd /mnt/casper/initrd \\\\\\n\\n-append \\'autoinstall ds=nocloud-net;s=http://_gateway:3003/\\'\\n\\nThis will boot, download the config from the server set up in the previous step, and run the install. The installer\\nreboots at the end but the -no-reboot flag to kvm means that kvm will exit when this happens. It should take about 5\\n\\nminutes.\\n\\n**Boot the installed system**\\n\\nkvm -no-reboot -m 2048 \\\\\\n\\n-drive file=image.img,format=raw,cache=none,if=virtio\\n\\nThis will boot into the freshly installed system and you should be able to log in as ubuntu / ubuntu .\\n## **Using another volume to provide the autoinstall config**\\n\\nThis is the method to use when you want to create media that you can just plug into a system to have it be installed.\\n\\n**Download the live-server ISO**\\n\\n[Go to the 22.10 ISO download page and download the latest Ubuntu 22.10 live-server ISO.](https://releases.ubuntu.com/22.10/)\\n\\n**Create your** user-data **and** meta-data **files**\\n\\nmkdir -p ~/cidata\\n\\ncd ~/cidata\\n\\ncat > user-data << \\'EOF\\'\\n\\n#cloud-config\\n\\nautoinstall:\\n\\nversion: 1\\n\\nidentity:\\n\\nhostname: ubuntu-server\\n\\npassword: \"$6$exDY1mhS4KUYCE/2$zmn9ToZwTKLhCw.b4/b.ZRTIZM30JZ4QrOQ2aOXJ8yk96xpcCof0kxKwuX1kqLG/ygbJ1f8wxED22bTL4F46\\n\\nusername: ubuntu\\n\\nEOF\\n\\ntouch meta-data',\n",
       " 'The crypted password is just “ubuntu”.\\n\\n**Create an ISO to use as a cloud-init data source**\\n\\nsudo apt install cloud-image-utils\\n\\ncloud-localds ~/seed.iso user-data meta-data\\n\\n**Create a target disk**\\n\\ntruncate -s 10G image.img\\n\\n**Run the install!**\\n\\nkvm -no-reboot -m 2048 \\\\\\n\\n-drive file=image.img,format=raw,cache=none,if=virtio \\\\\\n\\n-drive file=~/seed.iso,format=raw,cache=none,if=virtio \\\\\\n\\n-cdrom ~/Downloads/ubuntu-22.10-live-server-amd64.iso\\n\\nThis will boot and run the install. Unless you interrupt boot to add autoinstall to the kernel command line, the\\ninstaller will prompt for confirmation before touching the disk.\\n\\n77\\n\\n\\n-----\\n\\nThe installer reboots at the end but the -no-reboot flag to kvm means that kvm will exit when this happens.\\n\\nThe whole process should take about 5 minutes.\\n\\n**Boot the installed system**\\n\\nkvm -no-reboot -m 2048 \\\\\\n\\n-drive file=image.img,format=raw,cache=none,if=virtio\\n\\nThis will boot into the freshly installed system and you should be able to log in as ubuntu / ubuntu .\\n\\nThe intent of this page is to provide simple instructions to perform an autoinstall in a VM on your machine on s390x.\\n\\nThis page is just a slightly adapted page of the autoinstall quickstart page mapped to s390x.\\n## **Download an ISO**\\n\\nAt the time of writing (just after the kinetic release), the best place to go is here:\\n[https://cdimage.ubuntu.com/ubuntu/releases/22.10/release/](https://cdimage.ubuntu.com/ubuntu/releases/22.10/release/)\\n\\nwget https://cdimage.ubuntu.com/ubuntu/releases/22.10/release/ubuntu-22.10-live-server-s390x.iso -P ~/Downloads\\n## **Mount the ISO**\\n\\nmkdir -p ~/iso\\n\\nsudo mount -r ~/Downloads/ubuntu-22.10-live-server-s390x.iso ~/iso',\n",
       " '## **Write your autoinstall config**\\n\\nThis means creating a cloud-init #cloud-config file as follows:\\n\\nmkdir -p ~/www\\n\\ncd ~/www\\n\\ncat > user-data << \\'EOF\\'\\n\\n#cloud-config\\n\\nautoinstall:\\n\\nversion: 1\\n\\nidentity:\\n\\nhostname: ubuntu-server\\n\\npassword: \"$6$exDY1mhS4KUYCE/2$zmn9ToZwTKLhCw.b4/b.ZRTIZM30JZ4QrOQ2aOXJ8yk96xpcCof0kxKwuX1kqLG/ygbJ1f8wxED22bTL4F46\\n\\nusername: ubuntu\\n\\nEOF\\n\\ntouch meta-data\\n\\nThe crypted password is just “ubuntu”.\\n## **Serve the cloud-init config over HTTP**\\n\\nLeave this running in a new terminal window:\\n\\ncd ~/www\\n\\npython3 -m http.server 3003\\n## **Create a target disk**\\n\\nProceed with a second terminal window:\\n\\nsudo apt install qemu-utils\\n\\nqemu-img create -f qcow2 disk-image.qcow2 10G\\n\\nFormatting \\'disk-image.qcow2\\', fmt=qcow2 size=10737418240 cluster_size=65536 lazy_refcounts=off refcount_bits=16\\n\\nqemu-img info disk-image.qcow2\\n\\nimage: disk-image.qcow2\\n\\nfile format: qcow2\\n\\nvirtual size: 10 GiB (10737418240 bytes)\\n\\ndisk size: 196 KiB\\n\\ncluster_size: 65536\\n\\nFormat specific information:\\n\\n78\\n\\n\\n-----\\n\\ncompat: 1.1\\n\\nlazy refcounts: false\\n\\nrefcount bits: 16\\n\\ncorrupt: false\\n## **Run the install!**\\n\\nsudo apt install qemu-kvm\\n\\nYou may need to add the default user to the kvm group:\\n\\nsudo usermod -a -G kvm ubuntu\\n\\n**Note** :\\nYou will need to re-login to make the changes take effect.\\n\\nkvm -no-reboot -name auto-inst-test -nographic -m 2048 \\\\\\n\\n-drive file=disk-image.qcow2,format=qcow2,cache=none,if=virtio \\\\\\n\\n-cdrom ~/Downloads/ubuntu-22.10-live-server-s390x.iso \\\\\\n\\n-kernel ~/iso/boot/kernel.ubuntu \\\\\\n\\n-initrd ~/iso/boot/initrd.ubuntu \\\\\\n\\n-append \\'autoinstall ds=nocloud-net;s=http://_gateway:3003/ console=ttysclp0\\'',\n",
       " 'This will boot, download the config from the server set up in the previous step and run the install. The installer\\nreboots at the end but the -no-reboot flag to kvm means that kvm will exit when this happens. It should take about 5\\n\\nminutes.\\n## **Boot the installed system**\\n\\nkvm -no-reboot -name auto-inst-test -nographic -m 2048 \\\\\\n\\n-drive file=disk-image.qcow2,format=qcow2,cache=none,if=virtio\\n\\nThis will boot into the freshly installed system and you should be able to log in as ubuntu / ubuntu .\\n## **Overall format**\\n\\nThe autoinstall file is YAML. At top level it must be a mapping containing the keys described in this document.\\nUnrecognized keys are ignored.\\n## **Schema**\\n\\nAutoinstall configs are validated against a JSON schema before they are used.\\n## **Command lists**\\n\\nSeveral config keys are lists of commands to be executed. Each command can be a string (in which case it is executed\\nvia “sh -c”) or a list, in which case it is executed directly. Any command exiting with a non-zero return code is\\nconsidered an error and aborts the install (except for error-commands, where it is ignored).\\n## **Top-level keys**\\n\\n**version**\\n\\n**type:** integer\\n**default:** no default\\n\\nA future-proofing config file version field. Currently this must be “1”.\\n\\n**interactive-sections**\\n\\n**type:** list of strings\\n**default:** []\\n\\nA list of config keys to still show in the UI. So for example:\\n\\nversion: 1\\n\\ninteractive-sections:\\n\\n - network\\n\\nidentity:\\n\\n79\\n\\n\\n-----\\n\\nusername: ubuntu\\n\\npassword: $crypted_pass\\n\\nWould stop on the network screen and allow the user to change the defaults. If a value is provided for an interactive\\nsection it is used as the default.',\n",
       " 'You can use the special section name of “*” to indicate that the installer should ask all the usual questions – in this\\ncase, the autoinstall.yaml file is not really an “autoinstall” file at all, instead just a way to change the defaults in the\\nUI.\\n\\nNot all config keys correspond to screens in the UI. This documentation indicates if a given section can be interactive\\n\\nor not.\\n\\nIf there are any interactive sections at all, the reporting key is ignored.\\n\\n**early-commands**\\n\\n**type:** command list\\n**default:** no commands\\n\\n**can be interactive:** no\\n\\nA list of shell commands to invoke as soon as the installer starts, in particular before probing for block and network\\ndevices. The autoinstall config is available at /autoinstall.yaml (irrespective of how it was provided) and the file will\\nbe re-read after the early-commands have run to allow them to alter the config if necessary.\\n\\n**locale**\\n\\n**type:** string\\n**default:** en_US.UTF-8\\n**can be interactive:** yes, always interactive if any section is\\n\\nThe locale to configure for the installed system.\\n\\n**refresh-installer**\\n\\n**type:** mapping\\n**default:** see below\\n\\n**can be interactive:** yes\\n\\nControls whether the installer updates to a new version available in the given channel before continuing.\\n\\nThe mapping contains keys:\\n\\n**update**\\n\\n**type:** boolean\\n**default:** no\\n\\nWhether to update or not.\\n\\n**channel**\\n\\n**type:** string\\n**default:** \"stable/ubuntu-$REL\"\\n\\nThe channel to check for updates.\\n\\n**keyboard**\\n\\n**type:** mapping, see below\\n**default:** US English keyboard\\n**can be interactive:** yes\\n\\nThe layout of any attached keyboard. Often systems being automatically installed will not have a keyboard at all in',\n",
       " 'which case the value used here does not matter.\\n\\nThe mapping’s keys correspond to settings in the /etc/default/keyboard [configuration file. See its manual page for](http://manpages.ubuntu.com/manpages/bionic/en/man5/keyboard.5.html)\\nmore details.\\n\\nThe mapping contains keys:\\n\\n80\\n\\n\\n-----\\n\\n**layout**\\n\\n**type:** string\\n**default:** \"us\"\\n\\nCorresponds to the XKBLAYOUT setting.\\n\\n**variant**\\n\\n**type:** string\\n**default:** \"\"\\n\\nCorresponds to the XKBVARIANT setting.\\n\\n**toggle**\\n\\n**type:** string or null\\n**default:** null\\n\\nCorresponds to the value of grp: option from the XKBOPTIONS setting. Acceptable values are (but note that the\\ninstaller does not validate these): caps_toggle, toggle, rctrl_toggle, rshift_toggle, rwin_toggle, menu_toggle,\\n\\nalt_shift_toggle, ctrl_shift_toggle, ctrl_alt_toggle, alt_caps_toggle, lctrl_lshift_toggle, lalt_toggle,\\n\\nlctrl_toggle, lshift_toggle, lwin_toggle, sclk_toggle\\n\\nThe version of Subiquity released with 20.04 GA does not accept null for this field due to a bug.\\n\\n**source**\\n\\n**type:** mapping, see below\\n**default:** see below\\n\\n**can be interactive:** yes\\n\\n**search_drivers**\\n\\n**type:** boolean\\n**default:** true\\n\\nWhether the installer should search for available third-party drivers. When set to false, it disables the drivers screen\\nand section.\\n\\n**id**\\n\\n**type:** string\\n**default:** identifier of the first available source.\\n\\nIdentifier of the source to install (e.g., \"ubuntu-server-minimized\" ).\\n\\n**network**\\n\\n**type:** netplan-format mapping, see below\\n**default:** DHCP on interfaces named eth* or en*\\n\\n**can be interactive:** yes\\n\\n[Netplan-formatted network configuration.',\n",
       " 'This will be applied during installation as well as in the installed system.](https://netplan.readthedocs.io/en/stable/netplan-yaml/)\\nThe default is to interpret the config for the install media, which runs DHCPv4 on any interface with a name matching\\n“eth*” or “en*” but then disables any interface that does not receive an address.\\n\\nFor example, to run DHCPv6 on a particular NIC:\\n\\nnetwork:\\n\\nversion: 2\\n\\nethernets:\\n\\nenp0s31f6:\\n\\ndhcp6: yes\\n\\nNote that thanks to a bug, the version of Subiquity released with 20.04 GA forces you to write this with an extra\\n\\nnetwork: key like so:\\n\\nnetwork:\\n\\nnetwork:\\n\\nversion: 2\\n\\n81\\n\\n\\n-----\\n\\nethernets:\\n\\nenp0s31f6:\\n\\ndhcp6: yes\\n\\nLater versions support this syntax too for compatibility but if you can assume a newer version you should use the\\nformer.\\n\\n**proxy**\\n\\n**type:** URL or null\\n**default:** no proxy\\n**can be interactive:** yes\\n\\nThe proxy to configure both during installation and for apt and for snapd in the target system.\\n\\n**apt**\\n\\n**type:** mapping\\n**default:** see below\\n\\n**can be interactive:** yes\\n\\nApt configuration, used both during the install and once booted into the target system.\\n\\n[This section historically used the same format as curtin, which is documented here. Nonetheless, some key differences](https://curtin.readthedocs.io/en/latest/topics/apt_source.html)\\nwith the format supported by curtin have been introduced:\\n\\n  - Subiquity supports an alternative format for the primary section, allowing to configure a list of candidate primary\\nmirrors. During installation, subiquity will automatically test the specified mirrors and select the first one that\\nseems usable.',\n",
       " 'This new behavior is only activated when the primary section is wrapped in the mirror-selection\\n\\nsection.\\n\\n - The fallback key controls what subiquity should do if no primary mirror is usable.\\n\\n - The geoip key controls whether a geoip lookup is done to determine the correct country mirror.\\n\\nThe default is:\\n\\napt:\\n\\npreserve_sources_list: false\\n\\nmirror-selection:\\n\\nprimary:\\n\\n      - country-mirror\\n\\n      - arches: [i386, amd64]\\n\\nuri: \"http://archive.ubuntu.com/ubuntu\"\\n\\n      - arches: [s390x, arm64, armhf, powerpc, ppc64el, riscv64]\\n\\nuri: \"http://ports.ubuntu.com/ubuntu-ports\"\\n\\nfallback: abort\\n\\ngeoip: true\\n\\n**mirror-selection**\\n\\nif the primary section is contained within the mirror-selection section, the automatic mirror selection is enabled. This\\nis the default in new installations.\\n\\n**primary (when placed inside the** mirror-selection **section):**\\n\\n**type:** custom, see below\\n\\nIn the new format, the primary section expects a list of mirrors, which can be expressed in two different ways:\\n\\n  - the special value country-mirror\\n\\n  - a mapping with the following keys:\\n\\n**–** uri [: the URI of the mirror to use, e.g., “http://fr.archive.ubuntu.com/ubuntu”](http://fr.archive.ubuntu.com/ubuntu)\\n**–** arches : an optional list of architectures supported by the mirror. By default, this list contains the current\\nCPU architecture.\\n\\n**fallback**\\n\\n**type:** string (enumeration)\\n**default:** abort\\n\\n82\\n\\n\\n-----\\n\\nControls what subiquity should do if no primary mirror is usable.\\nSupported values are:\\n\\n - abort -> abort the installation\\n\\n - offline-install -> revert to an offline installation',\n",
       " ' - continue-anyway -> attempt to install the system anyway (not recommended, the installation will certainly fail)\\n\\n**geoip**\\n\\n**type:** boolean\\n**default:** : true\\n\\nIf geoip is true and one of the candidate primary mirrors has the special value country-mirror, a request is made to\\n\\nhttps://geoip.ubuntu.com/lookup . Subiquity then sets the mirror URI to http://CC.archive.ubuntu.com/ubuntu (or\\nsimilar for ports) where CC is the country code returned by the lookup. If this section is not interactive, the request\\nis timed out after 10 seconds.\\n\\nIf the legacy behavior (i.e., without mirror-selection) is in use, the geoip request is made if the mirror to be used is\\nthe default, and its URI ends up getting replaced by the proper country mirror URI.\\n\\nIf you just want to specify a mirror, you can use a configuration like this:\\n\\napt:\\n\\nmirror-selection:\\n\\nprimary:\\n\\n      - uri: YOUR_MIRROR_GOES_HERE\\n\\n      - country-mirror\\n\\n      - uri: http://archive.ubuntu.com/ubuntu\\n\\nTo add a ppa:\\n\\napt:\\n\\nsources:\\n\\ncurtin-ppa:\\n\\nsource: ppa:curtin-dev/test-archive\\n\\n**storage**\\n\\n**type:** mapping, see below\\n**default:** use “lvm” layout in a single disk system, no default in a multiple disk system\\n**can be interactive:** yes\\n\\nStorage configuration is a complex topic and the description of the desired configuration in the autoinstall file can also\\nbe complex. The installer supports “layouts”, simple ways of expressing common configurations.\\n\\n**Supported layouts**\\n\\nThe two supported layouts at the time of writing are “lvm” and “direct”.\\n\\nstorage:\\n\\nlayout:\\n\\nname: lvm\\n\\nstorage:\\n\\nlayout:\\n\\nname: direct',\n",
       " 'By default these will install to the largest disk in a system, but you can supply a match spec (see below) to indicate\\nwhich disk to use:\\n\\nstorage:\\n\\nlayout:\\n\\nname: lvm\\n\\nmatch:\\n\\nserial: CT*\\n\\nstorage:\\n\\nlayout:\\n\\nname: disk\\n\\nmatch:\\n\\nssd: yes\\n\\n83\\n\\n\\n-----\\n\\n(you can just say “ match: {} ” to match an arbitrary disk)\\n\\nWhen using the “lvm” layout, LUKS encryption can be enabled by supplying a password.\\n\\nstorage:\\n\\nlayout:\\n\\nname: lvm\\n\\npassword: LUKS_PASSPHRASE\\n\\nThe default is to use the lvm layout.\\n\\n**sizing-policy**\\n\\nThe lvm layout will, by default, attempt to leave room for snapshots and further expansion. A sizing-policy key may\\nbe supplied to control this behavior.\\n\\n**type:** string (enumeration)\\n**default:** scaled\\n\\nSupported values are:\\n\\n - scaled -> adjust space allocated to the root LV based on space available to the VG\\n\\n - all -> allocate all remaining VG space to the root LV\\n\\nThe scaling system is currently as follows:\\n\\n  - Less than 10 GiB: use all remaining space for root filesystem\\n\\n  - Between 10-20 GiB: 10 GiB root filesystem\\n\\n  - Between 20-200 GiB: use half of remaining space for root filesystem\\n\\n  - Greater than 200 GiB: 100 GiB root filesystem\\n\\n**Action-based config**\\n\\nFor full flexibility, the installer allows storage configuration to be done using a syntax which is a superset of that\\n[supported by curtin, described at https://curtin.readthedocs.io/en/latest/topics/storage.html.](https://curtin.readthedocs.io/en/latest/topics/storage.html)\\n\\nIf the “layout” feature is used to configure the disks, the “config” section will not be used.\\n\\n[As well as putting the list of actions under the ‘config’ key, the grub and swap curtin config items can be put here.',\n",
       " 'So](https://curtin.readthedocs.io/en/latest/topics/config.html#grub)\\na storage section might look like:\\n\\nstorage:\\n\\nswap:\\n\\nsize: 0\\n\\nconfig:\\n\\n    - type: disk\\n\\nid: disk0\\n\\nserial: ADATA_SX8200PNP_XXXXXXXXXXX\\n\\n    - type: partition\\n\\n...\\n\\nThe extensions to the curtin syntax are around disk selection and partition/logical volume sizing.\\n\\n**Disk selection extensions**\\n\\nCurtin supported identifying disks by serial (e.g. Crucial_CT512MX100SSD1_14250C57FECE ) or by path (e.g. /dev/sdc )\\nand the server installer supports this as well. The installer additionally supports a “match spec” on a disk action that\\nsupports more flexible matching.\\n\\nThe actions in the storage config are processed in the order they are in the autoinstall file. Any disk action is assigned\\na matching disk – chosen arbitrarily from the set of unassigned disks if there is more than one, and causing the\\ninstallation to fail if there is no unassigned matching disk.\\n\\nA match spec supports the following keys:\\n\\n - model: foo : matches a disk where ID_VENDOR=foo in udev, supporting globbing\\n\\n - path: foo : matches a disk where DEVPATH=foo in udev, supporting globbing (the globbing support distinguishes this from specifying path: foo directly in the disk action)\\n\\n - serial: foo : matches a disk where ID_SERIAL=foo in udev, supporting globbing (the globbing support distinguishes this from specifying serial: foo directly in the disk action)\\n\\n - ssd: yes|no : matches a disk that is or is not an SSD (vs a rotating drive)\\n\\n84\\n\\n\\n-----\\n\\n - size: largest|smallest : take the largest or smallest disk rather than an arbitrary one if there are multiple\\nmatches (support for smallest added in version 20.06.1)',\n",
       " 'A special sort of key is install-media: yes, which will take the disk the installer was loaded from (the ssd and size\\nselectors will never return this disk). If installing to the install media, care obviously needs to be take to not overwrite\\nthe installer itself!\\n\\nSo for example, to match an arbitrary disk it is simply:\\n\\n - type: disk\\n\\nid: disk0\\n\\nTo match the largest SSD:\\n\\n - type: disk\\n\\nid: big-fast-disk\\n\\nmatch:\\n\\nssd: yes\\n\\nsize: largest\\n\\nTo match a Seagate drive:\\n\\n - type: disk\\n\\nid: data-disk\\n\\nmatch:\\n\\nmodel: Seagate\\n\\n**Partition/logical volume extensions**\\n\\nThe size of a partition or logical volume in curtin is specified as a number of bytes. The autoinstall config is more\\nflexible:\\n\\n  - You can specify the size using the “1G”, “512M” syntax supported in the installer UI.\\n\\n  - You can specify the size as a percentage of the containing disk (or RAID), e.g. “50%”.\\n\\n  - For the last partition specified for a particular device, you can specify the size as “-1” to indicate that the\\npartition should fill the remaining space.\\n\\n - type: partition\\n\\nid: boot-partition\\n\\ndevice: root-disk\\n\\nsize: 10%\\n\\n - type: partition\\n\\nid: root-partition\\n\\nsize: 20G\\n\\n - type: partition\\n\\nid: data-partition\\n\\ndevice: root-disk\\n\\nsize: -1\\n\\n**identity**\\n\\n**type:** mapping, see below\\n**default:** no default\\n\\n**can be interactive:** yes\\n\\nConfigure the initial user for the system. This is the only config key that must be present (unless the user-data section\\nis present, in which case it is optional).\\n\\nA mapping that can contain keys, all of which take string values:\\n\\n**realname**\\n\\nThe real name for the user. This field is optional.\\n\\n**username**\\n\\nThe user name to create.\\n\\n85\\n\\n\\n-----\\n\\n**hostname**',\n",
       " \"The hostname for the system.\\n\\n**password**\\n\\nThe password for the new user, encrypted. This is required for use with sudo, even if SSH access is configured.\\n\\n[The crypted password string must conform to what passwd expects.](https://manpages.ubuntu.com/manpages/jammy/en/man1/passwd.1.html) Depending on the special characters in the\\npassword hash, quoting may be required, so it’s safest to just always include the quotes around the hash.\\n\\nSeveral tools can generate the crypted password, such as mkpasswd from the whois package, or openssl passwd .\\n\\nExample:\\n\\nidentity:\\n\\nrealname: 'Ubuntu User'\\n\\nusername: ubuntu\\n\\npassword: '$6$wdAcoXrU039hKYPd$508Qvbe7ObUnxoj15DRCkzC3qO7edjH0VV7BPNRDYK4QR8ofJaEEF2heacn0QgD.f8pO8SNp83XNdWG6tocBM\\n\\nhostname: ubuntu\\n\\n**active-directory**\\n\\n**type:** mapping, see below\\n**default:** no default\\n\\n**can be interactive:** yes\\n\\nAccepts data required to join the target system in an Active Directory domain.\\n\\nA mapping that can contain keys, all of which take string values:\\n\\n**admin-name**\\n\\nA domain account name with privilege to perform the join operation. That account’s password will be requested\\nduring runtime.\\n\\n**domain-name**\\n\\nThe Active Directory domain to join.\\n\\n**ubuntu-pro**\\n\\n**type:** mapping, see below\\n**default:** see below\\n\\n**can be interactive:** yes\\n\\n**token**\\n\\n**type:** string\\n**default:** no token\\n\\nA contract token to attach to an existing Ubuntu Pro subscription.\\n\\n**ssh**\\n\\n**type:** mapping, see below\\n**default:** see below\\n\\n**can be interactive:** yes\\n\\nConfigure SSH for the installed system. A mapping that can contain keys:\\n\\n**install-server**\\n\\n**type:** boolean\\n**default:** false\\n\\nWhether to install OpenSSH server in the target system.\\n\\n86\",\n",
       " '-----\\n\\n**authorized-keys**\\n\\n**type:** list of strings\\n**default:** []\\n\\nA list of SSH public keys to install in the initial user’s account.\\n\\n**allow-pw**\\n\\n**type:** boolean\\n**default:** true if authorized_keys is empty, false otherwise\\n\\n**codecs**\\n\\n**type:** mapping, see below\\n**default:** see below\\n\\n**can be interactive:** no\\n\\nConfigure whether common restricted packages (including codecs) from [multiverse] should be installed.\\n\\n**install**\\n\\n**type:** boolean\\n**default:** false\\n\\nWhether to install the ubuntu-restricted-addons package.\\n\\n**drivers**\\n\\n**type:** mapping, see below\\n**default:** see below\\n\\n**can be interactive:** yes\\n\\n**install**\\n\\n**type:** boolean\\n**default:** false\\n\\nWhether to install the available third-party drivers.\\n\\n**snaps**\\n\\n**type:** list\\n**default:** install no extra snaps\\n**can be interactive:** yes\\n\\nA list of snaps to install. Each snap is represented as a mapping with required name and optional channel (defaulting\\nto stable ) and classic (defaulting to false ) keys. For example:\\n\\nsnaps:\\n\\n  - name: etcd\\n\\nchannel: edge\\n\\nclassic: false\\n\\n**debconf-selections**\\n\\n**type:** string\\n**default:** no config\\n**can be interactive:** no\\n\\nThe installer will update the target with debconf set-selection values. Users will need to be familiar with the package\\ndebconf options.\\n\\n**packages**\\n\\n**type:** list\\n**default:** no packages\\n**can be interactive:** no\\n\\nA list of packages to install into the target system. More precisely, a list of strings to pass to “ apt-get install ”, so\\nthis includes things like task selection ( dns-server^ ) and installing particular versions of a package ( my-package=1-1 ).\\n\\n87\\n\\n\\n-----\\n\\n**kernel**',\n",
       " '**type:** mapping (mutually exclusive), see below\\n**default:** default kernel\\n\\n**can be interactive:** no\\n\\nWhich kernel gets installed. Either the name of the package or the name of the flavor must be specified.\\n\\n**package**\\n\\n**type:** string\\n\\nThe name of the package, e.g., linux-image-5.13.0-40-generic\\n\\n**flavor**\\n\\n**type:** string\\n\\nThe flavor of the kernel, e.g., generic or hwe .\\n\\n**timezone**\\n\\n**type:** string\\n**default:** no timezone\\n\\n**can be interactive:** no\\n\\nThe timezone to configure on the system. The special value “geoip” can be used to query the timezone automatically\\nover the network.\\n\\n**updates**\\n\\n**type:** string (enumeration)\\n**default:** security\\n**can be interactive:** no\\n\\nThe type of updates that will be downloaded and installed after the system install.\\nSupported values are:\\n\\n - security -> download and install updates from the -security pocket\\n\\n - all -> also download and install updates from the -updates pocket\\n\\n**shutdown**\\n\\n**type:** string (enumeration)\\n**default:** reboot\\n\\n**can be interactive:** no\\n\\nRequest the system to power off or reboot automatically after the installation has finished.\\nSupported values are:\\n\\n - reboot\\n\\n - poweroff\\n\\n**late-commands**\\n\\n**type:** command list\\n**default:** no commands\\n\\n**can be interactive:** no\\n\\nShell commands to run after the install has completed successfully and any updates and packages installed, just before\\nthe system reboots. They are run in the installer environment with the installed system mounted at /target . You can\\nrun curtin in-target -- $shell_command (with the version of subiquity released with 20.04 GA you need to specify',\n",
       " 'this as curtin in-target --target=/target -- $shell_command ) to run in the target system (similar to how plain\\n\\nin-target can be used in d-i preseed/late_command ).\\n\\n88\\n\\n\\n-----\\n\\n**error-commands**\\n\\n**type:** command list\\n**default:** no commands\\n\\n**can be interactive:** no\\n\\nShell commands to run after the install has failed. They are run in the installer environment, and the target system (or as much of it as the installer managed to configure) will be mounted at /target. Logs will be available at\\n\\n/var/log/installer in the live session.\\n\\n**reporting**\\n\\n**type:** mapping\\n**default:** type: print which causes output on tty1 and any configured serial consoles\\n**can be interactive:** no\\n\\nThe installer supports reporting progress to a variety of destinations. Note that this section is ignored if there are any\\ninteractive sections; it only applies to fully automated installs.\\n\\n[The config, and indeed the implementation, is 90% the same as that used by curtin.](https://curtin.readthedocs.io/en/latest/topics/reporting.html)\\n\\nEach key in the reporting mapping in the config defines a destination, where the type sub-key is one of:\\n\\n**The rsyslog reporter does not yet exist**\\n\\n - **print** : print progress information on tty1 and any configured serial console. There is no other configuration.\\n\\n - **rsyslog** : report progress via rsyslog. The **destination** key specifies where to send output.\\n\\n - **webhook** [: report progress via POSTing JSON reports to a URL. Accepts the same configuration as curtin.](https://curtin.readthedocs.io/en/latest/topics/reporting.html#webhook-reporter)\\n\\n - **none** : do not report progress. Only useful to inhibit the default output.\\n\\nExamples:',\n",
       " 'The default configuration is:\\n\\nreporting:\\n\\nbuiltin:\\n\\ntype: print\\n\\nReport to rsyslog:\\n\\nreporting:\\n\\ncentral:\\n\\ntype: rsyslog\\n\\ndestination: @192.168.0.1\\n\\nSuppress the default output:\\n\\nreporting:\\n\\nbuiltin:\\n\\ntype: none\\n\\nReport to a curtin-style webhook:\\n\\nreporting:\\n\\nhook:\\n\\ntype: webhook\\n\\nendpoint: http://example.com/endpoint/path\\n\\nconsumer_key: \"ck_foo\"\\n\\nconsumer_secret: \"cs_foo\"\\n\\ntoken_key: \"tk_foo\"\\n\\ntoken_secret: \"tk_secret\"\\n\\nlevel: INFO\\n\\n**user-data**\\n\\n**type:** mapping\\n**default:** {}\\n\\n**can be interactive:** no\\n\\nProvide cloud-init user data which will be merged with the user data the installer produces. If you supply this, you\\ndon’t need to supply an identity section (but then it’s your responsibility to make sure that you can log into the\\ninstalled system!).\\n\\n89\\n\\n\\n-----\\n\\n## **Introduction**\\n\\nThe server installer validates the provided autoinstall config against a JSON schema.\\n## **How the config is validated**\\n\\nAlthough the schema is presented below as a single document, and if you want to pre-validate your config you should\\nvalidate it against this document, the config is not actually validated against this document at run time. What happens\\ninstead is that some sections are loaded, validated, and applied first, before all other sections are validated. In detail:\\n\\n1. The reporting section is loaded, validated and applied.\\n2. The error commands are loaded and validated.\\n\\n3. The early commands are loaded and validated.\\n4. The early commands, if any, are run.\\n5. The config is reloaded, and now all sections are loaded and validated.\\n\\nThis is so that validation errors in most sections can be reported via the reporting and error-commands configuration,\\nas all other errors are.',\n",
       " '## **Schema**\\n\\n[The JSON schema for autoinstall data is as follows:](https://json-schema.org/)\\n\\n{\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"version\": {\\n\\n\"type\": \"integer\",\\n\\n\"minimum\": 1,\\n\\n\"maximum\": 1\\n\\n},\\n\\n\"early-commands\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": [\\n\\n\"string\",\\n\\n\"array\"\\n\\n],\\n\\n\"items\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"reporting\": {\\n\\n\"type\": \"object\",\\n\\n\"additionalProperties\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"type\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"type\"\\n\\n],\\n\\n\"additionalProperties\": true\\n\\n}\\n\\n},\\n\\n\"error-commands\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": [\\n\\n\"string\",\\n\\n\"array\"\\n\\n],\\n\\n90\\n\\n\\n-----\\n\\n\"items\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"user-data\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"packages\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"debconf-selections\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"locale\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"refresh-installer\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"update\": {\\n\\n\"type\": \"boolean\"\\n\\n},\\n\\n\"channel\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"additionalProperties\": false\\n\\n},\\n\\n\"kernel\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"package\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"flavor\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"oneOf\": [\\n\\n{\\n\\n\"type\": \"object\",\\n\\n\"required\": [\\n\\n\"package\"\\n\\n]\\n\\n},\\n\\n{\\n\\n\"type\": \"object\",\\n\\n\"required\": [\\n\\n\"flavor\"\\n\\n]\\n\\n}\\n\\n]\\n\\n},\\n\\n\"keyboard\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"layout\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\\n91\\n\\n\\n-----\\n\\n\"variant\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"toggle\": {\\n\\n\"type\": [\\n\\n\"string\",\\n\\n\"null\"\\n\\n]\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"layout\"\\n\\n],\\n\\n\"additionalProperties\": false\\n\\n},\\n\\n\"source\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"search_drivers\": {\\n\\n\"type\": \"boolean\"\\n\\n},\\n\\n\"id\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"network\": {\\n\\n\"oneOf\": [\\n\\n{',\n",
       " '\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"version\": {\\n\\n\"type\": \"integer\",\\n\\n\"minimum\": 2,\\n\\n\"maximum\": 2\\n\\n},\\n\\n\"ethernets\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"match\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"name\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"macaddress\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"driver\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"additionalProperties\": false\\n\\n}\\n\\n}\\n\\n},\\n\\n\"wifis\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"match\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"name\": {\\n\\n92\\n\\n\\n-----\\n\\n},\\n\\n{\\n\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"macaddress\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"driver\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"additionalProperties\": false\\n\\n}\\n\\n}\\n\\n},\\n\\n\"bridges\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"bonds\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"tunnels\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"vlans\": {\\n\\n\"type\": \"object\"\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"version\"\\n\\n]\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"network\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"version\": {\\n\\n\"type\": \"integer\",\\n\\n\"minimum\": 2,\\n\\n\"maximum\": 2\\n\\n},\\n\\n\"ethernets\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"match\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"name\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"macaddress\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"driver\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"additionalProperties\": false\\n\\n}\\n\\n}\\n\\n},\\n\\n\"wifis\": {\\n\\n\"type\": \"object\",\\n\\n93\\n\\n\\n-----\\n\\n\"properties\": {\\n\\n\"match\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"name\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"macaddress\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"driver\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"additionalProperties\": false\\n\\n}\\n\\n}\\n\\n},\\n\\n\"bridges\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"bonds\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"tunnels\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"vlans\": {\\n\\n\"type\": \"object\"\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"version\"\\n\\n]\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"network\"\\n\\n]\\n\\n}\\n\\n]\\n\\n},\\n\\n\"ubuntu-pro\": {',\n",
       " '\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"token\": {\\n\\n\"type\": \"string\",\\n\\n\"minLength\": 24,\\n\\n\"maxLength\": 30,\\n\\n\"pattern\": \"^C[1-9A-HJ-NP-Za-km-z]+$\",\\n\\n\"description\": \"A valid token starts with a C and is followed by 23 to 29 Base58 characters.\\\\nSee https://pkg\\n\\n}\\n\\n}\\n\\n},\\n\\n\"ubuntu-advantage\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"token\": {\\n\\n\"type\": \"string\",\\n\\n\"minLength\": 24,\\n\\n\"maxLength\": 30,\\n\\n\"pattern\": \"^C[1-9A-HJ-NP-Za-km-z]+$\",\\n\\n\"description\": \"A valid token starts with a C and is followed by 23 to 29 Base58 characters.\\\\nSee https://pkg\\n\\n94\\n\\n\\n-----\\n\\n}\\n\\n},\\n\\n\"deprecated\": true,\\n\\n\"description\": \"Compatibility only - use ubuntu-pro instead\"\\n\\n},\\n\\n\"proxy\": {\\n\\n\"type\": [\\n\\n\"string\",\\n\\n\"null\"\\n\\n],\\n\\n\"format\": \"uri\"\\n\\n},\\n\\n\"apt\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"preserve_sources_list\": {\\n\\n\"type\": \"boolean\"\\n\\n},\\n\\n\"primary\": {\\n\\n\"type\": \"array\"\\n\\n},\\n\\n\"mirror-selection\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"primary\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"anyOf\": [\\n\\n{\\n\\n\"type\": \"string\",\\n\\n\"const\": \"country-mirror\"\\n\\n},\\n\\n{\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"uri\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"arches\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"uri\"\\n\\n]\\n\\n}\\n\\n]\\n\\n}\\n\\n}\\n\\n}\\n\\n},\\n\\n\"geoip\": {\\n\\n\"type\": \"boolean\"\\n\\n},\\n\\n\"sources\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"disable_components\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n95\\n\\n\\n-----\\n\\n\"type\": \"string\",\\n\\n\"enum\": [\\n\\n\"universe\",\\n\\n\"multiverse\",\\n\\n\"restricted\",\\n\\n\"contrib\",\\n\\n\"non-free\"\\n\\n]\\n\\n}\\n\\n},\\n\\n\"preferences\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"package\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"pin\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"pin-priority\": {\\n\\n\"type\": \"integer\"\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"package\",\\n\\n\"pin\",\\n\\n\"pin-priority\"\\n\\n]\\n\\n}',\n",
       " '},\\n\\n\"fallback\": {\\n\\n\"type\": \"string\",\\n\\n\"enum\": [\\n\\n\"abort\",\\n\\n\"continue-anyway\",\\n\\n\"offline-install\"\\n\\n]\\n\\n}\\n\\n}\\n\\n},\\n\\n\"storage\": {\\n\\n\"type\": \"object\"\\n\\n},\\n\\n\"identity\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"realname\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"username\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"hostname\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"password\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"username\",\\n\\n\\n96\\n\\n\\n-----\\n\\n\"hostname\",\\n\\n\"password\"\\n\\n],\\n\\n\"additionalProperties\": false\\n\\n},\\n\\n\"ssh\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"install-server\": {\\n\\n\"type\": \"boolean\"\\n\\n},\\n\\n\"authorized-keys\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n},\\n\\n\"allow-pw\": {\\n\\n\"type\": \"boolean\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"snaps\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"name\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"channel\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"classic\": {\\n\\n\"type\": \"boolean\"\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"name\"\\n\\n],\\n\\n\"additionalProperties\": false\\n\\n}\\n\\n},\\n\\n\"codecs\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"install\": {\\n\\n\"type\": \"boolean\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"drivers\": {\\n\\n\"type\": \"object\",\\n\\n\"properties\": {\\n\\n\"install\": {\\n\\n\"type\": \"boolean\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"timezone\": {\\n\\n\"type\": \"string\"\\n\\n},\\n\\n\"updates\": {\\n\\n\\n97\\n\\n\\n-----\\n\\n\"type\": \"string\",\\n\\n\"enum\": [\\n\\n\"security\",\\n\\n\"all\"\\n\\n]\\n\\n},\\n\\n\"late-commands\": {\\n\\n\"type\": \"array\",\\n\\n\"items\": {\\n\\n\"type\": [\\n\\n\"string\",\\n\\n\"array\"\\n\\n],\\n\\n\"items\": {\\n\\n\"type\": \"string\"\\n\\n}\\n\\n}\\n\\n},\\n\\n\"shutdown\": {\\n\\n\"type\": \"string\",\\n\\n\"enum\": [\\n\\n\"reboot\",\\n\\n\"poweroff\"\\n\\n]\\n\\n}\\n\\n},\\n\\n\"required\": [\\n\\n\"version\"\\n\\n],\\n\\n\"additionalProperties\": true\\n\\n}\\n## **Regeneration**\\n\\nThe schema above can be regenerated by running “make schema” in a Subiquity source checkout.',\n",
       " 'This non-interactive installation uses ‘autoinstall’, which can be considered the successor to the Debian installer (d-i)\\nand preseed on Ubuntu. This is a detailed step-by-step guide, including output and logs (which are partially a bit\\nshortened, as indicated by ‘…’, to limit the size of this document).\\n\\nThe example z/VM guest here uses a direct-access storage device (DASD) and is connected to a regular (non-VLAN)\\nnetwork.\\n\\nFor a zFCP and a VLAN network example, please see the non-interactive IBM LPAR (s390x) installation using\\nautoinstall guide.\\n\\n  - Start with the preparation of the (FTP) install server (if it doesn’t already exist).\\n\\nuser@local:~$ ssh admin@installserver.local\\n\\nadmin@installserver:~$ mkdir -p /srv/ftp/ubuntu-daily-live-server-20.04\\n\\nadmin@installserver:~$ wget http://cdimage.ubuntu.com/ubuntu-server/focal/daily-live/current/focal\\nlive-server-s390x.iso --directory-prefix=/srv/ftp/ubuntu-daily-live-server-20.04\\n\\n--2020-06-26 12:18:48-- http://cdimage.ubuntu.com/ubuntu-server/focal/daily-live/current/focal-live\\nserver-s390x.iso\\n\\nResolving cdimage.ubuntu.com (cdimage.ubuntu.com)... 2001:67c:1560:8001::1d, 2001:67c:1360:8001::28, 2001:67c:1360\\n\\n...\\n\\nConnecting to cdimage.ubuntu.com (cdimage.ubuntu.com)|2001:67c:1560:8001::1d|:80... connected.\\n\\nHTTP request sent, awaiting response... 200 OK\\n\\nLength: 700952576 (668M) [application/x-iso9660-image]\\n\\nSaving to: ‘focal-live-server-s390x.iso’\\n\\nfocal-live-server-s 100%[===================>] 668.48M 2.73MB/s in 4m 54s\\n\\n2020-06-26 12:23:42 (2.27 MB/s) - ‘focal-live-server-s390x.iso’ saved [700952576/700952576]\\n\\nadmin@installserver:~$\\n\\n98\\n\\n\\n-----\\n\\n- The ISO image needs to be extracted now.',\n",
       " 'Since files in the boot folder need to be modified, loopback mount is\\nnot an option here:\\n\\nadmin@installserver:~$ cd /srv/ftp/ubuntu-daily-live-server-20.04\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ mkdir iso\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ sudo mount -o loop ./focal-live-server\\ns390x.iso ./iso\\n\\n[sudo] password for admin:\\n\\nmount: /home/user/iso-test/iso: WARNING: device write-protected, mounted read-only.\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ ls -l\\n\\ntotal 684530\\n\\n-rw-rw-r-- 1 user user 700952576 Jun 26 10:12 focal-live-server-s390x.iso\\n\\ndr-xr-xr-x 10 root root 2048 Jun 26 10:12 iso\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$\\n\\n- Now make sure an FTP server is running in the *installserver* with /srv/ftp as ftp-server root (as used in this\\nexample).\\n\\n- Next, prepare an *autoinstall* (HTTP) server. This hosts the configuration data for the non-interactive installation.\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ mkdir -p /srv/www/autoinstall/zvmguest\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ cd /srv/www/autoinstall/zvmguest\\n\\nadmin@installserver:/srv/www/autoinstall/zvmguest$\\n\\nadmin@installserver:/srv/www/autoinstall/zvmguest$ echo \"instance-id: $(uuidgen || openssl rand  \\nbase64 8)\" > meta-data\\n\\nadmin@installserver:/srv/www/autoinstall/zvmguest$ cat meta-data\\n\\ninstance-id: 2c2215fb-6a38-417f-b72f-376b1cc44f01\\n\\nadmin@installserver:/srv/www/autoinstall/zvmguest$\\n\\nadmin@installserver:/srv/www/autoinstall/zvmguest$ vi user-data\\n\\nadmin@installserver:/srv/www/autoinstall/zvmguest$ cat user-data\\n\\n#cloud-config\\n\\nautoinstall:\\n\\nversion: 1',\n",
       " 'refresh-installer:\\n\\nupdate: yes\\n\\nreporting:\\n\\nbuiltin:\\n\\ntype: print\\n\\napt:\\n\\npreserve_sources_list: false\\n\\nprimary:\\n\\n    - arches: [amd64, i386]\\n\\nuri: http://archive.ubuntu.com/ubuntu\\n\\n    - arches: [default]\\n\\nuri: http://ports.ubuntu.com/ubuntu-ports\\n\\nkeyboard:\\n\\nlayout: en\\n\\nvariant: us\\n\\nlocale: en_US\\n\\nidentity:\\n\\nhostname: zvmguest\\n\\npassword:\\n\\n\"$6$ebJ1f8wxED22bTL4F46P0\"\\n\\nusername: ubuntu\\n\\nuser-data:\\n\\ntimezone: America/Boston\\n\\nusers:\\n\\n      - name: ubuntu\\n\\npassword:\\n\\n\"$6$KwuxED22bTL4F46P0\"\\n\\nlock_passwd: false\\n\\nearly-commands:\\n\\n     - touch /tmp/lets_activate_the_s390x_devices\\n\\n99\\n\\n\\n-----\\n\\n     - chzdev dasd -e 1f00\\n\\n     - touch /tmp/s390x_devices_activation_done\\n\\nnetwork:\\n\\nversion: 2\\n\\nethernets:\\n\\nenc600:\\n\\naddresses: [10.11.12.23/24]\\n\\ngateway4: 10.11.12.1\\n\\nnameservers:\\n\\naddresses: [10.11.12.1]\\n\\nssh:\\n\\ninstall-server: true\\n\\nallow-pw: true\\n\\nauthorized-keys: [\\'ssh-rsa meQwtZ user@workstation # ssh-import-id lp:user\\']\\n\\nadmin@installserver:~$\\n\\n- For s390x installations, the early-commands section is the interesting part:\\n\\nearly-commands:\\n\\n   - touch /tmp/lets_activate_the_s390x_devices\\n\\n   - chzdev dasd -e 1f00\\n\\n   - touch /tmp/s390x_devices_activation_done\\n\\nThe first and last early-commands are optional; they only frame and indicate the real s390x command activation.\\n\\nIn this particular example a single DASD ECKD disk with the address 1f00 is enabled. zFCP disk storage\\ncan be enabled via their host (host-bus-adapters) addresses, for example *e000* ( chzdev zfcp -e e000 ) and *e100*\\n( chzdev zfcp -e e000 ). These have certain Logical Unit Numbers (LUNs) assigned, which are all automatically\\ndiscovered and activated by chzdev zfcp-lun -e --online .',\n",
       " 'Activation of a qeth device would look like this:\\n\\nchzdev qeth -e 0600 .\\n\\n[• For more details about the autoinstall config options, please have a look at the autoinstall reference and autoin-](https://ubuntu.com/server/docs/install/autoinstall-reference)\\n[stall schema page.](https://ubuntu.com/server/docs/install/autoinstall-schema)\\n\\n- Now make sure a HTTP server is running with /srv/www as web-server root (in this particular example).\\n\\n- Log in to your z/VM system using your preferred 3270 client – for example x3270 or c3270.\\n\\n- Transfer the installer kernel, initrd, parmfile and exec file to the z/VM system that is used for the installation.\\nPut these files (for example) on File Mode (Fm) *A* (a.k.a disk *A* ):\\n\\nlistfiles\\n\\nUBUNTU EXEC A1\\n\\nKERNEL UBUNTU A1\\n\\nINITRD UBUNTU A1\\n\\nPARMFILE UBUNTU A1\\n\\n- Now specify the necessary autoinstall parameters in the parmfile:\\n\\nxedit PARMFILE UBUNTU A\\n\\nPARMFILE UBUNTU O1 F 80 Trunc=80 Size=3 Line=0 Col=1 Alt=0\\n\\n00000 * * * Top of File * * *\\n\\n00001 ip=10.11.12.23::10.11.12.1:255.255.255.0:zvmguest:enc600:none:10.11.12.1\\n\\n00002 url=ftp://installserver.local:21/ubuntu-daily-live-server-20.04/focal-li\\n\\nve-ser\\n\\n00003 ver-s390x.iso autoinstall ds=nocloud-net;s=http://installserver.local:80\\n\\n00004 /autoinstall/zvmguest/ --- quiet\\n\\n00005 * * * End of File * * *\\n\\n**Note** :\\nIn case of any issues hitting the 80-character-per-line limit of the file, you can write parameters across two\\nlines as long as there are no unwanted white spaces. To view all 80 characters in one line, disable the prefix\\narea on the left. “prefix off | on” will be your friend – use it in the command area.',\n",
       " \"- You can now start the z/VM installation by executing the UBUNTU REXX script with UBUNTU .\\n\\n- Now monitor the initial program load (IPL) – a.k.a. the boot-up process – of the install system. This is quite\\ncrucial, because during this process a temporary installation password is generated and displayed. The line(s)\\nlook similar to this:\\n\\n100\\n\\n\\n-----\\n\\n...\\n\\n|37.487141| cloud-init¬1873|: Set the following 'random' passwords\\n\\n|37.487176| cloud-init¬1873|: installer: **i7UFdP8fhiVVMme3qqH8**\\n\\n...\\n\\nThis password is needed for remotely connecting to the installer via SSH in the next step.\\n\\n- So, start the REXX script:\\n\\nUBUNTU\\n\\n00: 0000004 FILES PURGED\\n\\n00: RDR FILE 1254 SENT FROM zvmguest PUN WAS 1254 RECS 102K CPY\\n\\n001 A NOHOLD NO\\n\\nKEEP\\n\\n00: RDR FILE 1258 SENT FROM zvmguest PUN WAS 1258 RECS 0003 CPY\\n\\n001 A NOHOLD NO\\n\\nKEEP\\n\\n00: RDR FILE 1262 SENT FROM zvmguest PUN WAS 1262 RECS 303K CPY\\n\\n001 A NOHOLD NO\\n\\nKEEP\\n\\n00: 0000003 FILES CHANGED\\n\\n00: 0000003 FILES CHANGED\\n\\n01: HCPGSP2627I The virtual machine is placed in CP mode due to a SIGP\\n\\ninitial C\\n\\nPU reset from CPU 00.\\n\\n02: HCPGSP2627I The virtual machine is placed in CP mode due to a SIGP\\n\\ninitial C\\n\\nPU reset from CPU 00.\\n\\n03: HCPGSP2627I The virtual machine is placed in CP mode due to a SIGP\\n\\ninitial C\\n\\nPU reset from CPU 00.\\n\\n¬ 0.403380| Initramfs unpacking failed: Decoding failed\\n\\nln: /tmp/mountroot-fail-hooks.d//scripts/init-premount/lvm2: No such file or dir\\n\\nectory\\n\\nQETH device 0.0.0600:0.0.0601:0.0.0602 configured\\n\\nIP-Config: enc600 hardware address 02:28:0b:00:00:51 mtu 1500\\n\\nIP-Config: enc600 guessed broadcast address 10.11.12.255\\n\\nIP-Config: enc600 complete:\",\n",
       " \"address: 10.11.12.23 broadcast: 10.210.210.255 netmask: 255.255.255.0\\n\\ngateway: 10.11.12.1 dns0 : 10.11.12.1 dns1 : 0.0.0.0\\n\\nhost : zvmguest\\n\\nrootserver: 0.0.0.0 rootpath:\\n\\nfilename :\\n\\nConnecting to installserver.local:21 (10.11.12.2:21)\\n\\nfocal-live-server-s3 2% ! ! 15.2M 0:00:48 ETA\\n\\nfocal-live-server-s3 16% !***** ! 126M 0:00:09 ETA\\n\\nfocal-live-server-s3 31% !********** ! 236M 0:00:06 ETA\\n\\nfocal-live-server-s3 46% !************** ! 347M 0:00:04 ETA\\n\\nfocal-live-server-s3 60% !******************* ! 456M 0:00:03 ETA\\n\\nfocal-live-server-s3 74% !*********************** ! 563M 0:00:02 ETA\\n\\nfocal-live-server-s3 88% !**************************** ! 667M 0:00:00 ETA\\n\\nfocal-live-server-s3 100% !********************************! 752M 0:00:00 ETA\\n\\nmount: mounting /cow on /root/cow failed: No such file or directory\\n\\nConnecting to plymouth: Connection refused\\n\\npasswd: password expiry information changed.\\n\\n¬ 16.748137| /dev/loop3: Can't open blockdev\\n\\n¬ 17.908156| systemd¬1|: Failed unmounting /cdrom.\\n\\n¬ ¬0;1;31mFAILED ¬0m| Failed unmounting ¬0;1;39m/cdrom ¬0m.\\n\\n¬ ¬0;32m OK ¬0m| Listening on ¬0;1;39mJournal Socket ¬0m.\\n\\nMounting ¬0;1;39mHuge Pages File System ¬0m...\\n\\nMounting ¬0;1;39mPOSIX Message Queue File System ¬0m...\\n\\nMounting ¬0;1;39mKernel Debug File System ¬0m...\\n\\n101\\n\\n\\n-----\\n\\nStarting ¬0;1;39mJournal Service ¬0m...\\n\\n...\\n\\n[ 61.190916] cloud-init[2076]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 running\\n\\n'modules:final' at Fri, 26 Jun 2020 11:02:01 +0000. Up 61.09 seconds.\\n\\n[ 61.191002] cloud-init[2076]: ci-info: no authorized SSH keys fingerprints fo\\n\\nund for user installer.\\n\\n[ 61.191071] cloud-init[2076]: Cloud-init v.\",\n",
       " \"20.1-10-g71af48df-0ubuntu5 finished at Fri, 26 Jun 2020 11:02:01 +0000\\n\\nDatasource DataSourceNoCloudNet [seed=cmdline,\\n\\n/var/lib/cloud/seed/nocloud,\\n\\nhttp://installserver.local:80/autoinstall/zvmguest/]\\n\\n[dsmode=net]. Up 61.18 seconds\\n\\n[ 61.191136] cloud-init[2076]: Welcome to Ubuntu Server Installer!\\n\\n[ 61.191202] cloud-init[2076]: Above you will find SSH host keys and a random\\n\\npassword set for the `installer` user. You can use these credentials to ssh-in\\n\\nand complete the installation. If you provided SSH keys in the cloud-init datasource,\\n\\nthey were also provisioned to the installer user.\\n\\n[ 61.191273] cloud-init[2076]: If you have access to the graphical console,\\n\\nlike TTY1 or HMC ASCII terminal you can complete the installation there too.\\n\\nIt is possible to connect to the installer over the network, which\\n\\nmight allow the use of a more capable terminal.\\n\\nTo connect, SSH to installer@10.11.12.23.\\n\\nThe password you should use is ''i7UFdP8fhiVVMme3qqH8''.\\n\\nThe host key fingerprints are:\\n\\nRSA SHA256:rBXLeUke3D4gKdsruKEajHjocxc9hr3PI\\n\\nECDSA SHA256:KZZYFswtKxFXWQPuQS9QpOBUoS6RHswis\\n\\nED25519 SHA256:s+5tZfagx0zffC6gYRGW3t1KcBH6f+Vt0\\n\\nUbuntu 20.04 LTS ubuntu-server sclp_line0\\n\\n- At short notice, you can even log in to the system with the user ‘installer’ and the temporary password that was\\ngiven at the end of the boot-up process (see above) of the installation system:\\n\\nuser@workstation:~$ ssh installer@zvmguest\\n\\nThe authenticity of host 'zvmguest (10.11.12.23)' can't be established.\\n\\nECDSA key fingerprint is SHA256:O/dU/D8jJAEGQcbqKGE9La24IRxUPLpzzs5li9F6Vvk.\\n\\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\",\n",
       " \"Warning: Permanently added 'zvmguest,10.11.12.23' (ECDSA) to the list of known hosts.\\n\\ninstaller@zvmguest's password:\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-37-generic s390x)\\n\\n  - Documentation: https://help.ubuntu.com\\n\\n  - Management: https://landscape.canonical.com\\n\\n  - Support: https://ubuntu.com/pro\\n\\nSystem information as of Fri Jun 26 11:08:18 UTC 2020\\n\\nSystem load: 1.25 Memory usage: 4% Processes: 192\\n\\nUsage of /home: unknown Swap usage: 0% Users logged in: 0\\n\\n0 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nThe list of available updates is more than a week old.\\n\\nTo check for new updates run: sudo apt update\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\n102\\n\\n\\n-----\\n\\nindividual files in /usr/share/doc/*/copyright.\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\\n\\napplicable law.\\n\\nthe installer running on /dev/tty1 will perform the autoinstall\\n\\npress enter to start a shell\\n\\n- Please note that it informs you about a currently-running autoinstall process:\\n\\nthe installer running on /dev/tty1 will perform the autoinstall\\n\\n- Nevertheless, we can quickly check some things – although, only until the autoinstall process is finished and the\\npost-install reboot has been triggered:\\n\\nroot@ubuntu-server:/# ls -l /tmp/lets_activate_the_s390x_devices\\n\\n-rw-r--r-- 1 root root 0 Jun 26 11:08 /tmp/lets_activate_the_s390x_devices\\n\\n-rw-r--r-- 1 root root 0 Jun 26 11:09 /tmp/s390x_devices_activation_done\\n\\nroot@ubuntu-server:/# lszdev | grep yes\\n\\ndasd-eckd 0.0.1f00 yes yes\\n\\nqeth 0.0.0600:0.0.0601:0.0.0602 yes no enc600\\n\\nroot@ubuntu-server:/#\",\n",
       " \"- If you wait long enough, you’ll see that the remote session gets closed:\\n\\nroot@ubuntu-server:/# Connection to zvmguest closed by remote host.\\n\\nConnection to zvmguest closed.\\n\\nuser@workstation:~$\\n\\n- As well as at the console:\\n\\nubuntu-server login:\\n\\n[[0;1;31mFAILED[0m] Failed unmounting [0;1;39m/cdrom[0m.\\n\\n[ 169.161139] sd-umoun[15600]: Failed to unmount /oldroot: Device or resource busy\\n\\n[ 169.161550] sd-umoun[15601]: Failed to unmount /oldroot/cdrom: Device or resource busy\\n\\n[ 169.168118] shutdown[1]: Failed to finalize file systems, loop devices, ignoring\\n\\nTotal: 282 Selected: 0\\n\\nCommand:\\n\\n- …and that the post-install reboot got triggered:\\n\\nMessage\\n\\nMounting [0;1;39mKernel Configuration File System[0m...\\n\\nStarting [0;1;39mApply Kernel Variables[0m...\\n\\n[[0;32m OK [0m] Finished [0;1;39mRemount Root and Kernel File Systems[0m.\\n\\n[[0;32m OK [0m] Finished [0;1;39mUncomplicated firewall[0m.\\n\\n[[0;32m OK [0m] Mounted [0;1;39mFUSE Control File System[0m.\\n\\n[[0;32m OK [0m] Mounted [0;1;39mKernel Configuration File System[0m.\\n\\n...\\n\\n[ 35.378928] cloud-init[2565]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 runnin\\n\\ng 'modules:final' at Fri, 26 Jun 2020 11:10:44 +0000. Up 35.29 seconds.\\n\\n[ 35.378978] cloud-init[2565]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 finish\\n\\ned at Fri, 26 Jun 2020 11:10:44 +0000. Datasource DataSourceNone. Up 35.37 seconds\\n\\n[ 35.379008] cloud-init[2565]: 2020-06-26 11:10:44,359 - cc_final_message.py[W\\n\\nARNING]: Used fallback datasource\\n\\n[[0;32m OK [0m] Finished [0;1;39mExecute cloud user/final scripts[0m.\\n\\n[[0;32m OK [0m] Reached target [0;1;39mCloud-init target[0m.\\n\\nzvmguest login:\",\n",
       " '- With the completion of the reboot the autoinstall is finished and the z/VM guest is ready to use:\\n\\nuser@workstation:~$ ssh-keygen -f \"/home/user/.ssh/known_hosts\" -R \"zvmguest\"\\n\\n# Host zvmguest found: line 163\\n\\n/home/user/.ssh/known_hosts updated.\\n\\n103\\n\\n\\n-----\\n\\nOriginal contents retained as /home/user/.ssh/known_hosts.old\\n\\nuser@workstation:~$ ssh ubuntu@zvmguest\\n\\nThe authenticity of host \\'zvmguest (10.11.12.23)\\' can\\'t be established.\\n\\nECDSA key fingerprint is SHA256:iGtCArEg+ZnoZlgtOkvmyy0gPY8UEI+f7zoISOF+m/0.\\n\\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\\n\\nWarning: Permanently added \\'zvmguest,10.11.12.23\\' (ECDSA) to the list of known hosts.\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-39-generic s390x)\\n\\n   - Documentation: https://help.ubuntu.com\\n\\n   - Management: https://landscape.canonical.com\\n\\n   - Support: https://ubuntu.com/pro\\n\\nSystem information as of Fri 26 Jun 2020 11:12:23 AM UTC\\n\\nSystem load: 0.21 Memory usage: 3% Processes: 189\\n\\nUsage of /: 28.2% of 30.88GB Swap usage: 0% Users logged in: 0\\n\\n10 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nTo see these additional updates run: apt list --upgradable\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\nindividual files in /usr/share/doc/*/copyright.\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY,\\n\\nto the extent permitted by applicable law.\\n\\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\\n\\nSee \"man sudo_root\" for details.\\n\\nubuntu@zvmguest:~$ uptime\\n\\n11:12:35 up 2 min, 1 user, load average: 0.18, 0.17, 0.08\\n\\nubuntu@zvmguest:~$ lsb_release -a',\n",
       " 'No LSB modules are available.\\n\\nDistributor ID: Ubuntu\\n\\nDescription: Ubuntu 20.04 LTS\\n\\nRelease: 20.04\\n\\nCodename: focal\\n\\nubuntu@zvmguest:~$ uname -a\\n\\nLinux zvmguest 5.4.0-39-generic #43-Ubuntu SMP Fri Jun 19 10:27:17\\n\\nUTC 2020 s390x s390x s390x\\n\\nGNU/Linux\\n\\nubuntu@zvmguest:~$ lszdev | grep yes\\n\\ndasd-eckd 0.0.1f00 yes yes\\n\\nqeth 0.0.0600:0.0.0601:0.0.0602 yes yes enc600\\n\\nubuntu@zvmguest:~$ exit\\n\\nlogout\\n\\nConnection to zvmguest closed.\\n\\nuser@workstation:~$\\n## **Some closing notes**\\n\\n  - It’s always best to use the latest installer and autoinstall components: either make sure the installer gets updated\\nto the latest level, or just use a current daily live-server image.\\n\\n  - The ISO image specified with the kernel parameters needs to fit in the boot folder. Its kernel and initrd are\\nspecified in the ‘Load from Removable Media and Server’ task at the hardware management console (HMC).\\n\\n  - In addition to activating disk storage resources in early-commands, other devices like OSA/qeth can be added\\nand activated there, too. This is not needed for the basic network device, as specified in the kernel parameters\\nthat are used for the installation (that one is automatically handled).\\n\\n  - If everything is properly set up – FTP server for the image, HTTP server for the autoinstall config files – the\\ninstallation can be as quick as 2 to 3 minutes (depending on the complexity of the autoinstall YAML file).\\n\\n104\\n\\n\\n-----\\n\\n  - There is a simple way of generating a sample autoinstall YAML file: one can perform an interactive Subiquity\\ninstallation, grab the file /var/log/installer/autoinstall-user-data, and use it as an example – but beware',\n",
       " 'that the early-commands entries to activate the s390x-specific devices need to be added manually!\\n\\nThis non-interactive installation uses ‘autoinstall’, which can be considered the successor to the Debian installer (d-i)\\nand preseed on Ubuntu. This is a detailed step-by-step guide, including output and logs (which are partially a bit\\nshortened, as indicated by ‘…’, to limit the size of this document).\\n\\nThe example logical partition (LPAR) here uses zFCP storage and is connected to a VLAN network.\\nFor a DASD and a non-VLAN network example, please see the non-interactive IBM z/VM (s390x) autoinstallation\\nguide.\\n\\n  - Start with the preparation of the (FTP) install server (if it doesn’t already exist).\\n\\nuser@local:~$ ssh admin@installserver.local\\n\\nadmin@installserver:~$ mkdir -p /srv/ftp/ubuntu-daily-live-server-20.04\\n\\nadmin@installserver:~$ wget http://cdimage.ubuntu.com/ubuntu-server/focal/daily-live/current/focal\\nlive-server-s390x.iso --directory-prefix=/srv/ftp/ubuntu-daily-live-server-20.04\\n\\n--2020-06-26 12:18:48-- http://cdimage.ubuntu.com/ubuntu-server/focal/daily-live/current/focal-live\\nserver-s390x.iso\\n\\nResolving cdimage.ubuntu.com (cdimage.ubuntu.com)... 2001:67c:1560:8001::1d, 2001:67c:1360:8001::28, 2001:67c:1360\\n\\n...\\n\\nConnecting to cdimage.ubuntu.com (cdimage.ubuntu.com)|2001:67c:1560:8001::1d|:80... connected.\\n\\nHTTP request sent, awaiting response... 200 OK\\n\\nLength: 700952576 (668M) [application/x-iso9660-image]\\n\\nSaving to: ‘focal-live-server-s390x.iso’\\n\\nfocal-live-server-s 100%[===================>] 668.48M 2.73MB/s in 4m 54s\\n\\n2020-06-26 12:23:42 (2.27 MB/s) - ‘focal-live-server-s390x.iso’ saved [700952576/700952576]\\n\\nadmin@installserver:~$',\n",
       " '  - The ISO image needs to be extracted now. Since files in its boot folder need to be modified, loopback mount is\\nnot an option here:\\n\\nadmin@installserver:~$ cd /srv/ftp/ubuntu-daily-live-server-20.04\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ mkdir iso\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ sudo mount -o loop ./focal-live-server\\ns390x.iso ./iso\\n\\n[sudo] password for admin:\\n\\nmount: /home/user/iso-test/iso: WARNING: device write-protected, mounted read-only.\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ ls -l\\n\\ntotal 684530\\n\\n-rw-rw-r-- 1 user user 700952576 Jun 26 10:12 focal-live-server-s390x.iso\\n\\ndr-xr-xr-x 10 root root 2048 Jun 26 10:12 iso\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ rsync -rtvz ./iso/ . && sync\\n\\nsending incremental file list\\n\\nskipping non-regular file \"ubuntu\"\\n\\nskipping non-regular file \"ubuntu-ports\"\\n\\n./\\n\\nREADME.diskdefines\\n\\nboot.catalog\\n\\nmd5sum.txt\\n\\nubuntu.ins\\n\\nskipping non-regular file \"dists/stable\"\\n\\nskipping non-regular file \"dists/unstable\"\\n\\n.disk/\\n\\n.disk/base_installable\\n\\n.disk/casper-uuid-generic\\n\\n.disk/cd_type\\n\\n.disk/info\\n\\nboot/\\n\\nboot/README.boot\\n\\nboot/initrd.off\\n\\nboot/initrd.siz\\n\\nboot/initrd.ubuntu\\n\\n105\\n\\n\\n-----\\n\\nboot/kernel.ubuntu\\n\\nboot/parmfile.ubuntu\\n\\nboot/ubuntu.exec\\n\\nboot/ubuntu.ikr\\n\\nboot/ubuntu.ins\\n\\ncasper/\\n\\n...\\n\\nsent 681,509,758 bytes received 1,857 bytes 22,344,643.11 bytes/sec\\n\\ntotal size is 700,317,941 speedup is 1.03\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ ls -l\\n\\ntotal 684578\\n\\ndr-xr-xr-x 2 user user 4096 Jun 26 10:12 boot\\n\\n-r--r--r-- 1 user user 2048 Jun 26 10:12 boot.catalog\\n\\ndr-xr-xr-x 3 user user 4096 Jun 26 10:12 casper',\n",
       " 'dr-xr-xr-x 3 user user 4096 Jun 26 10:11 dists\\n\\n-rw-rw-r-- 1 user user 700952576 Jun 26 10:12 focal-live-server-s390x.iso\\n\\ndr-xr-xr-x 2 user user 4096 Jun 26 10:11 install\\n\\ndr-xr-xr-x 10 root root 2048 Jun 26 10:12 iso\\n\\n-r--r--r-- 1 user user 4944 Jun 26 10:12 md5sum.txt\\n\\ndr-xr-xr-x 2 user user 4096 Jun 26 10:11 pics\\n\\ndr-xr-xr-x 3 user user 4096 Jun 26 10:11 pool\\n\\ndr-xr-xr-x 2 user user 4096 Jun 26 10:11 preseed\\n\\n-r--r--r-- 1 user user 236 Jun 26 10:11 README.diskdefines\\n\\n-r--r--r-- 1 user user 185 Jun 26 10:12 ubuntu.ins\\n\\n- Now create ins and parm files dedicated to the LPAR that will be installed (here zlinlpar ), based on the default\\nins and parm files that are shipped with the ISO image:\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ chmod -R +rw ./boot\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ cp ./boot/ubuntu.ins ./boot/ubuntu_zlinlpar.ins\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ cp ./boot/parmfile.ubuntu ./boot/parmfile.zlinlpar\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ vi ./boot/ubuntu_zlinlpar.ins\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ cat ./boot/ubuntu_zlinlpar.ins\\n\\n  - Ubuntu for z Series (default kernel)\\n\\nkernel.ubuntu 0x00000000\\n\\ninitrd.off 0x0001040c\\n\\ninitrd.siz 0x00010414\\n\\nparmfile.zlinlpar 0x00010480\\n\\ninitrd.ubuntu 0x01000000\\n\\nadmin@installserver:~$\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ vi ./boot/parmfile.zlinlpar\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ cat ./boot/parmfile.zlinlpar',\n",
       " 'ip=10.11.12.42::10.11.12.1:255.255.255.0:zlinlpar:encc000.4711:none:10.11.12.1 vlan=encc000.4711:encc000 url=http\\n\\ndaily-live-server-20.04/focal-live-server-s390x.iso autoinstall ds=nocloud-net;s=http://installserver.local:80/au\\n\\n-- quiet\\n\\n- Now make sure an FTP server is running in the *installserver* with /srv/ftp as ftp-server root (as used in this\\nexample).\\n\\n- Now prepare an *autoinstall* (HTTP) server, which hosts the configuration data for the non-interactive installation.\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ mkdir -p /srv/www/autoinstall/zlinlpar\\n\\nadmin@installserver:/srv/ftp/ubuntu-daily-live-server-20.04$ cd /srv/www/autoinstall/zlinlpar\\n\\nadmin@installserver:/srv/www/autoinstall/zlinlpar$\\n\\nadmin@installserver:/srv/www/autoinstall/zlinlpar$ echo \"instance-id: $(uuidgen || openssl rand  \\nbase64 8)\" > meta-data\\n\\nadmin@installserver:/srv/www/autoinstall/zlinlpar$ cat meta-data\\n\\ninstance-id: 2c2215fb-6a38-417f-b72f-376b1cc44f01\\n\\nadmin@installserver:/srv/www/autoinstall/zlinlpar$\\n\\nadmin@installserver:/srv/www/autoinstall/zlinlpar$ vi user-data\\n\\nadmin@installserver:/srv/www/autoinstall/zlinlpar$ cat user-data\\n\\n#cloud-config\\n\\n106\\n\\n\\n-----\\n\\nautoinstall:\\n\\nversion: 1\\n\\nrefresh-installer:\\n\\nupdate: yes\\n\\nreporting:\\n\\nbuiltin:\\n\\ntype: print\\n\\napt:\\n\\npreserve_sources_list: false\\n\\nprimary:\\n\\n    - arches: [amd64, i386]\\n\\nuri: http://archive.ubuntu.com/ubuntu\\n\\n    - arches: [default]\\n\\nuri: http://ports.ubuntu.com/ubuntu-ports\\n\\nkeyboard:\\n\\nlayout: en\\n\\nvariant: us\\n\\nlocale: en_US\\n\\nidentity:\\n\\nhostname: zlinlpar\\n\\npassword:\\n\\n\"$6$ebJ1f8wxED22bTL4F46P0\"\\n\\nusername: ubuntu\\n\\nuser-data:\\n\\ntimezone: America/Boston\\n\\nusers:\\n\\n      - name: ubuntu\\n\\npassword:\\n\\n\"$6$KwuxED22bTL4F46P0\"',\n",
       " \"lock_passwd: false\\n\\nearly-commands:\\n\\n     - touch /tmp/lets_activate_the_s390x_devices\\n\\n     - chzdev zfcp -e e000\\n\\n     - chzdev zfcp -e e100\\n\\n     - chzdev zfcp-lun -e --online\\n\\n     - touch /tmp/s390x_devices_activation_done\\n\\nnetwork:\\n\\nethernets:\\n\\nencc000: {}\\n\\nversion: 2\\n\\nvlans:\\n\\nencc000.4711:\\n\\naddresses: [10.11.12.42/24]\\n\\ngateway4: 10.11.12.1\\n\\nid: 4711\\n\\nlink: encc000\\n\\nnameservers:\\n\\naddresses: [10.11.12.1]\\n\\nssh:\\n\\ninstall-server: true\\n\\nallow-pw: true\\n\\nauthorized-keys: ['ssh-rsa meQwtZ user@workstation # ssh-import-id lp:user']\\n\\nadmin@installserver:~$\\n\\n- For s390x installations the early-commands section is the interesting part:\\n\\nearly-commands:\\n\\n   - touch /tmp/lets_activate_the_s390x_devices\\n\\n   - chzdev zfcp -e e000\\n\\n   - chzdev zfcp -e e100\\n\\n   - chzdev zfcp-lun -e --online\\n\\n   - touch /tmp/s390x_devices_activation_done\\n\\nThe first and last early-commands are optional; they only frame and indicate the real s390x command activation.\\n\\n107\\n\\n\\n-----\\n\\nIn this particular example, two zFCP hosts (host-bus-adapters) are enabled via their addresses *e000* ( chzdev\\n\\nzfcp -e e000 ) and *e100* ( chzdev zfcp -e e000 ). These have certain logical unit numbers (LUNs) assigned that\\nare all automatically discovered and activated by chzdev zfcp-lun -e --online .\\n\\nActivation of a direct-access storage device (DASD) would look like this: chzdev dasd -e 1f00, and a qeth device\\nactivation looks like: chzdev qeth -e c000 .\\n\\n**See also** :\\n[For more details about the autoinstall config options, please have a look at the autoinstall reference and](https://ubuntu.com/server/docs/install/autoinstall-reference)\\n[autoinstall schema pages.](https://ubuntu.com/server/docs/install/autoinstall-schema)\",\n",
       " \"- Now make sure a HTTP server is running with /srv/www as web-server root (in this particular example).\\n\\n- Next steps need to be done at the hardware management console (HMC). First, connect to the HMC and proceed\\nwith the ‘Load From Removable Media and Server’ task.\\n\\n- Then, start the ‘Load from Removable Media or Server’ task under ‘Recovery’ --> ‘Load from Removable Media\\nor Server’ on your specific LPAR that you are going to install, and fill out the following fields (the contents will\\nbe of course different on your system):\\n\\nFTP Source\\n\\nHost computer: installserver.local\\n\\nUser ID: ftpuser\\n\\nPassword: ********\\n\\nAccount (optional):\\n\\nFile location (optional): ubuntu-daily-live-server-20.04/boot\\n\\n- Now confirm the entered data and click ‘OK’.\\n\\n- At the ‘Load from Removable Media or Server - Select Software to Install’ screen, choose the LPAR that is going\\nto be installed, here:\\n\\nubuntu-daily-live-server-20.04/boot/ubuntu_zlinlpar.ins Ubuntu for z Series (default kernel)\\n\\n- Confirm again with ‘OK’.\\n\\n- And another confirmation about the ‘Load will cause jobs to be cancelled’.\\n\\n- Then, another ‘Yes’ – understanding that it’s a disruptive task:\\n\\nDisruptive Task Confirmation : Load from Removable Media or Server\\n\\n- Now monitor the ‘Load from Removable media or Server Progress’ screen and confirm it once again when the\\nstatus changes from ‘Please wait while the image is being loaded.’ to ‘Success’.\\n\\n- Then navigate to ‘Daily’ --> ‘Operating System Messages’ to monitor the initial program load (IPL) of the\\ninstall system …\\n\\nMessage\\n\\nchzdev: Unknown device type or device ID format: c000.4711\\n\\nUse 'chzdev --help' for more information\",\n",
       " \"QETH device 0.0.c000:0.0.c001:0.0.c002 configured\\n\\nIP-Config: encc000.4711 hardware address 1a:3c:99:55:2a:ef mtu 1500\\n\\nIP-Config: encc000.4711 guessed broadcast address 10.11.12.255\\n\\nIP-Config: encc000.4711 complete:\\n\\naddress: 10.11.12.42 broadcast: 10.11.12.255 netmask: 255.255.255.0\\n\\ngateway: 10.11.12.1 dns0 : 10.11.12.1 dns1 : 0.0.0.0\\n\\nhost : zlinlpar\\n\\nrootserver: 0.0.0.0 rootpath:\\n\\nfilename :\\n\\nConnecting to installserver.local:80 (installserver.local:80)\\n\\nfocal-live-server-s3 5% |* | 39.9M 0:00:15 ETA\\n\\nfocal-live-server-s3 22% |******* | 147M 0:00:07 ETA\\n\\nfocal-live-server-s3 38% |************ | 254M 0:00:04 ETA\\n\\nfocal-live-server-s3 53% |***************** | 355M 0:00:03 ETA\\n\\nfocal-live-server-s3 67% |********************* | 453M 0:00:02 ETA\\n\\nfocal-live-server-s3 81% |************************** | 545M 0:00:01 ETA\\n\\nfocal-live-server-s3 94% |***************************** | 633M 0:00:00 ETA\\n\\nfocal-live-server-s3 100% |******************************| 668M 0:00:00 ETA\\n\\n108\\n\\n\\n-----\\n\\nmount: mounting /cow on /root/cow failed: No such file or directory\\n\\nConnecting to plymouth: Connection refused\\n\\npasswd: password expiry information changed.\\n\\nUsing CD-ROM mount point /cdrom/\\n\\nIdentifying... [5d25356068b713167814807dd678c261-2]\\n\\nScanning disc for index files...\\n\\nFound 2 package indexes, 0 source indexes, 0 translation indexes and 1 signature\\n\\nFound label 'Ubuntu-Server 20.04 LTS _Focal Fossa_ - Release s390x (20200616)'\\n\\nThis disc is called:\\n\\n'Ubuntu-Server 20.04 LTS _Focal Fossa_ - Release s390x (20200616)'\\n\\n...\\n\\n[ 61.190916] cloud-init[2076]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 running\\n\\n'modules:final' at Fri, 26 Jun 2020 11:02:01 +0000. Up 61.09 seconds.\",\n",
       " \"[ 61.191002] cloud-init[2076]: ci-info: no authorized SSH keys fingerprints fo\\n\\nund for user installer.\\n\\n[ 61.191071] cloud-init[2076]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 finished at Fri, 26 Jun 2020 11:02:01 +0000\\n\\nDatasource DataSourceNoCloudNet [seed=cmdline,\\n\\n/var/lib/cloud/seed/nocloud,\\n\\nhttp://installserver.local:80/autoinstall/zlinlpar/]\\n\\n[dsmode=net]. Up 61.18 seconds\\n\\n[ 61.191136] cloud-init[2076]: Welcome to Ubuntu Server Installer!\\n\\n[ 61.191202] cloud-init[2076]: Above you will find SSH host keys and a random\\n\\npassword set for the `installer` user. You can use these credentials to ssh-in\\n\\nand complete the installation. If you provided SSH keys in the cloud-init datasource,\\n\\nthey were also provisioned to the installer user.\\n\\n[ 61.191273] cloud-init[2076]: If you have access to the graphical console,\\n\\nlike TTY1 or HMC ASCII terminal you can complete the installation there too.\\n\\nIt is possible to connect to the installer over the network, which\\n\\nmight allow the use of a more capable terminal.\\n\\nTo connect, SSH to installer@10.11.12.42.\\n\\nThe password you should use is ''i7UFdP8fhiVVMme3qqH8''.\\n\\nThe host key fingerprints are:\\n\\nRSA SHA256:rBXLeUke3D4gKdsruKEajHjocxc9hr3PI\\n\\nECDSA SHA256:KZZYFswtKxFXWQPuQS9QpOBUoS6RHswis\\n\\nED25519 SHA256:s+5tZfagx0zffC6gYRGW3t1KcBH6f+Vt0\\n\\nUbuntu 20.04 LTS ubuntu-server sclp_line0\\n\\n- At short notice, you can even log in to the system with the user ‘installer’ and the temporary password that was\\ngiven at the end of the boot-up process (see above) of the installation system:\\n\\nuser@workstation:~$ ssh installer@zlinlpar\\n\\nThe authenticity of host 'zlinlpar (10.11.12.42)' can't be established.\",\n",
       " \"ECDSA key fingerprint is SHA256:O/dU/D8jJAEGQcbqKGE9La24IRxUPLpzzs5li9F6Vvk.\\n\\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\\n\\nWarning: Permanently added 'zlinlpar,10.11.12.42' (ECDSA) to the list of known hosts.\\n\\ninstaller@zlinlpar's password:\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-37-generic s390x)\\n\\n  - Documentation: https://help.ubuntu.com\\n\\n  - Management: https://landscape.canonical.com\\n\\n  - Support: https://ubuntu.com/pro\\n\\nSystem information as of Fri Jun 26 11:08:18 UTC 2020\\n\\nSystem load: 1.25 Memory usage: 4% Processes: 192\\n\\nUsage of /home: unknown Swap usage: 0% Users logged in: 0\\n\\n0 updates can be installed immediately.\\n\\n109\\n\\n\\n-----\\n\\n0 of these updates are security updates.\\n\\nThe list of available updates is more than a week old.\\n\\nTo check for new updates run: sudo apt update\\n\\nThe programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\nindividual files in /usr/share/doc/*/copyright.\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\\n\\napplicable law.\\n\\nthe installer running on /dev/tty1 will perform the autoinstall\\n\\npress enter to start a shell\\n\\n- Notice that it informs you about a currently-running autoinstall process:\\n\\nthe installer running on /dev/tty1 will perform the autoinstall\\n\\n- Nevertheless, we can quickly check some things – though only until the autoinstall process has finished and the\\npost-install reboot has been triggered:\\n\\nroot@ubuntu-server:/# ls -l /tmp/lets_activate_the_s390x_devices\\n\\n-rw-r--r-- 1 root root 0 Jun 26 11:08 /tmp/lets_activate_the_s390x_devices\",\n",
       " '-rw-r--r-- 1 root root 0 Jun 26 11:09 /tmp/s390x_devices_activation_done\\n\\nroot@ubuntu-server:/# lszdev | grep yes\\n\\nzfcp-host 0.0.e000 yes yes\\n\\nzfcp-host 0.0.e100 yes yes\\n\\nzfcp-lun 0.0.e000:0x50050763060b16b6:0x4026400200000000 yes yes sdb sg1\\n\\nzfcp-lun 0.0.e000:0x50050763061b16b6:0x4026400200000000 yes yes sda sg0\\n\\nzfcp-lun 0.0.e100:0x50050763060b16b6:0x4026400200000000 yes yes sdd sg3\\n\\nzfcp-lun 0.0.e100:0x50050763061b16b6:0x4026400200000000 yes yes sdc sg2\\n\\nqeth 0.0.c000:0.0.c001:0.0.c002 yes no encc000\\n\\nroot@ubuntu-server:/#\\n\\n- If you wait long enough you’ll see the remote session get closed:\\n\\nroot@ubuntu-server:/# Connection to zlinlpar closed by remote host.\\n\\nConnection to zlinlpar closed.\\n\\nuser@workstation:~$\\n\\n- As well as at the console:\\n\\nubuntu-server login:\\n\\n[[0;1;31mFAILED[0m] Failed unmounting [0;1;39m/cdrom[0m.\\n\\n[ 169.161139] sd-umoun[15600]: Failed to unmount /oldroot: Device or resource busy\\n\\n[ 169.161550] sd-umoun[15601]: Failed to unmount /oldroot/cdrom: Device or resource busy\\n\\n[ 169.168118] shutdown[1]: Failed to finalize file systems, loop devices, ignoring\\n\\nTotal: 282 Selected: 0\\n\\nCommand:\\n\\n- …and that the post-install reboot gets triggered:\\n\\nMessage\\n\\nMounting [0;1;39mKernel Configuration File System[0m...\\n\\nStarting [0;1;39mApply Kernel Variables[0m...\\n\\n[[0;32m OK [0m] Finished [0;1;39mRemount Root and Kernel File Systems[0m.\\n\\n[[0;32m OK [0m] Finished [0;1;39mUncomplicated firewall[0m.\\n\\n[[0;32m OK [0m] Mounted [0;1;39mFUSE Control File System[0m.\\n\\n[[0;32m OK [0m] Mounted [0;1;39mKernel Configuration File System[0m.\\n\\n...\\n\\n[ 35.378928] cloud-init[2565]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 runnin\\n\\n110\\n\\n\\n-----',\n",
       " 'g \\'modules:final\\' at Fri, 26 Jun 2020 11:10:44 +0000. Up 35.29 seconds.\\n\\n[ 35.378978] cloud-init[2565]: Cloud-init v. 20.1-10-g71af48df-0ubuntu5 finish\\n\\ned at Fri, 26 Jun 2020 11:10:44 +0000. Datasource DataSourceNone. Up 35.37 seconds\\n\\n[ 35.379008] cloud-init[2565]: 2020-06-26 11:10:44,359 - cc_final_message.py[W\\n\\nARNING]: Used fallback datasource\\n\\n[[0;32m OK [0m] Finished [0;1;39mExecute cloud user/final scripts[0m.\\n\\n[[0;32m OK [0m] Reached target [0;1;39mCloud-init target[0m.\\n\\nzlinlpar login:\\n\\n- With the completion of the reboot, the autoinstall is finished and the LPAR is ready to use:\\n\\nuser@workstation:~$ ssh-keygen -f \"/home/user/.ssh/known_hosts\" -R \"zlinlpar\"\\n\\n# Host zlinlpar found: line 163\\n\\n/home/user/.ssh/known_hosts updated.\\n\\nOriginal contents retained as /home/user/.ssh/known_hosts.old\\n\\nuser@workstation:~$ ssh ubuntu@zlinlpar\\n\\nThe authenticity of host \\'zlinlpar (10.11.12.42)\\' can\\'t be established.\\n\\nECDSA key fingerprint is SHA256:iGtCArEg+ZnoZlgtOkvmyy0gPY8UEI+f7zoISOF+m/0.\\n\\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\\n\\nWarning: Permanently added \\'zlinlpar,10.11.12.42\\' (ECDSA) to the list of known hosts.\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-39-generic s390x)\\n\\n  - Documentation: https://help.ubuntu.com\\n\\n  - Management: https://landscape.canonical.com\\n\\n  - Support: https://ubuntu.com/pro\\n\\nSystem information as of Fri 26 Jun 2020 11:12:23 AM UTC\\n\\nSystem load: 0.21 Memory usage: 3% Processes: 189\\n\\nUsage of /: 28.2% of 30.88GB Swap usage: 0% Users logged in: 0\\n\\n10 updates can be installed immediately.\\n\\n0 of these updates are security updates.\\n\\nTo see these additional updates run: apt list --upgradable',\n",
       " 'The programs included with the Ubuntu system are free software;\\n\\nthe exact distribution terms for each program are described in the\\n\\nindividual files in /usr/share/doc/*/copyright.\\n\\nUbuntu comes with ABSOLUTELY NO WARRANTY,\\n\\nto the extent permitted by applicable law.\\n\\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\\n\\nSee \"man sudo_root\" for details.\\n\\nubuntu@zlinlpar:~$ uptime\\n\\n11:12:35 up 2 min, 1 user, load average: 0.18, 0.17, 0.08\\n\\nubuntu@zlinlpar:~$ lsb_release -a\\n\\nNo LSB modules are available.\\n\\nDistributor ID: Ubuntu\\n\\nDescription: Ubuntu 20.04 LTS\\n\\nRelease: 20.04\\n\\nCodename: focal\\n\\nubuntu@zlinlpar:~$ uname -a\\n\\nLinux zlinlpar 5.4.0-39-generic #43-Ubuntu SMP Fri Jun 19 10:27:17\\n\\nUTC 2020 s390x s390x s390x\\n\\nGNU/Linux\\n\\nubuntu@zlinlpar:~$ lszdev | grep yes\\n\\nzfcp-host 0.0.e000 yes yes\\n\\nzfcp-host 0.0.e100 yes yes\\n\\nzfcp-lun 0.0.e000:0x50050763060b16b6:0x4026400200000000 yes yes\\n\\nsdb sg1\\n\\nzfcp-lun 0.0.e000:0x50050763061b16b6:0x4026400200000000 yes yes\\n\\n111\\n\\n\\n-----\\n\\nsda sg0\\n\\nzfcp-lun 0.0.e100:0x50050763060b16b6:0x4026400200000000 yes yes\\n\\nsdc sg2\\n\\nzfcp-lun 0.0.e100:0x50050763061b16b6:0x4026400200000000 yes yes\\n\\nsdd sg3\\n\\nqeth 0.0.c000:0.0.c001:0.0.c002 yes yes encc000\\n\\nubuntu@zlinlpar:~$ exit\\n\\nlogout\\n\\nConnection to zlinlpar closed.\\n\\nuser@workstation:~$\\n## **Some closing notes**\\n\\n  - It’s always best to use the latest installer and autoinstall components. Be sure to update the installer to the\\nmost recent version, or just use a current daily live-server image.\\n\\n  - The ISO image specified with the kernel parameters needs to fit in the boot folder. Its kernel and initrd are',\n",
       " 'specified in the ‘Load from Removable Media and Server’ task at the hardware management console (HMC).\\n\\n  - In addition to activating disk storage resources in early-commands, other devices like OSA/qeth can be added\\nand activated there, too. This is not needed for the basic network device, as specified in the kernel parameters\\nused for the installation (that one is automatically handled).\\n\\n  - If everything is properly set up – FTP server for the image, HTTP server for the autoinstall config files – the\\ninstallation can be as quick as 2 to 3 minutes. Of course this depends on the complexity of the autoinstall YAML\\nfile.\\n\\n  - There is a simple way of generating a sample autoinstall YAML file; one can perform an interactive Subiquity\\ninstallation, grab the file /var/log/installer/autoinstall-user-data, and use it as an example – but beware\\nthat the early-commands entries to activate the s390x-specific devices need to be added manually!\\n## **What are ROCKs?**\\n\\nOrdinary software packages can often be installed in a variety of different types of environments that satisfy the given\\npackaging system. However, these environments can be quite varied, such as including versions of language runtimes,\\nsystem libraries, and other library dependencies that the software was not well tested with.\\n\\nSoftware containers address this by encapsulating both the software and the surrounding environment. Instead of\\ninstalling and maintaining a collection of software packages, the user runs and maintains a single container, instantiated\\nfrom a container image with the desired software already installed. The user relies on the provider of the container',\n",
       " \"image to perform the necessary software testing and maintenance updates. There is a rich ecosystem of container\\nproviders thanks to mainstream tools like Docker, and popular container registries like Docker Hub, Amazon ECR,\\netc., which make it easy for anyone to build and publish a container image. Unfortunately, with that freedom and\\nflexibility invariably comes unreliability of maintenance and inconsistency of implementation.\\n\\nThe *Open Container Initiative* (OCI) establishes standards for constructing container images that can be reliably\\ninstalled across a variety of compliant host environments.\\n\\n[Ubuntu’s LTS Docker Image Portfolio provides OCI-compliant images that receive stable security updates and pre-](https://ubuntu.com/security/docker-images)\\ndictable software updates, thus ensuring consistency in both maintenance schedule and operational interfaces for the\\nunderlying software your software builds on.\\n## **Container Creation and Deletion**\\n\\nOver the course of this tutorial we’ll explore deriving a customized Apache container, and then networking in a Postgres\\ncontainer backend for it. By the end you’ll have a working knowledge of how to set up a container-based environment\\nusing Canonical’s ROCKs.\\n\\nFirst the absolute basics. Let’s spin up a single container providing the Apache2 web server software:\\n\\n$ sudo apt-get update\\n\\n$ sudo apt-get -y install docker.io\\n\\n$ sudo docker run -d --name my-apache2-container -p 8080:80 ubuntu/apache2:2.4-22.04_beta\\n\\nUnable to find image 'ubuntu/apache2:2.4-22.04_beta' locally\\n\\n2.4-22.04_beta: Pulling from ubuntu/apache2\\n\\n13c61b50dd15: Pull complete\\n\\n34dadde438e6: Pull complete\\n\\nd8e11cec95e6: Pull complete\\n\\n112\\n\\n\\n-----\",\n",
       " 'Digest: sha256:11647ce68a130540150dfebbb755ee79c908103fafbf805074eb6513e6b9df83\\n\\nStatus: Downloaded newer image for ubuntu/apache2:2.4-22.04_beta\\n\\n4031e6ed24a6e08185efd1c60e7df50f8f60c21ed9961c858ca0cb6bb300a72a\\n\\nThis container, named my-apache2-container runs in an Ubuntu 22.04 LTS environment and can be accessed via local\\nport 8080. Load the website up in your local web browser:\\n\\n$ firefox http://localhost:8080\\n\\nIf you don’t have firefox handy, curl can be used instead:\\n\\n$ curl -s http://localhost:8080 | grep \"<title>\"\\n\\n<title>Apache2 Ubuntu Default Page: It works</title>\\n\\n[The run command had a number of parameters to it. The Usage section of Ubuntu’s Docker hub page for Apache2 has](https://hub.docker.com/r/ubuntu/apache2)\\n[a table with an overview of parameters specific to the image, and Docker itself has a formal reference of all available](https://docs.docker.com/engine/reference/commandline/run/)\\nparameters, but lets go over what we’re doing in this particular case:\\n\\n$ sudo docker run -d --name my-apache2-container -e TZ=UTC -p 8080:80 ubuntu/apache2:2.4-22.04_beta\\n\\nThe -d parameter causes the container to be detached so it runs in the background. If you omit this, then you’ll want\\nto use a different terminal window for interacting with the container. The --name parameter allows you to use a defined\\nname; if it’s omitted you can still reference the container by its Docker id. The -e option lets you set environment\\nvariables used when creating the container; in this case we’re just setting the timezone ( TZ ) to universal time ( UTC ).\\nThe -p parameter allows us to map port 80 of the container to 8080 on localhost, so we can reference the service as',\n",
       " 'http://localhost:8080 . The last parameter indicates what software image we want.\\n\\n[A variety of other container images are provided on Ubuntu’s Docker Hub and on Amazon ECR, including documen-](https://hub.docker.com/r/ubuntu/)\\ntation of supported customization parameters and debugging tips. This lists the different major/minor versions of\\neach piece of software, packaged on top of different Ubuntu LTS releases. So for example, in specifying our requested\\nimage as ubuntu/apache2:2.4-22.04_beta we used Apache2 version 2.4 running on a Ubuntu 22.04 environment.\\n\\nNotice that the image version we requested has _beta appended to it. This is called a *Channel Tag* . Like most software,\\nApache2 provides incremental releases numbered like 2.4.51, 2.4.52, and 2.4.53. Some of these releases are strictly\\nbugfix-only, or even just CVE security fixes; others may include new features or other improvements. If we think of\\nthe series of these incremental releases for Apache2 2.4 on Ubuntu 22.04 as running in a *Channel*, the *Channel Tags*\\npoint to the newest incremental release that’s been confirmed to the given level of stability. So, if a new incremental\\nrelease 2.4.54 becomes available, ubuntu/apache2:2.4-22.04_edge images would be updated to that version rapidly,\\nthen ubuntu/apache2:2.4-22.04_beta once it’s received some basic testing; eventually, if no problems are found, it\\nwill also be available in ubuntu/apache2:2.4-22.04_candidate and then in ubuntu/apache2:2.4-22.04_stable once it’s\\nvalidated as completely safe.\\n\\n113\\n\\n\\n-----\\n\\nFor convenience there’s also a latest tag and an edge tag which are handy for experimentation or where you don’t',\n",
       " 'care what version is used and just want the newest available. For example, to launch the latest version of Nginx, we\\ncan do so as before, but specifying latest instead of the version:\\n\\n$ sudo docker run -d --name my-nginx-container -e TZ=UTC -p 9080:80 ubuntu/nginx:latest\\n\\n4dac8d77645d7ed695bdcbeb3409a8eda942393067dad49e4ef3b8b1bdc5d584\\n\\n$ curl -s http://localhost:9080 | grep \"<title>\"\\n\\n<title>Welcome to nginx!</title>\\n\\nWe’ve also changed the port to 9080 instead of 8080 using the -p parameter, since port 8080 is still being used by\\nour apache2 container. If we were to try to also launch Nginx (or another Apache2 container) on port 8080, we’d get\\nan error message, Bind for 0.0.0.0:8080 failed: port is already allocated and then would need to remove the\\ncontainer and try again.\\n\\nSpeaking of removing containers, now that we know how to create generic default containers, let’s clean up:\\n\\n$ sudo docker ps\\n\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\n\\nd86e93c98e20 ubuntu/apache2:2.4-22.04_beta \"apache2-foreground\" 29 minutes ago Up 29 minutes 0.0.0.0:8080\\n>80/tcp, :::8080->80/tcp my-apache2-container\\n\\need23be5f65d ubuntu/nginx:latest \"/docker-entrypoint.…\" 18 minutes ago Up 18 minutes 0.0.0.0:9080\\n>80/tcp, :::9080->80/tcp my-nginx-container\\n\\n$ sudo docker stop my-apache2-container\\n\\n$ sudo docker rm my-apache2-container\\n\\n$ sudo docker stop my-nginx-container\\n\\n$ sudo docker rm my-nginx-container\\n\\nTo be able to actually use the containers, we’ll have to configure and customize them, which we’ll look at next.\\n\\nIn the last section we looked at the basics of how to start and stop containers. Here we’ll apply our own modifications\\nto the images.',\n",
       " 'You’ll recall we used the -p parameter to give the two containers we created different ports so they didn’t conflict\\nwith each other. We can think of this type of customization as a *container configuration*, as opposed to an *image*\\n*configuration* change defined in the Dockerfile settings for the image itself. From a single image definition we can\\ncreate an arbitrary number of different containers with different ports (or other pre-defined aspects), which are all\\notherwise reliably identical. A third approach is to modify the running container after it has been launched, applying\\nwhatever arbitrary changes we wish as *runtime modifications* .\\n\\n - **image configuration** : Done in Dockerfile, changes common to all container instances of that image. Requires\\nrebuilding the image.\\n\\n - **container configuration** : Done at container launch, allowing variation between instances of a given image.\\nRequires re-launching the container to change.\\n\\n - **runtime modifications** : Done dynamically after container launch. Does not require re-launching the container.\\n\\nThe second approach follows Docker’s immutable infrastructure principle, and is what the ROCKs system intends for\\nproduction environments. For the sake of this tutorial we’ll use the third approach for introductory purposes, building\\non that later to show how to achieve the same with only configuration at container creation time.\\n## **Setting up a Development Environment**\\n\\nSpeaking of doing things properly, let’s prepare a virtual machine (VM) to do our tutorial work in.\\n\\nWhile you can of course install the docker.io package directly on your desktop, as you may have done in the previous',\n",
       " 'section of this tutorial, using it inside a VM has a few advantages. First, it encapsulates the system changes you want\\nto experiment with, so that they don’t affect your desktop; if anything gets seriously messed up you just can delete\\nthe VM and start over. Second, it facilitates experimenting with different versions of Ubuntu, Docker, or other tools\\nthan would be available from your desktop. Third, since “The Cloud” is built with VM’s, developing in a VM from\\nthe start lets you more closely emulate likely types of environments you’ll be deploying to.\\n\\nThere are a number of different VM technologies available, any of which will suit our purposes, but for this tutorial\\n[we’ll set one up using Canonical’s Multipass software, which you can install on Windows using a downloadable installer,](https://multipass.run/docs/installing-on-windows)\\n[or on macOS via brew, or any flavor of Linux via snapd.](https://multipass.run/docs/installing-on-macos)\\n\\nHere’s how to launch a Ubuntu 22.04 VM with a bit of extra resources, and log in:\\n\\n114\\n\\n\\n-----\\n\\nhost> multipass launch --cpus 2 --mem 4G --disk 10G --name my-vm daily:20.04\\n\\nhost> multipass shell my-vm\\n\\nIf later you wish to suspend or restart the VM, use the stop/start commands:\\n\\nhost> multipass stop my-vm\\n\\nhost> multipass start my-vm\\n\\nGo ahead and set up your new VM devel environment with Docker, your preferred editor, and any other tools you\\nlike having on hand:\\n\\n$ sudo apt-get update\\n\\n$ sudo apt-get -y install docker.io\\n## **Data Customization**\\n\\nThe most basic customization for a webserver would be the index page. Let’s replace the default one with the typical\\nhello world example:',\n",
       " '$ echo \\'<html><title>Hello Docker...</title><body>Hello Docker!</body></html>\\' > index.html\\n\\nThe technique we’ll use to load this into the webserver container is called *bind mounting a volume*, and this is done\\nwith the -v (or --volume ) flag to docker run (not to be confused with docker -v which of course just prints the docker\\nversion). A *volume* is a file or directory tree or other data on the host we wish to provide via the container. A *bind*\\n*mount* means rather than copying the data *into* the container, we establish a linkage between the local file and the file\\nin the container. Have a look at how this works:\\n\\n$ sudo docker run -d --name my-apache2-container -e TZ=UTC -p 8080:80 -v \"${HOME}/index.html:/var/www/html/index.html\" ubu\\n\\n...\\n\\n$ curl http://localhost:8080\\n\\n<html><title>Hello Docker...</title></html>\\n\\n$ sudo docker inspect -f \"{{ .Mounts }}\" my-apache2-container\\n\\n[{bind /home/ubuntu/index.html /var/www/html/index.html true rprivate}]\\n\\nWatch what happens when we change the index.html contents:\\n\\n$ echo \\'<html><title>...good day!</title></html>\\' > index.html\\n\\n$ curl http://localhost:8080\\n\\n<html><title>...good day</title></html>\\n\\nThis linkage is two-way, which means that the container itself can change the data. (We mentioned *runtime modifica-*\\n*tions* earlier – this would be an example of doing that.)\\n\\n$ sudo docker exec -ti my-apache2-container /bin/bash\\n\\nroot@abcd12345678:/# echo \\'<html><title>Hello, again</title></html>\\' > /var/www/html/index.html\\n\\nroot@abcd12345678:/# exit\\n\\nexit\\n\\n$ curl http://localhost:8080\\n\\n<html><title>Hello, again</title></html>\\n\\nWhat if we don’t want that behavior, and don’t want to grant the container the ability to do so?',\n",
       " 'We can set the bind\\nmount to be read-only by appending :ro :\\n\\n$ sudo docker stop my-apache2-container\\n\\n$ sudo docker rm my-apache2-container\\n\\n$ sudo docker run -d --name my-apache2-container -e TZ=UTC -p 8080:80 -v ${HOME}/index.html:/var/www/html/index.html:ro ub\\n\\n$ sudo docker exec -ti my-apache2-container /bin/bash\\n\\nroot@abcd12345678:/# echo \\'<html><title>good day, sir!</title></html>\\' > /var/www/html/index.html\\n\\nbash: /var/www/html/index.html: Read-only file system\\n\\nroot@abcd12345678:/# exit\\n\\n$ curl http://localhost:8080\\n\\n115\\n\\n\\n-----\\n\\n<html><title>Hello, again</title></html>\\n\\nHowever, the read-only mount still sees changes on the host side:\\n\\n$ echo \\'<html><title>I said good day!</title></html>\\' > ./index.html\\n\\n$ curl http://localhost:8080\\n\\n<html><title>I said good day!</title></html>\\n\\nThis same approach can be used to seed database containers:\\n\\n$ echo \\'CREATE DATABASE my_db;\\' > my-database.sql\\n\\n$ sudo docker run -d --name my-database -e TZ=UTC \\\\\\n\\n-e POSTGRES_PASSWORD=mysecret \\\\\\n\\n-v $(pwd)/my-database.sql:/docker-entrypoint-initdb.d/my-database.sql:ro \\\\\\n\\nubuntu/postgres:latest\\n\\nThe docker-entrypoint-initdb.d/ directory we’re using here is special in that files ending in the .sql extension (or\\n\\n.sql.gz or .sql.xz ) will be executed to the database on container initialization. Bash scripts ( .sh ) can also be placed\\nin this directory to perform other initialization steps.\\n\\nLet’s verify the database’s creation:\\n\\n$ sudo docker exec -ti my-database su postgres --command \"psql my_db --command \\'SELECT * FROM pg_database WHERE datistempla\\n\\noid | datname | datdba | encoding | datcollate | datctype | datistemplate | datallowconn | datconnlimit | datlastsysoid |',\n",
       " \"------+----------+--------+----------+------------+------------+---------------+--------------+-----------\\n--+---------------+--------------+------------+---------------+-------\\n                                                      13761 | postgres | 10 | 6 | en_US.utf8 | en_US.utf8 | f | t |\\n\\n1 | 13760 | 727 | 1 | 1663 |\\n\\n                                                      16384 | my_db | 10 | 6 | en_US.utf8 | en_US.utf8 | f | t |\\n\\n1 | 13760 | 727 | 1 | 1663 |\\n\\n(2 rows)\\n## **Debugging Techniques**\\n\\nMost containers are configured to make pertinent status information (such as their error log) visible through Docker’s\\n\\nlogs command:\\n\\n$ sudo docker logs my-apache2-container\\n\\nAH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerN\\n\\n...\\n\\nSometimes this isn’t sufficient to diagnose a problem. In the previous example we shelled into our container to\\nexperiment with, via:\\n\\n$ sudo docker exec -it my-apache2-container /bin/bash\\n\\nroot@abcd12345678:/# cat /proc/cmdline\\n\\nBOOT_IMAGE=/boot/vmlinuz-5.15.0-25-generic root=LABEL=cloudimg-rootfs ro console=tty1 console=ttyS0\\n\\nThis places you inside a bash shell inside the container; commands you issue will be executed within the scope of the\\ncontainer. While tinkering around inside the container isn’t suitable for normal production operations, it can be a\\nhandy way to debug problems such as if you need to examine logs or system settings. For example, if you’re trying to\\nexamine the network:\\n\\nroot@abcd12345678:/# apt-get update && apt-get install -y iputils-ping iproute2\\n\\nroot@abcd12345678:/# ip addr | grep inet\\n\\ninet 127.0.0.1/8 scope host lo\",\n",
       " 'inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0\\n\\nroot@abcd12345678:/# ping my-apache2-container\\n\\nping: my-apache2-container: Name or service not known\\n\\nroot@abcd12345678:/# ping -c1 172.17.0.1 | tail -n2\\n\\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\\n\\nrtt min/avg/max/mdev = 0.194/0.194/0.194/0.000 ms\\n\\nroot@abcd12345678:/# ping -c1 172.17.0.2 | tail -n2\\n\\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\\n\\nrtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms\\n\\nroot@abcd12345678:/# ping -c1 172.17.0.3 | tail -n2\\n\\n1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms\\n\\n116\\n\\n\\n-----\\n\\nWe won’t use this container any further, so can remove it:\\n\\n$ sudo docker stop my-apache2-container\\n\\n$ sudo docker rm my-apache2-container\\n## **Network**\\n\\nIP addresses may be suitable for debugging purposes, but as we move beyond individual containers we’ll want to refer\\nto them by network hostnames. First we create the network itself:\\n\\n$ sudo docker network create my-network\\n\\nc1507bc90cfb6100fe0e696986eb99afe64985c7c4ea44ad319f8080e640616b\\n\\n$ sudo docker network list\\n\\nNETWORK ID NAME DRIVER SCOPE\\n\\n7e9ce8e7c0fd bridge bridge local\\n\\n6566772ff02f host host local\\n\\nc1507bc90cfb my-network bridge local\\n\\n8b992742eb38 none null local\\n\\nNow when creating containers we can attach them to this network:\\n\\n$ sudo docker run -d --name my-container-0 --network my-network ubuntu/apache2:latest\\n\\n$ sudo docker run -d --name my-container-1 --network my-network ubuntu/apache2:latest\\n\\n$ sudo docker exec -it my-container-0 /bin/bash\\n\\nroot@abcd12345678:/# apt-get update && apt-get install -y iputils-ping bind9-dnsutils',\n",
       " 'root@abcd12345678:/# ping my-container-1 -c 1| grep statistics -A1\\n\\n--- my-container-1 ping statistics --\\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\\n\\nroot@abcd12345678:/# dig +short my-container-0 my-container-1\\n\\n172.18.0.2\\n\\n172.18.0.3\\n\\nroot@abcd12345678:/# exit\\n\\n$ sudo docker stop my-container-0 my-container-1\\n\\n$ sudo docker rm my-container-0 my-container-1\\n\\nA common use case for networked containers is load balancing. Docker’s --network-alias option provides one means\\nof setting up *round-robin* load balancing at the network level during container creation:\\n\\n$ sudo docker run -d --name my-container-0 --network my-network --network-alias my-website -e TZ=UTC \\np 8080:80 -v ${HOME}/index.html:/var/www/html/index.html:ro ubuntu/apache2:latest\\n\\n$ sudo docker run -d --name my-container-1 --network my-network --network-alias my-website -e TZ=UTC \\np 8081:80 -v ${HOME}/index.html:/var/www/html/index.html:ro ubuntu/apache2:latest\\n\\n$ sudo docker run -d --name my-container-2 --network my-network --network-alias my-website -e TZ=UTC \\np 8082:80 -v ${HOME}/index.html:/var/www/html/index.html:ro ubuntu/apache2:latest\\n\\n$ sudo docker ps\\n\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\n\\n665cf336ba9c ubuntu/apache2:latest \"apache2-foreground\" 4 days ago Up 4 days 0.0.0.0:8082\\n>80/tcp, :::8082->80/tcp my-container-2\\n\\nfd952342b6f8 ubuntu/apache2:latest \"apache2-foreground\" 4 days ago Up 4 days 0.0.0.0:8081\\n>80/tcp, :::8081->80/tcp my-container-1\\n\\n0592e413e81d ubuntu/apache2:latest \"apache2-foreground\" 4 days ago Up 4 days 0.0.0.0:8080\\n>80/tcp, :::8080->80/tcp my-container-0',\n",
       " 'The my-website alias selects a different container for each request it handles, allowing load to be distributed across all\\nof them.\\n\\n$ sudo docker exec -it my-container-0 /bin/bash\\n\\nroot@abcd12345678:/# apt update; apt install -y bind9-dnsutils\\n\\nroot@abcd12345678:/# dig +short my-website\\n\\n172.18.0.3\\n\\n172.18.0.2\\n\\n117\\n\\n\\n-----\\n\\n172.18.0.4\\n\\nRun that command several times, and the output should display in a different order each time.\\n\\nroot@abcd12345678:/# dig +short my-website\\n\\n172.18.0.3\\n\\n172.18.0.4\\n\\n172.18.0.2\\n\\nroot@abcd12345678:/# dig +short my-website\\n\\n172.18.0.2\\n\\n172.18.0.3\\n\\n172.18.0.4\\n\\nroot@abcd12345678:/# exit\\n\\n$ sudo docker stop my-container-0 my-container-1 my-container-2\\n\\n$ sudo docker rm my-container-0 my-container-1 my-container-2\\n## **Installing Software**\\n\\nBy default Apache2 can serve static pages, but for more than that it’s necessary to enable one or more of its modules.\\nAs we mentioned above, there are three approaches you could take: Set things up at runtime by logging into the\\ncontainer and running commands directly; configuring the container at creation time; or, customizing the image\\ndefinition itself.\\n\\nIdeally, we’d use the second approach to pass a parameter or setup.sh script to install software and run a2enmod <mod>,\\nhowever the Apache2 image lacks the equivalent of Postgres’ /docker-entrypoint-initdb.d/ directory and automatic\\nprocessing of shell scripts. So for a production system you’d need to derive your own customized Apache2 image and\\nbuild containers from that.\\n\\nFor the purposes of this tutorial, though, we can use the runtime configuration approach just for experimental purposes.\\n\\nFirst, create our own config file that enables CGI support:',\n",
       " '$ cat > ~/my-apache2.conf << \\'EOF\\'\\n\\nUser ${APACHE_RUN_USER}\\n\\nGroup ${APACHE_RUN_GROUP}\\n\\nErrorLog ${APACHE_LOG_DIR}/error.log\\n\\nServerName localhost\\n\\nHostnameLookups Off\\n\\nLogLevel warn\\n\\nListen 80\\n\\n# Include module configuration:\\n\\nIncludeOptional mods-enabled/*.load\\n\\nIncludeOptional mods-enabled/*.conf\\n\\n<Directory />\\n\\nAllowOverride None\\n\\nRequire all denied\\n\\n</Directory>\\n\\n<Directory /var/www/html/>\\n\\nAllowOverride None\\n\\nRequire all granted\\n\\n</Directory>\\n\\n<Directory /var/www/cgi-bin/>\\n\\nAddHandler cgi-script .cgi\\n\\nAllowOverride None\\n\\nOptions +ExecCGI -MultiViews\\n\\nRequire all granted\\n\\n</Directory>\\n\\n<VirtualHost *:80>\\n\\nDocumentRoot /var/www/html/\\n\\nScriptAlias /cgi-bin/ /var/www/cgi-bin/\\n\\n118\\n\\n\\n-----\\n\\n</VirtualHost>\\n\\nEOF\\n\\nNext, copy the following into a file named fortune.cgi .\\n\\n$ cat > ~/fortune.cgi << \\'EOF\\'\\n\\n#!/usr/bin/env bash\\n\\necho -n -e \"Content-Type: text/plain\\\\n\\\\n\"\\n\\necho \"Hello ${REMOTE_ADDR}, I am $(hostname -f) at ${SERVER_ADDR}\"\\n\\necho \"Today is $(date)\"\\n\\nif [ -x /usr/games/fortune ]; then\\n\\n/usr/games/fortune\\n\\nfi\\n\\nEOF\\n\\n$ chmod a+x ~/fortune.cgi\\n\\nNow create our container:\\n\\n$ sudo docker run -d --name my-fortune-cgi -e TZ=UTC -p 9080:80 \\\\\\n\\n-v $(pwd)/my-apache2.conf:/etc/apache2/apache2.conf:ro \\\\\\n\\n-v $(pwd)/fortune.cgi:/var/www/cgi-bin/fortune.cgi:ro \\\\\\n\\nubuntu/apache2:latest\\n\\nc3709dc03f24fbf862a8d9499a03015ef7ccb5e76fdea0dc4ac62a4c853597bf\\n\\nNext, perform the runtime configuration steps:\\n\\n$ sudo docker exec -it my-fortune-cgi /bin/bash\\n\\nroot@abcd12345678:/# apt-get update && apt-get install -y fortune\\n\\nroot@abcd12345678:/# a2enmod cgid\\n\\nroot@abcd12345678:/# service apache2 force-reload\\n\\nFinally, restart the container so our changes take effect:',\n",
       " '$ sudo docker restart my-fortune-cgi\\n\\nmy-fortune-cgi\\n\\nLet’s test it out:\\n\\n$ curl http://localhost:9080/cgi-bin/fortune.cgi\\n\\nHello 172.17.0.1, I am 8ace48b71de7 at 172.17.0.2\\n\\nToday is Wed Jun 1 16:59:40 UTC 2022\\n\\nQ: Why is Christmas just like a day at the office?\\n\\nA: You do all of the work and the fat guy in the suit\\n\\ngets all the credit.\\n\\nFinally is cleanup, if desired:\\n\\n$ sudo docker stop my-fortune-cgi\\n\\n$ sudo docker rm my-fortune-cgi\\n## **Next**\\n\\nWhile it’s interesting to be able to customize a basic container, how can we do this without resorting to runtime\\nconfiguration? As well, a single container by itself is not terrible useful, so in the next section we’ll practice setting\\nup a database node to serve data to our webserver.\\n\\nThe prior section explained the use of a single container for running a single software instance, but the principle benefit\\nof using ROCKs is the ability to easily create and architecturally organize, or “orchestrate”, them to operate together\\nin a modular fashion.\\n\\nIf you set up a VM while following that section, you can continue to use that here, or if not feel free to create a new\\nVM for this section, using those same directions.\\n\\n**Colors Web App**\\n\\nThis section will demonstrate use of docker-compose to set up two nodes that inter-operate to implement a trivial CGI\\nweb app that lets the user select a background color from the standard rgb.txt color codes. Here’s the table definition\\nitself:\\n\\n119\\n\\n\\n-----\\n\\n$ cat > ~/my-color-database.sql <<\\'EOF\\'\\n\\nCREATE DATABASE my_color_db;\\n\\nCREATE TABLE \"color\"\\n\\n(\\n\\nid INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\\n\\nred INTEGER,\\n\\ngreen INTEGER,\\n\\nblue INTEGER,\\n\\ncolorname VARCHAR NOT NULL\\n\\n);',\n",
       " 'REVOKE ALL ON \"color\" FROM public;\\n\\nGRANT SELECT ON \"color\" TO \"postgres\";\\n\\nEOF\\n\\nFor the data, we’ll scarf up X11’s rgb.txt file, which should be readily at hand with most Ubuntu desktop installations:\\n\\n$ sudo apt-get install x11-common\\n\\n$ grep -v ^! /usr/share/X11/rgb.txt | \\\\\\n\\nawk \\'BEGIN{print \"INSERT INTO color(red, green, blue, colorname) VALUES\"}\\n\\n$1 != $2 || $2 != $3 {\\n\\nprintf(\" (%d, %d, %d, \\'\\\\\\'\\'\", $1, $2, $3);\\n\\nfor (i = 4; i <= NF; i++) {\\n\\nprintf(\"%s\", $i);\\n\\n}\\n\\nprintf(\"\\'\\\\\\'\\'),\\\\n\");\\n\\n}\\n\\nEND {print \" (0, 0, 0, \\'\\\\\\'\\'black\\'\\\\\\'\\');\"}\\' >> ~/my-color-database.sql\\n\\nHere’s the corresponding CGI script:\\n\\n$ cat > ~/my-colors.cgi <<\\'EOF\\'\\n\\n#!/usr/bin/env python3\\n\\nimport cgi\\n\\nimport psycopg2\\n\\n# Get web form data (if any)\\n\\nquery_form = cgi.FieldStorage()\\n\\nif \\'bgcolor\\' in query_form.keys():\\n\\nbgcolor = query_form[\"bgcolor\"].value\\n\\nelse:\\n\\nbgcolor = \\'FFFFFF\\'\\n\\nprint(\"Content-Type: text/html\\\\n\\\\n\");\\n\\n# Head\\n\\nbody_style = \"body { background-color: #%s; }\" %(bgcolor)\\n\\ntext_style = \".color-invert { filter: invert(1); mix-blend-mode: difference; }\"\\n\\nprint(f\"<html>\\\\n<head><style>\\\\n{body_style}\\\\n{text_style}\\\\n</style></head>\\\\n\")\\n\\nprint(\"<body>\\\\n<h1 class=\\\\\"color-invert\\\\\">Pick a background color:</h1>\\\\n\")\\n\\nprint(\"<table width=\\\\\"500\\\\\" cellspacing=\\\\\"0\\\\\" cellpadding=\\\\\"0\\\\\">\\\\n\")\\n\\nprint(\" <tr><th width=\\\\\"50\\\\\">Color</th><th>Name</th><th width=\\\\\"100\\\\\">Code</th></tr>\\\\n\")\\n\\n# Connect database\\n\\ndb = psycopg2.connect(host=\\'examples_postgres_1\\', user=\\'postgres\\', password=\\'myS&cret\\')\\n\\n# Display the colors\\n\\ncolors = db.cursor()\\n\\ncolors.execute(\"SELECT * FROM color;\")\\n\\nfor row in colors.fetchall():\\n\\ncode = \\'\\'.join(\\'{:02X}\\'.format(a) for a in row[1:4])\\n\\ncolor = row[4]\\n\\n120\\n\\n\\n-----',\n",
       " 'print(f\" <tr style=\\\\\"background-color:#{code}\\\\\">\\\\n\")\\n\\nprint(f\" <td><a href=\\\\\"my-colors.cgi?bgcolor={code}\\\\\">{color}</td>\\\\n\")\\n\\nprint(f\" <td>{code}</td></tr>\\\\n\")\\n\\n# Foot\\n\\nprint(\"</table>\\\\n\")\\n\\nprint(\"</body>\\\\n</html>\\\\n\")\\n\\nEOF\\n\\nBy default, Apache2 is configured to allow CGI scripts in the /usr/lib/cgi-bin system directory, but rather than\\ninstalling the script there, let’s use our own directory to serve from:\\n\\n$ cat > ~/my-apache.conf <<\\'EOF\\'\\n\\nUser ${APACHE_RUN_USER}\\n\\nGroup ${APACHE_RUN_GROUP}\\n\\nErrorLog ${APACHE_LOG_DIR}/error.log\\n\\nServerName localhost\\n\\nHostnameLookups Off\\n\\nLogLevel warn\\n\\nListen 80\\n\\n# Include module configuration:\\n\\nIncludeOptional mods-enabled/*.load\\n\\nIncludeOptional mods-enabled/*.conf\\n\\n<Directory />\\n\\nAllowOverride None\\n\\nRequire all denied\\n\\n</Directory>\\n\\n<Directory /var/www/html/>\\n\\nAllowOverride None\\n\\nRequire all granted\\n\\n</Directory>\\n\\n<Directory /var/www/cgi-bin/>\\n\\nAddHandler cgi-script .cgi\\n\\nAllowOverride None\\n\\nOptions +ExecCGI -MultiViews\\n\\nRequire all granted\\n\\n</Directory>\\n\\n<VirtualHost *:80>\\n\\nDocumentRoot /var/www/html/\\n\\nScriptAlias /cgi-bin/ /var/www/cgi-bin/\\n\\n</VirtualHost>\\n\\nEOF\\n\\n**Install Docker Compose**\\n\\nWith our web app developed, we’re ready to containerize it. We’ll install Docker Compose, pull in the two base images\\nfor the database and web server, and create our own containers with our web app files and configuration layered on\\ntop.\\n\\nFirst, install what we’ll need:\\n\\n$ sudo apt-get update\\n\\n$ sudo apt-get install -y docker.io docker-compose\\n\\n**Create Database Container**\\n\\nNext, prepare the Postgres container. Each of Ubuntu’s Docker Images has a git repository, referenced from the\\nrespective Docker Hub page.',\n",
       " \"These repositories include some example content that we can build from:\\n\\n$ git clone https://git.launchpad.net/~canonical-server/ubuntu-docker-images/+git/postgresql my-postgresql\\noci\\n\\n121\\n\\n\\n-----\\n\\n$ cd my-postgresql-oci/\\n\\n$ git checkout origin/14-22.04 -b my-postgresql-oci-branch\\n\\n$ find ./examples/ -type f\\n\\n./examples/README.md\\n\\n./examples/postgres-deployment.yml\\n\\n./examples/docker-compose.yml\\n\\n./examples/config/postgresql.conf\\n\\nNotice the two YAML files. The docker-compose.yml file lets us create a derivative container where we can insert our\\nown customizations such as config changes and our own SQL data to instantiate our database. (The other YAML file\\nis for Kubernetes-based deployments.)\\n\\n$ mv -iv ~/my-color-database.sql ./examples/\\n\\nrenamed '/home/ubuntu/my-color-database.sql' -> './examples/my-color-database.sql'\\n\\n$ git add ./examples/my-color-database.sql\\n\\nModify the services section of the file examples/docker-compose.yml to look like this:\\n\\nservices:\\n\\npostgres:\\n\\nimage: ubuntu/postgres:14-22.04_beta\\n\\nports:\\n\\n      - 5432:5432\\n\\nenvironment:\\n\\n      - POSTGRES_PASSWORD=myS&cret\\n\\nvolumes:\\n\\n      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf:ro\\n\\n      - ./my-color-database.sql:/docker-entrypoint-initdb.d/my-color-database.sql:ro\\n\\nThe volumes section of the file lets us bind files from our local git repository into our new container. Things like the\\n\\npostgresql.conf configuration file get installed to the normal system as you’d expect.\\n\\nBut the /docker-entrypoint-initdb.d/ directory will look unusual – this is a special directory provided by Ubuntu’s\",\n",
       " 'Postgres Docker container that will automatically run .sql (or .sql.gz or .sql.xz ) and .sh files through the psql\\ninterpreter during initialization, in POSIX alphanumerical order. In our case we have a single .sql file that we want\\ninvoked during initialization.\\n\\nUbuntu’s ROCKs are also built with environment variables to customize behavior; above we can see where we can\\nspecify our own password.\\n\\nCommit everything so far to our branch:\\n\\n$ git commit -a -m \"Add a color database definition\"\\n\\n[my-postgresql-oci-branch 0edeb20] Add a color database definition\\n\\n2 files changed, 549 insertions(+)\\n\\ncreate mode 100644 examples/my-color-database.sql\\n\\nNow we’re ready to create and start our application’s database container:\\n\\n$ cd ./examples/\\n\\n$ sudo docker-compose up -d\\n\\nPulling postgres (ubuntu/postgres:edge)...\\n\\n...\\n\\nCreating examples_postgres_1 ... done\\n\\n$ sudo docker-compose logs\\n\\n...\\n\\npostgres_1 | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/my-color-database.sql\\n\\n...\\n\\npostgres_1 | 2022-06-02 03:14:28.040 UTC [1] LOG: database system is ready to accept connections\\n\\nThe -d flag causes the container to run in the background (you might omit it if you want to run it in its own window\\nso you can watch the service log info live.)\\n\\nNote that if there is an error, such as a typo in your .sql file, you can’t just re-run docker-compose up (or restart )\\nbecause it’ll attempt to re-attach and may appear successful at first glance:\\n\\n...\\n\\npostgres_1 | psql:/docker-entrypoint-initdb.d/my-color-database.sql:10: ERROR: type \"sometypo\" does not exist\\n\\npostgres_1 | LINE 3: \"id\" SOMETYPO,\\n\\n122\\n\\n\\n-----\\n\\npostgres_1 | ^\\n\\nexamples_postgres_1 exited with code 3',\n",
       " \"$ sudo docker-compose up\\n\\nStarting examples_postgres_1 ... done\\n\\nAttaching to examples_postgres_1\\n\\npostgres_1 |\\n\\npostgres_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization\\n\\n...\\n\\npostgres_1 | 2022-06-02 04:00:51.400 UTC [25] LOG: database system was not properly shut down; automatic recovery in prog\\n\\n...\\n\\npostgres_1 | 2022-06-02 04:00:51.437 UTC [1] LOG: database system is ready to accept connections\\n\\nHowever, while there is a live database, our data didn’t load into it so it is invalid.\\n\\nInstead, always issue a down command before attempting a restart when fixing issues:\\n\\n$ sudo docker-compose down; sudo docker-compose up\\n\\n...\\n\\nNote that in our environment docker-compose needs to be run with root permissions; if it isn’t, you may see an error\\nsimilar to this:\\n\\nERROR: Couldn't connect to Docker daemon at http+docker://localhost - is it running?\\n\\nIf it's at a non-standard location, specify the URL with the DOCKER_HOST environment variable.\\n\\nAt this point we could move on to the webserver container, but we can double-check our work so far by installing the\\nPostgres client locally in the VM and running a sample query:\\n\\n$ sudo apt-get install postgresql-client\\n\\n$ psql -h localhost -U postgres\\n\\nPassword for user postgres: myS&cret\\n\\npostgres=# \\\\d\\n\\nList of relations\\n\\nSchema | Name | Type | Owner\\n\\n--------+--------------+----------+---------\\npublic | color | table | postgres\\n\\npublic | color_id_seq | sequence | postgres\\n\\n(2 rows)\\n\\npostgres=# SELECT * FROM color WHERE id<4;\\n\\nid | red | green | blue | colorname\\n\\n----+-----+-------+------+-----------\\n1 | 255 | 250 | 250 | snow\\n\\n2 | 248 | 248 | 255 | ghostwhite\\n\\n3 | 248 | 248 | 255 | GhostWhite\",\n",
       " '(3 rows)\\n\\n**Create Webserver Docker Container**\\n\\nNow we do the same thing for the Apache2 webserver.\\n\\nGet the example files from Canonical’s Apache2 image repository via git:\\n\\n$ cd ~\\n\\n$ git clone https://git.launchpad.net/~canonical-server/ubuntu-docker-images/+git/apache2 my-apache2-oci\\n\\n$ cd my-apache2-oci/\\n\\n$ git checkout origin/2.4-22.04 -b my-apache2-oci-branch\\n\\n$ find ./examples/ -type f\\n\\n./examples/apache2-deployment.yml\\n\\n./examples/README.md\\n\\n./examples/docker-compose.yml\\n\\n./examples/config/apache2.conf\\n\\n./examples/config/html/index.html\\n\\n$ mv -ivf ~/my-apache2.conf ./examples/config/apache2.conf\\n\\nrenamed \\'/home/ubuntu/my-apache2.conf\\' -> \\'./examples/config/apache2.conf\\'\\n\\n$ mv -iv ~/my-colors.cgi ./examples/\\n\\n123\\n\\n\\n-----\\n\\nrenamed \\'/home/ubuntu/my-colors.cgi\\' -> \\'examples/my-colors.cgi\\'\\n\\n$ chmod a+x ./examples/my-colors.cgi\\n\\n$ git add ./examples/config/apache2.conf ./examples/my-colors.cgi\\n\\nModify the examples/docker-compose.yml file to look like this:\\n\\nversion: \\'2\\'\\n\\nservices:\\n\\napache2:\\n\\nimage: ubuntu/apache2:2.4-22.04_beta\\n\\nports:\\n\\n      - 8080:80\\n\\nvolumes:\\n\\n      - ./config/apache2.conf:/etc/apache2/apache2.conf:ro\\n\\n      - ./config/html:/srv/www/html/index.html:ro\\n\\n      - ./my-colors.cgi:/var/www/cgi-bin/my-colors.cgi:ro\\n\\ncommand: bash -c \"apt-get update && apt-get -y install python3 python3-psycopg2; a2enmod cgid; apache2\\nforeground\"\\n\\nrestart: always\\n\\nCommit everything to the branch:\\n\\n$ git commit -a -m \"Add a color CGI web application\"\\n\\nNow launch the web server container:\\n\\n$ cd ./examples/\\n\\n$ sudo docker-compose up -d\\n\\nYou will now be able to connect to the service:\\n\\n$ firefox http://localhost:8080/cgi-bin/my-colors.cgi?bgcolor=FFDEAD\\n\\n124\\n\\n\\n-----',\n",
       " 'Click on one of the colors to see the background color change:\\n\\n125\\n\\n\\n-----\\n\\nOnce you’re done, if you wish you can cleanup the containers as before, or if you used Multipass you can shutdown\\nand delete the VM:\\n\\n$ exit\\n\\nhost> multipass stop my-vm\\n\\nhost> multipass delete my-vm\\n\\n**Next Steps**\\n\\nAs you can see, docker-compose makes it convenient to set up multi-container applications without needing to perform\\nruntime changes to the containers. As you can imagine, this can permit building a more sophisticated management\\nsystem to handle fail-over, load-balancing, scaling, upgrading old nodes, and monitoring status. But rather than\\nneeding to implement all of this directly on top of docker-container, you can next investigate Kubernetes-style cluster\\n[management software such as microk8s.](https://microk8s.io/docs)\\n\\nUbuntu features a comprehensive package management system for installing, upgrading, configuring, and removing\\nsoftware. In addition to providing access to an organized base of over 60,000 software packages for your Ubuntu\\ncomputer, the package management facilities also feature dependency resolution capabilities and software update\\nchecking.\\n\\nSeveral tools are available for interacting with Ubuntu’s package management system, from simple command-line\\nutilities that can be easily automated by system administrators, to an easy-to-use graphical interface for those new to\\nUbuntu.\\n## **Introduction**\\n\\nUbuntu’s package management system is derived from the same system used by the Debian GNU/Linux distribution.\\nThe package files contain all of the necessary files, metadata, and instructions to implement a particular functionality',\n",
       " 'or software application on your Ubuntu computer.\\n\\nDebian package files typically have the extension .deb, and usually exist in *repositories* which are collections of packages\\nfound online or on physical media, such as CD-ROM discs. Packages are normally in a pre-compiled binary format;\\nthus installation is quick and requires no compiling of software.\\n\\n126\\n\\n\\n-----\\n\\nMany packages use *dependencies* . Dependencies are additional packages required by the principal package in order to\\nfunction properly. For example, the speech synthesis package festival depends upon the package alsa-utils, which\\n[is a package supplying the Advanced Linux Sound Architecture (ALSA) sound library tools needed for audio playback.](https://www.alsa-project.org/wiki/Main_Page)\\nIn order for festival to function, it – and all of its dependencies – must be installed. The software management tools\\nin Ubuntu will do this automatically.\\n## **Advanced Packaging Tool – APT**\\n\\nThe apt command is a powerful command-line tool, which works with Ubuntu’s Advanced Packaging Tool (APT). The\\ncommands contained within apt provide the means for installing new software packages, upgrading existing software\\npackages, updating the package list index, and even upgrading the entire Ubuntu system.\\n\\nSome examples of popular uses for the apt utility include:\\n\\n - **Install a Package**\\nInstallation of packages using apt is quite simple. For example, to install the nmap network scanner, type the\\nfollowing:\\n\\nsudo apt install nmap\\n\\n**Tip**\\nYou can specify multiple packages to be installed or removed, by separating them with spaces.\\n\\n - **Remove a Package**\\nRemoval of a package (or packages) is also straightforward.',\n",
       " 'To remove the package installed in the previous\\nexample, simply type:\\n\\nsudo apt remove nmap\\n\\nAdding the --purge option to apt remove will remove the package configuration files as well. This may or may\\nnot be the desired effect, so use with caution.\\n\\n**Note** :\\n\\nWhile apt is a command-line tool, it is intended to be used interactively, and not to be called from\\nnon-interactive scripts. The apt-get command should be used in scripts (perhaps with the --quiet\\nflag). For basic commands the syntax of the two tools is identical.\\n\\n - **Update the package index**\\nThe APT package index is essentially a database of available packages from the repositories defined in the\\n\\n/etc/apt/sources.list file and in the /etc/apt/sources.list.d directory. To update the local package index\\nwith the latest changes made in the repositories, type the following:\\n\\nsudo apt update\\n\\n - **Upgrade packages**\\nInstalled packages on your computer may periodically have upgrades available from the package repositories\\n(e.g., security updates). To upgrade your system, first, update your package index with sudo apt update, and\\nthen type:\\n\\nsudo apt upgrade\\n\\n[For details on how to upgrade to a new Ubuntu release, see our guide on upgrading.](https://ubuntu.com/server/docs/upgrade-introduction)\\n\\nActions of the apt command, such as installation and removal of packages, are logged in the /var/log/dpkg.log log\\nfile.\\n\\n[For further information about the use of APT, read the comprehensive APT User’s Guide, or type](https://www.debian.org/doc/user-manuals#apt-guide) apt help .\\n## **Aptitude**\\n\\nLaunching Aptitude with no command-line options will give you a menu-driven, text-based frontend to the APT\\nsystem.',\n",
       " 'Many of the common package management functions, such as installation, removal, and upgrade, can be\\nperformed in Aptitude with single-key commands, which are typically lowercase letters.\\n\\nAptitude is best suited for use in a non-graphical terminal environment to ensure proper functioning of the command\\nkeys. You can start the menu-driven interface of Aptitude as a normal user by typing the following command at a\\nterminal prompt:\\n\\nsudo aptitude\\n\\n127\\n\\n\\n-----\\n\\nWhen Aptitude starts, you will see a menu bar at the top of the screen and two panes below the menu bar. The\\ntop pane contains package categories, such as *New Packages* and *Not Installed Packages* . The bottom pane contains\\ninformation related to the packages and package categories.\\n\\nUsing Aptitude for package management is relatively straightforward, and the user interface makes common tasks\\nsimple to perform. The following are examples of common package management functions as performed in Aptitude:\\n\\n - **Install Packages**\\nTo install a package, locate it via the *Not Installed Packages* package category by using the keyboard arrow keys\\nand the Enter key. Highlight the desired package, then press the + key. The package entry should turn *green*,\\nindicating it has been marked for installation. Now press g to be presented with a summary of package actions.\\nPress g again, and the package will be downloaded and installed. When finished, press Enter to return to the\\n\\nmenu.\\n\\n - **Remove Packages**\\nTo remove a package, locate it in the *Installed Packages* package category by using the keyboard arrow keys\\nand the Enter key. Highlight the package you want to remove, then press the - key. The package entry should',\n",
       " 'turn *pink*, indicating it has been marked for removal. Now press g to be presented with a summary of package\\nactions. Press g again, and the package will be removed. When finished, press Enter to return to the menu.\\n\\n - **Update Package Index**\\nTo update the package index, simply press the u key.\\n\\n - **Upgrade Packages**\\nTo upgrade packages, first update the package index as detailed above, and then press the U key to mark all\\npackages with updates. Now press g, which will present you with a summary of package actions. Press g again\\nto begin the download and installation. When finished, press Enter to return to the menu.\\n\\nThe first column of information displayed in the package list (in the top pane) lists the current state of the package\\n(when viewing packages). It uses the following key to describe the package state:\\n\\n - **i** : Installed package\\n\\n - **c** : Package not installed, but package configuration remains on the system\\n\\n - **p** : Purged from system\\n\\n - **v** : Virtual package\\n\\n - **B** : Broken package\\n\\n - **u** : Unpacked files, but package not yet configured\\n\\n - **C** : Half-configured - configuration failed and requires fix\\n\\n - **H** : Half-installed - removal failed and requires a fix\\n\\nTo exit Aptitude, simply press the q key and confirm you wish to exit. Many other functions are available from the\\nAptitude menu by pressing the F10 key.\\n\\n**Command Line Aptitude**\\n\\nYou can also use Aptitude as a command-line tool, similar to apt . To install the nmap package with all necessary\\ndependencies (as in the apt example), you would use the following command:\\n\\nsudo aptitude install nmap\\n\\nTo remove the same package, you would use the command:',\n",
       " 'sudo aptitude remove nmap\\n\\nConsult the Aptitude manpages for full details of Aptitude’s command-line options.\\n\\ndpkg\\n\\ndpkg is a package manager for *Debian* -based systems. It can install, remove, and build packages, but unlike other\\npackage management systems, it cannot automatically download and install packages – or their dependencies. **APT**\\n**and Aptitude are newer, and layer additional features on top of** dpkg **.** This section covers using dpkg to\\nmanage locally installed packages:\\n\\n  - To list *all* packages in the system’s package database, installed and uninstalled, from a terminal prompt type:\\n\\ndpkg -l\\n\\n128\\n\\n\\n-----\\n\\n  - Depending on the number of packages on your system, this can generate a large amount of output. Pipe the\\noutput through grep to see if a specific package is installed:\\n\\ndpkg -l | grep apache2\\n\\nReplace apache2 with any package name, part of a package name, or a regular expression.\\n\\n  - To list the files installed by a package, in this case the ufw package, enter:\\n\\ndpkg -L ufw\\n\\n  - If you are unsure which package installed a file, dpkg -S may be able to tell you. For example:\\n\\ndpkg -S /etc/host.conf\\n\\nbase-files: /etc/host.conf\\n\\nThe output shows that the /etc/host.conf belongs to the base-files package.\\n\\n**Note** :\\nMany files are automatically generated during the package install process, and even though they are\\non the filesystem, dpkg -S may not know which package they belong to.\\n\\n  - You can install a local .deb file by entering:\\n\\nsudo dpkg -i zip_3.0-4_amd64.deb\\n\\nChange zip_3.0-4_amd64.deb to the actual file name of the local .deb file you wish to install.\\n\\n  - You can uninstall a package by:\\n\\nsudo dpkg -r zip\\n\\n**Caution** :',\n",
       " 'Uninstalling packages using dpkg, is *NOT* recommended in most cases. It is better to use a package\\nmanager that handles dependencies to ensure that the system is in a consistent state. For example,\\nusing dpkg -r zip will remove the zip package, but any packages that depend on it will still be installed\\nand may no longer function correctly.\\n\\nFor more dpkg options see the manpage: man dpkg .\\n## **APT configuration**\\n\\nConfiguration of the APT system repositories is stored in the /etc/apt/sources.list file and the /etc/apt/sources.list.d\\ndirectory. An example of this file is referenced here, along with information on adding or removing repository references\\nfrom the file.\\n\\nYou can edit the file to enable and disable repositories. For example, to disable the requirement of inserting the\\nUbuntu CD-ROM whenever package operations occur, simply comment out the appropriate line for the CD-ROM,\\nwhich appears at the top of the file:\\n\\n# no more prompting for CD-ROM please\\n\\n# deb cdrom:[DISTRO-APT-CD-NAME - Release i386 (20111013.1)]/ DISTRO-SHORT-CODENAME main restricted\\n\\n**Extra repositories**\\n\\nIn addition to the officially-supported package repositories available for Ubuntu, there are also community-maintained\\nrepositories which add thousands more packages for potential installation. Two of the most popular are the *universe*\\nand *multiverse* repositories. These repositories are not officially supported by Ubuntu, but because they are maintained\\nby the community they generally provide packages which are safe for use with your Ubuntu computer.\\n\\n**Note** :\\n\\nPackages in the *multiverse* repository often have licensing issues that prevent them from being distributed',\n",
       " 'with a free operating system, and they may be illegal in your locality.\\n\\n**Warning** :\\nBe advised that neither *universe* nor *multiverse* contain officially-supported packages. In particular, there\\nmay not be security updates for these packages.\\n\\nMany other package sources are available – sometimes even offering only one package, as in the case of packages\\nprovided by the developer of a single application. You should always be very careful and cautious when using nonstandard package sources/repos, however. Research the packages and their origins carefully before performing any\\ninstallation, as some packages could render your system unstable or non-functional in some respects.\\n\\n129\\n\\n\\n-----\\n\\nBy default, the *universe* and *multiverse* repositories are enabled. If you would like to disable them, edit\\n\\n/etc/apt/sources.list and comment out the following lines:\\n\\ndeb http://archive.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME universe multiverse\\n\\ndeb-src http://archive.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME universe multiverse\\n\\ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME universe\\n\\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME universe\\n\\ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates universe\\n\\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates universe\\n\\ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME multiverse\\n\\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME multiverse\\n\\ndeb http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates multiverse\\n\\ndeb-src http://us.archive.ubuntu.com/ubuntu/ DISTRO-SHORT-CODENAME-updates multiverse',\n",
       " 'deb http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security universe\\n\\ndeb-src http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security universe\\n\\ndeb http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security multiverse\\n\\ndeb-src http://security.ubuntu.com/ubuntu DISTRO-SHORT-CODENAME-security multiverse\\n## **Automatic updates**\\n\\nThe unattended-upgrades package can be used to automatically install updated packages and can be configured to\\nupdate all packages or just install security updates. First, install the package by entering the following in a terminal:\\n\\nsudo apt install unattended-upgrades\\n\\nTo configure unattended-upgrades, edit /etc/apt/apt.conf.d/50unattended-upgrades and adjust the following to fit\\nyour needs:\\n\\nUnattended-Upgrade::Allowed-Origins {\\n\\n\"${distro_id}:${distro_codename}\";\\n\\n\"${distro_id}:${distro_codename}-security\";\\n\\n// \"${distro_id}:${distro_codename}-updates\";\\n\\n// \"${distro_id}:${distro_codename}-proposed\";\\n\\n// \"${distro_id}:${distro_codename}-backports\";\\n\\n};\\n\\nCertain packages can also be excluded and therefore will not be automatically updated. To block a package, add it\\nto the list:\\n\\nUnattended-Upgrade::Package-Blacklist {\\n\\n// \"vim\";\\n\\n// \"libc6\";\\n\\n// \"libc6-dev\";\\n\\n// \"libc6-i686\";\\n\\n};\\n\\n**Note** :\\nThe double “//” serve as comments, so whatever follows “//” will not be evaluated.\\n\\nTo enable automatic updates, edit /etc/apt/apt.conf.d/20auto-upgrades and set the appropriate APT configuration\\noptions:\\n\\nAPT::Periodic::Update-Package-Lists \"1\";\\n\\nAPT::Periodic::Download-Upgradeable-Packages \"1\";\\n\\nAPT::Periodic::AutocleanInterval \"7\";\\n\\nAPT::Periodic::Unattended-Upgrade \"1\";',\n",
       " 'The above configuration updates the package list, downloads, and installs available upgrades every day. These actions\\nare triggered by timer units at a set time but with a random delay: apt-daily.timer and apt-daily-upgrade.timer .\\nThese timers activate the correspondent services that run the /usr/lib/apt/apt.systemd.daily script.\\n\\nHowever, it may happen that if the server is off at the time the timer unit elapses, the timer will be triggered\\nimmediately at the next startup. As a result, they will often run on system startup\\nand thereby cause immediate activity and hold the apt-lock.\\n\\n130\\n\\n\\n-----\\n\\nIn many cases this is beneficial, but in some cases it might be counter-productive; examples are administrators with\\nmany shut-down machines or VM images that are only started for some quick action, which is delayed or even blocked\\nby the unattended upgrades. To adapt this behaviour, we can change/override the configuration of both APT’s timer\\nunits [ apt-daily-upgrade.timer, apt-daily.timer ]. To do so, use systemctl edit <timer_unit> and override the\\n*Persistent* attribute, for example with Persistent=delay :\\n\\n[Timer]\\n\\nPersistent=delay\\n\\nThe local download archive is cleaned every week. On servers upgraded to newer versions of Ubuntu, depending on\\nyour responses, the file listed above may not be there. In this case, creating a new file of the same name should also\\nwork.\\n\\n**Note** :\\nYou can read more about apt *Periodic* configuration options in the apt.conf(5) manpage and in the\\n\\n/usr/lib/apt/apt.systemd.daily script header.\\n\\nThe results of unattended-upgrades will be logged to /var/log/unattended-upgrades .\\n## **Notifications**',\n",
       " 'Configuring Unattended-Upgrade::Mail in /etc/apt/apt.conf.d/50unattended-upgrades will enable unattended\\nupgrades to email an administrator detailing any packages that need upgrading or have problems.\\n\\nAnother useful package is apticron . apticron will configure a cron job to email an administrator information about\\nany packages on the system that have updates available, as well as a summary of changes in each package.\\n\\nTo install the apticron package, enter the following command in a terminal:\\n\\nsudo apt install apticron\\n\\nOnce the package is installed, edit /etc/apticron/apticron.conf, to set the email address and other options:\\n\\nEMAIL=\"root@example.com\"\\n## **References**\\n\\nMost of the material covered in this chapter is available in man pages, many of which are available online.\\n\\n[• The Installing Software Ubuntu wiki page has more information.](https://help.ubuntu.com/community/InstallingSoftware)\\n\\n[• For more dpkg details see the dpkg man page.](http://manpages.ubuntu.com/cgi-bin/search.py?q=dpkg)\\n\\n[• The APT User’s Guide and apt man page contain useful information regarding APT usage.](https://www.debian.org/doc/user-manuals#apt-guide)\\n\\n[• For more info about systemd timer units (and systemd in general), visit the systemd man page and systemd.timer](https://manpages.ubuntu.com/cgi-bin/search.py?q=systemd)\\n\\n[man page.](https://manpages.ubuntu.com/cgi-bin/search.py?q=systemd.timer)\\n\\n[• See the Aptitude user’s manual for more Aptitude options.](https://www.debian.org/doc/user-manuals#aptitude-guide)\\n\\n[• The Adding Repositories HOWTO (Ubuntu Wiki) page contains more details on adding repositories.](https://help.ubuntu.com/community/Repositories/Ubuntu)',\n",
       " 'This article details how to upgrade an Ubuntu Server or Ubuntu cloud image to the next release.\\n## **Upgrade paths**\\n\\nUbuntu supports the ability to upgrade from one LTS to the next LTS in sequential order. For example, a user on\\nUbuntu 16.04 LTS can upgrade to Ubuntu 18.04 LTS, but cannot jump directly to Ubuntu 20.04 LTS. To do this, the\\nuser would need to upgrade twice: once to Ubuntu 18.04 LTS, and then upgrade again to Ubuntu 20.04 LTS.\\n\\nIt is recommended that users run an LTS release as it provides 5 years of standard support and security updates. After\\n[the initial standard support, an extended support period is available via an Ubuntu Pro subscription.](http://ubuntu.com/pro)\\n\\n[For a complete list of releases and current support status see the Ubuntu Wiki Releases page.](https://wiki.ubuntu.com/Releases)\\n## **Upgrade checklist**\\n\\nTo ensure a successful upgrade, please review the following items:\\n\\n  - Check the release notes for the new release for any known issues or important changes. Release notes for each\\n[release are found on the Ubuntu Wiki Releases page.](https://wiki.ubuntu.com/Releases)\\n\\n131\\n\\n\\n-----\\n\\n  - Fully update the system. The upgrade process works best when the current system has all the latest updates\\ninstalled. Users should confirm that these commands complete successfully and that no further updates are\\navailable. It is also suggested that users reboot the system after all the updates are applied to verify they are\\nrunning the latest kernel. To upgrade run the following commands:\\n\\nsudo apt update\\n\\nsudo apt upgrade\\n\\n  - Users should check that there is sufficient free disk space for the upgrade. Upgrading a system will make your',\n",
       " 'system download new packages, which is likely to be on the order of hundreds of new packages. Systems with\\nadditional software installed may therefore require a few gigabytes of free disk space.\\n\\n  - The upgrade process takes time to complete. Users should have dedicated time to participate in the upgrade\\n\\nprocess.\\n\\n  - Third-party software repositories and personal package archives (PPAs) are disabled during the upgrade. However, any software installed from these repositories is not removed or downgraded. Software installed from these\\nrepositories is the single most common cause of upgrade issues.\\n\\n  - Backup any and all data. Although upgrades are normally safe, there is always a chance that something may go\\nwrong. It is extremely important that the data is safely copied to a backup location to allow restoration if there\\nare any problems or complications during the upgrade process.\\n## **Upgrade**\\n\\nIt is recommended to upgrade the system using the do-release-upgrade command on Server edition and cloud images.\\nThis command can handle system configuration changes that are sometimes needed between releases.\\n\\n**do-release-upgrade**\\n\\nTo begin the process run the following command:\\n\\nsudo do-release-upgrade\\n\\nUpgrading to a development release of Ubuntu is available using the -d flag. However, using the development release\\n(or this flag) is not recommended for production environments.\\n\\nUpgrades from one LTS to the next LTS release are only available after the first point release. For example, Ubuntu\\n18.04 LTS will only upgrade to Ubuntu 20.04 LTS after the 20.04.1 point release. If users wish to update before the',\n",
       " \"point release (e.g., on a subset of machines to evaluate the LTS upgrade) users can force the upgrade via the -d flag.\\n\\n**Pre-upgrade summary**\\n\\nBefore making any changes the command will first do some checks to verify the system is ready to update. The user\\nwill be prompted with a summary of the upgrade before proceeding. If the user accepts the changes, the process will\\nbegin to update the system’s packages:\\n\\nDo you want to start the upgrade?\\n\\n5 installed packages are no longer supported by Canonical. You can\\n\\nstill get support from the community.\\n\\n4 packages are going to be removed. 117 new packages are going to be\\n\\ninstalled. 424 packages are going to be upgraded.\\n\\nYou have to download a total of 262 M. This download will take about\\n\\n33 minutes with a 1Mbit DSL connection and about 10 hours with a 56k\\n\\nmodem.\\n\\nFetching and installing the upgrade can take several hours. Once the\\n\\ndownload has finished, the process cannot be canceled.\\n\\nContinue [yN] Details [d]\\n\\n132\\n\\n\\n-----\\n\\n**Configuration changes**\\n\\nIt is possible during the upgrade process the user gets presented with a message to make decisions about package\\nupdates. These prompts occur when there are existing configuration files edited by the user and the new package\\nconfiguration file are different. Below is an example prompt:\\n\\nConfiguration file '/etc/ssh/ssh_config'\\n\\n==> Modified (by you or by a script) since installation.\\n\\n==> Package distributor has shipped an updated version.\\n\\nWhat would you like to do about it ? Your options are:\\n\\nY or I : install the package maintainer's version\\n\\nN or O : keep your currently-installed version\\n\\nD : show the differences between the versions\",\n",
       " \"Z : start a shell to examine the situation\\n\\nThe default action is to keep your current version.\\n\\n*** ssh_config (Y/I/N/O/D/Z) [default=N] ?\\n\\nUsers should look at the differences between the files and decide what to do. The default response is to keep the\\ncurrent version of the file. There are situations where accepting the new version, like with /boot/grub/menu.lst, is\\nrequired for the system to boot correctly with the new kernel.\\n\\n**Package removal**\\n\\nAfter all packages are updated the user will again remove any obsolete, no longer needed, packages:\\n\\nRemove obsolete packages?\\n\\n30 packages are going to be removed.\\n\\nContinue [yN] Details [d]\\n\\n**Reboot**\\n\\nFinally, when the upgrade is complete the user is prompted to reboot the system. The system is not considered\\nupgraded until a reboot occurs:\\n\\nSystem upgrade is complete.\\n\\nRestart required\\n\\nTo finish the upgrade, a restart is required.\\n\\nIf you select 'y' the system will be restarted.\\n\\nContinue [yN]\\n\\nUbuntu is an operating system with thousands of packages and snaps available to its users, but it is humanly (and\\nsometimes technically!) impossible to make all software out there available in the official repositories. There are\\nsituations where you may want to install a package that is not maintained by Ubuntu, but *is* maintained by a third\\nparty entity. We don’t recommend using third party APT repositories, but we know that users sometimes have no\\nother option – so let’s take a look at some of the pitfalls, alternatives, and mitigations.\\n## **Why not use third party APT repositories?**\\n\\nWhile having access to the software you want to use is great, it is crucial to understand the risks involved in using\",\n",
       " 'third party APT repositories.\\n\\n**Security risk**\\n\\nWhen using any software that you have not audited yourself, you must implicitly trust the publisher of that software\\nwith your data. However, with third party APT repositories, there are additional implications of this that are less\\nobvious.\\n\\nUnlike more modern packaging systems, APT repositories run code that is not sandboxed. When using software from\\nmore than one publisher, such as from your distribution as well as a third party, APT and dpkg provide no security\\nboundary between them.\\n\\n133\\n\\n\\n-----\\n\\nThis is important because in addition to trusting the publisher’s intentions, you are also implicitly trusting the quality\\nand competence of the publisher’s own information security, since an adversary can compromise your system indirectly\\nby compromising the software publisher’s infrastructure.\\n\\nFor example, consider users who use applications such as games where system security isn’t much of a concern, but also\\nuse their computers for something more security-sensitive such as online banking. A properly sandboxed packaging\\nsystem would mitigate an adversary compromising the game publisher in order to take over their users’ online banking\\nsessions, since the games wouldn’t have access to those sessions. But with APT repositories, the game can access your\\nonline banking session as well. Your system’s security – as a whole – has been downgraded to the level of the app\\npublisher that has the worst security; they may not consider their information security important because they aren’t\\na bank.\\n\\n**System integrity**',\n",
       " 'Even if you are certain that the third party APT repository can be trusted, you also need to take into account possible\\nconflicts that having an external package may bring to your system. Some third party packagers – but not all –\\nare careful to integrate their packages into Ubuntu in a way that they don’t conflict with official packages from the\\ndistribution, but it is technically impossible to predict future changes that might happen in future Ubuntu releases.\\nThis means that fundamentally there always is the possibility of conflict. The most common cause of system upgrade\\nfailure is the use of third party repositories that worked at the time but later conflicted with a subsequent upgrade.\\n\\nOne of the most common conflicts occurs when a third party package ships with a file that is also shipped by an\\nofficial Ubuntu package. In this case, having both packages installed simultaneously is impossible because dpkg will\\nprevent managed files from being overwritten. Another possible (and more subtle) issue can happen when the third\\nparty software interacts in a problematic way with an official package from Ubuntu. This can be harder to diagnose\\nand might cause more serious problems in the system, such as data loss and service unavailability.\\n\\nAs a general rule, if the third party package you are installing is interacting with or is a modified version of an existing\\nUbuntu package, you need to be more careful and do some preliminary research before using it in your system.\\n\\n**Lack of official Ubuntu support**\\n\\nIf you decide to install a third party package on your Ubuntu system, the Ubuntu community will struggle to offer',\n",
       " 'support for whatever failures you may encounter as a consequence, since it is out of their control and they are unlikely\\nto be familiar with it. In fact, if you experience a bug in an official Ubuntu package but it is later determined that\\nthe bug was caused by a third party package, the Ubuntu community may not be able to help you.\\n\\nIn other words, if you use a third party software you will have to contact its packagers for help if you experience any\\nproblem with it.\\n## **A better solution to third party APT repositories: snaps**\\n\\nAs we have seen, third party APT repositories are not simple and should be handled carefully. But there is an\\nalternative that is natively supported by Ubuntu and solves some of the issues affecting third party APT repositories:\\n\\n[snaps.](https://ubuntu.com/core/services/guide/snaps-intro)\\n\\nDue to the way they are architected, snaps already carry all of their dependencies inside them. When they are installed,\\nthey are placed in an isolated directory in the system, which means that they cannot conflict with existing Ubuntu\\npackages (or even with other snaps).\\n\\nWhen executed, a snap application is sandboxed and has limited access to the system resources. While still vulnerable\\nto some security threats, snaps offer a better isolation than third party APT repositories when it comes to the damage\\nthat can be done by an application.\\n\\n[Finally, if a snap is published in the snapstore, you will not need to go through the hassle of modifying](https://snapcraft.io/store) sources.list\\nor adding a new GPG key to the keyring. Everything will work “out of the box” when you run snap install .\\n## **Mitigating the risks**',\n",
       " 'If the software you want is not available as a snap, you may still need to use a third party APT repository. In that\\ncase, there are some mitigating steps you can take to help protect your system.\\n\\n**Security risk mitigation**\\n\\n  - If the package you want to install is Free Software/Open Source, then the risk can be reduced by carefully\\nexamining the source code of the entire software, including the packaging parts. The amount of work required\\nto do this assessment will depend on the size and complexity of the software, and is something that needs to\\n\\n134\\n\\n\\n-----\\n\\nbe performed by an expert whenever an update is available. Realistically, this kind of evaluation almost never\\nhappens due to the efforts and time required.\\n\\n  - The availability and cadence of fixes to security vulnerabilities should also be taken into account when assessing\\nthe quality and reliability of the third party APT repository. It is important to determine whether these fixes\\nare covered by the third party entity, and how soon they are released once they have been disclosed.\\n\\n  - In addition, you must ensure that the packages are cryptographically signed with the repository’s GPG key. This\\nrequirement helps to confirm the integrity of the package you are about to install on your system.\\n\\n**System integrity mitigation**\\n\\n  - Avoid release upgrades whenever possible, favouring redeployment onto a newer release instead. Third party\\nAPT repositories will often break at release time, and the only way to avoid this is to wait until the maintainers\\nof the repository have upgraded the software to be compatible with the release.\\n\\n  - Configure pinning (we show how to do this below).',\n",
       " 'Pinning is a way to assign a preference level to some (or\\nall) packages from a certain source; in this particular case, the intention is to reduce the preference of packages\\nprovided by an external repository so that official Ubuntu packages are not overwritten by mistake.\\n## **Dealing with third party APT repositories in Ubuntu**\\n\\nNow that we have discussed the risks and mitigations of using third party APT repositories, let’s take a look at how\\nwe can work with them in Ubuntu. Unless otherwise noted, all commands below are to be executed as the root user\\n(or using sudo with your regular user).\\n\\n**Add the repository**\\n\\nSeveral third party entities provide their own instructions on how to add their repositories to a system, but more often\\n[than not they don’t follow best practices when doing so.](https://wiki.debian.org/DebianRepository/UseThirdParty)\\n\\n**Fetch the GPG key**\\n\\nThe first step before adding a third party APT repository to your system is to fetch the GPG key for it. This key\\nmust be obtained from the third party entity; it should be available at the root of the repository’s URL, but you might\\nneed to contact them and ask for the key file.\\n\\nAlthough several third party guides instruct the user to use apt-key in order to add the GPG key to apt ’s keyring, this\\nis no longer recommended. Instead, you should explicitly list the key in the sources.list entry by using the signed-by\\noption (see below).\\n\\nThird party APT repositories should also provide a special package called REPONAME-archive-keyring whose purpose is\\nto provide updates to the GPG key used to sign the archive. Because this package is signed using the GPG key that',\n",
       " 'is not present in the system when we are initially configuring the repository, we need to manually download and put\\nit in the right place the first time. Assuming that REPONAME is externalrepo, something like the following should work:\\n\\nwget -O /usr/share/keyrings/externalrepo-archive-keyring.pgp https://thirdpartyrepo.com/ubuntu/externalrepo\\narchive-keyring.pgp\\n\\n**Sources.list entry**\\n\\nTo add a third party APT repository to your system, you will need to create a file under /etc/apt/sources.list.d/ with\\ninformation about the external archive. This file is usually named after the repository (in our example, externalrepo ).\\nThere are two standards the file can follow:\\n\\n  - A one-line entry, which is the most common. In this case, the extension of the file should be .list .\\n\\n - The deb822 format, which is more descriptive but less common. In this case, the extension of the file should be\\n\\n.sources .\\n\\nAn example of a one-line entry would be the following:\\n\\ndeb [signed-by=/usr/share/keyrings/externalrepo-archive-keyring.pgp] https://thirdpartyrepo.com/ubuntu/ jammy main\\n\\nAn example of a deb822 file for the same case would be the following:\\n\\nTypes: deb\\n\\nURIs: https://thirdpartyrepo.com/ubuntu\\n\\nSuites: jammy\\n\\nComponents: main\\n\\nSigned-By: /usr/share/keyrings/externalrepo-archive-keyring.pgp\\n\\n135\\n\\n\\n-----\\n\\nThere are cases when the third party APT repository may be served using HTTPS, in which case you will also need\\nto install the apt-transport-https package.\\n\\nAfter adding the repository information, you need to run apt update in order to install the third party packages. Also,',\n",
       " 'now that you have everything configured you should be able to install the externalrepo-archive-keyring package to\\nautomate the update of the GPG key.\\n\\n**Configure pinning for the repository**\\n\\nIn order to increase the security of your system and to prevent the conflict issues discussed in the “System integrity”\\nsection, we recommend that you configure pinning for the third party APT repository.\\n\\nYou can configure this preference level by creating a file under /etc/apt/preferences.d/ that is usually named after\\nthe repository name ( externalrepo in this case).\\n\\nIn our example, a file named /etc/apt/preferences.d/externalrepo should be created with the following contents:\\n\\nPackage: *\\n\\nPin: origin thirdpartyrepo.com\\n\\nPin-Priority: 100\\n\\n[There are several levels of pinning you can choose here; the Debian Reference guide has good documentation about](https://www.debian.org/doc/manuals/debian-reference/ch02.en.html#_tweaking_candidate_version)\\nthe topic. The level 100 used above means that users will be able to install packages from the repository and that\\nautomatic package upgrades are also enabled. If you want to be able to install packages but don’t want them to be\\nconsidered for automatic upgrades, you should use the level 1 .\\n\\n**How to remove a repository**\\n\\nIf you have enabled a third party APT repository but found yourself in a situation where you would like to remove it\\nfrom the system, there are a few steps you need to take to make sure that the third party packages are also uninstalled.\\n\\nThe first step is to remove the files created in the steps above. These are:\\n\\n  - The sources.list file, under /etc/apt/sources.list.d/ .',\n",
       " '  - The package pinning preference, under /etc/apt/preferences.d/ .\\n\\n  - If the third party APT repository does not provide the GPG key in a package, then you can also remove it\\nmanually from /usr/share/keyrings/ .\\n\\nBefore you run apt update, you might want to also remove the third party packages that were installed from the\\nrepository. The following one-liner will list all those packages:\\n\\napt remove --purge \\\\\\n\\n$(grep \"^Package: \" /var/lib/apt/lists/#<SELECT_THE_FILE_FOR_YOUR_REPOSITORY>#_*_Packages \\\\\\n\\n| cut -d \" \" -f2 | sort -u | \\\\\\n\\nxargs dpkg-query -W -f=\\'${binary:Package}\\\\t${db:Status-Abbrev}\\\\n\\' 2> /dev/null | \\\\\\n\\nawk \\'/\\\\tii $/{print $1}\\')\\n\\nMake sure to replace #<SELECT_THE_FILE_FOR_YOUR_REPOSITORY># with the right file for the third party APT repository.\\n\\nAfter that, you can safely run apt update .\\n## **A special case: Ubuntu PPAs**\\n\\nUbuntu PPAs can be considered as a special case of third party APT repositories. In fact, there are upstream projects\\nthat choose to ship their software through PPAs because of the existing tooling that allows users to easily add them\\nto their Ubuntu systems.\\n\\nIt is important to mention that the same points raised above regarding security, system integrity and lack of official\\nUbuntu support also apply to PPAs.\\n\\nIf you would like to install packages from a PPA, first you will need to add it to your system. For that, you can use\\nthe add-apt-repository command. Suppose you want to add a PPA from user thirdparty named externalrepo . You\\n\\ncan run:\\n\\nadd-apt-repository ppa:thirdparty/externalrepo\\n\\nThis command will automatically set up the GPG key, as discussed above. After that, you can run apt update and',\n",
       " 'install the third party packages provided by the PPA. Note that add-apt-repository will not adjust the repository\\npinning, so it is recommended that you go through that process manually.\\n\\n136\\n\\n\\n-----\\n\\nIf you decide you do not want to use the PPA anymore and would like to remove it (and its packages) from your\\nsystem, the easiest way to do it is by installing the ppa-purge package. You can then execute it and provide the PPA\\nreference as its argument. In our example, that would be:\\n\\nppa-purge ppa:thirdparty/externalrepo\\n\\n[The Ubuntu Project, and thus Ubuntu Server, uses Launchpad as its bug tracker. In order to file a bug, you will need](https://launchpad.net/)\\n[a Launchpad account. Create one here if necessary.](https://help.launchpad.net/YourAccount/NewAccount)\\n## Reporting bugs with apport-cli\\n\\nThe preferred way to report a bug is with the apport-cli command. It must be invoked on the machine affected by\\nthe bug because it collects information from the system on which it is being run and publishes it to the bug report\\non Launchpad. Getting that information to Launchpad can, therefore, be a challenge if the system is not running a\\ndesktop environment in order to use a browser (common with servers) or if it does not have Internet access. The steps\\nto take in these situations are described below.\\n\\n**Note** :\\nThe commands apport-cli and ubuntu-bug should give the same results on a command-line interface (CLI)\\nserver. The latter is actually a symlink to apport-bug which is intelligent enough to know whether a\\ndesktop environment is in use, and will choose apport-cli if not. Since server systems tend to be CLI-only\\n\\napport-cli was chosen from the outset in this guide.',\n",
       " 'Bug reports in Ubuntu need to be filed against a specific software package, so the name of the package (source package\\nor program name/path) affected by the bug needs to be supplied to apport-cli :\\n\\napport-cli PACKAGENAME\\n\\nOnce apport-cli has finished gathering information you will be asked what to do with it. For instance, to report a\\nbug in vim using apport-cli vim produces output like this:\\n\\n*** Collecting problem information\\n\\nThe collected information can be sent to the developers to improve the\\n\\napplication. This might take a few minutes.\\n\\n...\\n\\n*** Send problem report to the developers?\\n\\nAfter the problem report has been sent, please fill out the form in the\\n\\nautomatically opened web browser.\\n\\nWhat would you like to do? Your options are:\\n\\nS: Send report (2.8 KB)\\n\\nV: View report\\n\\nK: Keep report file for sending later or copying to somewhere else\\n\\nI: Cancel and ignore future crashes of this program version\\n\\nC: Cancel\\n\\nPlease choose (S/V/K/I/C):\\n\\nThe first three options are described below:\\n\\n - **S: Send report**\\nSubmits the collected information to Launchpad as part of the process of filing a new bug report. You will be\\ngiven the opportunity to describe the bug in your own words.\\n\\n*** Uploading problem information\\n\\nThe collected information is being sent to the bug tracking system.\\n\\nThis might take a few minutes.\\n\\n94%\\n\\n*** To continue, you must visit the following URL:\\n\\nhttps://bugs.launchpad.net/ubuntu/+source/vim/+filebug/09b2495a-e2ab-11e3-879b-68b5996a96c8?\\n\\nYou can launch a browser now, or copy this URL into a browser on another computer.\\n\\n137\\n\\n\\n-----\\n\\nChoices:\\n\\n1: Launch a browser now\\n\\nC: Cancel\\n\\nPlease choose (1/C): 1',\n",
       " 'The browser that will be used when choosing ‘1’ will be the one known on the system as www-browser via the\\n[Debian alternatives system. Examples of text-based browsers to install include links, elinks, lynx, and w3m.](https://manpages.ubuntu.com/manpages/jammy/en/man1/update-alternatives.1.html)\\nYou can also manually point an existing browser at the given URL.\\n\\n - **V: View**\\n\\nDisplays the collected information on the screen for review. This can be a lot of information. Press Enter to\\nscroll through the screens. Press q to quit and return to the choice menu.\\n\\n - **K: Keep**\\nWrites the collected information to disk. The resulting file can be later used to file the bug report, typically after\\ntransferring it to another Ubuntu system.\\n\\nWhat would you like to do? Your options are:\\n\\nS: Send report (2.8 KB)\\n\\nV: View report\\n\\nK: Keep report file for sending later or copying to somewhere else\\n\\nI: Cancel and ignore future crashes of this program version\\n\\nC: Cancel\\n\\nPlease choose (S/V/K/I/C): k\\n\\nProblem report file: /tmp/apport.vim.1pg92p02.apport\\n\\nTo report the bug, get the file onto an Internet-enabled Ubuntu system and apply apport-cli to it. This will\\ncause the menu to appear immediately (the information is already collected). You should then press s to send:\\n\\napport-cli apport.vim.1pg92p02.apport\\n\\nTo directly save a report to disk (without menus) you can run:\\n\\napport-cli vim --save apport.vim.test.apport\\n\\nReport names should end in *.apport* .\\n\\n**Note** :\\nIf this Internet-enabled system is non-Ubuntu/Debian, apport-cli is not available so the bug will need\\nto be created manually. An apport report is also not to be included as an attachment to a bug either',\n",
       " 'so it is completely useless in this scenario.\\n## **Reporting application crashes**\\n\\nThe software package that provides the apport-cli utility, apport, can be configured to automatically capture the\\nstate of a crashed application. This is enabled by default (in /etc/default/apport ).\\n\\nAfter an application crashes, if enabled, apport will store a crash report under /var/crash :\\n\\n-rw-r----- 1 peter whoopsie 150K Jul 24 16:17 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu\\ncached.1000.crash\\n\\nUse the apport-cli command without arguments to process any pending crash reports. It will offer to report them\\none by one, as in the following example:\\n\\napport-cli\\n\\n*** Send problem report to the developers?\\n\\nAfter the problem report has been sent, please fill out the form in the\\n\\nautomatically opened web browser.\\n\\nWhat would you like to do? Your options are:\\n\\nS: Send report (153.0 KB)\\n\\nV: View report\\n\\nK: Keep report file for sending later or copying to somewhere else\\n\\nI: Cancel and ignore future crashes of this program version\\n\\nC: Cancel\\n\\nPlease choose (S/V/K/I/C): s\\n\\n138\\n\\n\\n-----\\n\\nIf you send the report, as was done above, the prompt will be returned immediately and the /var/crash directory will\\nthen contain 2 extra files:\\n\\n-rw-r----- 1 peter whoopsie 150K Jul 24 16:17 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu\\ncached.1000.crash\\n\\n-rw-rw-r-- 1 peter whoopsie 0 Jul 24 16:37 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu\\ncached.1000.upload\\n\\n-rw------- 1 whoopsie whoopsie 0 Jul 24 16:37 _usr_lib_x86_64-linux-gnu_libmenu-cache2_libexec_menu\\ncached.1000.uploaded\\n\\nSending in a crash report like this will not immediately result in the creation of a new public bug.',\n",
       " 'The report will be\\nmade private on Launchpad, meaning that it will be visible to only a limited set of bug triagers. These triagers will\\nthen scan the report for possible private data before creating a public bug.\\n## **Resources**\\n\\n[• See the Reporting Bugs Ubuntu wiki page.](https://help.ubuntu.com/community/ReportingBugs)\\n\\n[• Also, the Apport page has some useful information. Though some of it pertains to using a GUI.](https://wiki.ubuntu.com/Apport)\\n\\nA ‘kernel crash dump’ refers to a portion of the contents of volatile memory (RAM) that is copied to disk whenever\\nthe execution of the kernel is disrupted. The following events can cause a kernel disruption:\\n\\n  - Kernel panic\\n\\n  - Non-maskable interrupts (NMI)\\n\\n  - Machine check exceptions (MCE)\\n\\n  - Hardware failure\\n\\n  - Manual intervention\\n\\nFor some of these events (kernel panic, NMI) the kernel will react automatically and trigger the crash dump mechanism\\nthrough *kexec* . In other situations a manual intervention is required in order to capture the memory. Whenever one\\nof the above events occurs, it is important to find out the root cause in order to prevent it from happening again. The\\ncause can be determined by inspecting the copied memory contents.\\n## **Kernel crash dump mechanism**\\n\\nWhen a kernel panic occurs, the kernel relies on the *kexec* mechanism to quickly reboot a new instance of the kernel\\nin a pre-reserved section of memory that had been allocated when the system booted (see below). This permits the\\nexisting memory area to remain untouched in order to safely copy its contents to storage.\\n## **Installation**\\n\\nThe kernel crash dump utility is installed with the following command:',\n",
       " 'sudo apt install linux-crashdump\\n\\n**Note** :\\n\\nStarting with 16.04, the kernel crash dump mechanism is enabled by default.\\n\\nDuring the installation, you will be prompted with the following dialogs.\\n\\n|------------------------| Configuring kexec-tools |------------------------|\\n\\n| |\\n\\n| |\\n\\n| If you choose this option, a system reboot will trigger a restart into a |\\n\\n| kernel loaded by kexec instead of going through the full system boot |\\n\\n| loader process. |\\n\\n| |\\n\\n| Should kexec-tools handle reboots (sysvinit only)? |\\n\\n| |\\n\\n| <Yes> <No> |\\n\\n| |\\n\\n|---------------------------------------------------------------------------|\\n\\nSelect ‘Yes’ to select kexec-tools for all reboots.\\n\\n139\\n\\n\\n-----\\n\\n|------------------------| Configuring kdump-tools |------------------------|\\n\\n| |\\n\\n| |\\n\\n| If you choose this option, the kdump-tools mechanism will be enabled. A |\\n\\n| reboot is still required in order to enable the crashkernel kernel |\\n\\n| parameter. |\\n\\n| |\\n\\n| Should kdump-tools be enabled be default? |\\n\\n| |\\n\\n| <Yes> <No> |\\n\\n| |\\n\\n|---------------------------------------------------------------------------|\\n\\n‘Yes’ should be selected here as well, to enable kdump-tools .\\n\\nIf you ever need to manually enable the functionality, you can use the dpkg-reconfigure kexec-tools and dpkg\\nreconfigure kdump-tools commands and answer ‘Yes’ to the questions. You can also edit /etc/default/kexec and set\\nparameters directly:\\n\\n# Load a kexec kernel (true/false)\\n\\nLOAD_KEXEC=true\\n\\nAs well, edit /etc/default/kdump-tools to enable kdump by including the following line:\\n\\nUSE_KDUMP=1\\n\\nIf a reboot has not been done since installation of the linux-crashdump package, a reboot will be required in order to',\n",
       " 'activate the crashkernel= boot parameter. Upon reboot, kdump-tools will be enabled and active.\\n\\nIf you enable kdump-tools after a reboot, you will only need to issue the kdump-config load command to activate the\\n\\nkdump mechanism.\\n\\nYou can view the current status of kdump via the command kdump-config show . This will display something like this:\\n\\nDUMP_MODE: kdump\\n\\nUSE_KDUMP: 1\\n\\nKDUMP_SYSCTL: kernel.panic_on_oops=1\\n\\nKDUMP_COREDIR: /var/crash\\n\\ncrashkernel addr:\\n\\n/var/lib/kdump/vmlinuz\\n\\nkdump initrd:\\n\\n/var/lib/kdump/initrd.img\\n\\ncurrent state: ready to kdump\\n\\nkexec command:\\n\\n/sbin/kexec -p --command-line=\"...\" --initrd=...\\n\\nThis tells us that we will find core dumps in /var/crash .\\n## **Configuration**\\n\\nIn addition to local dump, it is now possible to use the remote dump functionality to send the kernel crash dump to\\na remote server, using either the SSH or NFS protocols.\\n\\n**Local kernel crash dumps**\\n\\nLocal dumps are configured automatically and will remain in use unless a remote protocol is chosen. Many configuration\\noptions exist and are thoroughly documented in the /etc/default/kdump-tools file.\\n\\n**Remote kernel crash dumps using the SSH protocol**\\n\\nTo enable remote dumps using the SSH protocol, the /etc/default/kdump-tools must be modified in the following\\n\\nmanner:\\n\\n# --------------------------------------------------------------------------\\n# Remote dump facilities:\\n\\n# SSH - username and hostname of the remote server that will receive the dump\\n\\n# and dmesg files.\\n\\n# SSH_KEY - Full path of the ssh private key to be used to login to the remote\\n\\n# server. use kdump-config propagate to send the public key to the\\n\\n140\\n\\n\\n-----\\n\\n# remote server',\n",
       " '# HOSTTAG - Select if hostname of IP address will be used as a prefix to the\\n\\n# timestamped directory when sending files to the remote server.\\n\\n# \\'ip\\' is the default.\\n\\nSSH=\"ubuntu@kdump-netcrash\"\\n\\nThe only mandatory variable to define is SSH. It must contain the username and hostname of the remote server using\\nthe format {username}@{remote server} .\\n\\nSSH_KEY may be used to provide an existing private key to be used. Otherwise, the kdump-config propagate command\\nwill create a new keypair. The HOSTTAG variable may be used to use the hostname of the system as a prefix to the\\nremote directory to be created instead of the IP address.\\n\\nThe following example shows how kdump-config propagate is used to create and propagate a new keypair to the remote\\n\\nserver:\\n\\nsudo kdump-config propagate\\n\\nWhich produces an output like this:\\n\\nNeed to generate a new ssh key...\\n\\nThe authenticity of host \\'kdump-netcrash (192.168.1.74)\\' can\\'t be established.\\n\\nECDSA key fingerprint is SHA256:iMp+5Y28qhbd+tevFCWrEXykDd4dI3yN4OVlu3CBBQ4.\\n\\nAre you sure you want to continue connecting (yes/no)? yes\\n\\nubuntu@kdump-netcrash\\'s password:\\n\\npropagated ssh key /root/.ssh/kdump_id_rsa to server ubuntu@kdump-netcrash\\n\\nThe password of the account used on the remote server will be required in order to successfully send the public key to\\nthe server.\\n\\nThe kdump-config show command can be used to confirm that kdump is correctly configured to use the SSH protocol:\\n\\nkdump-config show\\n\\nWhose output appears like this:\\n\\nDUMP_MODE: kdump\\n\\nUSE_KDUMP: 1\\n\\nKDUMP_SYSCTL: kernel.panic_on_oops=1\\n\\nKDUMP_COREDIR: /var/crash\\n\\ncrashkernel addr: 0x2c000000\\n\\n/var/lib/kdump/vmlinuz: symbolic link to /boot/vmlinuz-4.4.0-10-generic',\n",
       " 'kdump initrd:\\n\\n/var/lib/kdump/initrd.img: symbolic link to /var/lib/kdump/initrd.img-4.4.0-10-generic\\n\\nSSH: ubuntu@kdump-netcrash\\n\\nSSH_KEY: /root/.ssh/kdump_id_rsa\\n\\nHOSTTAG: ip\\n\\ncurrent state: ready to kdump\\n\\n**Remote kernel crash dumps using the NFS protocol**\\n\\nTo enable remote dumps using the NFS protocol, the /etc/default/kdump-tools must be modified in the following\\n\\nmanner:\\n\\n# NFS - Hostname and mount point of the NFS server configured to receive\\n\\n# the crash dump. The syntax must be {HOSTNAME}:{MOUNTPOINT}\\n\\n# (e.g. remote:/var/crash)\\n\\n#\\n\\nNFS=\"kdump-netcrash:/var/crash\"\\n\\nAs with the SSH protocol, the HOSTTAG variable can be used to replace the IP address by the hostname as the prefix\\nof the remote directory.\\n\\nThe kdump-config show command can be used to confirm that kdump is correctly configured to use the NFS protocol :\\n\\nkdump-config show\\n\\nWhich produces an output like this:\\n\\nDUMP_MODE: kdump\\n\\nUSE_KDUMP: 1\\n\\nKDUMP_SYSCTL: kernel.panic_on_oops=1\\n\\n141\\n\\n\\n-----\\n\\nKDUMP_COREDIR: /var/crash\\n\\ncrashkernel addr: 0x2c000000\\n\\n/var/lib/kdump/vmlinuz: symbolic link to /boot/vmlinuz-4.4.0-10-generic\\n\\nkdump initrd:\\n\\n/var/lib/kdump/initrd.img: symbolic link to /var/lib/kdump/initrd.img-4.4.0-10-generic\\n\\nNFS: kdump-netcrash:/var/crash\\n\\nHOSTTAG: hostname\\n\\ncurrent state: ready to kdump\\n## **Verification**\\n\\nTo confirm that the kernel dump mechanism is enabled, there are a few things to verify. First, confirm that the\\n\\ncrashkernel boot parameter is present (note that the following line has been split into two to fit the format of this\\ndocument):\\n\\ncat /proc/cmdline\\n\\nBOOT_IMAGE=/vmlinuz-3.2.0-17-server root=/dev/mapper/PreciseS-root ro\\n\\ncrashkernel=384M-2G:64M,2G-:128M',\n",
       " 'The crashkernel parameter has the following syntax:\\n\\ncrashkernel=<range1>:<size1>[,<range2>:<size2>,...][@offset]\\n\\nrange=start-[end] \\'start\\' is inclusive and \\'end\\' is exclusive.\\n\\nSo for the crashkernel parameter found in /proc/cmdline we would have :\\n\\ncrashkernel=384M-2G:64M,2G-:128M\\n\\nThe above value means:\\n\\n  - if the RAM is smaller than 384M, then don’t reserve anything (this is the “rescue” case)\\n\\n  - if the RAM size is between 386M and 2G (exclusive), then reserve 64M\\n\\n  - if the RAM size is larger than 2G, then reserve 128M\\n\\nSecond, verify that the kernel has reserved the requested memory area for the kdump kernel by running:\\n\\ndmesg | grep -i crash\\n\\nWhich produces the following output in this case:\\n\\n...\\n\\n[ 0.000000] Reserving 64MB of memory at 800MB for crashkernel (System RAM: 1023MB)\\n\\nFinally, as seen previously, the kdump-config show command displays the current status of the kdump-tools configuration\\n\\n:\\n\\nkdump-config show\\n\\nWhich produces:\\n\\nDUMP_MODE: kdump\\n\\nUSE_KDUMP: 1\\n\\nKDUMP_SYSCTL: kernel.panic_on_oops=1\\n\\nKDUMP_COREDIR: /var/crash\\n\\ncrashkernel addr: 0x2c000000\\n\\n/var/lib/kdump/vmlinuz: symbolic link to /boot/vmlinuz-4.4.0-10-generic\\n\\nkdump initrd:\\n\\n/var/lib/kdump/initrd.img: symbolic link to /var/lib/kdump/initrd.img-4.4.0-10-generic\\n\\ncurrent state: ready to kdump\\n\\nkexec command:\\n\\n/sbin/kexec -p --command-line=\"BOOT_IMAGE=/vmlinuz-4.4.0-10-generic root=/dev/mapper/VividS--vg\\nroot ro debug break=init console=ttyS0,115200 irqpoll maxcpus=1 nousb systemd.unit=kdump-tools.service\" -\\ninitrd=/var/lib/kdump/initrd.img /var/lib/kdump/vmlinuz\\n## **Testing the crash dump mechanism**\\n\\n**Warning** :\\nTesting the crash dump mechanism **will cause a system reboot** .',\n",
       " 'In certain situations, this can cause\\n\\n142\\n\\n\\n-----\\n\\ndata loss if the system is under heavy load. If you want to test the mechanism, make sure that the system\\nis idle or under very light load.\\n\\nVerify that the *SysRQ* mechanism is enabled by looking at the value of the /proc/sys/kernel/sysrq kernel parameter:\\n\\ncat /proc/sys/kernel/sysrq\\n\\nIf a value of *0* is returned, the dump and then reboot feature is disabled. A value greater than *1* indicates that a\\nsub-set of sysrq features is enabled. See /etc/sysctl.d/10-magic-sysrq.conf for a detailed description of the options\\nand their default values. Enable dump then reboot testing with the following command:\\n\\nsudo sysctl -w kernel.sysrq=1\\n\\nOnce this is done, you must become root, as just using sudo will not be sufficient. As the *root* user, you will have to\\nissue the command echo c > /proc/sysrq-trigger . If you are using a network connection, you will lose contact with\\nthe system. This is why it is better to do the test while being connected to the system console. This has the advantage\\nof making the kernel dump process visible.\\n\\nA typical test output should look like the following :\\n\\nsudo -s\\n\\n[sudo] password for ubuntu:\\n\\n# echo c > /proc/sysrq-trigger\\n\\n[ 31.659002] SysRq : Trigger a crash\\n\\n[ 31.659749] BUG: unable to handle kernel NULL pointer dereference at (null)\\n\\n[ 31.662668] IP: [<ffffffff8139f166>] sysrq_handle_crash+0x16/0x20\\n\\n[ 31.662668] PGD 3bfb9067 PUD 368a7067 PMD 0\\n\\n[ 31.662668] Oops: 0002 [#1] SMP\\n\\n[ 31.662668] CPU 1\\n\\n....\\n\\nThe rest of the output is truncated, but you should see the system rebooting and somewhere in the log, you will see\\nthe following line :\\n\\nBegin: Saving vmcore from kernel crash ...',\n",
       " 'Once completed, the system will reboot to its normal operational mode. You will then find the kernel crash dump file,\\nand related subdirectories, in the /var/crash directory by running, e.g. ls /var/crash, which produces the following:\\n\\n201809240744 kexec_cmd linux-image-4.15.0-34-generic-201809240744.crash\\n\\nIf the dump does not work due to an ‘out of memory’ (OOM) error, then try increasing the amount of reserved memory\\nby editing /etc/default/grub.d/kdump-tools.cfg . For example, to reserve 512 megabytes:\\n\\nGRUB_CMDLINE_LINUX_DEFAULT=\"$GRUB_CMDLINE_LINUX_DEFAULT crashkernel=384M-:512M\"\\n\\nYou can then run sudo update-grub, reboot afterwards, and then test again.\\n## **Resources**\\n\\nKernel crash dump is a vast topic that requires good knowledge of the Linux kernel. You can find more information\\non the topic here:\\n\\n[• Kdump kernel documentation.](http://www.kernel.org/doc/Documentation/kdump/kdump.txt)\\n\\n[• Analyzing Linux Kernel Crash (Based on Fedora, it still gives a good walkthrough of kernel dump analysis)](http://www.dedoimedo.com/computers/crash-analyze.html)\\n\\nThe Lightweight Directory Access Protocol, or LDAP, is a protocol for querying and modifying a X.500-based directory\\n[service running over TCP/IP. The current LDAP version is LDAPv3, as defined in RFC 4510, and the implementation](http://tools.ietf.org/html/rfc4510)\\nused in Ubuntu is OpenLDAP.\\n\\nThe LDAP protocol *accesses* directories. It’s common to refer to a directory an *LDAP directory* or *LDAP database* as\\na shorthand – although technically incorrect, this shorthand is so widely used\\nthat it’s understood as such.\\n## **Key concepts and terms**',\n",
       " '  - A directory is a tree of data *entries* that is hierarchical in nature and is called the Directory Information Tree\\n(DIT).\\n\\n  - An entry consists of a set of *attributes* .\\n\\n  - An attribute has a *key* (a name/description) and one or more *values* .\\n\\n143\\n\\n\\n-----\\n\\n  - Every attribute must be defined in at least one *objectClass* .\\n\\n  - Attributes and objectClasses are defined in *schemas* (an objectClass is considered as a special kind of attribute).\\n\\n  - Each entry has a unique identifier: its *Distinguished Name* (DN or dn). This, in turn, consists of a *Relative*\\n*Distinguished Name* (RDN) followed by the parent entry’s DN.\\n\\n  - The entry’s DN is not an attribute. It is not considered part of the entry itself.\\n\\n**Note** :\\n\\nThe terms *object*, *container*, and *node* have certain connotations but they all essentially mean the same\\nthing as *entry* (the technically correct term).\\n\\nFor example, below we have a single entry consisting of 11 attributes where the following is true:\\n\\n - DN is “cn=John Doe,dc=example,dc=com”\\n\\n - RDN is “cn=John Doe”\\n\\n - parent DN is “dc=example,dc=com”\\n\\ndn: cn=John Doe,dc=example,dc=com\\n\\ncn: John Doe\\n\\ngivenName: John\\n\\nsn: Doe\\n\\ntelephoneNumber: +1 888 555 6789\\n\\ntelephoneNumber: +1 888 555 1232\\n\\nmail: john@example.com\\n\\nmanager: cn=Larry Smith,dc=example,dc=com\\n\\nobjectClass: inetOrgPerson\\n\\nobjectClass: organizationalPerson\\n\\nobjectClass: person\\n\\nobjectClass: top\\n\\nThe above entry is in *LDIF* format (LDAP Data Interchange Format). Any information that you feed into your DIT\\n[must also be in such a format. It is defined in RFC 2849.](https://datatracker.ietf.org/doc/html/rfc2849)',\n",
       " 'Such a directory accessed via LDAP is good for anything that involves a large number of access requests to a mostlyread, attribute-based (name:value) backend, and that can benefit from a hierarchical structure. Examples include an\\naddress book, company directory, a list of email addresses, and a mail server’s configuration.\\n## **References**\\n\\n[• The OpenLDAP administrators guide](https://openldap.org/doc/admin25/)\\n\\n[• RFC 4515: LDAP string representation of search filters](http://www.rfc-editor.org/rfc/rfc4515.txt)\\n\\n[• Zytrax’s LDAP for Rocket Scientists; a less pedantic but comprehensive treatment of LDAP](http://www.zytrax.com/books/ldap/)\\n\\nOlder references that might still be useful:\\n\\n[• O’Reilly’s LDAP System Administration (textbook; 2003)](http://www.oreilly.com/catalog/ldapsa/)\\n\\n[• Packt’s Mastering OpenLDAP (textbook; 2007)](http://www.packtpub.com/OpenLDAP-Developers-Server-Open-Source-Linux/book)\\n\\n[Installing slapd (the Stand-alone LDAP Daemon) creates a minimal working configuration with a top level entry, and](https://www.openldap.org/software/man.cgi?query=slapd)\\nan administrator’s Distinguished Name (DN).\\n\\nIn particular, it creates a database instance that you can use to store your data. However, the **suffix** (or **base DN** ) of\\nthis instance will be determined from the domain name of the host. If you want something different, you can change\\nit right after the installation (before it contains any useful data).\\n\\n**Note** :\\nThis guide will use a database suffix of dc=example,dc=com . You can change this to match your particular\\nsetup.\\n## **Install slapd**\\n\\nYou can install the server and the main command line utilities with the following command:',\n",
       " 'sudo apt install slapd ldap-utils\\n\\n144\\n\\n\\n-----\\n\\n**Change the instance suffix (optional)**\\n\\nIf you want to change your Directory Information Tree (DIT) suffix, now would be a good time since changing it\\ndiscards your existing one. To change the suffix, run the following command:\\n\\nsudo dpkg-reconfigure slapd\\n\\nTo switch your DIT suffix to dc=example,dc=com, for example, so you can follow this guide more closely, answer\\n\\nexample.com when asked about the DNS domain name.\\n\\nThroughout this guide we will issue many commands with the LDAP utilities. To save some typing, we can configure\\nthe OpenLDAP libraries with certain defaults in /etc/ldap/ldap.conf (adjust these entries for your server name and\\ndirectory suffix):\\n\\nBASE dc=example,dc=com\\n\\nURI ldap://ldap01.example.com\\n## **Configuration options**\\n\\nslapd is designed to be configured within the service itself by dedicating a separate DIT for that purpose. This allows for\\ndynamic configuration of slapd without needing to restart the service or edit config files. This configuration database\\nconsists of a collection of text-based LDIF files located under /etc/ldap/slapd.d, but these should never be edited\\ndirectly. This way of working is known by several names: the “slapd-config” method, the “Real Time Configuration\\n(RTC)” method, or the “cn=config” method. You can still use the traditional flat-file method ( slapd.conf ) but that\\nwill not be covered in this guide.\\n\\nRight after installation, you will get two databases, or suffixes: one for your data, which is based on your host’s domain\\n( dc=example,dc=com ), and one for your configuration, with its root at cn=config . To change the data on each we need',\n",
       " 'different credentials and access methods:\\n\\n - dc=example,dc=com\\nThe administrative user for this suffix is cn=admin,dc=example,dc=com and its password is the one selected during\\nthe installation of the slapd package.\\n\\n - cn=config\\nThe configuration of slapd itself is stored under this suffix. Changes to it can be made by the special DN gid\\nNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth . This is how the local system’s root user ( uid=0/gid=0 )\\nis seen by the directory when using SASL EXTERNAL authentication through the ldapi:/// transport via\\nthe /run/slapd/ldapi Unix socket. Essentially what this means is that only the local root user can update the\\n\\ncn=config database. More details later.\\n\\n**Example “slapd-config” DIT**\\n\\nThis is what the **slapd-config** DIT looks like via the LDAP protocol (listing only the DNs):\\n\\n$ sudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config dn\\n\\ndn: cn=config\\n\\ndn: cn=module{0},cn=config\\n\\ndn: cn=schema,cn=config\\n\\ndn: cn={0}core,cn=schema,cn=config\\n\\ndn: cn={1}cosine,cn=schema,cn=config\\n\\ndn: cn={2}nis,cn=schema,cn=config\\n\\ndn: cn={3}inetorgperson,cn=schema,cn=config\\n\\ndn: olcDatabase={-1}frontend,cn=config\\n\\ndn: olcDatabase={0}config,cn=config\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nWhere the entries mean the following:\\n\\n - cn=config : Global settings\\n\\n - cn=module{0},cn=config : A dynamically loaded module\\n\\n - cn=schema,cn=config : Contains hard-coded system-level schema\\n\\n - cn={0}core,cn=schema,cn=config : The hard-coded *core* schema\\n\\n - cn={1}cosine,cn=schema,cn=config : The Cosine schema\\n\\n - cn={2}nis,cn=schema,cn=config : The Network Information Services (NIS) schema',\n",
       " ' - cn={3}inetorgperson,cn=schema,cn=config : The InetOrgPerson schema\\n\\n - olcDatabase={-1}frontend,cn=config : Frontend database, default settings for other databases\\n\\n - olcDatabase={0}config,cn=config : slapd configuration database ( cn=config )\\n\\n145\\n\\n\\n-----\\n\\n - olcDatabase={1}mdb,cn=config : Your database instance ( dc=example,dc=com )\\n\\n**Example “dc=example,dc=com” DIT**\\n\\nThis is what the dc=example,dc=com DIT looks like:\\n\\n$ ldapsearch -x -LLL -H ldap:/// -b dc=example,dc=com dn\\n\\ndn: dc=example,dc=com\\n\\ndn: cn=admin,dc=example,dc=com\\n\\nWhere the entries mean the following:\\n\\n - dc=example,dc=com : Base of the DIT\\n\\n - cn=admin,dc=example,dc=com : Administrator (rootDN) for this DIT (set up during package install)\\n\\nNotice how we used two different authentication mechanisms:\\n\\n - -x\\n\\nThis is called a “simple bind”, and is essentially a plain text authentication. Since no **Bind DN** was provided\\n(via -D ), this became an *anonymous* bind. Without -x, the default is to use a Simple Authentication Security\\nLayer (SASL) bind.\\n\\n - -Y EXTERNAL\\nThis is using a SASL bind (no -x was provided), and further specifying the EXTERNAL type. Together with -H\\n\\nldapi:///, this uses a local Unix socket connection.\\n\\nIn both cases we only got the results that the server access-control lists (ACLs) allowed us to see, based on who we\\nare. A very handy tool to verify the authentication is ldapwhoami, which can be used as follows:\\n\\n$ ldapwhoami -x\\n\\nanonymous\\n\\n$ ldapwhoami -x -D cn=admin,dc=example,dc=com -W\\n\\nEnter LDAP Password:\\n\\ndn:cn=admin,dc=example,dc=com\\n\\nWhen you use simple bind ( -x ) and specify a Bind DN with -D as your authentication DN, the server will look for',\n",
       " 'a userPassword attribute in the entry, and use that to verify the credentials. In this particular case above, we used\\nthe database **Root DN** entry, i.e., the actual administrator, and that is a special case whose password is set in the\\nconfiguration when the package is installed.\\n\\n**Note** :\\n\\nA simple bind without some sort of transport security mechanism is **clear text**, meaning the credentials\\nare transmitted in the clear. You should add Transport Layer Security (TLS) support to your OpenLDAP\\nserver as soon as possible.\\n\\n**Example SASL EXTERNAL**\\n\\nHere are the SASL EXTERNAL examples:\\n\\n$ ldapwhoami -Y EXTERNAL -H ldapi:/// -Q\\n\\ndn:gidNumber=1000+uidNumber=1000,cn=peercred,cn=external,cn=auth\\n\\n$ sudo ldapwhoami -Y EXTERNAL -H ldapi:/// -Q\\n\\ndn:gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\\n\\nWhen using SASL EXTERNAL via the ldapi:/// transport, the Bind DN becomes a combination of the uid and gid\\nof the connecting user, followed by the suffix cn=peercred,cn=external,cn=auth . The server ACLs know about this,\\nand grant the local root user complete write access to cn=config via the SASL mechanism.\\n## **Populate the directory**\\n\\nLet’s introduce some content to our directory. We will add the following:\\n\\n  - A node called **People**, to store users\\n\\n146\\n\\n\\n-----\\n\\n**–** A user called **john**\\n\\n  - A node called **Groups**, to store groups\\n\\n**–** A group called **miners**\\n\\nCreate the following LDIF file and call it add_content.ldif :\\n\\ndn: ou=People,dc=example,dc=com\\n\\nobjectClass: organizationalUnit\\n\\nou: People\\n\\ndn: ou=Groups,dc=example,dc=com\\n\\nobjectClass: organizationalUnit\\n\\nou: Groups\\n\\ndn: cn=miners,ou=Groups,dc=example,dc=com\\n\\nobjectClass: posixGroup\\n\\ncn: miners',\n",
       " 'gidNumber: 5000\\n\\ndn: uid=john,ou=People,dc=example,dc=com\\n\\nobjectClass: inetOrgPerson\\n\\nobjectClass: posixAccount\\n\\nobjectClass: shadowAccount\\n\\nuid: john\\n\\nsn: Doe\\n\\ngivenName: John\\n\\ncn: John Doe\\n\\ndisplayName: John Doe\\n\\nuidNumber: 10000\\n\\ngidNumber: 5000\\n\\nuserPassword: {CRYPT}x\\n\\ngecos: John Doe\\n\\nloginShell: /bin/bash\\n\\nhomeDirectory: /home/john\\n\\n**Note** :\\n\\nIt’s important that uid and gid values in your directory do not collide with local values. You can use high\\nnumber ranges, such as starting at 5000 or even higher.\\n\\nAdd the content:\\n\\n$ ldapadd -x -D cn=admin,dc=example,dc=com -W -f add_content.ldif\\n\\nEnter LDAP Password: ********\\n\\nadding new entry \"ou=People,dc=example,dc=com\"\\n\\nadding new entry \"ou=Groups,dc=example,dc=com\"\\n\\nadding new entry \"cn=miners,ou=Groups,dc=example,dc=com\"\\n\\nadding new entry \"uid=john,ou=People,dc=example,dc=com\"\\n\\nWe can check that the information has been correctly added with the ldapsearch utility. For example, let’s search for\\nthe “john” entry, and request the cn and gidnumber attributes:\\n\\n$ ldapsearch -x -LLL -b dc=example,dc=com \\'(uid=john)\\' cn gidNumber\\n\\ndn: uid=john,ou=People,dc=example,dc=com\\n\\ncn: John Doe\\n\\ngidNumber: 5000\\n\\nHere we used an LDAP “filter”: (uid=john) . LDAP filters are very flexible and can become complex. For example, to\\nlist the group names of which **john** is a member, we could use the filter:\\n\\n(&(objectClass=posixGroup)(memberUid=john))\\n\\nThat is a logical “AND” between two attributes. Filters are very important in LDAP and mastering their syntax is\\nextremely helpful. They are used for simple queries like this, but can also select what content is to be replicated to a\\n[secondary server, or even in complex ACLs.',\n",
       " 'The full specification is defined in RFC 4515.](http://www.rfc-editor.org/rfc/rfc4515.txt)\\n\\n147\\n\\n\\n-----\\n\\nNotice we set the userPassword field for the “john” entry to the cryptic value {CRYPT}x . This essentially is an invalid\\npassword, because no hashing will produce just x . It’s a common pattern when adding a user entry without a default\\npassword. To change the password to something valid, you can now use ldappasswd :\\n\\n$ ldappasswd -x -D cn=admin,dc=example,dc=com -W -S uid=john,ou=people,dc=example,dc=com\\n\\nNew password:\\n\\nRe-enter new password:\\n\\nEnter LDAP Password:\\n\\n**Note** :\\n\\nRemember that simple binds are insecure and you should add TLS support to your server as soon as\\npossible!\\n## **Change the configuration**\\n\\nThe slapd-config DIT can also be queried and modified. Here are some common operations.\\n\\n**Add an index**\\n\\nUse ldapmodify to add an “Index” to your {1}mdb,cn=config database definition (for dc=example,dc=com ). Create a file\\ncalled uid_index.ldif, and add the following contents:\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: mail eq,sub\\n\\nThen issue the command:\\n\\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f uid_index.ldif\\n\\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\\n\\nYou can confirm the change in this way:\\n\\n$ sudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \\\\\\n\\ncn=config \\'(olcDatabase={1}mdb)\\' olcDbIndex\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nolcDbIndex: objectClass eq\\n\\nolcDbIndex: cn,uid eq\\n\\nolcDbIndex: uidNumber,gidNumber eq\\n\\nolcDbIndex: member,memberUid eq\\n\\nolcDbIndex: mail eq,sub\\n\\n**Change the RootDN password:**\\n\\nFirst, run slappasswd to get the hash for the new password you want:\\n\\n$ slappasswd\\n\\nNew password:\\n\\nRe-enter new password:',\n",
       " '{SSHA}VKrYMxlSKhONGRpC6rnASKNmXG2xHXFo\\n\\nNow prepare a changerootpw.ldif file with this content:\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nreplace: olcRootPW\\n\\nolcRootPW: {SSHA}VKrYMxlSKhONGRpC6rnASKNmXG2xHXFo\\n\\nFinally, run the ldapmodify command:\\n\\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f changerootpw.ldif\\n\\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\\n\\nWe still have the actual cn=admin,dc=example,dc=com DN in the dc=example,dc=com database, so let’s change that too.\\nSince this is a regular entry in this database suffix, we can use ldappasswd :\\n\\n148\\n\\n\\n-----\\n\\n$ ldappasswd -x -D cn=admin,dc=example,dc=com -W -S\\n\\nNew password:\\n\\nRe-enter new password:\\n\\nEnter LDAP Password: <-- current password, about to be changed\\n\\n**Add a schema**\\n\\nSchemas can only be added to cn=config if they are in LDIF format. If not, they will first have to be converted. You\\ncan find unconverted schemas in addition to converted ones in the /etc/ldap/schema directory.\\n\\n**Note** :\\nIt is not trivial to remove a schema from the slapd-config database. Practice adding schemas on a test\\nsystem.\\n\\nIn the following example we’ll add one of the pre-installed policy schemas in /etc/ldap/schema/ . The pre-installed\\nschemas exists in both converted ( .ldif ) and native ( .schema ) formats, so we don’t have to convert them and can use\\n\\nldapadd directly:\\n\\n$ sudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/corba.ldif\\n\\nadding new entry \"cn=corba,cn=schema,cn=config\"\\n\\nIf the schema you want to add does not exist in LDIF format, a nice conversion tool that can be used is provided in\\nthe schema2ldif package.\\n## **Logging**',\n",
       " 'Activity logging for slapd is very useful when implementing an OpenLDAP-based solution – and it must be manually\\nenabled after software installation. Otherwise, only rudimentary messages will appear in the logs. Logging, like any\\nother such configuration, is enabled via the slapd-config database.\\n\\nOpenLDAP comes with multiple logging levels, with each level containing the lower one (additive). A good level to\\ntry is **stats** [. The slapd-config man page has more to say on the different subsystems.](https://manpages.ubuntu.com/manpages/slapd-config.html)\\n\\n**Example logging with the stats level**\\n\\nCreate the file logging.ldif with the following contents:\\n\\ndn: cn=config\\n\\nchangetype: modify\\n\\nreplace: olcLogLevel\\n\\nolcLogLevel: stats\\n\\nImplement the change:\\n\\nsudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f logging.ldif\\n\\nThis will produce a significant amount of logging and you will want to revert back to a less verbose level once your\\nsystem is in production. While in this verbose mode your host’s syslog engine (rsyslog) may have a hard time keeping\\nup and may drop messages like this:\\n\\nrsyslogd-2177: imuxsock lost 228 messages from pid 2547 due to rate-limiting\\n\\nYou may consider a change to rsyslog’s configuration. In /etc/rsyslog.conf, put:\\n\\n# Disable rate limiting\\n\\n# (default is 200 messages in 5 seconds; below we make the 5 become 0)\\n\\n$SystemLogRateLimitInterval 0\\n\\nAnd then restart the rsyslog daemon:\\n\\nsudo systemctl restart syslog.service\\n## **Next steps**\\n\\nNow that you have successfully installed LDAP, you may want to about access control.\\n\\nThe management of what type of access (read, write, etc) users should be granted for resources is known as **access**\\n**control** .',\n",
       " 'The configuration directives involved are called **access control lists** or ACLs.\\n\\n149\\n\\n\\n-----\\n\\nWhen we installed the slapd package, various ACLs were set up automatically. We will look at a few important\\nconsequences of those defaults and, in so doing, we’ll get an idea of how ACLs work and how they’re configured.\\n\\nTo get the effective ACL for an LDAP query we need to look at the ACL entries of both the database being queried,\\nand those of the special frontend database instance. Note that the ACLs belonging to the frontend database are always\\nappended to the database-specific ACLs, and the first match ‘wins’.\\n## **Getting the ACLs**\\n\\nThe following commands will give, respectively, the ACLs of the mdb database ( dc=example,dc=com ) and those of the\\nfrontend database:\\n\\n$ sudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \\\\\\n\\ncn=config \\'(olcDatabase={1}mdb)\\' olcAccess\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nolcAccess: {0}to attrs=userPassword by self write by anonymous auth by * none\\n\\nolcAccess: {1}to attrs=shadowLastChange by self write by * read\\n\\nolcAccess: {2}to * by * read\\n\\n$ sudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \\\\\\n\\ncn=config \\'(olcDatabase={-1}frontend)\\' olcAccess\\n\\ndn: olcDatabase={-1}frontend,cn=config\\n\\nolcAccess: {0}to * by dn.exact=gidNumber=0+uidNumber=0,cn=peercred,cn=external\\n\\n,cn=auth manage by * break\\n\\nolcAccess: {1}to dn.exact=\"\" by * read\\n\\nolcAccess: {2}to dn.base=\"cn=Subschema\" by * read\\n\\n**Note** :\\n\\nThe RootDN always has full rights to its database and does not need to be included in any ACL.\\n## **Interpreting the results**\\n\\nThe first two ACLs are crucial:\\n\\nolcAccess: {0}to attrs=userPassword by self write by anonymous auth by * none',\n",
       " 'olcAccess: {1}to attrs=shadowLastChange by self write by * read\\n\\nThis can be represented differently for easier reading:\\n\\nto attrs=userPassword\\n\\nby self write\\n\\nby anonymous auth\\n\\nby * none\\n\\nto attrs=shadowLastChange\\n\\nby self write\\n\\nby * read\\n\\nThese ACLs enforce the following:\\n\\n  - Anonymous ‘auth’ access is provided to the **userPassword** attribute so that users can authenticate, or *bind* .\\nPerhaps counter-intuitively, ‘by anonymous auth’ is needed even when anonymous access to the DIT is unwanted,\\notherwise this would be a chicken-and-egg problem: before authentication, all users are anonymous.\\n\\n  - The ‘by self write’ ACL grants write access to the **userPassword** attribute to users who authenticated as the\\nDN where the attribute lives. In other words, users can update the **userPassword** attribute of their own entries.\\n\\n - The **userPassword** attribute is otherwise inaccessible by all other users, with the exception of the RootDN,\\nwho always has access and doesn’t need to be mentioned explicitly.\\n\\n  - In order for users to change their own password, using passwd or other utilities, the user’s own **shad-**\\n**owLastChange** attribute needs to be writable. All other directory users get to read this attribute’s\\n\\ncontents.\\n\\nThis DIT can be searched anonymously because of ‘to * by * read’ in this ACL, which grants read access to everything\\nelse, by anyone (including anonymous):\\n\\n150\\n\\n\\n-----\\n\\nto *\\n\\nby * read\\n\\nIf this is unwanted then you need to change the ACL. To force authentication during a bind request you can alternatively\\n(or in combination with the modified ACL) use the olcRequire: authc directive.\\n## **SASL identity**',\n",
       " \"There is no administrative account (“RootDN”) created for the slapd-config database. There is, however, a SASL\\nidentity that is granted full access to it. It represents the localhost’s superuser ( root / sudo ). Here it is:\\n\\ndn.exact=gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\\n\\nThe following command will display the ACLs of the slapd-config database:\\n\\n$ sudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \\\\\\n\\ncn=config '(olcDatabase={0}config)' olcAccess\\n\\ndn: olcDatabase={0}config,cn=config\\n\\nolcAccess: {0}to * by dn.exact=gidNumber=0+uidNumber=0,cn=peercred,\\n\\ncn=external,cn=auth manage by * break\\n\\nSince this is a SASL identity we need to use a SASL **mechanism** when invoking the LDAP utility in question – the\\n**EXTERNAL** mechanism (see the previous command for an example). Note that:\\n\\n  - You must use sudo to become the root identity in order for the ACL to match.\\n\\n - The EXTERNAL mechanism works via **Interprocess Communication** (IPC, UNIX domain sockets). This\\nmeans you must use the ldapi URI format.\\n\\nA succinct way to get all the ACLs is like this:\\n\\n$ sudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \\\\\\n\\ncn=config '(olcAccess=*)' olcAccess olcSuffix\\n## **Further reading**\\n\\n[• See the man page for slapd.access.](http://manpages.ubuntu.com/manpages/slapd.access.html)\\n\\n[• The access control topic in the OpenLDAP administrator’s guide.](https://openldap.org/doc/admin25/guide.html#Access%20Control)\\n\\nThe LDAP service becomes increasingly important as more networked systems begin to depend on it. In such an\\nenvironment, it is standard practice to build redundancy (high availability) into LDAP to prevent havoc should the\\nLDAP server become unresponsive.\",\n",
       " 'This is done through *LDAP replication* .\\n\\nReplication is achieved via the Sync replication engine, *syncrepl* . This allows changes to be synchronised using a\\n*Consumer* - *Provider* [model. A detailed description of this replication mechanism can be found in the OpenLDAP](https://openldap.org/doc/admin24/guide.html#LDAP%20Sync%20Replication)\\n[administrator’s guide and in its defining RFC 4533.](https://openldap.org/doc/admin24/guide.html#LDAP%20Sync%20Replication)\\n\\nThere are two ways to use this replication:\\n\\n - *Standard replication* : Changed entries are sent to the consumer in their entirety. For example, if the *userPassword*\\nattribute of the *uid=john,ou=people,dc=example,dc=com* entry changed, then the whole entry is sent to the\\n\\nconsumer.\\n\\n - *Delta replication* : Only the actual change is sent, instead of the whole entry.\\n\\nThe delta replication sends less data over the network, but is more complex to set up. We will show both in this guide.\\n\\n**Important** :\\nYou **must** have Transport Layer Security (TLS) enabled already. Please consult the LDAP with TLS guide\\nfor details of how to set this up.\\n## **Provider configuration - replication user**\\n\\nBoth replication strategies will need a replication user, as well as updates to the ACLs and limits regarding this user.\\nTo create the replication user, save the following contents to a file called replicator.ldif :\\n\\ndn: cn=replicator,dc=example,dc=com\\n\\nobjectClass: simpleSecurityObject\\n\\nobjectClass: organizationalRole\\n\\ncn: replicator\\n\\ndescription: Replication user\\n\\n151\\n\\n\\n-----\\n\\nuserPassword: {CRYPT}x\\n\\nThen add it with ldapadd :\\n\\n$ ldapadd -x -ZZ -D cn=admin,dc=example,dc=com -W -f replicator.ldif\\n\\nEnter LDAP Password:',\n",
       " 'adding new entry \"cn=replicator,dc=example,dc=com\"\\n\\nNow set a password for it with ldappasswd :\\n\\n$ ldappasswd -x -ZZ -D cn=admin,dc=example,dc=com -W -S cn=replicator,dc=example,dc=com\\n\\nNew password:\\n\\nRe-enter new password:\\n\\nEnter LDAP Password:\\n\\nThe next step is to give this replication user the correct privileges, i.e.:\\n\\n  - Read access to the content that we want replicated\\n\\n  - No search limits on this content\\n\\nFor that we need to update the ACLs on the provider. Since ordering matters, first check what the existing ACLs\\nlook like on the *dc=example,dc=com* tree:\\n\\n$ sudo ldapsearch -Q -Y EXTERNAL -H ldapi:/// -LLL -b cn=config \\'(olcSuffix=dc=example,dc=com)\\' olcAccess\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nolcAccess: {0}to attrs=userPassword by self write by anonymous auth by * none\\n\\nolcAccess: {1}to attrs=shadowLastChange by self write by * read\\n\\nolcAccess: {2}to * by * read\\n\\nWhat we need is to insert a new rule before the first one, and also adjust the limits for the replicator user. Prepare\\nthe replicator-acl-limits.ldif file with this content:\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nadd: olcAccess\\n\\nolcAccess: {0}to *\\n\\nby dn.exact=\"cn=replicator,dc=example,dc=com\" read\\n\\nby * break\\n\\n\\nadd: olcLimits\\n\\nolcLimits: dn.exact=\"cn=replicator,dc=example,dc=com\"\\n\\ntime.soft=unlimited time.hard=unlimited\\n\\nsize.soft=unlimited size.hard=unlimited\\n\\nAnd add it to the server:\\n\\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f replicator-acl-limits.ldif\\n\\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\\n## **Provider configuration - standard replication**\\n\\nThe remaining configuration for the provider using standard replication is to add the *syncprov* overlay on top of the',\n",
       " '*dc=example,dc=com* database.\\n\\nCreate a file called provider_simple_sync.ldif with this content:\\n\\n# Add indexes to the frontend db.\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: entryCSN eq\\n\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: entryUUID eq\\n\\n#Load the syncprov module.\\n\\ndn: cn=module{0},cn=config\\n\\nchangetype: modify\\n\\nadd: olcModuleLoad\\n\\nolcModuleLoad: syncprov\\n\\n152\\n\\n\\n-----\\n\\n# syncrepl Provider for primary db\\n\\ndn: olcOverlay=syncprov,olcDatabase={1}mdb,cn=config\\n\\nchangetype: add\\n\\nobjectClass: olcOverlayConfig\\n\\nobjectClass: olcSyncProvConfig\\n\\nolcOverlay: syncprov\\n\\nolcSpCheckpoint: 100 10\\n\\nolcSpSessionLog: 100\\n\\n**Customisation warning** :\\nThe LDIF above has some parameters that you should review before deploying in production on your\\ndirectory. In particular – olcSpCheckpoint and olcSpSessionLog .\\n[Please see the slapo-syncprov(5) man page. In general,](http://manpages.ubuntu.com/manpages/focal/man5/slapo-syncprov.5.html) olcSpSessionLog should be equal to (or preferably\\n[larger than) the number of entries in your directory. Also see ITS #8125 for details on an existing bug.](https://www.openldap.org/its/index.cgi/?findid=8125)\\n\\nAdd the new content:\\n\\nsudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f provider_simple_sync.ldif\\n\\nThe Provider is now configured.\\n## **Consumer configuration - standard replication**\\n\\nInstall the software by going through enable TLS.\\n\\nCreate an LDIF file with the following contents and name it consumer_simple_sync.ldif :\\n\\ndn: cn=module{0},cn=config\\n\\nchangetype: modify\\n\\nadd: olcModuleLoad\\n\\nolcModuleLoad: syncprov\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: entryUUID eq\\n\\n\\nadd: olcSyncrepl',\n",
       " 'olcSyncrepl: rid=0\\n\\nprovider=ldap://ldap01.example.com\\n\\nbindmethod=simple\\n\\nbinddn=\"cn=replicator,dc=example,dc=com\" credentials=<secret>\\n\\nsearchbase=\"dc=example,dc=com\"\\n\\nschemachecking=on\\n\\ntype=refreshAndPersist retry=\"60 +\"\\n\\nstarttls=critical tls_reqcert=demand\\n\\n\\nadd: olcUpdateRef\\n\\nolcUpdateRef: ldap://ldap01.example.com\\n\\nEnsure the following attributes have the correct values:\\n\\n - **provider** : Provider server’s hostname – ldap01.example.com in this example – or IP address. It must match\\nwhat is presented in the provider’s SSL certificate.\\n\\n - **binddn** : The bind DN for the *replicator* user.\\n\\n - **credentials** : The password you selected for the replicator user.\\n\\n - **searchbase** : The database suffix you’re using, i.e., content that is to be replicated.\\n\\n - **olcUpdateRef** : Provider server’s hostname or IP address, given to clients if they try to write to this consumer.\\n\\n - **rid** : Replica ID, a unique 3-digit ID that identifies the replica. Each consumer should have at least one rid.\\n\\n**Note** :\\nNote that a successful encrypted connection via *START_TLS* is being enforced in this configuration, to\\navoid sending the credentials in the clear across the network. See LDAP with TLS for details on how to\\nset up OpenLDAP with trusted SSL certificates.\\n\\n153\\n\\n\\n-----\\n\\nAdd the new configuration:\\n\\nsudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f consumer_simple_sync.ldif\\n\\nNow you’re done! The *dc=example,dc=com* tree should now be synchronising.\\n## **Provider configuration - delta replication**\\n\\nThe remaining provider configuration for delta replication is:\\n\\n  - Create a new database called *accesslog*',\n",
       " ' - Add the syncprov overlay on top of the *accesslog* and *dc=example,dc=com* databases\\n\\n - Add the accesslog overlay on top of the *dc=example,dc=com* database\\n\\n**Add** syncprov **and** accesslog **overlays and DBs**\\n\\nCreate an LDIF file with the following contents and name it provider_sync.ldif :\\n\\n# Add indexes to the frontend db.\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: entryCSN eq\\n\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: entryUUID eq\\n\\n#Load the syncprov and accesslog modules.\\n\\ndn: cn=module{0},cn=config\\n\\nchangetype: modify\\n\\nadd: olcModuleLoad\\n\\nolcModuleLoad: syncprov\\n\\n\\nadd: olcModuleLoad\\n\\nolcModuleLoad: accesslog\\n\\n# Accesslog database definitions\\n\\ndn: olcDatabase={2}mdb,cn=config\\n\\nobjectClass: olcDatabaseConfig\\n\\nobjectClass: olcMdbConfig\\n\\nolcDatabase: {2}mdb\\n\\nolcDbDirectory: /var/lib/ldap/accesslog\\n\\nolcSuffix: cn=accesslog\\n\\nolcRootDN: cn=admin,dc=example,dc=com\\n\\nolcDbIndex: default eq\\n\\nolcDbIndex: entryCSN,objectClass,reqEnd,reqResult,reqStart\\n\\nolcAccess: {0}to * by dn.exact=\"cn=replicator,dc=example,dc=com\" read by * break\\n\\nolcLimits: dn.exact=\"cn=replicator,dc=example,dc=com\"\\n\\ntime.soft=unlimited time.hard=unlimited\\n\\nsize.soft=unlimited size.hard=unlimited\\n\\n# Accesslog db syncprov.\\n\\ndn: olcOverlay=syncprov,olcDatabase={2}mdb,cn=config\\n\\nchangetype: add\\n\\nobjectClass: olcOverlayConfig\\n\\nobjectClass: olcSyncProvConfig\\n\\nolcOverlay: syncprov\\n\\nolcSpNoPresent: TRUE\\n\\nolcSpReloadHint: TRUE\\n\\n# syncrepl Provider for primary db\\n\\ndn: olcOverlay=syncprov,olcDatabase={1}mdb,cn=config\\n\\nchangetype: add\\n\\nobjectClass: olcOverlayConfig\\n\\nobjectClass: olcSyncProvConfig\\n\\n154\\n\\n\\n-----\\n\\nolcOverlay: syncprov\\n\\nolcSpCheckpoint: 100 10\\n\\nolcSpSessionLog: 100',\n",
       " '# accesslog overlay definitions for primary db\\n\\ndn: olcOverlay=accesslog,olcDatabase={1}mdb,cn=config\\n\\nobjectClass: olcOverlayConfig\\n\\nobjectClass: olcAccessLogConfig\\n\\nolcOverlay: accesslog\\n\\nolcAccessLogDB: cn=accesslog\\n\\nolcAccessLogOps: writes\\n\\nolcAccessLogSuccess: TRUE\\n\\n# scan the accesslog DB every day, and purge entries older than 7 days\\n\\nolcAccessLogPurge: 07+00:00 01+00:00\\n\\n**Customisation warning** :\\nThe LDIF above has some parameters that you should review before deploying in production on your\\ndirectory. In particular – olcSpCheckpoint, olcSpSessionLog .\\n[Please see the slapo-syncprov(5) manpage. In general,](http://manpages.ubuntu.com/manpages/focal/man5/slapo-syncprov.5.html) olcSpSessionLog should be equal to (or preferably\\n[larger than) the number of entries in your directory. Also see ITS #8125 for details on an existing bug.](https://www.openldap.org/its/index.cgi/?findid=8125)\\nFor olcAccessLogPurge [, please check the slapo-accesslog(5) manpage.](http://manpages.ubuntu.com/manpages/focal/man5/slapo-accesslog.5.html)\\n\\nCreate a directory:\\n\\nsudo -u openldap mkdir /var/lib/ldap/accesslog\\n\\nAdd the new content:\\n\\nsudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f provider_sync.ldif\\n\\nThe Provider is now configured.\\n## **Consumer configuration**\\n\\nInstall the software by going through enable TLS.\\n\\nCreate an LDIF file with the following contents and name it consumer_sync.ldif :\\n\\ndn: cn=module{0},cn=config\\n\\nchangetype: modify\\n\\nadd: olcModuleLoad\\n\\nolcModuleLoad: syncprov\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: entryUUID eq\\n\\n\\nadd: olcSyncrepl\\n\\nolcSyncrepl: rid=0\\n\\nprovider=ldap://ldap01.example.com\\n\\nbindmethod=simple',\n",
       " 'binddn=\"cn=replicator,dc=example,dc=com\" credentials=<secret>\\n\\nsearchbase=\"dc=example,dc=com\"\\n\\nlogbase=\"cn=accesslog\"\\n\\nlogfilter=\"(&(objectClass=auditWriteObject)(reqResult=0))\"\\n\\nschemachecking=on\\n\\ntype=refreshAndPersist retry=\"60 +\"\\n\\nsyncdata=accesslog\\n\\nstarttls=critical tls_reqcert=demand\\n\\n\\nadd: olcUpdateRef\\n\\nolcUpdateRef: ldap://ldap01.example.com\\n\\nEnsure the following attributes have the correct values:\\n\\n - **provider** : Provider server’s hostname – ldap01.example.com in this example – or IP address. It must match\\nwhat is presented in the provider’s SSL certificate.\\n\\n155\\n\\n\\n-----\\n\\n - **binddn** : The bind DN for the *replicator* user.\\n\\n - **credentials** : The password you selected for the replicator user.\\n\\n - **searchbase** : The database suffix you’re using, i.e., content that is to be replicated.\\n\\n - **olcUpdateRef** : Provider server’s hostname or IP address, given to clients if they try to write to this consumer.\\n\\n - **rid** : Replica ID, a unique 3-digit ID that identifies the replica. Each consumer should have at least one rid.\\n\\n**Note** :\\nNote that a successful encrypted connection via *START_TLS* is being enforced in this configuration, to\\navoid sending the credentials in the clear across the network. See LDAP with TLS for details on how to\\nset up OpenLDAP with trusted SSL certificates.\\n\\nAdd the new configuration:\\n\\nsudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f consumer_sync.ldif\\n\\nYou’re done! The *dc=example,dc=com* tree should now be synchronising.\\n## **Testing**\\n\\nOnce replication starts, you can monitor it by running:\\n\\n$ ldapsearch -z1 -LLL -x -s base -b dc=example,dc=com contextCSN\\n\\ndn: dc=example,dc=com\\n\\ncontextCSN: 20200423222317.722667Z#000000#000#000000',\n",
       " \"On both the provider and the consumer. Once the *contextCSN* value for both match, both trees are in sync. Every\\ntime a change is done in the provider, this value will change and so should the one in the consumer(s).\\n\\nIf your connection is slow and/or your LDAP database large, it might take a while for the consumer’s *contextCSN*\\nmatch the provider’s. But, you will know it is progressing since the consumer’s *contextCSN* will be steadily increasing.\\n\\nIf the consumer’s *contextCSN* is missing or does not match the provider, you should stop and figure out the issue before\\ncontinuing. Try checking the slapd entries in /var/log/syslog in the provider to see if the consumer’s authentication\\nrequests were successful, or that its requests to retrieve data return no errors. In particular, verify that you can\\nconnect to the provider from the consumer as the replicator BindDN using *START_TLS* :\\n\\nldapwhoami -x -ZZ -D cn=replicator,dc=example,dc=com -W -h ldap01.example.com\\n\\nFor our example, you should now see the *john* user in the replicated tree:\\n\\n$ ldapsearch -x -LLL -b dc=example,dc=com -h ldap02.example.com '(uid=john)' uid\\n\\ndn: uid=john,ou=People,dc=example,dc=com\\n\\nuid: john\\n## **References**\\n\\n[• Replication types, OpenLDAP Administrator’s Guide](https://openldap.org/doc/admin24/guide.html#Configuring%20the%20different%20replication%20types)\\n\\n[• LDAP Sync Replication - OpenLDAP Administrator’s Guide](https://openldap.org/doc/admin24/guide.html#LDAP%20Sync%20Replication)\\n\\n[• RFC 4533.](http://www.rfc-editor.org/rfc/rfc4533.txt)\\n\\nOnce you have a working LDAP server, you will need to install libraries on the client that know how and when to\\ncontact it.\",\n",
       " 'On Ubuntu, this was traditionally done by installing the libnss-ldap package, but nowadays you should\\nuse the System Security Services Daemon (SSSD). To find out how to use LDAP with SSSD, refer to our SSSD and\\nLDAP guide.\\n## User and group management - ldapscripts\\n\\nA common use case for an LDAP server is to store Unix user and group information in the directory. There are many\\ntools out there, and big deployments will usually develop their own. However, as a quick and easy way to get started\\nstoring user and group information in OpenLDAP, you can use the ldapscripts package.\\n\\n**Install ldapscripts**\\n\\nYou can install ldapscripts by running the following command:\\n\\nsudo apt install ldapscripts\\n\\nThen edit the file /etc/ldapscripts/ldapscripts.conf to arrive at something similar to the following:\\n\\n156\\n\\n\\n-----\\n\\nSERVER=ldap://ldap01.example.com\\n\\nLDAPBINOPTS=\"-ZZ\"\\n\\nBINDDN=\\'cn=admin,dc=example,dc=com\\'\\n\\nBINDPWDFILE=\"/etc/ldapscripts/ldapscripts.passwd\"\\n\\nSUFFIX=\\'dc=example,dc=com\\'\\n\\nGSUFFIX=\\'ou=Groups\\'\\n\\nUSUFFIX=\\'ou=People\\'\\n\\nMSUFFIX=\\'ou=Computers\\'\\n\\n**Notes** :\\n\\n    - Adjust *SERVER* and related *SUFFIX* options to suit your directory structure.\\n\\n    - Here, we are forcing use of *START_TLS* ( -ZZ parameter). Refer to LDAP with TLS to learn how to\\nset up the server with TLS support.\\n\\nStore the cn=admin password in the /etc/ldapscripts/ldapscripts.passwd file and make sure it’s only readable by the\\n*root* local user:\\n\\nsudo chmod 400 /etc/ldapscripts/ldapscripts.passwd\\n\\nThe scripts are now ready to help manage your directory.\\n## **Manage users and groups with ldapscripts**\\n\\nHere are some brief examples you can use to manage users and groups using ldapscripts .\\n\\n**Create a new user**',\n",
       " \"sudo ldapaddgroup george\\n\\nsudo ldapadduser george george\\n\\nThis will create a group and user with name “george” and set the user’s primary group ( **gid** ) to “george” as well.\\n\\n**Change a user’s password**\\n\\n$ sudo ldapsetpasswd george\\n\\nChanging password for user uid=george,ou=People,dc=example,dc=com\\n\\nNew Password:\\n\\nRetype New Password:\\n\\nSuccessfully set password for user uid=george,ou=People,dc=example,dc=com\\n## **Delete a user**\\n\\nsudo ldapdeleteuser george\\n\\nNote that this won’t delete the user’s primary group, but will remove the user from supplementary ones.\\n## **Add a group**\\n\\nsudo ldapaddgroup qa\\n## **Delete a group**\\n\\nsudo ldapdeletegroup qa\\n## **Add a user to a group**\\n\\nsudo ldapaddusertogroup george qa\\n\\nYou should now see a **memberUid** attribute for the “qa” group with a value of “george”.\\n## **Remove a user from a group**\\n\\nsudo ldapdeleteuserfromgroup george qa\\n\\nThe **memberUid** attribute should now be removed from the “qa” group.\\n\\n157\\n\\n\\n-----\\n\\n## Manage user attributes with ldapmodifyuser\\n\\nThe ldapmodifyuser script allows you to add, remove, or replace a user's attributes. The script uses the same\\n\\nsyntax as the ldapmodify‘ utility. For example:\\n\\nsudo ldapmodifyuser george\\n\\n# About to modify the following entry :\\n\\ndn: uid=george,ou=People,dc=example,dc=com\\n\\nobjectClass: account\\n\\nobjectClass: posixAccount\\n\\ncn: george\\n\\nuid: george\\n\\nuidNumber: 10001\\n\\ngidNumber: 10001\\n\\nhomeDirectory: /home/george\\n\\nloginShell: /bin/bash\\n\\ngecos: george\\n\\ndescription: User account\\n\\nuserPassword:: e1NTSEF9eXFsTFcyWlhwWkF1eGUybVdFWHZKRzJVMjFTSG9vcHk=\\n\\n# Enter your modifications here, end with CTRL-D.\\n\\ndn: uid=george,ou=People,dc=example,dc=com\\n\\nreplace: gecos\\n\\ngecos: George Carlin\",\n",
       " 'The user’s **gecos** should now be “George Carlin”.\\n## ldapscripts templates\\n\\nA nice feature of ldapscripts is the template system. Templates allow you to customise the attributes of user, group,\\nand machine objects. For example, to enable the **user** template, edit /etc/ldapscripts/ldapscripts.conf by changing:\\n\\nUTEMPLATE=\"/etc/ldapscripts/ldapadduser.template\"\\n\\nThere are sample templates in the /usr/share/doc/ldapscripts/examples directory. Copy or rename the ldapad\\nduser.template.sample file to /etc/ldapscripts/ldapadduser.template :\\n\\nsudo cp /usr/share/doc/ldapscripts/examples/ldapadduser.template.sample \\\\\\n\\n/etc/ldapscripts/ldapadduser.template\\n\\nEdit the new template to add the desired attributes. The following will create new users with an **objectClass** of\\n“inetOrgPerson”:\\n\\ndn: uid=<user>,<usuffix>,<suffix>\\n\\nobjectClass: inetOrgPerson\\n\\nobjectClass: posixAccount\\n\\ncn: <user>\\n\\nsn: <ask>\\n\\nuid: <user>\\n\\nuidNumber: <uid>\\n\\ngidNumber: <gid>\\n\\nhomeDirectory: <home>\\n\\nloginShell: <shell>\\n\\ngecos: <user>\\n\\ndescription: User account\\n\\ntitle: Employee\\n\\nNotice the <ask> option used for the **sn** attribute. This will make ldapadduser prompt you for its value.\\n\\nThere are utilities in the package that were not covered here. This command will output a list of them:\\n\\ndpkg -L ldapscripts | grep /usr/sbin\\n## **Next steps**\\n\\nNow that you know how to set up and modify users and groups, you may wish to learn more about how access control\\nworks. If you’re already familiar with this topic, it’s a good idea to secure your LDAP communication by setting up\\nTransport Layer Security (TLS).\\n\\n158\\n\\n\\n-----\\n\\nWhen authenticating to an OpenLDAP server it is best to do so using an encrypted session.',\n",
       " 'This can be accomplished\\nusing Transport Layer Security (TLS).\\n\\nHere, we will be our own *Certificate Authority* (CA) and then create and sign our LDAP server certificate as that CA.\\nThis guide will use the certtool utility to complete these tasks. For simplicity, this is being done on the OpenLDAP\\nserver itself, but your real internal CA should be elsewhere.\\n\\nInstall the gnutls-bin and ssl-cert packages:\\n\\nsudo apt install gnutls-bin ssl-cert\\n\\nCreate a private key for the Certificate Authority:\\n\\nsudo certtool --generate-privkey --bits 4096 --outfile /etc/ssl/private/mycakey.pem\\n\\nCreate the template/file /etc/ssl/ca.info to define the CA:\\n\\ncn = Example Company\\n\\nca\\n\\ncert_signing_key\\n\\nexpiration_days = 3650\\n\\nCreate the self-signed CA certificate:\\n\\nsudo certtool --generate-self-signed \\\\\\n\\n--load-privkey /etc/ssl/private/mycakey.pem \\\\\\n\\n--template /etc/ssl/ca.info \\\\\\n\\n--outfile /usr/local/share/ca-certificates/mycacert.crt\\n\\n**Note** :\\nYes, the --outfile path is correct. We are writing the CA certificate to /usr/local/share/ca\\ncertificates . This is where *update-ca-certificates* will pick up trusted local CAs from. To pick up CAs\\nfrom /usr/share/ca-certificates, a call to dpkg-reconfigure ca-certificates is necessary.\\n\\nRun update-ca-certificates to add the new CA certificate to the list of trusted CAs. Note the one added CA:\\n\\n$ sudo update-ca-certificates\\n\\nUpdating certificates in /etc/ssl/certs...\\n\\n1 added, 0 removed; done.\\n\\nRunning hooks in /etc/ca-certificates/update.d...\\n\\ndone.\\n\\nThis also creates a /etc/ssl/certs/mycacert.pem symlink pointing to the real file in /usr/local/share/ca-certificates .\\n\\nMake a private key for the server:\\n\\nsudo certtool --generate-privkey \\\\',\n",
       " '--bits 2048 \\\\\\n\\n--outfile /etc/ldap/ldap01_slapd_key.pem\\n\\n**Note** :\\nReplace ldap01 in the filename with your server’s hostname. Naming the certificate and key for the host\\nand service that will be using them will help keep things clear.\\n\\nCreate the /etc/ssl/ldap01.info info file containing:\\n\\norganization = Example Company\\n\\ncn = ldap01.example.com\\n\\ntls_www_server\\n\\nencryption_key\\n\\nsigning_key\\n\\nexpiration_days = 365\\n\\nThe above certificate is good for 1 year, and it’s valid only for the ldap01.example.com hostname. You can adjust this\\naccording to your needs.\\n\\nCreate the server’s certificate:\\n\\nsudo certtool --generate-certificate \\\\\\n\\n--load-privkey /etc/ldap/ldap01_slapd_key.pem \\\\\\n\\n--load-ca-certificate /etc/ssl/certs/mycacert.pem \\\\\\n\\n--load-ca-privkey /etc/ssl/private/mycakey.pem \\\\\\n\\n--template /etc/ssl/ldap01.info \\\\\\n\\n159\\n\\n\\n-----\\n\\n--outfile /etc/ldap/ldap01_slapd_cert.pem\\n\\nAdjust permissions and ownership:\\n\\nsudo chgrp openldap /etc/ldap/ldap01_slapd_key.pem\\n\\nsudo chmod 0640 /etc/ldap/ldap01_slapd_key.pem\\n\\nYour server is now ready to accept the new TLS configuration.\\n\\nCreate the file certinfo.ldif with the following contents (adjust paths and filenames accordingly):\\n\\ndn: cn=config\\n\\nadd: olcTLSCACertificateFile\\n\\nolcTLSCACertificateFile: /etc/ssl/certs/mycacert.pem\\n\\n\\nadd: olcTLSCertificateFile\\n\\nolcTLSCertificateFile: /etc/ldap/ldap01_slapd_cert.pem\\n\\n\\nadd: olcTLSCertificateKeyFile\\n\\nolcTLSCertificateKeyFile: /etc/ldap/ldap01_slapd_key.pem\\n\\nUse the ldapmodify command to tell slapd about our TLS work via the *slapd-config* database:\\n\\nsudo ldapmodify -Y EXTERNAL -H ldapi:/// -f certinfo.ldif',\n",
       " 'If you need access to *LDAPS* (LDAP over SSL), then you need to edit /etc/default/slapd and include ldaps:/// in\\n\\nSLAPD_SERVICES like below:\\n\\nSLAPD_SERVICES=\"ldap:/// ldapi:/// ldaps:///\"\\n\\nAnd restart slapd with: sudo systemctl restart slapd .\\n\\nNote that *StartTLS* will be available without the change above, and does NOT need a slapd restart.\\n\\nTest *StartTLS* :\\n\\n$ ldapwhoami -x -ZZ -H ldap://ldap01.example.com\\n\\nanonymous\\n\\nTest *LDAPS* :\\n\\n$ ldapwhoami -x -H ldaps://ldap01.example.com\\n\\nanonymous\\n## **Certificate for an OpenLDAP replica**\\n\\nTo generate a certificate pair for an OpenLDAP replica (consumer), create a holding directory (which will be used for\\nthe eventual transfer) and run the following:\\n\\nmkdir ldap02-ssl\\n\\ncd ldap02-ssl\\n\\ncerttool --generate-privkey \\\\\\n\\n--bits 2048 \\\\\\n\\n--outfile ldap02_slapd_key.pem\\n\\nCreate an info file, ldap02.info, for the Consumer server, adjusting its values according to your requirements:\\n\\norganization = Example Company\\n\\ncn = ldap02.example.com\\n\\ntls_www_server\\n\\nencryption_key\\n\\nsigning_key\\n\\nexpiration_days = 365\\n\\nCreate the Consumer’s certificate:\\n\\nsudo certtool --generate-certificate \\\\\\n\\n--load-privkey ldap02_slapd_key.pem \\\\\\n\\n--load-ca-certificate /etc/ssl/certs/mycacert.pem \\\\\\n\\n--load-ca-privkey /etc/ssl/private/mycakey.pem \\\\\\n\\n--template ldap02.info \\\\\\n\\n--outfile ldap02_slapd_cert.pem\\n\\n160\\n\\n\\n-----\\n\\n**Note** :\\nWe had to use sudo to get access to the CA’s private key. This means the generated certificate file is owned\\nby root. You should change that ownership back to your regular user before copying these files over to the\\nConsumer.\\n\\nGet a copy of the CA certificate:\\n\\ncp /etc/ssl/certs/mycacert.pem .\\n\\nWe’re done.',\n",
       " 'Now transfer the ldap02-ssl directory to the Consumer. Here we use scp (adjust accordingly):\\n\\ncd ..\\n\\nscp -r ldap02-ssl user@consumer:\\n\\nOn the Consumer side, install the certificate files you just transferred:\\n\\nsudo cp ldap02_slapd_cert.pem ldap02_slapd_key.pem /etc/ldap\\n\\nsudo chgrp openldap /etc/ldap/ldap02_slapd_key.pem\\n\\nsudo chmod 0640 /etc/ldap/ldap02_slapd_key.pem\\n\\nsudo cp mycacert.pem /usr/local/share/ca-certificates/mycacert.crt\\n\\nsudo update-ca-certificates\\n\\nCreate the file certinfo.ldif with the following contents (adjust accordingly regarding paths and filenames, if needed):\\n\\ndn: cn=config\\n\\nadd: olcTLSCACertificateFile\\n\\nolcTLSCACertificateFile: /etc/ssl/certs/mycacert.pem\\n\\n\\nadd: olcTLSCertificateFile\\n\\nolcTLSCertificateFile: /etc/ldap/ldap02_slapd_cert.pem\\n\\n\\nadd: olcTLSCertificateKeyFile\\n\\nolcTLSCertificateKeyFile: /etc/ldap/ldap02_slapd_key.pem\\n\\nConfigure the *slapd-config* database:\\n\\nsudo ldapmodify -Y EXTERNAL -H ldapi:/// -f certinfo.ldif\\n\\nLike before, if you want to enable *LDAPS*, edit /etc/default/slapd and add ldaps:/// to SLAPD_SERVICES, and then\\nrestart slapd .\\n\\nTest *StartTLS* :\\n\\n$ ldapwhoami -x -ZZ -H ldap://ldap02.example.com\\n\\nanonymous\\n\\nTest *LDAPS* :\\n\\n$ ldapwhoami -x -H ldaps://ldap02.example.com\\n\\nanonymous\\n\\nNow we have LDAP running just the way we want, it is time to ensure we can save all of our work and restore it as\\nneeded.\\n\\nWhat we need is a way to back up the directory database(s) – specifically the configuration backend ( *cn=config* ) and\\nthe DIT ( *dc=example,dc=com* ). If we are going to backup those databases into, say, /export/backup, we could use\\n\\nslapcat as shown in the following script, called /usr/local/bin/ldapbackup :\\n\\n#!/bin/bash',\n",
       " 'set -e\\n\\nBACKUP_PATH=/export/backup\\n\\nSLAPCAT=/usr/sbin/slapcat\\n\\nnice ${SLAPCAT} -b cn=config > ${BACKUP_PATH}/config.ldif\\n\\nnice ${SLAPCAT} -b dc=example,dc=com > ${BACKUP_PATH}/example.com.ldif\\n\\nchown root:root ${BACKUP_PATH}/*\\n\\nchmod 600 ${BACKUP_PATH}/*.ldif\\n\\n**Note** :\\nThese files are uncompressed text files containing everything in your directory including the tree layout,\\n\\n161\\n\\n\\n-----\\n\\nusernames, and every password. So, you might want to consider making /export/backup an encrypted\\npartition and even having the script encrypt those files as it creates them. Ideally you should do both, but\\nthat depends on your security requirements.\\n\\nThen, it is just a matter of having a cron script to run this program as often as you feel comfortable with. For many, once\\na day suffices. For others, more often is required. Here is an example of a cron script called /etc/cron.d/ldapbackup\\nthat is run every night at 22:45h:\\n\\nMAILTO=backup-emails@domain.com\\n\\n45 22 * * * root /usr/local/bin/ldapbackup\\n\\nNow the files are created, they should be copied to a backup server.\\n\\nAssuming we did a fresh reinstall of LDAP, the restore process could be something like this:\\n\\n#!/bin/bash\\n\\nset -e\\n\\nBACKUP_PATH=/export/backup\\n\\nSLAPADD=/usr/sbin/slapadd\\n\\nif [ -n \"$(ls -l /var/lib/ldap/* 2>/dev/null)\" -o -n \"$(ls -l /etc/ldap/slapd.d/* 2>/dev/null)\" ]; then\\n\\necho Run the following to remove the existing db:\\n\\necho sudo systemctl stop slapd.service\\n\\necho sudo rm -rf /etc/ldap/slapd.d/* /var/lib/ldap/*\\n\\nexit 1\\n\\nfi\\n\\nsudo systemctl stop slapd.service || :\\n\\nsudo slapadd -F /etc/ldap/slapd.d -b cn=config -l /export/backup/config.ldif',\n",
       " 'sudo slapadd -F /etc/ldap/slapd.d -b dc=example,dc=com -l /export/backup/example.com.ldif\\n\\nsudo chown -R openldap:openldap /etc/ldap/slapd.d/\\n\\nsudo chown -R openldap:openldap /var/lib/ldap/\\n\\nsudo systemctl start slapd.service\\n\\nThis is a simplistic backup strategy, of course. It’s being shown here as a reference for the basic tooling you can use\\nfor backups and restores.\\n\\nKerberos is a network authentication system based on the principal of a trusted third party. The other two parties\\nbeing the user and the service the user wishes to authenticate to. Not all services and applications can use Kerberos,\\nbut for those that can, it brings the network environment one step closer to being Single Sign On (SSO).\\n\\nThis section covers installation and configuration of a Kerberos server, and some example client configurations.\\n## **Overview**\\n\\nIf you are new to Kerberos there are a few terms that are good to understand before setting up a Kerberos server.\\nMost of the terms will relate to things you may be familiar with in other environments:\\n\\n - *Principal:* any users, computers, and services provided by servers need to be defined as Kerberos Principals.\\n\\n - *Instances:* are a variation for service principals. For example, the principal for an NFS service will have an\\ninstance for the hostname of the server, like nfs/server.example.com@REALM . Similarly admin privileges on a\\nprincipal use an instance of /admin, like john/admin@REALM, differentiating it from john@REALM . These variations\\nfit nicely with ACLs.\\n\\n - *Realms:* the unique realm of control provided by the Kerberos installation. Think of it as the domain or group\\nyour hosts and users belong to.',\n",
       " 'Convention dictates the realm should be in uppercase. By default, Ubuntu will\\nuse the DNS domain converted to uppercase ( EXAMPLE.COM ) as the realm.\\n\\n - *Key Distribution Center:* (KDC) consist of three parts: a database of all principals, the authentication server,\\nand the ticket granting server. For each realm there must be at least one KDC.\\n\\n - *Ticket Granting Ticket:* issued by the Authentication Server (AS), the Ticket Granting Ticket (TGT) is encrypted\\nin the user’s password which is known only to the user and the KDC. This is the starting point for a user to\\nacquire additional tickets for the services being accessed.\\n\\n - *Ticket Granting Server:* (TGS) issues service tickets to clients upon request.\\n\\n - *Tickets:* confirm the identity of the two principals. One principal being a user and the other a service requested\\nby the user. Tickets establish an encryption key used for secure communication during the authenticated session.\\n\\n162\\n\\n\\n-----\\n\\n - *Keytab Files:* contain encryption keys for a service or host extracted from the KDC principal database.\\n\\nTo put the pieces together, a Realm has at least one KDC, preferably more for redundancy, which contains a database\\nof Principals. When a user principal logs into a workstation that is configured for Kerberos authentication, the KDC\\nissues a Ticket Granting Ticket (TGT). If the user supplied credentials match, the user is authenticated and can then\\nrequest tickets for Kerberized services from the Ticket Granting Server (TGS). The service tickets allow the user to\\nauthenticate to the service without entering another username and password.\\n## **Resources**',\n",
       " '[• For more information on MIT’s version of Kerberos, see the MIT Kerberos site.](http://web.mit.edu/Kerberos/)\\n\\n  - Also, feel free to stop by the *#ubuntu-server* and *#kerberos* [IRC channels on Libera.Chat if you have Kerberos](https://libera.chat/)\\nquestions.\\n\\n[• Another guide for installing Kerberos on Debian, includes PKINIT](http://techpubs.spinlocksolutions.com/dklar/kerberos.html)\\n## **Installation**\\n\\nFor this discussion, we will create a MIT Kerberos domain with the following features (edit them to fit your needs):\\n\\n - *Realm:* EXAMPLE.COM\\n\\n - *Primary KDC:* kdc01.example.com\\n\\n - *Secondary KDC:* kdc02.example.com\\n\\n - *User principal:* ubuntu\\n\\n - *Admin principal:* ubuntu/admin\\n\\nBefore installing the Kerberos server, a properly configured DNS server is needed for your domain. Since the Kerberos\\nRealm by convention matches the domain name, this section uses the EXAMPLE.COM domain configured in the section\\nPrimary Server of the DNS documentation.\\n\\nAlso, Kerberos is a time sensitive protocol. If the local system time between a client machine and the server differs\\nby more than five minutes (by default), the workstation will not be able to authenticate. To correct the problem all\\nhosts should have their time synchronized using the same *Network Time Protocol (NTP)* server. Check out the NTP\\nchapter for more details.\\n\\nThe first step in creating a Kerberos Realm is to install the krb5-kdc and krb5-admin-server packages. From a terminal\\n\\nenter:\\n\\nsudo apt install krb5-kdc krb5-admin-server\\n\\nYou will be asked at the end of the install to supply the hostname for the Kerberos and Admin servers, which may or\\nmay not be the same server, for the realm.',\n",
       " 'Since we are going to create the realm, and thus these servers, type in the\\nfull hostname of this server.\\n\\n**Note**\\n\\nBy default the realm name will be domain name of the KDC server.\\n\\nNext, create the new realm with the kdb5_newrealm utility:\\n\\nsudo krb5_newrealm\\n\\nIt will ask you for a database master password, which is used to encrypt the local database. Chose a secure password:\\nits strength is not verified for you.\\n## **Configuration**\\n\\nThe questions asked during installation are used to configure the /etc/krb5.conf and /etc/krb5kdc/kdc.conf files. The\\nformer is used by the kerberos 5 libraries, and the latter configures the KDC. If you need to adjust the Key Distribution\\nCenter (KDC) settings simply edit the file and restart the krb5-kdc daemon. If you need to reconfigure Kerberos from\\nscratch, perhaps to change the realm name, you can do so by typing\\n\\nsudo dpkg-reconfigure krb5-kdc\\n\\n**Note**\\n\\nThe manpage for krb5.conf is in the krb5-doc package.\\n\\n163\\n\\n\\n-----\\n\\nLet’s create our first principal. Since there is no principal create yet, we need to use kadmin.local, which uses a local\\nunix socket to talk to the KDC, and requires root privileges:\\n\\n$ sudo kadmin.local\\n\\nAuthenticating as principal root/admin@EXAMPLE.COM with password.\\n\\nkadmin.local: addprinc ubuntu\\n\\nWARNING: no policy specified for ubuntu@EXAMPLE.COM; defaulting to no policy\\n\\nEnter password for principal \"ubuntu@EXAMPLE.COM\":\\n\\nRe-enter password for principal \"ubuntu@EXAMPLE.COM\":\\n\\nPrincipal \"ubuntu@EXAMPLE.COM\" created.\\n\\nkadmin.local: quit\\n\\nTo be able to use kadmin remotely, we should create an *admin principal* . Convention suggests it should be an *admin*',\n",
       " '*instance*, as that also makes creating generic ACLs easier. Let’s create an *admin* instance for the *ubuntu* principal:\\n\\n$ sudo kadmin.local\\n\\nAuthenticating as principal root/admin@EXAMPLE.COM with password.\\n\\nkadmin.local: addprinc ubuntu/admin\\n\\nWARNING: no policy specified for ubuntu/admin@EXAMPLE.COM; defaulting to no policy\\n\\nEnter password for principal \"ubuntu/admin@EXAMPLE.COM\":\\n\\nRe-enter password for principal \"ubuntu/admin@EXAMPLE.COM\":\\n\\nPrincipal \"ubuntu/admin@EXAMPLE.COM\" created.\\n\\nkadmin.local: quit\\n\\nNext, the new admin principal needs to have the appropriate Access Control List (ACL) permissions. The permissions\\nare configured in the /etc/krb5kdc/kadm5.acl file:\\n\\nubuntu/admin@EXAMPLE.COM \\nYou can also use a more generic form for this ACL:\\n\\n*/admin@EXAMPLE.COM \\nThe above will grant all privileges to any *admin instance* of a principal. See the [kadm5.acl](http://manpages.ubuntu.com/manpages/jammy/man5/kadm5.acl.5.html) manpage for details.\\n\\nNow restart the krb5-admin-server for the new ACL to take affect:\\n\\nsudo systemctl restart krb5-admin-server.service\\n\\nThe new user principal can be tested using the kinit utility:\\n\\n$ kinit ubuntu/admin\\n\\nPassword for ubuntu/admin@EXAMPLE.COM:\\n\\nAfter entering the password, use the klist utility to view information about the Ticket Granting Ticket (TGT):\\n\\n$ klist\\n\\nTicket cache: FILE:/tmp/krb5cc_1000\\n\\nDefault principal: ubuntu/admin@EXAMPLE.COM\\n\\nValid starting Expires Service principal\\n\\n04/03/20 19:16:57 04/04/20 05:16:57 krbtgt/EXAMPLE.COM@EXAMPLE.COM\\n\\nrenew until 04/04/20 19:16:55\\n\\nWhere the cache filename krb5cc_1000 is composed of the prefix krb5cc_ and the user id (uid), which in this case is\\n\\n1000 .',\n",
       " 'kinit will inspect /etc/krb5.conf to find out which KDC to contact, and its address. The KDC can also be found via\\nDNS lookups for special TXT and SRV records. You can add these records to your example.com DNS zone:\\n\\n_kerberos._udp.EXAMPLE.COM. IN SRV 1 0 88 kdc01.example.com.\\n\\n_kerberos._tcp.EXAMPLE.COM. IN SRV 1 0 88 kdc01.example.com.\\n\\n_kerberos._udp.EXAMPLE.COM. IN SRV 10 0 88 kdc02.example.com.\\n\\n_kerberos._tcp.EXAMPLE.COM. IN SRV 10 0 88 kdc02.example.com.\\n\\n_kerberos-adm._tcp.EXAMPLE.COM. IN SRV 1 0 749 kdc01.example.com.\\n\\n_kpasswd._udp.EXAMPLE.COM. IN SRV 1 0 464 kdc01.example.com.\\n\\nSee the DNS chapter for detailed instructions on setting up DNS.\\n\\nA very quick and useful way to troubleshoot what kinit is doing is to set the environment variable KRB5_TRACE to a\\nfile, or stderr, and it will show extra information. The output is quite verbose:\\n\\n164\\n\\n\\n-----\\n\\n$ KRB5_TRACE=/dev/stderr kinit ubuntu/admin\\n\\n[2898] 1585941845.278578: Getting initial credentials for ubuntu/admin@EXAMPLE.COM\\n\\n[2898] 1585941845.278580: Sending unauthenticated request\\n\\n[2898] 1585941845.278581: Sending request (189 bytes) to EXAMPLE.COM\\n\\n[2898] 1585941845.278582: Resolving hostname kdc01.example.com\\n\\n(...)\\n\\nYour new Kerberos Realm is now ready to authenticate clients.\\n\\nThe specific steps to enable Kerberos for a service can vary a bit, but in general the following is needed:\\n\\n  - a principal for the service: usually service/host@REALM\\n\\n  - a keytab accessible to the service wherever it’s running: usually in /etc/krb5.keytab\\n\\nFor example, let’s create a principal for an LDAP service running on the ldap-server.example.com host:\\n\\nubuntu@ldap-server:~$ sudo kadmin -p ubuntu/admin',\n",
       " 'Authenticating as principal ubuntu/admin with password.\\n\\nPassword for ubuntu/admin@EXAMPLE.COM:\\n\\nkadmin: addprinc -randkey ldap/ldap-server.example.com\\n\\nNo policy specified for ldap/ldap-server.example.com@EXAMPLE.COM; defaulting to no policy\\n\\nPrincipal \"ldap/ldap-server.example.com@EXAMPLE.COM\" created.\\n\\nLet’s dig a bit into what is happening here:\\n\\n - the kadmin command is being run in the *ldap-server* machine, not on the KDC. We are using kadmin remotely\\n\\n  - it’s being run with sudo, the reason will become clear later\\n\\n  - we are logged in on the server as *ubuntu*, but specifying a *ubuntu/admin* principal. Remember the *ubuntu*\\nprincipal has no special privileges\\n\\n  - the name of the principal we are creating follows the pattern *service/hostname*\\n\\n  - in order to select a random secret, we pass the -randkey parameter. Otherwise we would be asked to type in a\\npassword.\\n\\nWith the principal created, we need to extract the key from the KDC and store it in the *ldap-server* host, so that the\\n*ldap* service can use it to authenticate itself with the KDC. Still in the same *kadmin* session:\\n\\nkadmin: ktadd ldap/ldap-server.example.com\\n\\nEntry for principal ldap/ldap-server.example.com with kvno 2, encryption type aes256-cts-hmac-sha1\\n96 added to keytab FILE:/etc/krb5.keytab.\\n\\nEntry for principal ldap/ldap-server.example.com with kvno 2, encryption type aes128-cts-hmac-sha1\\n96 added to keytab FILE:/etc/krb5.keytab.\\n\\nThis is why he needed to run kadmin with sudo : so that it can write to /etc/krb5.keytab . This is the system keytab\\nfile, which is the default file for all keys that might be needed for services on this host. And we can list them with\\n\\nklist .',\n",
       " 'Back in the shell:\\n\\n$ sudo klist -k\\n\\nKeytab name: FILE:/etc/krb5.keytab\\n\\nKVNO Principal\\n\\n---- -------------------------------------------------------------------------\\n2 ldap/ldap-server.example.com@EXAMPLE.COM\\n\\n2 ldap/ldap-server.example.com@EXAMPLE.COM\\n\\nIf you don’t have the kadmin utility on the target host, one alternative is to extract the keys on a different host and\\ninto a different file, and then transfer this file *securely* to the target server. For example:\\n\\nkadmin: ktadd -k /home/ubuntu/ldap.keytab ldap/ldap-server.example.com\\n\\nEntry for principal ldap/ldap-server.example.com with kvno 3, encryption type aes256-cts-hmac-sha1\\n96 added to keytab WRFILE:/home/ubuntu/ldap.keytab.\\n\\nEntry for principal ldap/ldap-server.example.com with kvno 3, encryption type aes128-cts-hmac-sha1\\n96 added to keytab WRFILE:/home/ubuntu/ldap.keytab.\\n\\n**Note**\\n\\nNotice how the kvno changed from 2 to 3 in the example above, when using ktadd a second time? This is\\nthe key version, and it basically invalidated the key with kvno 2 that was extracted before. Everytime a\\nkey is extracted with ktadd, its version is bumped and that invalidates the previous ones!\\n\\nIn this case, as long as the target location is writable, you don’t even have to run kadmin with sudo .\\n\\nThen use scp to transfer it to the target host:\\n\\n165\\n\\n\\n-----\\n\\n$ scp /home/ubuntu/ldap.keytab ldap-server.example.com:\\n\\nAnd over there copy it to /etc/krb5.keytab, making sure it’s mode 0600 and owned by root:root .\\n\\nEncryption is at the heart of Kerberos, and it supports multiple cryptographic algorithms. The default choices are\\ngood enough for most deployments, but specific situations might need to tweak these settings.',\n",
       " 'This document will explain the basic configuration parameters of Kerberos that control the selection of encryption\\nalgorithms used in a Kerberos deployment.\\n## **Server-side configuration**\\n\\nThere are two main server-side configuration parameters that control the encryption types used on the server for its\\ndatabase and its collection or principals. Both exist in /etc/krb5kdc/kdc.conf inside the [realms] section and are as\\nfollows:\\n\\n - master_key_type\\nSpecifies the key type of the master key. This is used to encrypt the database, and the default is aes256-cts\\nhmac-sha1-96 .\\n\\n - supported_enctypes\\nSpecifies the default key/salt combinations of principals for this realm. The default is aes256-cts-hmac-sha1\\n96:normal aes128-cts-hmac-sha1-96:normal, and the encryption types should be listed in order of preference.\\n\\n[Possible values for the encryption algorithms are listed in the MIT documentation on encryption types, and the salt](https://web.mit.edu/kerberos/krb5-latest/doc/admin/conf_files/kdc_conf.html#encryption-types)\\n[types can be seen in the MIT keysalt lists.](https://web.mit.edu/kerberos/krb5-latest/doc/admin/conf_files/kdc_conf.html#keysalt-lists)\\n\\nHere is an example showing the default values (other settings removed for brevity):\\n\\n[realms]\\n\\nEXAMPLE.INTERNAL = {\\n\\n(...)\\n\\nmaster_key_type = aes256-cts\\n\\nsupported_enctypes = aes256-cts-hmac-sha1-96:normal aes128-cts-hmac-sha1-96:normal\\n\\n(...)\\n\\n}\\n\\nThe master key is created once per realm, when the realm is bootstrapped. That is usually done with the krb5_newrealm\\ntool (see Kerberos Server for details). You can check the master key type with either of these commands on the KDC\\n\\nserver:\\n\\n$ sudo kadmin.local',\n",
       " 'kadmin.local: getprinc K/M\\n\\nPrincipal: K/M@EXAMPLE.INTERNAL\\n\\n(...)\\n\\nNumber of keys: 1\\n\\nKey: vno 1, aes256-cts-hmac-sha1-96\\n\\n(...)\\n\\n$ sudo klist -ke /etc/krb5kdc/stash\\n\\nKeytab name: FILE:/etc/krb5kdc/stash\\n\\nKVNO Principal\\n\\n---- -------------------------------------------------------------------------\\n1 K/M@EXAMPLE.INTERNAL (aes256-cts-hmac-sha1-96)\\n\\nWhen a new Kerberos principal is created through the kadmind service (via the kadmin or kadmin.local utilities), the\\ntypes of encryption keys it will get are controlled via the supported_enctypes configuration parameter.\\n\\nFor example, let’s create an ubuntu principal, and check the keys that were created for it (output abbreviated):\\n\\n$ sudo kadmin.local\\n\\nAuthenticating as principal root/admin@EXAMPLE.INTERNAL with password.\\n\\nkadmin.local: addprinc ubuntu\\n\\nNo policy specified for ubuntu@EXAMPLE.INTERNAL; defaulting to no policy\\n\\nEnter password for principal \"ubuntu@EXAMPLE.INTERNAL\":\\n\\nRe-enter password for principal \"ubuntu@EXAMPLE.INTERNAL\":\\n\\nPrincipal \"ubuntu@EXAMPLE.INTERNAL\" created.\\n\\nkadmin.local: getprinc ubuntu\\n\\nPrincipal: ubuntu@EXAMPLE.INTERNAL\\n\\n166\\n\\n\\n-----\\n\\n(...)\\n\\nNumber of keys: 2\\n\\nKey: vno 1, aes256-cts-hmac-sha1-96\\n\\nKey: vno 1, aes128-cts-hmac-sha1-96\\n\\n(...)\\n\\nTwo keys were created for the ubuntu principal, following the default setting of supported_enctypes in kdc.conf for\\nthis realm.\\n\\n**Note** :\\nThe server config supported_enctypes has the *default* list of key types that are created for a principal. This\\nlist applies to the moment when that principal is **created** by kadmind . Changing that setting after the fact\\nwon’t affect the keys that the principal in question has after that event.',\n",
       " 'In particular, principals can be\\ncreated with specific key types regardless of the supported_enctypes setting. See the -e parameter for the\\n[kadmin add_principal command.](https://web.mit.edu/kerberos/krb5-latest/doc/admin/database.html#add-principal)\\n\\nIf we had supported_enctypes set to aes256-sha2:normal aes128-sha2:normal camellia256-cts:normal in kdc.conf,\\nthen the ubuntu principal would get three key types:\\n\\nkadmin.local: getprinc ubuntu\\n\\nPrincipal: ubuntu@EXAMPLE.INTERNAL\\n\\n(...)\\n\\nNumber of keys: 3\\n\\nKey: vno 1, aes256-cts-hmac-sha384-192\\n\\nKey: vno 1, aes128-cts-hmac-sha256-128\\n\\nKey: vno 1, camellia256-cts-cmac\\n\\n**Note** :\\n\\nBootstrapping a new Kerberos realm via the krb5_newrealm command also creates some system principals\\nrequired by Kerberos, such as kadmin/admin, kadmin/changepw and others. They will all also get the same\\nnumber of keys each: one per encryption type in supported_enctypes .\\n## **Client-side configuration**\\n\\nWhen we say “client-side”, we really mean “applications linked with the Kerberos libraries”. These live on the server\\ntoo, so keep that in mind.\\n\\nThe encryption types supported by the Kerberos libraries are defined in the /etc/krb5.conf file, inside the [libde\\nfaults] section, via the permitted_enctypes parameter.\\n\\nExample:\\n\\n[libdefaults]\\n\\n(...)\\n\\npermitted_enctypes = aes256-cts-hmac-sha1-96 aes128-cts-hmac-sha1-96\\n\\nThis parameter contains a space-separated list of encryption type names, in order of preference. Default value: aes256\\ncts-hmac-sha1-96 aes128-cts-hmac-sha1-96 aes256-cts-hmac-sha384-192 aes128-cts-hmac-sha256-128 des3-cbc-sha1\\n\\narcfour-hmac-md5 camellia256-cts-cmac camellia128-cts-cmac .',\n",
       " '[Possible values for the encryption algorithms are listed in the MIT documentation (same ones as for the KDC).](https://web.mit.edu/kerberos/krb5-latest/doc/admin/conf_files/kdc_conf.html#encryption-types)\\n\\n**Note** :\\n\\nThere are more encryption-related parameters in krb5.conf, but most take their defaults from permit\\nted_enctypes [. See the MIT libdefaults documentation for more information.](https://web.mit.edu/kerberos/krb5-latest/doc/admin/conf_files/krb5_conf.html#libdefaults)\\n## **Putting it all together**\\n\\nWhen a client performs Kerberos authentication and requests a ticket from the KDC, the encryption type used in that\\nticket is decided by picking the common set of:\\n\\n  - The encryption types supported by the server for that principal\\n\\n  - The encryption types supported by the client\\n\\nIf there is no common algorithm between what the client accepts, and what the server has to offer for that specific\\nprincipal, then kinit will fail.\\n\\nFor example, if the principal on the server has:\\n\\nkadmin.local: getprinc ubuntu\\n\\nPrincipal: ubuntu@EXAMPLE.INTERNAL\\n\\n167\\n\\n\\n-----\\n\\n(...)\\n\\nNumber of keys: 2\\n\\nKey: vno 1, aes256-cts-hmac-sha384-192\\n\\nKey: vno 1, aes128-cts-hmac-sha256-128\\n\\nAnd the client’s krb5.conf has:\\n\\npermitted_enctypes = aes256-sha1 aes128-sha1\\n\\nThen kinit will fail, because the client only supports sha1 variants, and the server only has sha2 to offer for that\\nparticular principal the client is requesting:\\n\\n$ kinit ubuntu\\n\\nkinit: Generic error (see e-text) while getting initial credentials\\n\\nThe server log ( journalctl -u krb5-admin-server.service ) will have more details about the error:',\n",
       " 'Apr 19 19:31:49 j-kdc krb5kdc[8597]: AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1\\n96(17)}) fd42:78f4:b1c4:3964:216:3eff:feda:118c: GET_LOCAL_TGT: ubuntu@EXAMPLE.INTERNAL for krbtgt/EXAMPLE.INTERNAL@EXA\\n\\nThis log says that there was an AS-REQ request which accepted two encryption types, but there was no matching key\\ntype on the server database for that principal.\\n## **Changing encryption types**\\n\\nChanging encryption types of an existing Kerberos realm is no small task. Just changing the configuration settings\\nwon’t recreate existing keys, nor add new ones. The modifications have to be done in incremental steps.\\n\\n[MIT Kerberos has a guide on updating encryption types that covers many scenarios, including deployments with](https://web.mit.edu/kerberos/krb5-latest/doc/admin/enctypes.html#migrating-away-from-older-encryption-types)\\nmultiple replicating servers:\\n# **References**\\n\\n[• Encryption types in MIT Kerberos](https://web.mit.edu/kerberos/krb5-latest/doc/admin/enctypes.html)\\n\\n - krb5.conf [encryption related configurations options](https://web.mit.edu/kerberos/krb5-latest/doc/admin/enctypes.html#configuration-variables)\\n\\n[• Migrating away from older encryption types](https://web.mit.edu/kerberos/krb5-latest/doc/admin/enctypes.html#migrating-away-from-older-encryption-types)\\n\\n - kdc.conf [manpage](https://manpages.ubuntu.com/manpages/jammy/man5/kdc.conf.5.html)\\n\\n - [krb5.conf](https://manpages.ubuntu.com/manpages/jammy/man5/krb5.conf.5.html) manpage\\n\\n[• Kerberos V5 concepts](https://web.mit.edu/kerberos/krb5-latest/doc/basic/index.html)\\n\\nOnce you have one Key Distribution Center (KDC) on your network, it is good practice to have a Secondary KDC',\n",
       " 'in case the primary becomes unavailable. Also, if you have Kerberos clients that are in different networks (possibly\\nseparated by routers using NAT), it is wise to place a secondary KDC in each of those networks.\\n\\n**Note**\\n\\nThe native replication mechanism explained here relies on a cronjob, and essentially dumps the DB on the\\nprimary and loads it back up on the secondary. You may want to take a look at using the *kldap* backend\\nwhich can use the OpenLDAP replication mechanism. It is explained further below.\\n\\nFirst, install the packages, and when asked for the Kerberos and Admin server names enter the name of the Primary\\nKDC:\\n\\nsudo apt install krb5-kdc krb5-admin-server\\n\\nOnce you have the packages installed, create the host principals for both KDCs. From a terminal prompt, enter:\\n\\n$ kadmin -q \"addprinc -randkey host/kdc01.example.com\"\\n\\n$ kadmin -q \"addprinc -randkey host/kdc02.example.com\"\\n\\n**Note**\\n\\nThe kadmin command defaults to using a principal like *username/admin@EXAMPLE.COM*, where *user-*\\n*name* is your current shell user. If you need to override that, use -p <principal-you-want>\\n\\nExtract the *key* file for the *kdc02* principal, which is this server we are on::\\n\\n$ sudo kadmin -p ubuntu/admin -q \"ktadd host/kdc02.example.com\"\\n\\nNext, there needs to be a kpropd.acl file on each KDC that lists all KDCs for the Realm. For example, on both\\nprimary and secondary KDC, create /etc/krb5kdc/kpropd.acl :\\n\\n168\\n\\n\\n-----\\n\\nhost/kdc01.example.com@EXAMPLE.COM\\n\\nhost/kdc02.example.com@EXAMPLE.COM\\n\\n**Note**\\n\\nIt’s customary to allow both KDCs because one may want to switch their roles if one goes bad. For such\\nan eventuality, both are already listed here.',\n",
       " 'Create an empty database on the *Secondary KDC* :\\n\\n$ sudo kdb5_util create -s\\n\\nNow install kpropd daemon, which listens for connections from the kprop utility from the primary kdc:\\n\\n$ sudo apt install krb5-kpropd\\n\\nThe service will be running right after installation.\\n\\nFrom a terminal on the *Primary KDC*, create a dump file of the principal database:\\n\\n$ sudo kdb5_util dump /var/lib/krb5kdc/dump\\n\\nStill on the *Primary KDC*, extract its *key* :\\n\\n$ sudo kadmin.local -q \"ktadd host/kdc01.example.com\"\\n\\nOn the *Primary KDC*, run the kprop utility to push the database dump made before to the Secondary KDC:\\n\\n$ sudo kprop -r EXAMPLE.COM -f /var/lib/krb5kdc/dump kdc02.example.com\\n\\nDatabase propagation to kdc02.example.com: SUCCEEDED\\n\\nNote the *SUCCEEDED* message, which signals that the propagation worked. If there is an error message check\\n\\n/var/log/syslog on the secondary KDC for more information.\\n\\nYou may also want to create a cron job to periodically update the database on the Secondary KDC. For example, the\\nfollowing will push the database every hour:\\n\\n# m h dom mon dow command\\n\\n0 - - - - root /usr/sbin/kdb5_util dump /var/lib/krb5kdc/dump && /usr/sbin/kprop -r EXAMPLE.COM \\nf /var/lib/krb5kdc/dump kdc02.example.com\\n\\nFinally, start the krb5-kdc daemon on the Secondary KDC:\\n\\n$ sudo systemctl start krb5-kdc.service\\n\\n**Note**\\n\\nThe *Secondary KDC* does not run an admin server, since it’s a read-only copy\\n\\nFrom now on, you can specify both KDC servers in /etc/krb5.conf for the EXAMPLE.COM realm, in any host participating\\nin this realm (including kdc01 and kdc02 ), but remember that there can only be one admin server and that’s the one\\nrunning on kdc01 :\\n\\n[realms]\\n\\nEXAMPLE.COM = {',\n",
       " 'kdc = kdc01.example.com\\n\\nkdc = kdc02.example.com\\n\\nadmin_server = kdc01.example.com\\n\\n}\\n\\nThe *Secondary KDC* should now be able to issue tickets for the Realm. You can test this by stopping the krb5-kdc\\ndaemon on the Primary KDC, then by using kinit to request a ticket. If all goes well you should receive a ticket from\\nthe Secondary KDC. Otherwise, check /var/log/syslog and /var/log/auth.log in the Secondary KDC.\\n\\nThis section covers configuring a Linux system as a Kerberos client. This will allow access to any kerberized services\\nonce a user has successfully logged into the system.\\n\\nNote that Kerberos alone is not enough for a user to exist in a Linux system. Meaning, we cannot just point the\\nsystem at a kerberos server and expect all the kerberos principals to be able to *login* on the linux system, simply\\nbecause these users do not *exist* locally. Kerberos only provides authentication: it doesn’t know about user groups,\\nLinux uids and gids, home directories, etc. Normally another network source is used for this information, such as an\\nLDAP or Windows server, and, in the old days, NIS was used for that as well.\\n\\n169\\n\\n\\n-----\\n\\n## **Installation**\\n\\nIf you have local users matching the principals in a Kerberos realm, and just want to switch the authentication from\\nlocal to remote using Kerberos, you can follow this section. This is not a very usual scenario, but serves to highlight\\nthe separation between user authentication and user information (full name, uid, gid, home directory, groups, etc). If\\nyou just want to be able to grab tickets and use them, it’s enough to install krb5-user and run kinit .',\n",
       " 'We are going to use sssd with a trick so that it will fetch the user information from the local system files, instead of\\na remote source which is the common case.\\n\\nTo install the packages enter the following in a terminal prompt:\\n\\n$ sudo apt install krb5-user sssd-krb5\\n\\nYou will be prompted for the addresses of your KDCs and admin servers. If you have been following this chapter so\\nfar, the KDCs will be: kdc01.example.com kdc02.example.com (space separated)\\n\\nAnd the admin server will be: kdc01.example.com . Remember that kdc02 is a read-only copy of the primary KDC, so\\nit doesn’t run an admin server.\\n\\n**Note**\\n\\nIf you have added the appropriate SRV records to DNS, none of those prompts will need answering.\\n## **Configuration**\\n\\nIf you missed the questions earlier, you can reconfigure the package to fill them in again: sudo dpkg-reconfigure\\n\\nkrb5-config .\\n\\nYou can test the kerberos configuration by requesting a ticket using the kinit utility. For example:\\n\\n$ kinit ubuntu\\n\\nPassword for ubuntu@EXAMPLE.COM:\\n\\n**Note**\\n\\nkinit doesn’t need for the principal to exist as a local user in the system. In fact, you can kinit any\\nprincipal you want. If you don’t specify one, then the tool will use the username of whoever is running\\n\\nkinit .\\n\\nThe only remaining configuration now is for sssd . Create the file /etc/sssd/sssd.conf with the following content:\\n\\n[sssd]\\n\\nconfig_file_version = 2\\n\\nservices = pam\\n\\ndomains = example.com\\n\\n[pam]\\n\\n[domain/example.com]\\n\\nid_provider = proxy\\n\\nproxy_lib_name = files\\n\\nauth_provider = krb5\\n\\nkrb5_server = kdc01.example.com,kdc02.example.com\\n\\nkrb5_kpasswd = kdc01.example.com\\n\\nkrb5_realm = EXAMPLE.COM',\n",
       " 'The above configuration will use kerberos for *authentication* ( auth_provider ), but will use the local system users for\\nuser and group information ( id_provider ).\\n\\nAdjust the permissions of the config file and start sssd :\\n\\n$ sudo chown root:root /etc/sssd/sssd.conf\\n\\n$ sudo chmod 0600 /etc/sssd/sssd.conf\\n\\n$ sudo systemctl start sssd\\n\\nJust by having installed sssd and its dependencies, PAM will already have been configured to use sssd, with a fallback\\nto local user authentication. To try it out, if this is a workstation, simply switch users (in the GUI), or open a login\\nterminal (CTRL-ALT-<number>), or spawn a login shell with sudo login, and try logging in using the name of a\\nkerberos principal. Remember that this user must already exist on the local system:\\n\\n170\\n\\n\\n-----\\n\\n$ sudo login\\n\\nfocal-krb5-client login: ubuntu\\n\\nPassword:\\n\\nWelcome to Ubuntu Focal Fossa (development branch) (GNU/Linux 5.4.0-21-generic x86_64)\\n\\n(...)\\n\\nLast login: Thu Apr 9 21:23:50 UTC 2020 from 10.20.20.1 on pts/0\\n\\n$ klist\\n\\nTicket cache: FILE:/tmp/krb5cc_1000_NlfnSX\\n\\nDefault principal: ubuntu@EXAMPLE.COM\\n\\nValid starting Expires Service principal\\n\\n04/09/20 21:36:12 04/10/20 07:36:12 krbtgt/EXAMPLE.COM@EXAMPLE.COM\\n\\nrenew until 04/10/20 21:36:12\\n\\nAnd you will have a Kerberos ticket already right after login.\\n\\nKerberos supports a few database backends. The default one is what we have been using so far, called *db2* [. The DB](https://web.mit.edu/kerberos/krb5-latest/doc/admin/dbtypes.html)\\n[Types documentation shows all the options, one of which is LDAP.](https://web.mit.edu/kerberos/krb5-latest/doc/admin/dbtypes.html)',\n",
       " 'There are several reasons why one would want to have the Kerberos principals stored in LDAP as opposed to a local\\non-disk database. There are also cases when it is not a good idea. Each site has to evaluate the pros and cons. Here\\nare a few:\\n\\n  - the OpenLDAP replication is faster and more robust then the native Kerberos one, based on a cron job\\n\\n  - setting things up with the LDAP backend isn’t exactly trivial and shouldn’t be attempted by administrators\\nwithout prior knowledge of OpenLDAP\\n\\n[• as highlighted in LDAP section of DB Types, since](https://web.mit.edu/kerberos/krb5-latest/doc/admin/dbtypes.html#ldap-module-kldap) krb5kdc is single threaded there may be higher latency in\\nservicing requests when using the OpenLDAP backend\\n\\n  - if you already have OpenLDAP setup for other things, like storing users and groups, adding the Kerberos\\nattributes to the same mix might be beneficial and can provide a nice integrated story\\n\\nThis section covers configuring a primary and secondary kerberos server to use OpenLDAP for the principal database.\\n[Note that as of version 1.18, the KDC from MIT Kerberos does not support a primary KDC using a read-only](https://krbdev.mit.edu/rt/Ticket/Display.html?id=7754)\\nconsumer (secondary) LDAP server. What we have to consider here is that a Primary KDC is read-write, and it needs\\na read-write backend. The Secondaries can use both a read-write and read-only backend, because they are expected\\nto be read-only. Therefore there are only some possible layouts we can use:\\n\\n1. Simple case: Primary KDC connected to primary OpenLDAP, Secondary KDC connected to both Primary and\\nSecondary OpenLDAP\\n2.',\n",
       " 'Extended simple case: Multiple Primary KDCs connected to one Primary OpenLDAP, and multiple Secondary\\nKDCs connected to Primary and Secondary OpenLDAP\\n3. OpenLDAP with multi-master replication: multiple primary KDCs connected to all primary OpenLDAP servers\\n\\nWe haven’t covered OpenLDAP multi-master replication in this guide, so we will show the first case only. The second\\nscenario is an extension: just add another primary KDC to the mix, talking to the same primary OpenLDAP server.\\n## **Configuring OpenLDAP**\\n\\nWe are going to install the OpenLDAP server on the same host as the KDC, to simplify the communication between\\nthem. In such a setup, we can use the *ldapi:///* transport, which is via an unix socket, and don’t need to setup SSL\\ncertificates to secure the communication between the Kerberos services and OpenLDAP. Note, however, that SSL is\\nstill needed for the OpenLDAP replication. See LDAP with TLS for details.\\n\\nIf you want to use an existing OpenLDAP server that you have somewhere else, that’s of course also possible, but\\nkeep in mind that you should then use SSL for the communication between the KDC and this OpenLDAP server.\\n\\nFirst, the necessary *schema* needs to be loaded on an OpenLDAP server that has network connectivity to the Primary\\nand Secondary KDCs. The rest of this section assumes that you also have LDAP replication configured between at\\nleast two servers. For information on setting up OpenLDAP see OpenLDAP Server.\\n\\n**Note**\\n\\ncn=admin,dc=example,dc=com is a default admin user that is created during the installation of the slapd\\npackage (the OpenLDAP server). The domain component will change for your server, so adjust accordingly.',\n",
       " '  - Install the necessary packages (it’s assumed that OpenLDAP is already installed):\\n\\nsudo apt install krb5-kdc-ldap krb5-admin-server\\n\\n171\\n\\n\\n-----\\n\\n- Next, extract the kerberos.schema.gz file:\\n\\nsudo cp /usr/share/doc/krb5-kdc-ldap/kerberos.schema.gz /etc/ldap/schema/\\n\\nsudo gunzip /etc/ldap/schema/kerberos.schema.gz\\n\\n- The *kerberos* schema needs to be added to the *cn=config* tree. This schema file needs to be converted to LDIF\\nformat before it can be added. For that we will use a helper tool, called schema2ldif, provided by the package\\nof the same name which is available in the Universe archive:\\n\\nsudo apt install schema2ldif\\n\\n- To import the kerberos schema, run:\\n\\n$ sudo ldap-schema-manager -i kerberos.schema\\n\\nSASL/EXTERNAL authentication started\\n\\nSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\\n\\nSASL SSF: 0\\n\\nexecuting \\'ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/kerberos.ldif\\'\\n\\nSASL/EXTERNAL authentication started\\n\\nSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\\n\\nSASL SSF: 0\\n\\nadding new entry \"cn=kerberos,cn=schema,cn=config\"\\n\\n- With the new schema loaded, let’s index an attribute often used in searches:\\n\\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// <<EOF\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nadd: olcDbIndex\\n\\nolcDbIndex: krbPrincipalName eq,pres,sub\\n\\nEOF\\n\\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\\n\\n- Let’s create LDAP entries for the Kerberos administrative entities that will contact the OpenLDAP server to\\nperform operations. There are two:\\n\\n**–**\\n***ldap_kdc_dn*** : needs to have read rights on the realm container, principal container and realm sub-trees.',\n",
       " 'If ***disable_last_success*** and ***disable_lockout*** are not set, however, then ***ldap_kdc_dn*** needs write\\naccess to the kerberos container just like the admin dn below.\\n\\n**–**\\n***ldap_kadmind_dn*** : needs to have read and write rights on the realm container, principal container and\\nrealm sub-trees\\n\\nHere is the command to create these entities:\\n\\n$ ldapadd -x -D cn=admin,dc=example,dc=com -W <<EOF\\n\\ndn: uid=kdc-service,dc=example,dc=com\\n\\nuid: kdc-service\\n\\nobjectClass: account\\n\\nobjectClass: simpleSecurityObject\\n\\nuserPassword: {CRYPT}x\\n\\ndescription: Account used for the Kerberos KDC\\n\\ndn: uid=kadmin-service,dc=example,dc=com\\n\\nuid: kadmin-service\\n\\nobjectClass: account\\n\\nobjectClass: simpleSecurityObject\\n\\nuserPassword: {CRYPT}x\\n\\ndescription: Account used for the Kerberos Admin server\\n\\nEOF\\n\\nEnter LDAP Password:\\n\\nadding new entry \"uid=kdc-service,dc=example,dc=com\"\\n\\nadding new entry \"uid=kadmin-service,dc=example,dc=com\"\\n\\nNow let’s set a password for them. Note that first the tool asks for the password you want for the specified user\\ndn, and then for the password of the ***cn=admin*** dn:\\n\\n$ ldappasswd -x -D cn=admin,dc=example,dc=com -W -S uid=kdc-service,dc=example,dc=com\\n\\nNew password: <-- password you want for uid-kdc-service\\n\\n172\\n\\n\\n-----\\n\\nRe-enter new password:\\n\\nEnter LDAP Password: <-- password for the dn specified with the -D option\\n\\nRepeat for the uid=kadmin-service dn. These passwords will be needed later.\\n\\nYou can test these with ldapwhoami :\\n\\n$ ldapwhoami -x -D uid=kdc-service,dc=example,dc=com -W\\n\\nEnter LDAP Password:\\n\\ndn:uid=kdc-service,dc=example,dc=com\\n\\n  - Finally, update the Access Control Lists (ACL).',\n",
       " 'These can be tricky, as it highly depends on what you have\\ndefined already. By default, the slapd package configures your database with the following ACLs:\\n\\nolcAccess: {0}to attrs=userPassword by self write by anonymous auth by * none\\n\\nolcAccess: {1}to attrs=shadowLastChange by self write by * read\\n\\nolcAccess: {2}to * by * read\\n\\nWe need to insert new rules before the final to * by * read one, to control access to the Kerberos related entries\\nand attributes:\\n\\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// <<EOF\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nadd: olcAccess\\n\\nolcAccess: {2}to attrs=krbPrincipalKey\\n\\nby anonymous auth\\n\\nby dn.exact=\"uid=kdc-service,dc=example,dc=com\" read\\n\\nby dn.exact=\"uid=kadmin-service,dc=example,dc=com\" write\\n\\nby self write\\n\\nby * none\\n\\n   \\nadd: olcAccess\\n\\nolcAccess: {3}to dn.subtree=\"cn=krbContainer,dc=example,dc=com\"\\n\\nby dn.exact=\"uid=kdc-service,dc=example,dc=com\" read\\n\\nby dn.exact=\"uid=kadmin-service,dc=example,dc=com\" write\\n\\nby * none\\n\\nEOF\\n\\nmodifying entry \"olcDatabase={1}mdb,cn=config\"\\n\\nThis will make the existing {2} rule become {4} . Check with sudo slapcat -b cn=config (the output below was\\nreformatted a bit for clarity):\\n\\nolcAccess: {0}to attrs=userPassword\\n\\nby self write\\n\\nby anonymous auth\\n\\nby * none\\n\\nolcAccess: {1}to attrs=shadowLastChange\\n\\nby self write\\n\\nby * read\\n\\nolcAccess: {2}to attrs=krbPrincipalKey by anonymous auth\\n\\nby dn.exact=\"uid=kdc-service,dc=example,dc=com\" read\\n\\nby dn.exact=\"uid=kadmin-service,dc=example,dc=com\" write\\n\\nby self write\\n\\nby * none\\n\\nolcAccess: {3}to dn.subtree=\"cn=krbContainer,dc=example,dc=com\"\\n\\nby dn.exact=\"uid=kdc-service,dc=example,dc=com\" read\\n\\nby dn.exact=\"uid=kadmin-service,dc=example,dc=com\" write\\n\\nby * none',\n",
       " 'olcAccess: {4}to * by * read\\n\\nThat’s it, your LDAP directory is now ready to serve as a Kerberos principal database.\\n## **Primary KDC Configuration (LDAP)**\\n\\nWith OpenLDAP configured it is time to configure the KDC. In this example we are doing it in the same OpenLDAP\\nserver to take advantage of local unix socket communication.\\n\\n  - Reconfigure the krb5-config package if neededd to get a good starting point with /etc/krb5.conf :\\n\\n173\\n\\n\\n-----\\n\\nsudo dpkg-reconfigure krb5-config\\n\\n- Now edit /etc/krb5.conf adding the database_module option to the EXAMPLE.COM realm section:\\n\\n[realms]\\n\\nEXAMPLE.COM = {\\n\\nkdc = kdc01.example.com\\n\\nkdc = kdc02.example.com\\n\\nadmin_server = kdc01.example.com\\n\\ndefault_domain = example.com\\n\\ndatabase_module = openldap_ldapconf\\n\\n}\\n\\nThen also add these new sections:\\n\\n[dbdefaults]\\n\\nldap_kerberos_container_dn = cn=krbContainer,dc=example,dc=com\\n\\n[dbmodules]\\n\\nopenldap_ldapconf = {\\n\\ndb_library = kldap\\n\\n# if either of these is false, then the ldap_kdc_dn needs to\\n\\n# have write access\\n\\ndisable_last_success = true\\n\\ndisable_lockout = true\\n\\n# this object needs to have read rights on\\n\\n# the realm container, principal container and realm sub-trees\\n\\nldap_kdc_dn = \"uid=kdc-service,dc=example,dc=com\"\\n\\n# this object needs to have read and write rights on\\n\\n# the realm container, principal container and realm sub-trees\\n\\nldap_kadmind_dn = \"uid=kadmin-service,dc=example,dc=com\"\\n\\nldap_service_password_file = /etc/krb5kdc/service.keyfile\\n\\nldap_servers = ldapi:///\\n\\nldap_conns_per_server = 5\\n\\n}\\n\\n- Next, use the kdb5_ldap_util utility to create the realm:\\n\\n$ sudo kdb5_ldap_util -D cn=admin,dc=example,dc=com create -subtrees dc=example,dc=com -r EXAMPLE.COM \\ns -H ldapi:///',\n",
       " 'Password for \"cn=admin,dc=example,dc=com\":\\n\\nInitializing database for realm \\'EXAMPLE.COM\\'\\n\\nYou will be prompted for the database Master Password.\\n\\nIt is important that you NOT FORGET this password.\\n\\nEnter KDC database master key:\\n\\nRe-enter KDC database master key to verify:\\n\\n- Create a stash of the password used to bind to the LDAP server. Run it once for each *ldap_kdc_dn* and\\n*ldap_kadmin_dn* ::\\n\\nsudo kdb5_ldap_util -D cn=admin,dc=example,dc=com stashsrvpw -f /etc/krb5kdc/service.keyfile uid=kdc\\nservice,dc=example,dc=com\\n\\nsudo kdb5_ldap_util -D cn=admin,dc=example,dc=com stashsrvpw -f /etc/krb5kdc/service.keyfile uid=kadmin\\nservice,dc=example,dc=com\\n\\n**Note**\\n\\nThe /etc/krb5kdc/service.keyfile file now contains clear text versions of the passwords used by the\\nKDC to contact the LDAP server!\\n\\n- Create a /etc/krb5kdc/kadm5.acl file for the admin server, if you haven’t already:\\n\\n*/admin@EXAMPLE.COM  \\n- Start the Kerberos KDC and admin server:\\n\\n174\\n\\n\\n-----\\n\\nsudo systemctl start krb5-kdc.service krb5-admin-server.service\\n\\nYou can now add Kerberos principals to the LDAP database, and they will be copied to any other LDAP servers\\nconfigured for replication. To add a principal using the kadmin.local utility enter:\\n\\n$ sudo kadmin.local\\n\\nAuthenticating as principal root/admin@EXAMPLE.COM with password.\\n\\nkadmin.local: addprinc ubuntu\\n\\nWARNING: no policy specified for ubuntu@EXAMPLE.COM; defaulting to no policy\\n\\nEnter password for principal \"ubuntu@EXAMPLE.COM\":\\n\\nRe-enter password for principal \"ubuntu@EXAMPLE.COM\":\\n\\nPrincipal \"ubuntu@EXAMPLE.COM\" created.\\n\\nkadmin.local:',\n",
       " 'The above will create an ubuntu *principal* with a *dn* of krbPrincipalName=ubuntu@EXAMPLE.COM,cn=EXAMPLE.COM,cn=krbContainer,dc\\nLet’s say, however, that you already have an user in your directory, and it’s in uid=testuser1,ou=People,dc=example,dc=com,\\nhow to add the kerberos attributes to it? You use the -x parameter to specify the location. For the *ldap_kadmin_dn*\\nto be able to write to it, we first need to update the ACLs:\\n\\n$ sudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// <<EOF\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nadd: olcAccess\\n\\nolcAccess: {4}to dn.subtree=“ou=People,dc=example,dc=com”\\n\\nby dn.exact=”uid=kdc-service,dc=example,dc=com” read\\n\\nby dn.exact=”uid=kadmin-service,dc=example,dc=com” write\\n\\nby * break\\n\\nEOF\\n\\nAnd now we can specify the new location:\\n\\n$ sudo kadmin.local\\n\\nAuthenticating as principal root/admin@EXAMPLE.COM with password.\\n\\nkadmin.local: addprinc -x dn=uid=testuser1,ou=People,dc=example,dc=com testuser1\\n\\nWARNING: no policy specified for testuser1@EXAMPLE.COM; defaulting to no policy\\n\\nEnter password for principal \"testuser1@EXAMPLE.COM\":\\n\\nRe-enter password for principal \"testuser1@EXAMPLE.COM\":\\n\\nPrincipal \"testuser1@EXAMPLE.COM\" created.\\n\\nSince the specified *dn* already exists, kadmin.local will just add the required kerberos attributes to this existing entry.\\nIf it didn’t exist, it would be created from scratch, with just the kerberos attributes, like what happened with the\\n\\nubuntu example above, but in the specified location.\\n\\n**Note**\\n\\nThe *ldap_kadmin_dn* DN ( *uid=kadmin-service* in our example) does not have write access to the location\\nspecified by the -x parameter, you will get an Insufficient access error.',\n",
       " 'Both places are visible for kinit, since, when the realm was created with kdb5_ldap_util, the default value for the\\nsearch scope and base were taken: subtree, and dc=example,dc=com .\\n## **Secondary KDC Configuration (LDAP)**\\n\\nThe setup of the secondary KDC (and its OpenLDAP replica) is very similar. Once you have the OpenLDAP replication\\nsetup, repeat these steps on the secondary:\\n\\n  - install *krb5-kdc-ldap*, *ldap-utils* . Do not install *krb5-admin-server* .\\n\\n  - load the kerberos schema using schema2ldif\\n\\n  - add the index for *krbPrincipalName*\\n\\n - add the ACLs\\n\\n  - configure *krb5.conf* in the same way, initially. If you want and if you configured SSL properly, you can add\\n\\nldaps://kdc01.example.com to the ldap_servers list after ldapi:///, so that the Secondary KDC can have two\\nLDAP backends at its disposal\\n\\n - **DO NOT** run kdb5_ldap_util . There is no need to create the database since it’s being replicated from the\\nPrimary\\n\\n  - copy over the following files from the Primary KDC and place them in the same location on the Secondary:\\n\\n**–**\\n/etc/krb5kdc/stash\\n\\n**–**\\n/etc/krb5kdc/service.keyfile\\n\\n  - start the KDC: sudo systemctl start krb5-kdc.service\\n\\n175\\n\\n\\n-----\\n\\n## **Resources**\\n\\n[• Configuring Kerberos with OpenLDAP back-end](https://web.mit.edu/kerberos/krb5-latest/doc/admin/conf_ldap.html#conf-ldap)\\n\\n[• MIT Kerberos backend types](https://web.mit.edu/kerberos/krb5-latest/doc/admin/dbtypes.html)\\n\\nSSSD stands for System Security Services Daemon and it’s actually a collection of daemons that handle authentication,\\nauthorization, and user and group information from a variety of network sources. At its core it has support for:\\n\\n  - Active Directory\\n\\n - LDAP',\n",
       " ' - Kerberos\\n\\nSSSD provides PAM and NSS modules to integrate these remote sources into your system and allow remote users to\\nlogin and be recognized as valid users, including group membership. To allow for disconnected operation, SSSD also\\ncan also cache this information, so that users can continue to login in the event of a network failure, or other problem\\nof the same sort.\\n\\nThis guide will focus on the most common scenarios where SSSD is deployed.\\n## **References**\\n\\n[• Upstream project: https://sssd.io/](https://sssd.io/)\\n\\nThis section describes the use of sssd to authenticate user logins against an Active Directory via using sssd’s “ad”\\nprovider. At the end, Active Directory users will be able to login on the host using their AD credentials. Group\\nmembership will also be maintained.\\n## **Group Policies for Ubuntu**\\n\\nSSSD manages user authentication and sets initial security policies.\\n\\nADSys serves as a Group Policy client for Ubuntu, streamlining the configuration of Ubuntu systems within a Microsoft\\nActive Directory environment.\\n\\n[If you are interested in Group Policies support for Ubuntu, detailed information can be found in the ADSys documen-](https://canonical-adsys.readthedocs-hosted.com/en/stable/)\\n[tation on Active Directory GPO support.](https://canonical-adsys.readthedocs-hosted.com/en/stable/)\\n## **Prerequisites, Assumptions, and Requirements**\\n\\n  - This guide does not explain Active Directory, how it works, how to set one up, or how to maintain it.\\n\\n  - This guide assumes that a working Active Directory domain is already configured and you have access to the\\ncredentials to join a machine to that domain.',\n",
       " '  - The domain controller is acting as an authoritative DNS server for the domain.\\n\\n  - The domain controller is the primary DNS resolver (check with systemd-resolve --status )\\n\\n  - System time is correct and in sync, maintained via a service like *chrony* or *ntp*\\n\\n  - The domain used in this example is ad1.example.com .\\n## **Software Installation**\\n\\nInstall the following packages:\\n\\nsudo apt install sssd-ad sssd-tools realmd adcli\\n## **Join the domain**\\n\\nWe will use the realm command, from the realmd package, to join the domain and create the sssd configuration.\\n\\nLet’s verify the domain is discoverable via DNS:\\n\\n$ sudo realm -v discover ad1.example.com\\n\\n - Resolving: _ldap._tcp.ad1.example.com\\n\\n - Performing LDAP DSE lookup on: 10.51.0.5\\n\\n - Successfully discovered: ad1.example.com\\n\\nad1.example.com\\n\\ntype: kerberos\\n\\nrealm-name: AD1.EXAMPLE.COM\\n\\n176\\n\\n\\n-----\\n\\ndomain-name: ad1.example.com\\n\\nconfigured: no\\n\\nserver-software: active-directory\\n\\nclient-software: sssd\\n\\nrequired-package: sssd-tools\\n\\nrequired-package: sssd\\n\\nrequired-package: libnss-sss\\n\\nrequired-package: libpam-sss\\n\\nrequired-package: adcli\\n\\nrequired-package: samba-common-bin\\n\\nThis performs several checks and determines the best software stack to use with sssd. sssd can install the missing\\npackages via *packagekit*, but we installed them already previously.\\n\\nNow let’s join the domain:\\n\\n$ sudo realm join ad1.example.com\\n\\nPassword for Administrator:\\n\\nThat was quite uneventful. If you want to see what it was doing, pass the -v option:\\n\\n$ sudo realm join -v ad1.example.com\\n\\n - Resolving: _ldap._tcp.ad1.example.com\\n\\n - Performing LDAP DSE lookup on: 10.51.0.5\\n\\n - Successfully discovered: ad1.example.com',\n",
       " \"Password for Administrator:\\n\\n - Unconditionally checking packages\\n\\n - Resolving required packages\\n\\n - LANG=C /usr/sbin/adcli join --verbose --domain ad1.example.com --domain-realm AD1.EXAMPLE.COM --domain\\ncontroller 10.51.0.5 --login-type user --login-user Administrator --stdin-password\\n\\n - Using domain name: ad1.example.com\\n\\n - Calculated computer account name from fqdn: AD-CLIENT\\n\\n - Using domain realm: ad1.example.com\\n\\n - Sending NetLogon ping to domain controller: 10.51.0.5\\n\\n - Received NetLogon info from: SERVER1.ad1.example.com\\n\\n - Wrote out krb5.conf snippet to /var/cache/realmd/adcli-krb5-hUfTUg/krb5.d/adcli-krb5-conf-hv2kzi\\n\\n - Authenticated as user: Administrator@AD1.EXAMPLE.COM\\n\\n - Looked up short domain name: AD1\\n\\n - Looked up domain SID: S-1-5-21-2660147319-831819607-3409034899\\n\\n - Using fully qualified name: ad-client.ad1.example.com\\n\\n - Using domain name: ad1.example.com\\n\\n - Using computer account name: AD-CLIENT\\n\\n - Using domain realm: ad1.example.com\\n\\n - Calculated computer account name from fqdn: AD-CLIENT\\n\\n - Generated 120 character computer password\\n\\n - Using keytab: FILE:/etc/krb5.keytab\\n\\n - Found computer account for AD-CLIENT$ at: CN=AD-CLIENT,CN=Computers,DC=ad1,DC=example,DC=com\\n\\n - Sending NetLogon ping to domain controller: 10.51.0.5\\n\\n - Received NetLogon info from: SERVER1.ad1.example.com\\n\\n - Set computer password\\n\\n - Retrieved kvno '3' for computer account in directory: CN=AD-CLIENT,CN=Computers,DC=ad1,DC=example,DC=com\\n\\n - Checking RestrictedKrbHost/ad-client.ad1.example.com\\n\\n - Added RestrictedKrbHost/ad-client.ad1.example.com\\n\\n - Checking RestrictedKrbHost/AD-CLIENT\\n\\n - Added RestrictedKrbHost/AD-CLIENT\",\n",
       " ' - Checking host/ad-client.ad1.example.com\\n\\n - Added host/ad-client.ad1.example.com\\n\\n - Checking host/AD-CLIENT\\n\\n - Added host/AD-CLIENT\\n\\n - Discovered which keytab salt to use\\n\\n - Added the entries to the keytab: AD-CLIENT$@AD1.EXAMPLE.COM: FILE:/etc/krb5.keytab\\n\\n - Added the entries to the keytab: host/AD-CLIENT@AD1.EXAMPLE.COM: FILE:/etc/krb5.keytab\\n\\n - Added the entries to the keytab: host/ad-client.ad1.example.com@AD1.EXAMPLE.COM: FILE:/etc/krb5.keytab\\n\\n - Added the entries to the keytab: RestrictedKrbHost/AD-CLIENT@AD1.EXAMPLE.COM: FILE:/etc/krb5.keytab\\n\\n - Added the entries to the keytab: RestrictedKrbHost/ad-client.ad1.example.com@AD1.EXAMPLE.COM: FILE:/etc/krb5.keytab\\n\\n177\\n\\n\\n-----\\n\\n - /usr/sbin/update-rc.d sssd enable\\n\\n - /usr/sbin/service sssd restart\\n\\n - Successfully enrolled machine in realm\\n\\nBy default, *realm* will use the *Administrator* account of the domain to request the join. If you need to use another\\naccount, pass it to the tool with the -U option.\\n\\nAnother popular way of joining a domain is using an *OTP*, or *One Time Password*, token. For that, use the --one\\ntime-password option.\\n## **SSSD Configuration**\\n\\nThe *realm* tool already took care of creating an sssd configuration, adding the pam and nss modules, and starting the\\nnecessary services.\\n\\nLet’s take a look at /etc/sssd/sssd.conf :\\n\\n[sssd]\\n\\ndomains = ad1.example.com\\n\\nconfig_file_version = 2\\n\\nservices = nss, pam\\n\\n[domain/ad1.example.com]\\n\\ndefault_shell = /bin/bash\\n\\nkrb5_store_password_if_offline = True\\n\\ncache_credentials = True\\n\\nkrb5_realm = AD1.EXAMPLE.COM\\n\\nrealmd_tags = manages-system joined-with-adcli\\n\\nid_provider = ad\\n\\nfallback_homedir = /home/%u@%d\\n\\nad_domain = ad1.example.com',\n",
       " 'use_fully_qualified_names = True\\n\\nldap_id_mapping = True\\n\\naccess_provider = ad\\n\\n**Note**\\n\\nSomething very important to remember is that this file must have permissions *0600* and ownership *root:root*,\\nor else sssd won’t start!\\n\\nLet’s highlight a few things from this config:\\n\\n - *cache_credentials* : this allows logins when the AD server is unreachable\\n\\n  - home directory: it’s by default /home/<user>@<domain> . For example, the AD user *john* will have a home directory\\nof */home/john@ad1.example.com*\\n\\n - *use_fully_qualified_names* : users will be of the form *user@domain*, not just *user* . This should only be changed\\nif you are certain no other domains will ever join the AD forest, via one of the several possible trust relationships\\n## **Automatic home directory creation**\\n\\nWhat the realm tool didn’t do for us is setup pam_mkhomedir, so that network users can get a home directory when\\nthey login. This remaining step can be done by running the following command:\\n\\nsudo pam-auth-update --enable mkhomedir\\n## **Checks**\\n\\nYou should now be able to fetch information about AD users. In this example, *John Smith* is an AD user:\\n\\n$ getent passwd john@ad1.example.com\\n\\njohn@ad1.example.com:*:1725801106:1725800513:John Smith:/home/john@ad1.example.com:/bin/bash\\n\\nLet’s see his groups:\\n\\n$ groups john@ad1.example.com\\n\\njohn@ad1.example.com : domain users@ad1.example.com engineering@ad1.example.com\\n\\n**Note**\\n\\nIf you just changed the group membership of a user, it may be a while before sssd notices due to caching.\\n\\n178\\n\\n\\n-----\\n\\nFinally, how about we try a login:\\n\\n$ sudo login\\n\\nad-client login: john@ad1.example.com\\n\\nPassword:\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-24-generic x86_64)',\n",
       " \"...\\n\\nCreating directory '/home/john@ad1.example.com'.\\n\\njohn@ad1.example.com@ad-client:~$\\n\\nNotice how the home directory was automatically created.\\n\\nYou can also use ssh, but note that the command will look a bit funny because of the multiple *@* signs:\\n\\n$ ssh john@ad1.example.com@10.51.0.11\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-24-generic x86_64)\\n\\n(...)\\n\\nLast login: Thu Apr 16 21:22:55 2020\\n\\njohn@ad1.example.com@ad-client:~$\\n\\n**Note**\\n\\nIn the ssh example, public key authentication was used, so no password was required. Remember that ssh\\npassword authentication is by default disabled in /etc/ssh/sshd_config .\\n## **Kerberos Tickets**\\n\\nIf you install krb5-user, your AD users will also get a kerberos ticket upon logging in:\\n\\njohn@ad1.example.com@ad-client:~$ klist\\n\\nTicket cache: FILE:/tmp/krb5cc_1725801106_9UxVIz\\n\\nDefault principal: john@AD1.EXAMPLE.COM\\n\\nValid starting Expires Service principal\\n\\n04/16/20 21:32:12 04/17/20 07:32:12 krbtgt/AD1.EXAMPLE.COM@AD1.EXAMPLE.COM\\n\\nrenew until 04/17/20 21:32:12\\n\\n**Note**\\n\\n*realm* also configured /etc/krb5.conf for you, so there should be no further configuration prompts when\\ninstalling krb5-user\\n\\nLet’s test with *smbclient* using kerberos authentication to list he shares of the domain controller:\\n\\njohn@ad1.example.com@ad-client:~$ smbclient -k -L server1.ad1.example.com\\n\\nSharename Type Comment\\n\\n--------- ---- ------\\nADMIN$ Disk Remote Admin\\n\\nC$ Disk Default share\\n\\nIPC$ IPC Remote IPC\\n\\nNETLOGON Disk Logon server share\\n\\nSYSVOL Disk Logon server share\\n\\nSMB1 disabled -- no workgroup available\\n\\nNotice how we now have a ticket for the *cifs* service, which was used for the share list above:\\n\\njohn@ad1.example.com@ad-client:~$ klist\",\n",
       " 'Ticket cache: FILE:/tmp/krb5cc_1725801106_9UxVIz\\n\\nDefault principal: john@AD1.EXAMPLE.COM\\n\\nValid starting Expires Service principal\\n\\n04/16/20 21:32:12 04/17/20 07:32:12 krbtgt/AD1.EXAMPLE.COM@AD1.EXAMPLE.COM\\n\\nrenew until 04/17/20 21:32:12\\n\\n04/16/20 21:32:21 04/17/20 07:32:12 cifs/server1.ad1.example.com@AD1.EXAMPLE.COM\\n## **Desktop Ubuntu Authentication**\\n\\nThe desktop login only shows local users in the list to pick from, and that’s on purpose.\\n\\n179\\n\\n\\n-----\\n\\nTo login with an Active Directory user for the first time, follow these steps:\\n\\n  - click on the “Not listed?” option:\\n\\n[click-not-listed](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/2/291d9ae9e6db85986154208a843963a6bd4eb350.png)\\n\\nclick-not-listed1024×768 14.5 KB\\n\\n  - type in the login name followed by the password:\\n\\n[type-in-username](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/6/6940e589fd250228137dce0ac847583b52217583.png)\\n\\ntype-in-username1024×768 16.3 KB\\n\\n  - the next time you login, the AD user will be listed as if it was a local user:\\n\\n[next-time](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/9/9c1744413a7e6fad58fc38f3dfbe21ea2d1541e2.png)\\n\\nnext-time1024×768 16.1 KB\\n## **Known Issues**\\n\\nWhen logging in on a system joined with an Active Directory domain, sssd (the package responsible for this integration)\\nwill try to apply Group Policies by default. There are cases where if a specific policy is missing, the login will be\\ndenied.\\n[This is being tracked in bug #1934997. Until the fix becomes available, please see comment #5 in that bug report for](https://bugs.launchpad.net/ubuntu/+source/sssd/+bug/1934997)\\nexisting workarounds.\\n## **Resources**',\n",
       " '[• GitHub SSSD Project](https://github.com/SSSD/sssd)\\n\\n[• Active Directory DNS Zone Entries](https://technet.microsoft.com/en-us/library/cc759550%28v=ws.10%29.aspx)\\n\\nSSSD can also use LDAP for authentication, authorization, and user/group information. In this section we will\\nconfigure a host to authenticate users from an OpenLDAP directory.\\n## **Prerequisites, Assumptions, and Requirements**\\n\\nFor this setup, we need:\\n\\n  - an existing OpenLDAP server with SSL enabled and using the RFC2307 schema for users and groups\\n\\n  - a client host where we will install the necessary tools and login as a user from the LDAP server\\n## **Software Installation**\\n\\nInstall the following packages:\\n\\nsudo apt install sssd-ldap ldap-utils\\n## **SSSD Configuration**\\n\\nCreate the /etc/sssd/sssd.conf configuration file, with permissions *0600* and ownership *root:root*, and this content:\\n\\n[sssd]\\n\\nconfig_file_version = 2\\n\\ndomains = example.com\\n\\n[domain/example.com]\\n\\nid_provider = ldap\\n\\nauth_provider = ldap\\n\\nldap_uri = ldap://ldap01.example.com\\n\\ncache_credentials = True\\n\\nldap_search_base = dc=example,dc=com\\n\\nMake sure to start the *sssd* service:\\n\\nsudo systemctl start sssd.service\\n\\n180\\n\\n\\n-----\\n\\n**Note**\\n\\n*sssd* will use *START_TLS* by default for authentication requests against the LDAP server (the\\n*auth_provider* ), but not for the *id_provider* . If you want to also enable *START_TLS* for the *id_provider*,\\nspecify ldap_id_use_start_tls = true .\\n## **Automatic home directory creation**\\n\\nTo enable automatic home directory creation, run the following command:\\n\\nsudo pam-auth-update --enable mkhomedir\\n## **Check SSL setup on the client**',\n",
       " 'The client must be able to use *START_TLS* when connecting to the LDAP server, with full certificate checking. This\\n\\nmeans:\\n\\n  - the client host knows and trusts the CA that signed the LDAP server certificate\\n\\n  - the server certificate was issued for the correct host ( ldap01.example.com in this guide)\\n\\n  - the time is correct on all hosts performing the TLS connection\\n\\n  - and, of course, that neither certificate (CA or server’s) expired\\n\\nIf using a custom CA, an easy way to have a host trust it is to place it in /usr/local/share/ca-certificates/ with a\\n\\n.crt extension and run sudo update-ca-certificates .\\n\\nAlternatively, you can edit /etc/ldap/ldap.conf and point TLS_CACERT to the CA public key file.\\n\\n**Note**\\n\\nYou may have to restart sssd after these changes: sudo systemctl restart sssd\\n\\nOnce that is all done, check that you can connect to the LDAP server using verified SSL connections:\\n\\n$ ldapwhoami -x -ZZ -H ldap://ldap01.example.com\\n\\nanonymous\\n\\nand for ldaps (if enabled in /etc/default/slapd ):\\n\\n$ ldapwhoami -x -H ldaps://ldap01.example.com\\n\\nThe -ZZ parameter tells the tool to use *START_TLS*, and that it must not fail. If you have LDAP logging enabled\\non the server, it will show something like this:\\n\\nslapd[779]: conn=1032 op=0 STARTTLS\\n\\nslapd[779]: conn=1032 op=0 RESULT oid= err=0 text=\\n\\nslapd[779]: conn=1032 fd=15 TLS established tls_ssf=256 ssf=256\\n\\nslapd[779]: conn=1032 op=1 BIND dn=\"\" method=128\\n\\nslapd[779]: conn=1032 op=1 RESULT tag=97 err=0 text=\\n\\nslapd[779]: conn=1032 op=2 EXT oid=1.3.6.1.4.1.4203.1.11.3\\n\\nslapd[779]: conn=1032 op=2 WHOAMI\\n\\nslapd[779]: conn=1032 op=2 RESULT oid= err=0 text=',\n",
       " \"*START_TLS* with *err=0* and *TLS established* is what we want to see there, and, of course, the *WHOAMI* extended\\noperation.\\n## **Final verification**\\n\\nIn this example, the LDAP server has the following user and group entry we are going to use for testing:\\n\\ndn: uid=john,ou=People,dc=example,dc=com\\n\\nuid: john\\n\\nobjectClass: inetOrgPerson\\n\\nobjectClass: posixAccount\\n\\ncn: John Smith\\n\\nsn: Smith\\n\\ngivenName: John\\n\\nmail: john@example.com\\n\\nuserPassword: johnsecret\\n\\nuidNumber: 10001\\n\\ngidNumber: 10001\\n\\nloginShell: /bin/bash\\n\\n181\\n\\n\\n-----\\n\\nhomeDirectory: /home/john\\n\\ndn: cn=john,ou=Group,dc=example,dc=com\\n\\ncn: john\\n\\nobjectClass: posixGroup\\n\\ngidNumber: 10001\\n\\nmemberUid: john\\n\\ndn: cn=Engineering,ou=Group,dc=example,dc=com\\n\\ncn: Engineering\\n\\nobjectClass: posixGroup\\n\\ngidNumber: 10100\\n\\nmemberUid: john\\n\\nThe user *john* should be known to the system:\\n\\nubuntu@ldap-client:~$ getent passwd john\\n\\njohn:*:10001:10001:John Smith:/home/john:/bin/bash\\n\\nubuntu@ldap-client:~$ id john\\n\\nuid=10001(john) gid=10001(john) groups=10001(john),10100(Engineering)\\n\\nAnd we should be able to authenticate as *john* :\\n\\nubuntu@ldap-client:~$ sudo login\\n\\nldap-client login: john\\n\\nPassword:\\n\\nWelcome to Ubuntu Focal Fossa (development branch) (GNU/Linux 5.4.0-24-generic x86_64)\\n\\n(...)\\n\\nCreating directory '/home/john'.\\n\\njohn@ldap-client:~$\\n\\nFinally, we can mix it all together in a setup that is very similar to Active Directory in terms of the technologies used:\\nuse LDAP for users and groups, and Kerberos for authentication.\\n## **Prerequisites, Assumptions, and Requirements**\\n\\nFor this setup, we will need:\\n\\n  - an existing OpenLDAP server using the RFC2307 schema for users and groups. SSL support is recommended,\",\n",
       " 'but not strictly necessary because authentication in this setup is being done via Kerberos, and not LDAP.\\n\\n  - a Kerberos server. It doesn’t have to be using the OpenLDAP backend\\n\\n  - a client host where we will install and configure SSSD\\n## **Software Installation**\\n\\nOn the client host, install the following packages:\\n\\nsudo apt install sssd-ldap sssd-krb5 ldap-utils krb5-user\\n\\nYou may be asked about the default Kerberos realm. For this guide, we are using EXAMPLE.COM .\\n\\nAt this point, you should alreaedy be able to obtain tickets from your Kerberos server, assuming DNS records point\\nat it like explained elsewhere in this guide:\\n\\n$ kinit ubuntu\\n\\nPassword for ubuntu@EXAMPLE.COM:\\n\\nubuntu@ldap-krb-client:~$ klist\\n\\nTicket cache: FILE:/tmp/krb5cc_1000\\n\\nDefault principal: ubuntu@EXAMPLE.COM\\n\\nValid starting Expires Service principal\\n\\n04/17/20 19:51:06 04/18/20 05:51:06 krbtgt/EXAMPLE.COM@EXAMPLE.COM\\n\\nrenew until 04/18/20 19:51:05\\n\\nBut we want to be able to login as an LDAP user, authenticated via Kerberos. Let’s continue with the configuration.\\n\\n182\\n\\n\\n-----\\n\\n## **SSSD Configuration**\\n\\nCreate the /etc/sssd/sssd.conf configuration file, with permissions *0600* and ownership *root:root*, and this content:\\n\\n[sssd]\\n\\nconfig_file_version = 2\\n\\ndomains = example.com\\n\\n[domain/example.com]\\n\\nid_provider = ldap\\n\\nldap_uri = ldap://ldap01.example.com\\n\\nldap_search_base = dc=example,dc=com\\n\\nauth_provider = krb5\\n\\nkrb5_server = kdc01.example.com,kdc02.example.com\\n\\nkrb5_kpasswd = kdc01.example.com\\n\\nkrb5_realm = EXAMPLE.COM\\n\\ncache_credentials = True\\n\\nThis example uses two KDCs, which made it necessary to also specify the *krb5_kpasswd* server because the second',\n",
       " \"KDC is a replica and is not running the admin server.\\n\\nStart the *sssd* service:\\n\\nsudo systemctl start sssd.service\\n## **Automatic home directory creation**\\n\\nTo enable automatic home directory creation, run the following command:\\n\\nsudo pam-auth-update --enable mkhomedir\\n## **Final verification**\\n\\nIn this example, the LDAP server has the following user and group entry we are going to use for testing:\\n\\ndn: uid=john,ou=People,dc=example,dc=com\\n\\nuid: john\\n\\nobjectClass: inetOrgPerson\\n\\nobjectClass: posixAccount\\n\\ncn: John Smith\\n\\nsn: Smith\\n\\ngivenName: John\\n\\nmail: john@example.com\\n\\nuidNumber: 10001\\n\\ngidNumber: 10001\\n\\nloginShell: /bin/bash\\n\\nhomeDirectory: /home/john\\n\\ndn: cn=john,ou=Group,dc=example,dc=com\\n\\ncn: john\\n\\nobjectClass: posixGroup\\n\\ngidNumber: 10001\\n\\nmemberUid: john\\n\\ndn: cn=Engineering,ou=Group,dc=example,dc=com\\n\\ncn: Engineering\\n\\nobjectClass: posixGroup\\n\\ngidNumber: 10100\\n\\nmemberUid: john\\n\\nNote how the *john* user has no *userPassword* attribute.\\n\\nThe user *john* should be known to the system:\\n\\nubuntu@ldap-client:~$ getent passwd john\\n\\njohn:*:10001:10001:John Smith:/home/john:/bin/bash\\n\\nubuntu@ldap-client:~$ id john\\n\\n183\\n\\n\\n-----\\n\\nuid=10001(john) gid=10001(john) groups=10001(john),10100(Engineering)\\n\\nLet’s try a login as this user:\\n\\nubuntu@ldap-krb-client:~$ sudo login\\n\\nldap-krb-client login: john\\n\\nPassword:\\n\\nWelcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-24-generic x86_64)\\n\\n(...)\\n\\nCreating directory '/home/john'.\\n\\njohn@ldap-krb-client:~$ klist\\n\\nTicket cache: FILE:/tmp/krb5cc_10001_BOrxWr\\n\\nDefault principal: john@EXAMPLE.COM\\n\\nValid starting Expires Service principal\\n\\n04/17/20 20:29:50 04/18/20 06:29:50 krbtgt/EXAMPLE.COM@EXAMPLE.COM\\n\\nrenew until 04/18/20 20:29:50\",\n",
       " 'john@ldap-krb-client:~$\\n\\nWe logged in using the kerberos password, and user/group information from the LDAP server.\\n## **SSSD and KDC spoofing**\\n\\nWhen using SSSD to manage kerberos logins on a Linux host, there is an attack scenario you should be aware of:\\nKDC spoofing.\\n\\nThe objective of the attacker is to login on a workstation that is using Kerberos authentication. Let’s say he knows\\n\\njohn is a valid user on that machine.\\n\\nThe attacker first deploys a rogue KDC server in the network, and creates the john principal there with a password\\nof his choosing. What he has to do now is to have his rogue KDC respond to the login request from the workstation,\\nbefore (or instead of) the real KDC. If the workstation isn’t authenticating the KDC, it will accept the reply from the\\nrogue server and let john in.\\n\\nThere is a configuration parameter that can be set to protect the workstation from this attack. It will have SSSD\\nauthenticate the KDC, and block the login if the KDC cannot be verified. This option is called krb5_validate, and\\nit’s false by default.\\n\\nTo enable it, edit /etc/sssd/sssd.conf and add this line to the domain section:\\n\\n[sssd]\\n\\nconfig_file_version = 2\\n\\ndomains = example.com\\n\\n[domain/example.com]\\n\\nid_provider = ldap\\n\\n...\\n\\nkrb5_validate = True\\n\\nThe second step is to create a host principal on the KDC for this workstation. This is how the KDC’s authenticity is\\nverified. It’s like a “machine account”, with a shared secret that the attacker cannot control and replicate in his rogue\\nKDC…The host principal has the format host/<fqdn>@REALM .\\n\\nAfter the host principal is created, its keytab needs to be stored on the workstation. This two step process can be',\n",
       " 'easily done on the workstation itself via kadmin (not kadmin.local ) to contact the KDC remotely:\\n\\n$ sudo kadmin -p ubuntu/admin\\n\\nkadmin: addprinc -randkey host/ldap-krb-client.example.com@EXAMPLE.COM\\n\\nWARNING: no policy specified for host/ldap-krb-client.example.com@EXAMPLE.COM; defaulting to no policy\\n\\nPrincipal \"host/ldap-krb-client.example.com@EXAMPLE.COM\" created.\\n\\nkadmin: ktadd -k /etc/krb5.keytab host/ldap-krb-client.example.com\\n\\nEntry for principal host/ldap-krb-client.example.com with kvno 6, encryption type aes256-cts-hmac-sha1\\n96 added to keytab WRFILE:/etc/krb5.keytab.\\n\\nEntry for principal host/ldap-krb-client.example.com with kvno 6, encryption type aes128-cts-hmac-sha1\\n96 added to keytab WRFILE:/etc/krb5.keytab.\\n\\nThen exit the tool and make sure the permissions on the keytab file are tight:\\n\\n184\\n\\n\\n-----\\n\\nsudo chmod 0600 /etc/krb5.keytab\\n\\nsudo chown root:root /etc/krb5.keytab\\n\\nYou can also do it on the KDC itself using kadmin.local, but you will have to store the keytab temporarily in another\\nfile and securely copy it over to the workstation.\\n\\nOnce these steps are complete, you can restart sssd on the workstation and perform the login. If the rogue KDC\\npicks the attempt up and replies, it will fail the host verification. With debugging we can see that happening on the\\nworkstation:\\n\\n==> /var/log/sssd/krb5_child.log <==\\n\\n(Mon Apr 20 19:43:58 2020) [[sssd[krb5_child[2102]]]] [validate_tgt] (0x0020): TGT failed verification using key for [host\\n\\nkrb-client.example.com@EXAMPLE.COM].\\n\\n(Mon Apr 20 19:43:58 2020) [[sssd[krb5_child[2102]]]] [get_and_save_tgt] (0x0020): 1741: [-1765328377][Server host/ldap',\n",
       " 'krb-client.example.com@EXAMPLE.COM not found in Kerberos database]\\n\\nAnd the login is denied. If the real KDC picks it up, however, the host verification succeeds:\\n\\n==> /var/log/sssd/krb5_child.log <==\\n\\n(Mon Apr 20 19:46:22 2020) [[sssd[krb5_child[2268]]]] [validate_tgt] (0x0400): TGT verified using key for [host/ldap\\nkrb-client.example.com@EXAMPLE.COM].\\n\\nAnd the login is accepted.\\n\\nHere are some tips to help troubleshoot sssd.\\n\\ndebug_level\\n\\nThe debug level of sssd can be changed on-the-fly via sssctl, from the sssd-tools package:\\n\\nsudo apt install sssd-tools\\n\\nsssctl debug-level <new-level>\\n\\nOr change add it to the config file and restart sssd:\\n\\n[sssd]\\n\\nconfig_file_version = 2\\n\\ndomains = example.com\\n\\n[domain/example.com]\\n\\ndebug_level = 6\\n\\n...\\n\\nEither will yield more logs in /var/log/sssd/*.log and can help identify what is going on. The sssctl approach has\\nthe clear advantage of not having to restart the service.\\n## **Caching**\\n\\nCaching is useful to speed things up, but it can get in the way big time when troubleshooting. It’s useful to be able\\nto remove the cache while chasing down a problem. This can also be done with the sssctl tool from the sssd-tools\\npackage.\\n\\nYou can either remove the whole cache:\\n\\n# sssctl cache-remove\\n\\nCreating backup of local data...\\n\\nSSSD backup of local data already exists, override? (yes/no) [no] yes\\n\\nRemoving cache files...\\n\\nSSSD= needs to be running. Start SSSD now? (yes/no) [yes] yes\\n\\nOr just one element:\\n\\nsssctl cache-expire -u john\\n\\nOr expire everything:\\n\\nsssctl cache-expire -E\\n## **DNS**\\n\\nKerberos is quite sensitive to DNS issues. If you suspect something related to DNS, here are two suggestions:\\n\\n185\\n\\n\\n-----\\n\\n**FQDN hostname**',\n",
       " 'Make sure hostname -f returns a fully qualified domain name. Set it in /etc/hostname if necessary, and use sudo\\n\\nhostnamectl set-hostname <fqdn> to set it at runtime.\\n\\n**Reverse name lookup**\\n\\nYou can try disabling a default reverse name lookup that the krb5 libraries do, by editing (or creating) /etc/krb5.conf\\nand setting rdns = false in the [libdefaults] section:\\n\\n[libdefaults]\\n\\nrdns = false\\n\\nWireGuard is a simple, fast and modern VPN implementation, widely deployed and cross-platform.\\n\\nVPNs have traditionally been hard to understand, configure and deploy. WireGuard removed most of that complexity\\nby focusing on its single task, and leaving out things like key distribution and pushed configurations. You get a\\nnetwork interface which encrypts and verifies the traffic, and the remaining tasks like setting up addresses, routing,\\n[etc, are left to the usual system tools like ip-route(8) and ip-address(8).](https://manpages.ubuntu.com/manpages/jammy/man8/ip-route.8.html)\\n\\nSetting up the cryptographic keys is very much similar to configuring ssh for key based authentication: each side of\\nthe connection has its own private and public key, and the peers’ public key, and this is enough to start encrypting\\nand verifying the exchanged traffic.\\n\\nFor more details on how WireGuard works, and information on its availability in other platforms, please see the\\nreferences section.\\n## **WireGuard Concepts**\\n\\nIt helps to think of WireGuard primarly as a network interface, like any other. It will have the usual attributes, like\\nIP address, CIDR, and there will be some routing associated with it. But it also has WireGuard specific attributes,\\nwhich handle the VPN part of things.',\n",
       " 'All of this can be configured via different tools. WireGuard itself ships its own tools in the userspace package\\n\\nwireguard-tools : [wg(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html) and [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) . But these are not strictly needed: any userspace with the right privileges\\nand kernel calls can configure a WireGuard interface. For example, systemd-networkd and network-manager can do it\\non their own, without the WireGuad userspace utilities.\\n\\nImportant attributes of a WireGuard interface are:\\n\\n - *private key* : together with the corresponding public key, they are used to authenticate and encrypt data. This\\nis generated with the wg genkey command.\\n\\n - *listen port* : the UDP port that WireGuard will be listening to for incoming traffic.\\n\\n  - List of *peers*, each one with:\\n\\n**–**\\n*public key* : the public counterpart of the private key. Generated from the private key of that peer, using\\nthe wg pubkey command.\\n\\n**–**\\n*endpoint* : where to send the encrypted traffic to. This is optional, but at least one of the corresponding\\npeers must have it to bootstrap the connection.\\n**–** *allowed IPs* : list of inner tunnel destination networks or addresses for this peer when sending traffic, or,\\nwhen receiving traffic, which source networks or addresses are allowed to send traffic to us.\\n\\n**NOTE**\\n\\nCryptography is not simple. When we say that, for example, a private key is used to decrypt or sign\\ntraffic, and a public key is used to encrypt or verify the authenticity of traffic, this is a simplification and is\\nhiding a lot of important details.',\n",
       " 'WireGuard has a detailed explanation of its protocols and cryptography\\n[handling in their website, at https://www.wireguard.com/protocol/](https://www.wireguard.com/protocol/)\\n\\nThese parameters can be set with the low-level [wg(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html) tool, directly via the command line or with a configuration file.\\nThis tool, however, doesn’t handle the non-WireGuard settings of the interface. It won’t assign an IP address to it,\\nfor example, nor setup routing. For this reason, it’s more common to use [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) .\\n\\n[wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) will handle the lifecycle of the WireGuard interface. It can bring it up or down, setup routing, execute\\narbitrary commands before or after the interface is up, and more. It augments the configuration file that [wg(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html) can\\nuse, with its own extra settings, which is important to keep in mind when feeding that file to [wg(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html), as it will contain\\nsettings [wg(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html) knows nothing about.\\n\\nThe [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) configuration file can have an arbitrary name, and can even be placed anywhere on the system, but\\nthe best practice is:\\n\\n  - Place the file in /etc/wireguard .\\n\\n186\\n\\n\\n-----\\n\\n  - Name it after the interface it controls.\\n\\nFor example, a file called /etc/wireguard/wg0.conf will have the needed configurations setting for a WireGuard network',\n",
       " 'interface called wg0 . By following this practice, you get the benefit of being able to call wg-quick with just the interface\\n\\nname:\\n\\n$ sudo wg-quick up wg0\\n\\nAnd that will bring the wg0 interface up, give it an IP address, setup routing, and configure the WireGuard specific\\nparameters for it to work. This interface is usually called wg0, but can have any valid network interface name, like\\n\\noffice (it doesn’t need an index number after the name), home1, etc. It can help to give it a meaningful name if you\\nplan to connect to multiple peers.\\n\\nLet’s go over an example of such a configuration file:\\n\\n[Interface]\\n\\nPrivateKey = eJdSgoS7BZ/uWkuSREN+vhCJPPr3M3UlB3v1Su/amWk=\\n\\nListenPort = 51000\\n\\nAddress = 10.10.11.10/24\\n\\n[Peer]\\n\\n# office\\n\\nPublicKey = xeWmdxiLjgebpcItF1ouRo0ntrgFekquRJZQO+vsQVs=\\n\\nEndpoint = wg.example.com:51000 # fake endpoint, just an example\\n\\nAllowedIPs = 10.10.11.0/24, 10.10.10.0/24\\n\\nIn the [Interface] section:\\n\\n - Address : this is the IP address, and CIDR, that the WireGuard interface will be setup with.\\n\\n - ListenPort : the UDP port WireGuard will use for traffic (listening and sending).\\n\\n - PrivateKey : the secret key used to decrypt traffic destined to this interface.\\n\\nThe *peers* list, each one in its own [Peer] section (example above has just one), comes next:\\n\\n - PublicKey : the key that will be used to encrypt traffic to this peer.\\n\\n - Endpoint : where to send encrypted traffic to.\\n\\n - AllowedIPs : when sending traffic, this is the list of target addresses that identify this peer. When receiving\\ntraffic, it’s the list of addresses that are allowed to be the source of the traffic.',\n",
       " 'To generate the keypairs for each peer, the [wg(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html) command is used:\\n\\n$ umask 077\\n\\n$ wg genkey > wg0.key\\n\\n$ wg pubkey < wg0.key > wg0.pub\\n\\nAnd then the contents of wg0.key and wg0.pub can be used in the configuration file.\\n\\nThis is what it looks like when this interface is brought up by [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) :\\n\\n$ sudo wg-quick up wg0\\n\\n[#] ip link add wg0 type wireguard\\n\\n[#] wg setconf wg0 /dev/fd/63\\n\\n[#] ip -4 address add 10.10.11.10/24 dev wg0\\n\\n[#] ip link set mtu 1420 up dev wg0\\n\\n[#] ip -4 route add 10.10.10.0/24 dev wg0\\n\\nThis is what [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) did for us:\\n\\n - Created the WireGuard wg0 interface.\\n\\n  - Configured it with the data from the configuration file.\\n\\n - Added the IP/CIDR from the Address field to the wg0 interface.\\n\\n  - Calculated a proper MTU (which can be overridden in the config if needed)\\n\\n  - Added a route for AllowedIPs .\\n\\nNote that in this example AllowedIPs is a list of two CIDR network blocks, but [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) only added a route\\nfor 10.10.10.0/24 and skipped 10.10.11.0/24 . That’s because the Address was already specified as a /24 one. Had\\nwe specified the address as 10.10.11.10/32 instead, then [wg-quick(8)](https://manpages.ubuntu.com/manpages/jammy/man8/wg-quick.8.html) would have added a route for 10.10.11.0/24\\nexplicitly.\\n\\nTo better understand how AllowedIPs work, let’s go through a quick example.\\n\\n187\\n\\n\\n-----\\n\\nLet’s say this system wants to send traffic to 10.10.10.201/24 .',\n",
       " 'There is a route for it which says to use the wg0 interface\\nfor that:\\n\\n$ ip route get 10.10.10.201\\n\\n10.10.10.201 dev wg0 src 10.10.11.10 uid 1000\\n\\ncache\\n\\nSince wg0 is a WireGuard interface, it will consult its configuration to see if any peer has that target address in the\\n\\nAllowedIPs list. Turns out one peer has it, in which case the traffic will:\\n\\na) Be authenticated as us, and encrypted for that peer.\\nb) Sent away via the configured Endpoint .\\n\\nNow let’s picture the reverse. This system received traffic on the ListenPort UDP port. If it can be decrypted, and\\nverified as having come from one of the listed peers using its respective public key, and if the source IP matches the\\ncorresponding AllowedIPs list, then the traffic is accepted.\\n\\nWhat if there is no Endpoint ? Well, to bootstrap the VPN, at least one of the peers must have an Endpoint, or else\\nit won’t know where to send the traffic to, and you will get an error saying “Destination address required” (see the\\ntroubleshooting section for details).\\n\\nBut once the peers know each other, the one that didn’t have an Endpoint setting in the interface will remember where\\nthe traffic came from, and use that address as the current endpoint. This has a very nice side effect of automatically\\ntracking the so called “road warrior” peer, which keeps changing its IP. That is very common with laptops that keep\\nbeing suspended and awakened in a new network, and then try to establish the VPN again from that new address.\\n\\n**Peers**\\n\\nYou will notice that the term “peers” is used preferably to “server” or “client”. Other terms used in some VPN',\n",
       " 'documentation are “left” and “right”, which is already starting to convey that the difference between a “server” and\\na “client” is a bit blurry. It only matters, if at all, at the start of the traffic exchange: who sends the first packet of\\ndata. In that sense, “servers” expect to sit idle and wait for connections to be initiated to them, and “clients” are the\\ninitiators. For example, a laptop on a public cafe initiating a connection to the company VPN peer. The laptop needs\\nto know the address of that peer, because it’s initiating the exchange. But the “server” doesn’t need to know the IP\\nof the laptop beforehand.\\n\\nOn a site to site VPN, however, when two separate networks are connected through the tunnel, who is the server, and\\nwho is the client? Both, so it’s best to call them “peers” instead.\\n## **Putting it all together**\\n\\nKey takeaways from this introduction:\\n\\n  - Each peer participating in the WireGuard VPN has a private key and a public key.\\n\\n - AllowedIPs is used as a routing key when sending traffic, and as an ACL when receiving traffic.\\n\\n  - To establish a VPN with a remote peer, you need its public key. Likewise, the remote peer will need your public\\nkey.\\n\\n  - At least one of the peers needs an Endpoint configured in order to be able to initiate the VPN.\\n\\nTo help better understand these and other concepts, we will create some WireGuard VPNs in the next sections,\\nillustrating some common setups.\\n\\n**NOTE**\\n\\nThroghout this guide, we will sometimes mention a VPN “connection”. This is technically false, as WireGuard uses UDP and there is no persistent connection. The term is used just to facilitate understanding,',\n",
       " 'and means that the peers in the examples know each other and have completed a handshake already.\\n# **References**\\n\\n[• See the WireGuard website for more detailed information.](https://www.wireguard.com)\\n\\n[• The WireGuard Quickstart has a good introduction and demo.](https://www.wireguard.com/quickstart/)\\n\\n[• wg(8) and wg-quick(8) manual pages.](https://manpages.ubuntu.com/manpages/jammy/man8/wg.8.html)\\n\\n[• Detailed explanation of the algorithms used by WireGuard.](https://www.wireguard.com/protocol/)\\n\\nTo help understand the WireGuard concepts, we will show some practical setups that hopefully match many scenarios\\nout there.\\n\\n188\\n\\n\\n-----\\n\\nThis is probably the most common setup for a VPN: connecting a single system to a remote site, and getting access\\nto the remote network “as if you were there”.\\n\\nWhere to place the remote WireGuard endpoint in the network will vary a lot depending on the topology. It can be\\nin a firewall box, the router itself, or some random system in the middle of the network.\\n\\nHere we will cover a simpler case more resembling what a home network could be like:\\n\\npublic internet\\n\\nxxxxxx ppp0 ┌────────┐\\n\\n┌────┐ xx xxxx ──┤router │\\n\\n│ ├─ppp0 xxx xx └───┬────┘\\n\\n│ │ xx x │ home 10.10.10.0/24\\n\\n│ │ xxx xxx └───┬─────────┬─────────┐\\n\\n└────┘ xxxxx │ │ │\\n\\n┌─┴─┐ ┌─┴─┐ ┌─┴─┐\\n\\n│ │ │ │ │ │\\n\\n│pi4│ │NAS│ │...│\\n\\n│ │ │ │ │ │\\n\\n└───┘ └───┘ └───┘\\n\\nThis diagram represents a typical simple home network setup. You have a router/modem, usually provided by the\\nISP (Internet Service Provider), and some internal devices like a Raspberry PI perhaps, a NAS (Network Attached\\nStorage), and some other device.',\n",
       " 'There are basically two approaches that can be taken here: install WireGuard on the router, or on another system in\\nthe home network. We will discuss both in the following sections.\\n\\nNote that in this scenario the “fixed” side, the home network, normally won’t have a WireGuard Endpoint configured,\\nas the peer is typically “on the road” and will have a dynamic IP address.\\n\\nIn this diagram, we are depicting a home network with some devices and a router where we can install WireGuard.\\n\\npublic internet ┌───wg0 10.10.11.1/24\\n\\n10.10.11.2/24 │ VPN network\\n\\nhome0│ xxxxxx ppp0 ┌───────┴┐\\n\\n┌─┴──┐ xx xxxxx ──────┤router │\\n\\n│ ├─wlan0 xx xx └───┬────┘ home network, .home domain\\n\\n│ │ xx x │.1 10.10.10.0/24\\n\\n│ │ xxx xxx └───┬─────────┬─────────┐\\n\\n└────┘ xxxxxx │ │ │\\n\\nLaptop in ┌─┴─┐ ┌─┴─┐ ┌─┴─┐\\n\\nCoffee shop │ │ │ │ │ │\\n\\n│pi4│ │NAS│ │...│\\n\\n│ │ │ │ │ │\\n\\n└───┘ └───┘ └───┘\\n\\nOf course, this setup is only possible if you can install software on the router. Most of the time, when it’s provided\\nby your ISP, you can’t. But some ISPs allow their device to be put in a bridge mode, in which case you can use your\\nown device (a computer, or a Raspberry PI, or something else) as the routing device.\\n\\nSince the router is the default gateway of the network already, this means you can create a whole new network for your\\nVPN users. You also won’t have to create any (D)NAT rules since the router is directly reachable from the Internet.\\n\\nLet’s define some addresses, networks, and terms used in this guide:\\n\\n - *laptop in coffee shop* : just your normal user at a coffee shop, using the provided WiFi access to connect to their\\nhome network. This will be one of our *peers* in the VPN setup.',\n",
       " ' - home0 : this will be the WireGuard interface on the laptop. It’s called home0 to convey the information that it is\\nused to connect to the *home* network.\\n\\n - *router* : the existing router at the home network. It has a public interface ppp0 that has a routable but dynamic\\nIPv4 address (not CGNAT), and an internal interface at 10.10.10.1/24 which is the default gateway for the\\nhome network.\\n\\n - *home network* : the existing home network, 10.10.10.0/24 in this example, with existing devices that the user\\nwishes to access remotely over the WireGuard VPN.\\n\\n - 10.10.11.0/24 : the WireGuard VPN network. This is a whole new network that was created just for the VPN\\n\\nusers.\\n\\n189\\n\\n\\n-----\\n\\n - wg0 on the *router* : this is the WireGuard interface that we will bring up on the router, at the 10.10.11.1/24\\naddress. It is the gateway for the 10.10.11.0/24 VPN network.\\n\\nWith this topology, if, say, the NAS wants to send traffic to *10.10.11.2/24*, it will send it to the default gateway (since\\nthe NAS has no specific route to *10.10.11.0/24* ), and the gateway will know how to send it to *10.10.11.2/24* because\\nit has the wg0 interface on that network.\\n## **Configuration**\\n\\nFirst, we need to create keys for the peers of this setup. We need one pair of keys for the laptop, and another for the\\nhome router:\\n\\n$ umask 077\\n\\n$ wg genkey > laptop-private.key\\n\\n$ wg pubkey < laptop-private.key > laptop-public.key\\n\\n$ wg genkey > router-private.key\\n\\n$ wg pubkey < router-private.key > router-public.key\\n\\nLet’s create the router wg0 interface configuration file. The file will be /etc/wireguard/wg0.conf and have these\\n\\ncontents:\\n\\n[Interface]\\n\\nPrivateKey = <contents-of-router-private.key>',\n",
       " 'ListenPort = 51000\\n\\nAddress = 10.10.11.1/24\\n\\n[Peer]\\n\\nPublicKey = <contents-of-laptop-public.key>\\n\\nAllowedIPs = 10.10.11.2\\n\\nThere is no Endpoint configured for the laptop peer, because we don’t know what IP address it will have beforehand,\\nnor will that IP address be always the same. This laptop can be connecting from a coffee shop free wifi, or an airport\\nlounge, or a friend’s house.\\n\\nNot having an endpoint here also means that the home network side will never be able to *initiate* the VPN connection.\\nIt will sit and wait, and can only *respond* to VPN handshake requests, at which time it will learn the endpoint from\\nthe peer and use that until it changes (the peer reconnects from a different site) or times out.\\n\\n**IMPORTANT**\\n\\nThis configuration file contains a secret: **PrivateKey** . Make sure to adjust its permissions accordingly,\\nlike:\\n\\nsudo chmod 0600 /etc/wireguard/wg0.conf\\n\\nsudo chown root: /etc/wireguard/wg0.conf\\n\\nWhen activated, this will bring up a wg0 interface with the address 10.10.11.1/24, listening on port *51000/udp*, and\\nadd a route for the 10.10.11.0/24 network using that interface.\\n\\nThe *Peer* section is identifying a peer via its public key, and listing who can connect from that peer. This AllowedIPs\\nsetting has two meanings:\\n\\n  - When sending packets, the AllowedIPs list serves as a routing table, indicating that this peer’s public key should\\nbe used to encrypt the traffic.\\n\\n  - When receiving packets, AllowedIPs behaves like an access control list. After decryption, the traffic is only\\nallowed if it matches the list.\\n\\nFinally, the ListenPort parameter specifies the *UDP* port on which WireGuard will listen for traffic. This port will',\n",
       " 'have to be allowed in the firewall rules of the router. There is no default nor a standard port for WireGuard, so you\\ncan pick any value you prefer.\\n\\nNow let’s create a similar configuration on the other peer, the laptop. Here the interface is called home0, so the\\nconfiguration file is /etc/wireguard/home0.conf :\\n\\n[Interface]\\n\\nPrivateKey = <contents-of-laptop-private.key>\\n\\nListenPort = 51000\\n\\nAddress = 10.10.11.2/24\\n\\n[Peer]\\n\\nPublicKey = <contents-of-router-public.key>\\n\\n190\\n\\n\\n-----\\n\\nEndpoint = <home-ppp0-IP-or-hostname>:51000\\n\\nAllowedIPs = 10.10.11.0/24,10.10.10.0/24\\n\\n**IMPORTANT**\\n\\nLike before, this configuration file contains a secret: **PrivateKey** . Make sure to adjust its permissions\\naccordingly, like:\\n\\nsudo chmod 0600 /etc/wireguard/home0.conf\\n\\nsudo chown root: /etc/wireguard/home0.conf\\n\\nWe have given this laptop the 10.10.11.2/24 address. It could have been any valid address in the 10.10.11.0/24\\nnetwork, as long as it doesn’t collide with an existing one, and is allowed in the router’s peer’s *AllowedIPs* list.\\n\\n**NOTE**\\n\\nYou may have noticed by now that address allocation is manual, and not via something like DHCP. Keep\\ntabs on it!\\n\\nIn the [Peer] stanza for the laptop we have:\\n\\n - The usual PublicKey item, which identifies the peer. Traffic to this peer will be encrypted using this public key.\\n\\n - Endpoint : this tells WireGuard where to actually send the encrypted traffic to. Since in our scenario the laptop\\nwill be initiating connections, it has to know the public IP address of the home router. If your ISP gave you\\na fixed IP address, great, nothing else to do. If however you have a dynamic IP address, one that changes',\n",
       " 'everytime you establish a new connection, then you will have to setup some sort of dynamic DNS service. There\\nare many free such services available on the Internet, but setting one up is out of scope for this guide.\\n\\n - In AllowedIPs we list our destinations. The VPN network 10.10.11.0/24 is listed so that we can ping wg0 on the\\nhome router as well as other devices on the same VPN, and the actual home network, which is 10.10.10.0/24 .\\n\\nIf we had used 0.0.0.0/0 alone in AllowedIPs, then the VPN would become our default gateway, and all traffic would\\nbe sent to this peer. See Default Gateway for details on that type of setup.\\n## **Testing**\\n\\nWith these configuration files in place, it’s time to bring the WireGuard interfaces up.\\n\\nOn the home router, run:\\n\\n$ sudo wg-quick up wg0\\n\\n[#] ip link add wg0 type wireguard\\n\\n[#] wg setconf wg0 /dev/fd/63\\n\\n[#] ip -4 address add 10.10.11.1/24 dev wg0\\n\\n[#] ip link set mtu 1378 up dev wg0\\n\\nVerify you have a wg0 interface with an address of 10.10.11.1/24 :\\n\\n$ ip a show dev wg0\\n\\n9: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1378 qdisc noqueue state UNKNOWN group default qlen 1000\\n\\nlink/none\\n\\ninet 10.10.11.1/24 scope global wg0\\n\\nvalid_lft forever preferred_lft forever\\n\\nVerify you have a wg0 interface up with an address of 10.10.11.1/24 :\\n\\n$ ip a show dev wg0\\n\\n9: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1378 qdisc noqueue state UNKNOWN group default qlen 1000\\n\\nlink/none\\n\\ninet 10.10.11.1/24 scope global wg0\\n\\nvalid_lft forever preferred_lft forever\\n\\nAnd a route to the 10.10.1.0/24 network via the wg0 interface:\\n\\n$ ip route | grep wg0\\n\\n10.10.11.0/24 dev wg0 proto kernel scope link src 10.10.11.1',\n",
       " 'And wg show should show some status information, but no connected peer yet:\\n\\n$ sudo wg show\\n\\ninterface: wg0\\n\\npublic key: <router public key>\\n\\nprivate key: (hidden)\\n\\nlistening port: 51000\\n\\n191\\n\\n\\n-----\\n\\npeer: <laptop public key>\\n\\nallowed ips: 10.10.11.2/32\\n\\nIn particular, verify that the public keys listed match what you created and expect.\\n\\nBefore we start the interface on the other peer, it helps to leave the above show command running continuously, so we\\ncan see when there are changes:\\n\\n$ sudo watch wg show\\n\\nNow start the interface on the laptop:\\n\\n$ sudo wg-quick up home0\\n\\n[#] ip link add home0 type wireguard\\n\\n[#] wg setconf home0 /dev/fd/63\\n\\n[#] ip -4 address add 10.10.11.2/24 dev home0\\n\\n[#] ip link set mtu 1420 up dev home0\\n\\n[#] ip -4 route add 10.10.10.0/24 dev home0\\n\\nSimilarly, verify the interface’s IP and added routes:\\n\\n$ ip a show dev home0\\n\\n24: home0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000\\n\\nlink/none\\n\\ninet 10.10.11.2/24 scope global home0\\n\\nvalid_lft forever preferred_lft forever\\n\\n$ ip route | grep home0\\n\\n10.10.10.0/24 dev home0 scope link\\n\\n10.10.11.0/24 dev home0 proto kernel scope link src 10.10.11.2\\n\\nUp to this point, the wg show output on the home router probably didn’t change. That’s because we haven’t sent any\\ntraffic to the home network, which didn’t trigger the VPN yet. By default, WireGuard is very “quiet” on the network.\\n\\nIf we trigger some traffic, however, the VPN will “wake up”. Let’s ping the internal address of the home router a few\\n\\ntimes:\\n\\n$ ping -c 3 10.10.10.1\\n\\nPING 10.10.10.1 (10.10.10.1) 56(84) bytes of data.\\n\\n64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=603 ms',\n",
       " '64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=300 ms\\n\\n64 bytes from 10.10.10.1: icmp_seq=3 ttl=64 time=304 ms\\n\\nNote how the first ping was slower. That’s because the VPN was “waking up” and being established. Afterwards,\\nwith the tunnel already established, the latency reduced.\\n\\nAt the same time, the wg show output on the home router will have changed to something like this:\\n\\n$ sudo wg show\\n\\ninterface: wg0\\n\\npublic key: <router public key>\\n\\nprivate key: (hidden)\\n\\nlistening port: 51000\\n\\npeer: <laptop public key>\\n\\nendpoint: <laptop public IP>:51000\\n\\nallowed ips: 10.10.11.2/32\\n\\nlatest handshake: 1 minute, 8 seconds ago\\n\\ntransfer: 564 B received, 476 B sent\\n\\nSometimes it’s not possible to install WireGuard on the home router itself. Perhaps it’s a closed system to which you\\ndo not have access, or there is no easy build for that architecture, or any other of the many possible reasons.\\n\\nBut you do have a spare system inside your network that you could use. Here we are going to show one way to\\nmake this work. There are others, but we believe this to be the one less involved as it only requires a couple of (very\\ncommon) changes in the router itself: NAT port forwarding, and DHCP range editing.\\n\\nTo recap, our home network has the 10.10.10.0/24 address, and we want to connect to it from a remote location and\\nbe “inserted” into that network as if we were there:\\n\\n192\\n\\n\\n-----\\n\\npublic internet\\n\\n10.10.10.3/24\\n\\nhome0│ xxxxxx ppp0 ┌────────┐\\n\\n┌─┴──┐ xx xxxxx ──────┤router │\\n\\n│ ├─ppp0 xxx xx └───┬────┘ home network, .home domain\\n\\n│ │ xx x │ 10.10.10.0/24\\n\\n│ │ xxx xxx └───┬─────────┬─────────┐\\n\\n└────┘ xxxxxx │ │ │\\n\\n┌─┴─┐ ┌─┴─┐ ┌─┴─┐\\n\\nwg0 ──┤ │ │ │ │ │\\n\\n10.10.10.10/32 │pi4│ │NAS│ │...│',\n",
       " '│ │ │ │ │ │\\n\\n└───┘ └───┘ └───┘\\n\\nReserved for VPN users:\\n\\n10.10.10.2-9\\n## **Router changes**\\n\\nSince in this scenario we don’t have a new network dedicated to our VPN users, we need to “carve out” a section of\\nthe home network and reserve it to the VPN.\\n\\nThe easiest way to reserve IPs for the VPN is to change the router configuration (assuming it’s responsible for DHCP\\nin this network) and tell its DHCP server to only hand out addresses from a specific range, leaving a “hole” for our\\nVPN users.\\n\\nFor example, in the case of the 10.10.10.0/24 network, the DHCP server on the router might already be configured\\nto hand out IP addresses from 10.10.10.2 through 10.10.10.254 . We can carve out a “hole” for our VPN users by\\nreducing the DHCP range, like below:\\n\\nNetwork 10.10.10.0/24\\nUsable Addresses 10.10.10.2             - 10.10.10.254 ( .1 is the router)\\nDHCP Range 10.10.10.50          - 10.10.10.254\\nVPN Range 10.10.10.10           - 10.10.10.59\\n\\nOr any other layout that is better suited for your case. In that way, the router will never hand out a DHCP address\\nthat conflicts with one that we selected for a VPN user.\\n\\nThe second change we need to do in the router is to port forward the WireGuard traffic to the internal system that will\\nbe the endpoint. In the diagram above, we selected the 10.10.10.10 system to be the internal WireGuard endpoint,\\nand we will run it on the *51000/udp* port. Therefore, you need to configure the router to forward all *51000/udp* traffic\\nto 10.10.10.10 on the same *51000/udp* port.\\n\\nFinally, we also need to allow hosts on the internet to send traffic to the router on the *51000/udp* port we selected\\nfor WireGuard.',\n",
       " 'This is done in the firewall rules of the device. Sometimes just doing the port forwarding from before\\nalso configures the firewall to allow that traffic, but better check.\\n\\nNow we are ready to configure the internal endpoint.\\n## **Configuring the internal WireGuard endpoint**\\n\\nInstall the wireguard package:\\n\\n$ sudo apt install wireguard\\n\\nGenerate the keys for this host:\\n\\n$ umask 077\\n\\n$ wg genkey > internal-private.key\\n\\n$ wg pubkey < internal-private.key > internal-public.key\\n\\nAnd create the /etc/wireguard/wg0.conf file with these contents:\\n\\n[Interface]\\n\\nAddress = 10.10.10.10/32\\n\\nListenPort = 51000\\n\\nPrivateKey = <contents of internal-private.key>\\n\\n193\\n\\n\\n-----\\n\\n[Peer]\\n\\n# laptop\\n\\nPublicKey = <contents of laptop-public.key>\\n\\nAllowedIPs = 10.10.10.11/32 # any available IP in the VPN range\\n\\n**NOTE**\\n\\nJust like in the *Peer to Site* scenario with WireGuard on the router, there is no Endpoint configuration\\nhere for the laptop peer, because we don’t know where it will be connecting from beforehand.\\n\\nThe final step is to configure this internal system as a router for the VPN users. For that, we need to enable a couple\\nof settings:\\n\\n - ip_forward : to enable forwarding (aka, routing) of traffic between interfaces.\\n\\n - proxy_arp : to reply to *arp* requests on behalf of the VPN systems, as if they were locally present on the network\\nsegment.\\n\\nTo do that, and make it persist across reboots, create the file /etc/sysctl.d/70-wireguard-routing.conf file with this\\n\\ncontent:\\n\\nnet.ipv4.ip_forward = 1\\n\\nnet.ipv4.conf.all.proxy_arp = 1\\n\\nThen run this command to apply those settings:\\n\\n$ sudo sysctl -p /etc/sysctl.d/70-wireguard-routing.conf -w\\n\\nNow the WireGuard interface can be brought up:',\n",
       " '$ sudo wg-quick up wg0\\n## **Configuring the peer**\\n\\nThe peer configuration will be very similar to what was done before. What changes will be the address, since now it\\nwon’t be on an exclusive network for the VPN, but have an address carved out of the home network block.\\n\\nLet’s call this new configuration file /etc/wireguard/home_internal.conf :\\n\\n[Interface]\\n\\nListenPort = 51000\\n\\nAddress = 10.10.10.11/24\\n\\nPrivateKey = <contents of the private key for this system>\\n\\n[Peer]\\n\\nPublicKey = <contents of internal-public.key>\\n\\nEndpoint = <home-ppp0-IP-or-hostname>:51000\\n\\nAllowedIPs = 10.10.10.0/24\\n\\nAnd bring up this WireGuard interface:\\n\\n$ sudo wg-quick up home_internal\\n\\n**NOTE**\\n\\nThere is no need to add an index number to the end of the interface name. That is a convention, but not\\nstrictly a requirement.\\n## **Testing**\\n\\nWith the WireGuard interfaces up on both peers, traffic should flow seamlessly in the 10.10.10.0/24 network between\\nremote and local systems.\\n\\nMore specifically, it’s best to test the non-trivial cases, that is, traffic between the remote peer and a host other than\\nthe one with the WireGuard interface on the home network.\\n\\nAnother usual VPN configuration where one could deploy WireGuard is to connect two distinct networks over the\\ninternet. Here is a simplified diagram:\\n\\n194\\n\\n\\n-----\\n\\n┌───────WireGuard tunnel ──────┐\\n\\n│ 10.10.9.0/31 │\\n\\n│ │\\n\\n10.10.9.0 wgA│ xx │wgB 10.10.9.1\\n\\n┌─┴─┐ xxx xxxx ┌─┴─┐\\n\\nalpha site │ │ext xx xx ext│ │ beta site\\n\\n│ ├─── x x ───┤ │\\n\\n10.10.10.0/24 │ │ xx xx │ │ 10.10.11.0/24\\n\\n│ │ x x │ │\\n\\n└─┬─┘ x x └─┬─┘\\n\\n10.10.10.1│ xx x │10.10.11.1\\n\\n...┌─────────┬────┘ xx xxx xx └───┬─────────┐...\\n\\n│ │ xx xxxxx │ │\\n\\n│ │ │ │',\n",
       " '┌─┴─┐ ┌─┴─┐ public internet ┌─┴─┐ ┌─┴─┐\\n\\n│ │ │ │ │ │ │ │\\n\\n└───┘ └───┘ └───┘ └───┘\\n\\nThe goal here is to seamlessly integrate network *alpha* with network *beta*, so that systems on the *alpha* site can\\ntransparently access systems on the *beta* site, and vice-versa.\\n\\nSuch a setup as a few particular details:\\n\\n  - Both peers are likely to be always up and running.\\n\\n  - We can’t assume one side will always be the initiator, like the laptop in a coffee shop scenario.\\n\\n  - Because of the above, both peers should have a static endpoint, like a fixed IP address, or valid domain name.\\n\\n  - Since we are not assigning VPN IPs to all systems on each side, the VPN network here will be very small (a /31,\\nwhich allows for two IPs) and only used for routing. The only systems with an IP in the VPN network are the\\ngateways themselves.\\n\\n  - There will be no NAT applied to traffic going over the WireGuard network. Therefore, the networks of both\\nsites ***must*** be different and not overlap.\\n\\nThis is what an *mtr* report from a system in the *beta* network to an *alpha* system will look like:\\n\\nubuntu@b1:~$ mtr -n -r 10.10.10.230\\n\\nStart: 2022-09-02T18:56:51+0000\\n\\nHOST: b1 Loss% Snt Last Avg Best Wrst StDev\\n\\n1.|-- 10.10.11.1 0.0% 10 0.1 0.1 0.1 0.2 0.0\\n\\n2.|-- 10.10.9.0 0.0% 10 299.6 299.3 298.3 300.0 0.6\\n\\n3.|-- 10.10.10.230 0.0% 10 299.1 299.1 298.0 300.2 0.6\\n\\n**NOTE**\\n\\nTechnically, a /31 CIDR network has no usable IP addresses, since the first one is the network address, and\\n[the second (and last) one is the broadcast address. RFC 3021 however allows for it, but if you encounter](https://www.ietf.org/rfc/rfc3021.txt)',\n",
       " 'routing or other networking issues, switch to a /30 CIDR and its two valid host ips.\\n## **Configuring WireGuard**\\n\\nOn the system that is the gateway for each site, and has internet connectivity, we start by installing WireGuard and\\ngenerating the keys. For the *alpha* site:\\n\\n$ sudo apt install wireguard\\n\\n$ wg genkey | sudo tee /etc/wireguard/wgA.key\\n\\n$ sudo cat /etc/wireguard/wgA.key | wg pubkey | sudo tee /etc/wireguard/wgA.pub\\n\\nAnd the configuration on *alpha* will be:\\n\\n[Interface]\\n\\nPostUp = wg set %i private-key /etc/wireguard/%i.key\\n\\nAddress = 10.10.9.0/31\\n\\nListenPort = 51000\\n\\n[Peer]\\n\\n# beta site\\n\\nPublicKey = <contents of /etc/wireguard/wgB.pub>\\n\\nAllowedIPs = 10.10.11.0/24,10.10.9.0/31\\n\\nEndpoint = <beta-gw-ip>:51000\\n\\n195\\n\\n\\n-----\\n\\nOn the gateway for the *beta* site we take similar steps:\\n\\n$ sudo apt install wireguard\\n\\n$ wg genkey | sudo tee /etc/wireguard/wgB.key\\n\\n$ sudo cat /etc/wireguard/wgB.key | wg pubkey | sudo tee /etc/wireguard/wgB.pub\\n\\nAnd create the corresponding configuration file for *beta* :\\n\\n[Interface]\\n\\nAddress = 10.10.9.1/31\\n\\nPostUp = wg set %i private-key /etc/wireguard/%i.key\\n\\nListenPort = 51000\\n\\n[Peer]\\n\\n# alpha site\\n\\nPublicKey = <contents of /etc/wireguard/wgA.pub>\\n\\nAllowedIPs = 10.10.10.0/24,10.10.9.0/31\\n\\nEndpoint = <alpha-gw-ip>:51000\\n\\n**IMPORTANT**\\n\\nWireGuard is being setup on the gateways for these two networks. As such, there are no changes needed\\non individual hosts of each network, but keep in mind that the WireGuard tunneling and encryption is\\nonly happening between the *alpha* and *beta* gateways, and **NOT** between the hosts of each network.\\n## **Bringing the interfaces up**',\n",
       " 'Since this VPN is permanent between static sites, it’s best to use the *systemd* unit file for wg-quick to bring the\\ninterfaces up and control them in general. In particular, we want them to be brought up automatically on reboot\\n\\nevents.\\n\\nOn *alpha* :\\n\\n$ sudo systemctl enable --now wg-quick@wgA\\n\\nAnd similarly on *beta* :\\n\\n$ sudo systemctl enable --now wg-quick@wgB\\n\\nThis both enables the interface on reboot, and starts it right away.\\n## **Firewall and routing**\\n\\nBoth gateways probably already have some routing and firewall rules. These might need changes depending on how\\nthey are setup.\\n\\nThe individual hosts on each network won’t need any changes regarding the remote *alpha* or *beta* networks, because\\nthey will just send that traffic to the default gateway (as any other non-local traffic), which knows how to route it\\nbecause of the routes that wg-quick added.\\n\\nIn the configuration we did so far, there have been no restrictions in place, so traffic between both sites flows without\\nimpediments.\\n\\nIn general, what needs to be done or checked is:\\n\\n  - Make sure both gateways can contact each other on the specified endpoint addresses and UDP port. In the case\\nof this example, that’s port 51000 . For extra security, create a firewall rule only allowing each peer to contact\\nthis port, instead of the Internet at large.\\n\\n  - Do NOT masquerade or NAT the traffic coming from the internal network and going out via the WireGuard\\ninterface towards the other site. This is purely routed traffic.\\n\\n  - There shouldn’t be any routing changes needed on the gateways, since wg-quick takes care of adding the route',\n",
       " 'for the remote site, but do check the routing table to see if it makes sense ( ip route and ip route | grep wg are\\na good start).\\n\\n  - You may have to create new firewall rules if you need to restrict traffic between the *alpha* and *beta* networks.\\nFor example, if you want to prevent *ssh* between the sites, you could add a firewall rule like this one to *alpha* :\\n\\n$ sudo iptables -A FORWARD -i wgA -p tcp --dport 22 -j REJECT\\n\\nAnd similarly on *beta* :\\n\\n196\\n\\n\\n-----\\n\\n$ sudo iptables -A FORWARD -i wgB -p tcp --dport 22 -j REJECT\\n\\nYou can add these as PostUp actions in the WireGuard interface config. Just don’t forget the remove them in\\nthe corresponding PreDown hook, or you will end up with multiple rules.\\n\\nWireGuard can also be setup to route all traffic through the VPN, and not just specific remote networks. There could\\nbe many reasons for this, but mostly they are related to privacy.\\n\\nHere we will assume a scenario where the local network is considered untrusted, and we want to leak as little information\\nas possible about our behavior on the Internet. This could be the case of an airport, or a coffee shop, a conference, a\\nhotel, or any other public network.\\n\\npublic untrusted ┌──wg0 10.90.90.2/24\\n\\n10.90.90.1/24 network/internet │ VPN network\\n\\nwg0│ xxxxxx ┌──────┴─┐\\n\\n┌─┴──┐ xx xxxxx ──────┤VPN gw │\\n\\n│ ├─wlan0 xx xx eth0 └────────┘\\n\\n│ │ xx x\\n\\n│ │ xxx xxx\\n\\n└────┘ xxxxxx\\n\\nLaptop\\n\\nFor this to work best, we need a system we can reach on the internet and that we control. Most commonly this can\\nbe a simple small VM in a public cloud, but a home network also works. Here we will assume it’s a brand new system\\nthat will be configured from scratch for this very specific purpose.',\n",
       " '## **WireGuard Configuration**\\n\\nLet’s start the configuration by installing WireGuard and generating the keys.\\n\\nOn the client:\\n\\n$ sudo apt install wireguard\\n\\n$ umask 077\\n\\n$ wg genkey > wg0.key\\n\\n$ wg pubkey < wg0.key > wg0.pub\\n\\n$ sudo mv wg0.key wg0.pub /etc/wireguard\\n\\nAnd on the gateway server:\\n\\n$ sudo apt install wireguard\\n\\n$ umask 077\\n\\n$ wg genkey > gateway0.key\\n\\n$ wg pubkey < gateway0.key > gateway0.pub\\n\\n$ sudo mv gateway0.key gateway0.pub /etc/wireguard\\n\\nOn the client, we will create /etc/wireguard/wg0.conf :\\n\\n[Interface]\\n\\nPostUp = wg set %i private-key /etc/wireguard/wg0.key\\n\\nListenPort = 51000\\n\\nAddress = 10.90.90.1/24\\n\\n[Peer]\\n\\nPublicKey = <contents of gateway0.pub>\\n\\nEndpoint = <public IP of gateway server>\\n\\nAllowedIPs = 0.0.0.0/0\\n\\nKey points here:\\n\\n  - We selected the 10.90.90.1/24 IP address for the WireGuard interface. This can be any private IP address, as\\nlong as it doesn’t conflict with the network you are on, so double check that. If it needs changing, don’t forget\\nto also change the IP for the WireGuard interface on the gateway server.\\n\\n - The AllowedIPs value is 0.0.0.0/0, which means “all IPv4 addresses”.\\n\\n  - We are using PostUp to load the private key instead of specifying it directly in the configuration file, so we don’t\\nhave to set the permissions on the config file to 0600 .\\n\\nThe counterpart configuration on the gateway server is /etc/wireguard/gateway0.conf with these contents:\\n\\n[Interface]\\n\\nPostUp = wg set %i private-key /etc/wireguard/%i.key\\n\\n197\\n\\n\\n-----\\n\\nAddress = 10.90.90.2/24\\n\\nListenPort = 51000\\n\\n[Peer]\\n\\nPublicKey = <contents of wg0.pub>\\n\\nAllowedIPs = 10.90.90.1/32',\n",
       " 'Since we don’t know from where this remote peer will be connecting, there is no Endpoint setting for it, and the\\nexpectation is that the peer will be the one initiating the VPN.\\n\\nThis finishes the WireGuard configuration on both ends, but there is one extra step we need to take on the gateway\\n\\nserver.\\n## **Routing and masquerading**\\n\\nThe WireGuard configuration that we did so far is enough to send the traffic from the client in the untrusted network,\\nto the gateway server. But what about from there on? There are two extra configs we need to make on the gateway\\n\\nserver:\\n\\n  - Masquerade (or apply source NAT rules) the traffic from 10.90.90.1/24 .\\n\\n  - Enable IPv4 forwarding so our gateway server acts as a router.\\n\\nTo enable routing, create /etc/sysctl.d/70-wireguard-routing.conf with this content:\\n\\nnet.ipv4.ip_forward = 1\\n\\nAnd run:\\n\\n$ sudo sysctl -p /etc/sysctl.d/70-wireguard-routing.conf -w\\n\\nTo masquerade the traffic from the VPN, one simple rule is needed:\\n\\n$ sudo iptables -t nat -A POSTROUTING -s 10.90.90.0/24 -o eth0 -j MASQUERADE\\n\\nReplace eth0 with the name of the network interface on the gateway server, if it’s different.\\n\\nTo have this rule persist across reboots, you can add it to /etc/rc.local (create the file if it doesn’t exist and make\\nit executable):\\n\\n#!/bin/sh\\n\\niptables -t nat -A POSTROUTING -s 10.90.90.0/24 -o eth0 -j MASQUERADE\\n\\nThis completes the gateway server configuration.\\n## **Testing**\\n\\nLet’s bring up the WireGuard interfaces on both peers.\\n\\nOn the gateway server:\\n\\n$ sudo wg-quick up gateway0\\n\\n[#] ip link add gateway0 type wireguard\\n\\n[#] wg setconf gateway0 /dev/fd/63\\n\\n[#] ip -4 address add 10.90.90.2/24 dev gateway0',\n",
       " '[#] ip link set mtu 1378 up dev gateway0\\n\\n[#] wg set gateway0 private-key /etc/wireguard/gateway0.key\\n\\nAnd on the client:\\n\\n$ sudo wg-quick up wg0\\n\\n[#] ip link add wg0 type wireguard\\n\\n[#] wg setconf wg0 /dev/fd/63\\n\\n[#] ip -4 address add 10.90.90.1/24 dev wg0\\n\\n[#] ip link set mtu 1420 up dev wg0\\n\\n[#] wg set wg0 fwmark 51820\\n\\n[#] ip -4 route add 0.0.0.0/0 dev wg0 table 51820\\n\\n[#] ip -4 rule add not fwmark 51820 table 51820\\n\\n[#] ip -4 rule add table main suppress_prefixlength 0\\n\\n[#] sysctl -q net.ipv4.conf.all.src_valid_mark=1\\n\\n[#] nft -f /dev/fd/63\\n\\n[#] wg set wg0 private-key /etc/wireguard/wg0.key\\n\\n198\\n\\n\\n-----\\n\\nFrom the client you should now be able to verify that your traffic reaching out to the internet is going through the\\ngateway server via the WireGuard VPN. For example:\\n\\n$ mtr -r 1.1.1.1\\n\\nStart: 2022-09-01T12:42:59+0000\\n\\nHOST: laptop.lan Loss% Snt Last Avg Best Wrst StDev\\n\\n1.|-- 10.90.90.2 0.0% 10 184.9 185.5 184.9 186.9 0.6\\n\\n2.|-- 10.48.128.1 0.0% 10 185.6 185.8 185.2 188.3 0.9\\n\\n(...)\\n\\n7.|-- one.one.one.one 0.0% 10 186.2 186.3 185.9 186.6 0.2\\n\\nAbove, hop 1 is the gateway0 interface on the gateway server, then 10.48.128.1 is the default gateway for that server,\\nthen come some in between hops, and the final hit is the target.\\n\\nIf you just look at the output of ip route, however, it’s not immediately obvious that the WireGuard VPN is the\\ndefault gateway:\\n\\n$ ip route\\n\\ndefault via 192.168.122.1 dev enp1s0 proto dhcp src 192.168.122.160 metric 100\\n\\n10.90.90.0/24 dev wg0 proto kernel scope link src 10.90.90.1\\n\\n192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.160 metric 100\\n\\n192.168.122.1 dev enp1s0 proto dhcp scope link src 192.168.122.160 metric 100',\n",
       " 'That’s because WireGuard is using *fwmarks* and policy routing. WireGuard cannot simply set the wg0 interface as the\\ndefault gateway: that traffic needs to reach the specified endpoint on port 51000/UDP outside of the VPN tunnel.\\n\\nIf you want to dive deeper into how this works, check ip rule list, ip route list table 51820, and consult the\\ndocumentation on “Linux Policy Routing”.\\n## **DNS leaks**\\n\\nThe traffic is now being routed through the VPN to the gateway server that you control, and from there on to the\\nInternet at large. The local network you are in cannot see the contents of that traffic, because it’s encrypted. But you\\nare still leaking information about the sites you access via DNS.\\n\\nWhen the laptop got its IP address in the local (untrusted) network it is sitting in, it likely also got a pair of IPs\\nfor DNS servers to use. These might be servers from that local network, or other DNS servers from the internet like\\n\\n1.1.1.1 or 8.8.8.8 . When you access an internet site, a DNS query will be sent to those servers to discover their IP\\naddresses. Sure, that traffic goes over the VPN, but at some point it exits the VPN, and then reaches those servers,\\nwhich will then know what you are trying to access.\\n\\n[There are DNS leak detectors out there, and if you want a quick check you can try out https://dnsleaktest.com. It](https://dnsleaktest.com)\\nwill tell you which DNS servers your connection is using, and it’s up to you if you trust them or not. You might\\nbe surprised that, even if you are in a conference network for example, using a default gateway VPN like the one\\ndescribed here, you are still using the DNS servers from the conference infrastructure.',\n",
       " 'In a way, the DNS traffic is\\nleaving your machine encrypted, and then coming back in clear text to the local DNS server.\\n\\nThere are basically two things you can do about this: select a specific DNS server to use for your VPN connection, or\\ninstall your own DNS server.\\n\\n**Selecting a DNS server**\\n\\nIf you can use a DNS server that you trust, or don’t mind using, that is probably the easiest solution. Many people\\nwould start with the DNS server assigned to the gateway server used for the VPN. This address can be checked by\\nrunning the following command in a shell on the gateway server:\\n\\n$ resolvectl status\\n\\nGlobal\\n\\nProtocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\\n\\nresolv.conf mode: stub\\n\\nLink 2 (ens2)\\n\\nCurrent Scopes: DNS\\n\\nProtocols: +DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\\n\\nCurrent DNS Server: 10.48.0.5\\n\\nDNS Servers: 10.48.0.5\\n\\nDNS Domain: openstacklocal\\n\\n199\\n\\n\\n-----\\n\\nLink 5 (gateway0)\\n\\nCurrent Scopes: none\\n\\nProtocols: -DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\\n\\nLook for Current DNS Server . In the example above, it’s 10.48.0.5 .\\n\\nLet’s change the WireGuard wg0 interface config to use that DNS server. Edit /etc/wireguard/wg0.conf and add a\\nsecond PostUp line with the resolvectl command like below:\\n\\n[Interface]\\n\\nPostUp = wg set %i private-key /etc/wireguard/wg0.key\\n\\nPostUp = resolvectl dns %i 10.48.0.5; resolvectl domain %i \\\\~.\\n\\nListenPort = 51000\\n\\nAddress = 10.90.90.1/24\\n\\n[Peer]\\n\\nPublicKey = <contents of gateway0.pub>\\n\\nEndpoint = <public IP of gateway server>\\n\\nAllowedIPs = 0.0.0.0/0\\n\\nYou can run that resolvectl command by hand if you want to avoid having to restart the WireGuard VPN:',\n",
       " '$ sudo resolvectl dns wg0 10.48.0.5; sudo resolvectl domain wg0 \\\\~.\\n\\nOr just restart the WireGuard interface:\\n\\n$ sudo wg-quick down wg0; sudo wg-quick up wg0\\n\\nAnd if you check again for DNS leaks, this time you’ll that you are only using the DNS server you specified.\\n\\n**Installing your own DNS server**\\n\\nIf you don’t want to use even the DNS server from the hosting provider where you have your gateway server, another\\nalternative is to install your own DNS server.\\n\\nThere are multiple choices out there for this: bind9 and unbound are quite popular, and easy to find quick tutorials\\nand instructions on how to do it.\\n\\nHere we will proceed with bind9, which is in the Ubuntu Main repository.\\n\\nOn the gateway server, install the bind9 package:\\n\\n$ sudo apt install bind9\\n\\nAnd that’s it for the server part.\\n\\nOn the client, add a PostUp line specifying this IP (or change the line we added in the previous section):\\n\\n[Interface]\\n\\nPostUp = wg set %i private-key /etc/wireguard/wg0.key\\n\\nPostUp = resolvectl dns %i 10.90.90.2; resolvectl domain %i \\\\~.\\n\\nListenPort = 51000\\n\\nAddress = 10.90.90.1/24\\n\\n[Peer]\\n\\nPublicKey = <contents of gateway0.pub>\\n\\nEndpoint = <public IP of gateway server>\\n\\nAllowedIPs = 0.0.0.0/0\\n\\nAnd restart the WireGuard interface. Now your VPN client will be using the gateway server as the DNS server.\\n\\nHere are some common tasks and other helpful tips that can help you in your WireGuard deployment.\\n## **Controlling the WireGuard interface with systemd**\\n\\nThe wg-quick tool is a simple way to bring the WireGuard interface up and down. That control is also exposed via a\\nsystemd service, which means the standard systemctl tool can be used.',\n",
       " 'Probably the best benefit of this is to be able to configure the interface to be brought up automatically when the\\nsystem is booted up. For example, to configure the wg0 interface to be brought up at boot:\\n\\n$ sudo systemctl enable wg-quick@wg0\\n\\n200\\n\\n\\n-----\\n\\nThe name of the systemd service follows the WireGuard interface name, and multiple such services can be enabled/started at the same time. You can also use the systemctl status, start, stop, reload and restart commands\\nto control the WireGuard interface and query its status:\\n\\n$ sudo systemctl reload wg-quick@wg0\\n\\nThe reload action does what we expect: it reloads the configuration of the interface without disrupting existing\\nWireGuard tunnels. To add or remove peers, reload is sufficient, but if wg-quick options, like PostUp, Address or\\nothers alike, are changed, then a restart is needed.\\n## **DNS resolving**\\n\\nLet’s say when you are inside the home network, literally at home, you can connect to your other systems via DNS\\nnames, because your router at *10.10.10.1* can act as an internal DNS server. It would be nice to have this capability\\nalso when connected via the WireGuard VPN.\\n\\nTo do that, we can add a PostUp command to the WireGuard configuration to run a command for us right after the\\nVPN is established. This command can be anything you would run in a shell, as root. We can use that to adjust the\\nDNS resolver configuration of the laptop that is remotely connected to the home network.\\n\\nFor example, if we have a WireGuard setup as follows:\\n\\n - home0 WIreGuard interface.\\n\\n - .home DNS domain for the remote network.\\n\\n - 10.10.10.1/24 is the DNS server for the .home domain, reachable after the VPN is established.',\n",
       " 'We can add this PostUp command to the home0.conf configuration file to have our systemd-based resolver use 10.10.10.1\\nas the DNS server for any queries for the .home domain:\\n\\n[Interface]\\n\\n...\\n\\nPostUp = resolvectl dns %i 10.10.10.1; resolvectl domain %i \\\\~home\\n\\nFor PostUp (and PostDown, see the wg-quick(8) manpage for details), the %i text is replaced with the WireGuard\\ninterface name. In this case, that would be home0 .\\n\\nThese two resolvectl commands tell the local *systemd-resolved* resolver to a) associate the DNS server at 10.10.10.1\\nto the home0 interface; and b) associate the home domain to the home0 interface.\\n\\nWhen you bring the home0 WireGuard interface up again, it will run the resolvectl commands:\\n\\n$ sudo wg-quick up home0\\n\\n[#] ip link add home0 type wireguard\\n\\n[#] wg setconf home0 /dev/fd/63\\n\\n[#] ip -4 address add 10.10.11.2/24 dev home0\\n\\n[#] ip link set mtu 1420 up dev home0\\n\\n[#] ip -4 route add 10.10.10.0/24 dev home0\\n\\n[#] resolvectl dns home0 10.10.10.1; resolvectl domain home0 \\\\~home\\n\\nYou can verify that it worked by pinging some hostname in your home network, or checking the DNS resolution status\\nfor the home0 interface:\\n\\n$ resolvectl status home0\\n\\nLink 26 (home0)\\n\\nCurrent Scopes: DNS\\n\\nProtocols: -DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\\n\\nCurrent DNS Server: 10.10.10.1\\n\\nDNS Servers: 10.10.10.1\\n\\nDNS Domain: ~home\\n\\nIf you are using *systemctl* to control the WireGuard interface, this is the type of change (adding or changing PostUp )\\nwhere the reload action won’t be enough, and you actually need to issue a restart .\\n\\n**NOTE**\\n\\nThe wg-quick(8) manpage documents the DNS setting of the WireGuard interface which has the same',\n",
       " 'purpose, but only works if you have resolveconf installed. Ubuntu systems by default don’t, and rely on\\n\\nsystemd-resolved instead.\\n\\n201\\n\\n\\n-----\\n\\n## **Adding another peer**\\n\\nTo add another peer to an existing WireGuard setup, we have to:\\n\\n1. generate a new keypair for the new peer\\n2. create a new [Peer] section on the “other side” of the WireGuard setup\\n3. pick a new IP for the new peer\\n\\nLet’s call the new system ontheroad, and generate the keys for it:\\n\\n$ umask 077\\n\\n$ wg genkey > ontheroad-private.key\\n\\n$ wg pubkey < ontheroad-private.key > ontheroad-public.key\\n\\n$ ls -la ontheroad.*\\n\\n-rw------- 1 ubuntu ubuntu 45 Aug 22 20:12 ontheroad-private.key\\n\\n-rw------- 1 ubuntu ubuntu 45 Aug 22 20:13 ontheroad-public.key\\n\\nAs for its IP address, let’s pick 10.10.11.3/24 for it, which is the next one in sequence of one of the previous examples\\nin this guide:\\n\\n[Interface]\\n\\nPrivateKey = <contents-of-ontheroad-private.key>\\n\\nListenPort = 51000\\n\\nAddress = 10.10.11.3/24\\n\\n[Peer]\\n\\nPublicKey = <contents-of-router-public.key>\\n\\nEndpoint = <home-ppp0-IP-or-hostname>:51000\\n\\nAllowedIPs = 10.10.11.0/24,10.10.10.0/24\\n\\nThe only difference between this config and one for an existing system in this same WireGuard setup will be PrivateKey\\nand Address .\\n\\nOn the “other side”, we add the new [Peer] section to the existing config:\\n\\n[Interface]\\n\\nPrivateKey = <contents-of-router-private.key>\\n\\nListenPort = 51000\\n\\nAddress = 10.10.11.1/24\\n\\n[Peer]\\n\\n# laptop\\n\\nPublicKey = <contents-of-laptop-public.key>\\n\\nAllowedIPs = 10.10.11.2\\n\\n[Peer]\\n\\n# ontheroad\\n\\nPublicKey = <contents-of-ontheroad-public.key>\\n\\nAllowedIPs = 10.10.11.3',\n",
       " 'To update the interface with the new peer without disrupting existing connections, we use the reload action of the\\nsystemd unit:\\n\\n$ systemctl reload wg-quick@wg0\\n\\n**NOTE**\\nFor this case of a “server” or “vpn gateway”, where we are just adding another peer to an existing config,\\nthe systemctl reload action will work well enough to insert the new peer in the wireguard configuration.\\nBut it won’t create new routes, or do any of the other steps that wg-quick does. Depending on the setup,\\nyou might need a full restart so that wg-quick can fully do its job.\\n## **Adding a smartphone peer**\\n\\nWireGuard can be installed on many different platforms, and smartphones are included. The usptream installation\\n[page has links for Android and iOS apps.](https://www.wireguard.com/install/#android-play-store-f-droid)\\n\\nSuch a mobile client can be configured more easily with the use of QR codes.\\n\\nWe start by creating the new peer’s config normally, as if it were any other system (generate keys, pick an IP address,\\netc). Then, to convert that configuration file to a QR code, install the qrencode package:\\n\\n202\\n\\n\\n-----\\n\\n$ sudo apt install qrencode\\n\\nAnd run the following command (assuming the config was written to phone.conf ):\\n\\n$ cat phone.conf | qrencode -t ansiutf8\\n\\nThat will generate a QR code in the terminal, ready for scanning with the smartphone app. Note that there is no\\nneed for a graphical environment, and this command can be run remotely over SSH for example.\\n\\nNote that you need to put the private key contents directly into that configuration file, and not use PostUp to load it\\nfrom a separate file.\\n\\n**IMPORTANT**',\n",
       " 'Treat this QR code as a secret, as it contains the private key for the wireguard interface!\\n\\nHere are some security tips for your WireGuard deployment.\\n## **Traffic goes both ways**\\n\\nRemember that the VPN traffic goes both ways. Once you are connected to the remote network, it means any device\\non that network can connect back to you! That is, unless you create specific firewall rules for this VPN network.\\n\\nSince WireGuard is “just” an interface, you can create normal firewall rules for its traffic, and control the access to\\nthe network resources as usual. This is done more easily if you have a dedicated network for the VPN clients.\\n## **Using PreSharedKey**\\n\\nYou may add another layer of cryptographic protection to your VPN with the PreSharedKey option. Its usage is\\noptional, and adds a layer of symmetric-key cryptography to the traffic between specific peers.\\n\\nSuch a key can be generated with the genpsk command:\\n\\n$ wg genpsk\\n\\nvxlX6eMMin8uhxbKEhe/iOxi8ru+q1qWzCdjESXoFZY=\\n\\nAnd then used in a [Peer] section, like this:\\n\\n[Peer]\\n\\nPublicKey = ....\\n\\nEndpoint = ....\\n\\nAllowedIPs = ....\\n\\nPresharedKey = vxlX6eMMin8uhxbKEhe/iOxi8ru+q1qWzCdjESXoFZY=\\n\\n**Both sides need to have the same** PresharedKey **in their respective** [Peer] **sections.**\\n## **Preventing accidental leakage of the private keys**\\n\\nWhen troubleshooting WireGuard, it’s common to post the contents of the interface configuration file somewhere for\\nothers to help, like in a mailing list, or internet forum. Since the private key is listed in that file, one has to remember\\nto strip or obfuscate it before sharing, or else the secret is leaked.',\n",
       " 'To avoid such mistakes, we can remove the private key from the configuration and leave it in its own file. This can be\\ndone via a **PostUp** hook. For example, let’s update the home0.conf file to use such a hook:\\n\\n[Interface]\\n\\nListenPort = 51000\\n\\nAddress = 10.10.11.3/24\\n\\nPostUp = wg set %i private-key /etc/wireguard/%i.key\\n\\n[Peer]\\n\\nPublicKey = <contents-of-router-public.key>\\n\\nEndpoint = 10.48.132.39:51000\\n\\nAllowedIPs = 10.10.11.0/24,10.10.10.0/24\\n\\nThe %i macro is replaced by the WireGuard interface name ( home0 in this case). When the interface comes up, the\\n\\nPostUp shell commands will be executed with that substitution in place, and the private key for this interface will be\\nset with the contents of the /etc/wireguard/home0.key file.\\n\\nThere are some other advantages to this method, and perhaps one disadvantage.\\n\\nPros:\\n\\n203\\n\\n\\n-----\\n\\n  - The configuration file can now safely be stored in version control, like a git repository, without fear of leaking\\nthe private key (unless you also use the PreSharedKey option, which is also a secret).\\n\\n  - Since the key is now stored in a file, you can give that file a meaningful name, which helps to avoid mixups with\\nkeys and peers when setting up WireGuard.\\n\\nCons:\\n\\n  - You cannot directly use the qrcode tool to convert this image to a qrcode and use it to configure the mobile\\nversion of WireGuard, because that tool won’t go after the private key in that separate file.\\n\\nGeneral troubleshooting checklist:\\n\\n  - Verify public and private keys. When dealing with multiple peers, it’s easy to mix these up, specially because\\nthe contents of these keys is just random data. There is nothing identifying them, and public and private keys',\n",
       " 'are basically the same format-wise.\\n\\n  - Verify AllowedIPs list on all peers.\\n\\n - Check with ip route and ip addr show dev <wg-interface> if the routes and IPs are set as you expect.\\n\\n  - Double check that you have /proc/sys/net/ipv4/ip_forward set to 1 where needed.\\n\\n  - When injecting the VPN users into an existing network, without routing, make sure /proc/sys/net/ipv4/conf/all/proxy_arp\\n\\nis set to 1 .\\n\\n  - Make sure the above /proc entries are in /etc/sysctl.conf or a file in /etc/sysctl.d so that they persist reboots.\\n\\nIt can be helpful to leave a terminal open with the watch wg command. Here is a sample output showing a system\\nwith two peers configured, where only one has established the VPN so far:\\n\\nEvery 2.0s: wg j-wg: Fri Aug 26 17:44:37 2022\\n\\ninterface: wg0\\n\\npublic key: +T3T3HTMeyrEDvim8FBxbYjbz+/POeOtG3Rlvl9kJmM=\\n\\nprivate key: (hidden)\\n\\nlistening port: 51000\\n\\npeer: 2cJdFcNzXv4YUGyDTahtOfrbsrFsCByatPnNzKTs0Qo=\\n\\nendpoint: 10.172.196.106:51000\\n\\nallowed ips: 10.10.11.2/32\\n\\nlatest handshake: 3 hours, 27 minutes, 35 seconds ago\\n\\ntransfer: 3.06 KiB received, 2.80 KiB sent\\n\\npeer: ZliZ1hlarZqvfxPMyME2ECtXDk611NB7uzLAD4McpgI=\\n\\nallowed ips: 10.10.11.3/32\\n## **Kernel debug messages**\\n\\nWireGuard is also silent when it comes to logging. Being a kernel module essentially, we need to explicitly enable\\nverbose logging of its module. This is done with the following command:\\n\\n$ echo \"module wireguard +p\" | sudo tee /sys/kernel/debug/dynamic_debug/control\\n\\nThis will write WireGuard logging messages to the kernel log, which can be watched live with:\\n\\n$ sudo dmesg -wT\\n\\nTo disable logging, run this:\\n\\n$ echo \"module wireguard -p\" | sudo tee /sys/kernel/debug/dynamic_debug/control',\n",
       " '## **Destination address required**\\n\\nIf you ping an IP and get back an error like this:\\n\\n$ ping 10.10.11.2\\n\\nPING 10.10.11.2 (10.10.11.2) 56(84) bytes of data.\\n\\nFrom 10.10.11.1 icmp_seq=1 Destination Host Unreachable\\n\\nping: sendmsg: Destination address required\\n\\nThis is happening because the WireGuard interface selected for this destination doesn’t know the endpoint for it. In\\nother words, it doesn’t know where to send the encrypted traffic.\\n\\nOne common scenario for this is on a peer where there is no Endpoint configuration, which is perfectly valid, and the\\nhost is trying to send traffic to that peer. Let’s take the coffee shop scenario we described earlier as an example.\\n\\n204\\n\\n\\n-----\\n\\nThe laptop is connected to the VPN and exchanging traffic as usual. Then it stops for a bit (the person went to get\\none more cup). Traffic ceases (WireGuard is silent, remember). If the WireGuard on the home router is now restarted,\\nwhen it comes back up, it won’t know how to reach the laptop, because it was never contacted by it before. This\\nmeans that at this time, if the home router tries to send traffic to the laptop in the coffee shop, it will get the above\\n\\nerror.\\n\\nNow the laptop user comes back, and generates some traffic to the home network (remember: the laptop has the home\\nnetwork’s Endpoint value). The VPN “wakes up”, data is exchanged, handshakes completed, and now the home router\\nknows the Endpoint associated with the laptop, and can again initiate new traffic to it without issues.\\n\\nAnother possibility is that one of the peers is behind a NAT, and there wasn’t enough traffic for the stateful firewall\\nto consider the “connection” alive, and it dropped the NAT mapping it had.',\n",
       " 'In this case, the peer might benefit from\\nthe PersistentKeepalive configuration, which makes WireGuard send a *keepalive* probe every so many seconds.\\n## **Required key not available**\\n\\nThis error:\\n\\n$ ping 10.10.11.1\\n\\nPING 10.10.11.1 (10.10.11.1) 56(84) bytes of data.\\n\\nFrom 10.10.11.2 icmp_seq=1 Destination Host Unreachable\\n\\nping: sendmsg: Required key not available\\n\\nCan happen when you have a route directing traffic to the WireGuard interface, but that interface does not have the\\ntarget address listed in its AllowedIPs configuration.\\n\\nIf you have enabled kernel debugging for WireGuard, you will also see a message like this one in the dmesg output:\\n\\nwireguard: home0: No peer has allowed IPs matching 10.10.11.1\\n\\n[QEMU is a machine emulator that can run operating systems and programs for one machine on a different machine.](http://wiki.qemu.org/Main_Page)\\n[However, it is more often used as a virtualiser in collaboration with KVM kernel components. In that case it uses the](https://www.linux-kvm.org/page/Main_Page)\\nhardware virtualisation technology to virtualise guests.\\n\\n[Although QEMU has a command line interface and a monitor to interact with running guests, they are typically only](https://qemu-project.gitlab.io/qemu/system/invocation.html)\\nused for development purposes. libvirt provides an abstraction from specific versions and hypervisors and encapsulates\\nsome workarounds and best practices.\\n## **Running QEMU/KVM**\\n\\nWhile there *are* more user-friendly and comfortable ways, the quickest way to get started with QEMU is by directly\\nrunning it from the netboot ISO. You can achieve this by running the following command:\\n\\n**Warning** :',\n",
       " 'This example is just for illustration purposes - it is not generally recommended without verifying the\\nchecksums; Multipass and UVTool are much better ways to get actual guests easily.\\n\\nsudo qemu-system-x86_64 -enable-kvm -cdrom http://archive.ubuntu.com/ubuntu/dists/bionic-updates/main/installer\\namd64/current/images/netboot/mini.iso\\n\\nDownloading the ISO provides for faster access at runtime. We can now allocate the space for the VM:\\n\\nqemu-img create -f qcow2 disk.qcow 5G\\n\\nAnd then we can use the disk space we have just allocated for storage by adding the argument: -drive\\n\\nfile=disk.qcow,format=qcow2 .\\n\\n[These tools can do much more, as you’ll discover in their respective (long) manpages. They can also be made more](https://manpages.ubuntu.com/)\\n[consumable for specific use-cases and needs through a vast selection of auxiliary tools - for example virt-manager for](https://virt-manager.org/)\\n[UI-driven use through libvirt. But in general, it comes down to:](https://libvirt.org/)\\n\\nqemu-system-x86_64 options image[s]\\n\\n[So take a look at the QEMU manpage,](http://manpages.ubuntu.com/manpages/bionic/man1/qemu-system.1.html) [qemu-img](http://manpages.ubuntu.com/manpages/bionic/man1/qemu-img.1.html) [and the QEMU documentation and see which options best suit your](https://www.qemu.org/documentation/)\\nneeds.\\n## **Next steps**\\n\\nQEMU can be extended in many different ways. If you’d like to take QEMU further, you might want to check out\\nthis follow-up guide on virtualizing graphics using QEMU/KVM, or this guide on how you can use QEMU to create\\nMicroVMs.\\n\\n205\\n\\n\\n-----\\n\\n[Multipass is the recommended method for creating Ubuntu VMs on Ubuntu.',\n",
       " 'It’s designed for developers who want a](https://multipass.run)\\nfresh Ubuntu environment with a single command, and it works on Linux, Windows and macOS.\\n\\nOn Linux it’s available as a snap:\\n\\nsudo snap install multipass\\n\\nIf you’re running an older version of Ubuntu where snapd isn’t pre-installed, you will need to install it first:\\n\\nsudo apt update\\n\\nsudo apt install snapd\\n## **Find available images**\\n\\nTo find available images you can use the multipass find command, which will produce a list like this:\\n\\nImage Aliases Version Description\\n\\nsnapcraft:core18 18.04 20201111 Snapcraft builder for Core 18\\n\\nsnapcraft:core20 20.04 20210921 Snapcraft builder for Core 20\\n\\nsnapcraft:core22 22.04 20220426 Snapcraft builder for Core 22\\n\\nsnapcraft:devel 20221128 Snapcraft builder for the devel series\\n\\ncore core16 20200818 Ubuntu Core 16\\n\\ncore18 20211124 Ubuntu Core 18\\n\\n18.04 bionic 20221117 Ubuntu 18.04 LTS\\n\\n20.04 focal 20221115.1 Ubuntu 20.04 LTS\\n\\n22.04 jammy,lts 20221117 Ubuntu 22.04 LTS\\n\\n22.10 kinetic 20221101 Ubuntu 22.10\\n\\ndaily:23.04 devel,lunar 20221127 Ubuntu 23.04\\n\\nappliance:adguard-home 20200812 Ubuntu AdGuard Home Appliance\\n\\nappliance:mosquitto 20200812 Ubuntu Mosquitto Appliance\\n\\nappliance:nextcloud 20200812 Ubuntu Nextcloud Appliance\\n\\nappliance:openhab 20200812 Ubuntu openHAB Home Appliance\\n\\nappliance:plexmediaserver 20200812 Ubuntu Plex Media Server Appliance\\n\\nanbox-cloud-appliance latest Anbox Cloud Appliance\\n\\ncharm-dev latest A development and testing environment for charmers\\n\\ndocker latest A Docker environment with Portainer and related tools\\n\\njellyfin latest Jellyfin is a Free Software Media System that puts you in control of managi',\n",
       " 'minikube latest minikube is local Kubernetes\\n## **Launch a fresh instance of the Ubuntu Jammy (22.04) LTS**\\n\\nYou can launch a fresh instance by specifying either the image name from the list (in this example, 22.04) or using an\\nalias, if the image has one.\\n\\n$ multipass launch 22.04\\n\\nLaunched: cleansing-guanaco\\n\\nThis command is equivalent to: multipass launch jammy or multipass launch lts in the list above. It will launch an\\ninstance based on the specified image, and provide it with a random name – in this case, cleansing-guanaco .\\n## **Check out the running instances**\\n\\nYou can check out the currently running instance(s) by using the ”multipass list‘ command:\\n\\n$ multipass list\\n\\nName State IPv4 Image\\n\\ncleansing-guanaco Running 10.140.26.17 Ubuntu 22.04 LTS\\n## **Learn more about the VM instance you just launched**\\n\\nYou can use the multipass info command to find out more details about the VM instance parameters:\\n\\n$ multipass info cleansing-guanaco\\n\\nName: cleansing-guanaco\\n\\nState: Running\\n\\nIPv4: 10.140.26.17\\n\\nRelease: Ubuntu 22.04.1 LTS\\n\\nImage hash: dc5b5a43c267 (Ubuntu 22.04 LTS)\\n\\n206\\n\\n\\n-----\\n\\nLoad: 0.45 0.19 0.07\\n\\nDisk usage: 1.4G out of 4.7G\\n\\nMemory usage: 168.3M out of 969.5M\\n\\nMounts: -## **Connect to a running instance**\\n\\nTo enter the VM you created, use the shell command:\\n\\n$ multipass shell cleansing-guanaco\\n\\nWelcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.15.0-53-generic x86_64)\\n\\n(...)\\n\\nubuntu@cleansing-guanaco:~$\\n\\n**Disconnect from the instance**\\n\\nDon’t forget to log out (or Ctrl + D) when you are done, or you may find yourself heading all the way down the\\nInception levels…\\n## **Run commands inside an instance from outside**',\n",
       " '$ multipass exec cleansing-guanaco -- lsb_release -a\\n\\nNo LSB modules are available.\\n\\nDistributor ID: Ubuntu\\n\\nDescription: Ubuntu 22.04.1 LTS\\n\\nRelease: 22.04\\n\\nCodename: jammy\\n## **Stop or start an instance**\\n\\nYou can stop an instance to save resources using the stop command:\\n\\n$ multipass stop cleansing-guanaco\\n\\nYou can start it back up again using the start command:\\n\\n$ multipass start cleansing-guanaco\\n## **Delete the instance**\\n\\nOnce you are finished with the instance, you can delete it as follows:\\n\\n$ multipass delete cleansing-guanaco\\n\\nIt will now show up as deleted when you use the list command:\\n\\n$ multipass list\\n\\nName State IPv4 Image\\n\\ncleansing-guanaco Deleted -- Not Available\\n\\nAnd when you want to completely get rid of it (and any other deleted instances), you can use the purge command:\\n\\n$ multipass purge\\n\\nWhich we can check again using list :\\n\\n$ multipass list\\n\\nNo instances found.\\n## **Integrate with the rest of your virtualization**\\n\\nYou might have other virtualization already based on libvirt, either through using the similar older uvtool or through\\n[the more common virt-manager.](https://virt-manager.org/)\\n\\nYou might, for example, want those guests to be on the same bridge to communicate with each other, or if you need\\naccess to the graphical output for some reason.\\n\\nFortunately it is possible to integrate this by using the libvirt backend of Multipass:\\n\\n$ sudo multipass set local.driver=libvirt\\n\\n[Now when you start a guest you can also access it via tools like virt-manager or](https://virt-manager.org/) virsh :\\n\\n207\\n\\n\\n-----\\n\\n$ multipass launch lts\\n\\nLaunched: engaged-amberjack\\n\\n$ virsh list\\n\\nId Name State\\n\\n---------------------------------------------------',\n",
       " '15 engaged-amberjack running\\n\\nFor more detailed and comprehensive instructions on changing your drivers, refer to the Multipass drivers documen\\ntation.\\n## **Get help**\\n\\nYou can use the following commands on the CLI:\\n\\nmultipass help\\n\\nmultipass help <command>\\n\\nmultipass help --all\\n\\n[Or, check out the Multipass documentation for more details on how to use it.](https://discourse.ubuntu.com/t/-/8294)\\n\\nWith Ubuntu being one of the most popular operating systems on many cloud platforms, the availability of stable\\nand secure cloud images has become very important. Since Ubuntu 12.04, the use of cloud images outside of a cloud\\ninfrastructure has been improved so that it is now possible to use those images to create a virtual machine without\\nneeding a complete installation.\\n## Creating virtual machines using uvtool\\n\\nStarting with Ubuntu 14.04 LTS, a tool called uvtool has greatly facilitated the creation of virtual machines (VMs)\\nusing cloud images. uvtool provides a simple mechanism for synchronising cloud images locally and using them to\\ncreate new VMs in minutes.\\n## Install uvtool packages\\n\\nThe following packages and their dependencies are required in order to use uvtool :\\n\\n - uvtool\\n\\n - uvtool-libvirt\\n\\nTo install uvtool, run:\\n\\nsudo apt -y install uvtool\\n\\nThis will install uvtool ’s main commands, uvt-simplestreams-libvirt and uvt-kvm .\\n## Get the Ubuntu cloud image with uvt-simplestreams-libvirt\\n\\nThis is one of the major simplifications that uvtool provides. It knows where to find the cloud images so you only\\nneed one command to get a new cloud image. For instance, if you want to synchronise all cloud images for the amd64\\narchitecture, the uvtool command would be:',\n",
       " 'uvt-simplestreams-libvirt --verbose sync arch=amd64\\n\\nAfter all the images have been downloaded from the Internet, you will have a complete set of locally-stored cloud\\nimages. To see what has been downloaded, use the following command:\\n\\nuvt-simplestreams-libvirt query\\n\\nWhich will provide you with a list like this:\\n\\nrelease=bionic arch=amd64 label=daily (20191107)\\n\\nrelease=focal arch=amd64 label=daily (20191029)\\n\\n...\\n\\nIn the case where you want to synchronise only one specific cloud image, you need to use the release= and arch= filters\\nto identify which image needs to be synchronised.\\n\\nuvt-simplestreams-libvirt sync release=DISTRO-SHORT-CODENAME arch=amd64\\n\\n208\\n\\n\\n-----\\n\\nFurthermore, you can provide an alternative URL to fetch images from. A common case is the daily image, which\\nhelps you get the very latest images, or if you need access to the not-yet-released development release of Ubuntu. As\\nan example:\\n\\nuvt-simplestreams-libvirt sync --source http://cloud-images.ubuntu.com/daily [... further options]\\n## **Create a valid SSH key**\\n\\nTo connect to the virtual machine once it has been created, you must first have a valid SSH key available for the\\nUbuntu user. If your environment does not have an SSH key, you can create one using the ssh-keygen command,\\nwhich will produce similar output to this:\\n\\nGenerating public/private rsa key pair.\\n\\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\\n\\nEnter passphrase (empty for no passphrase):\\n\\nEnter same passphrase again:\\n\\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa.\\n\\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\\n\\nThe key fingerprint is:',\n",
       " '4d:ba:5d:57:c9:49:ef:b5:ab:71:14:56:6e:2b:ad:9b ubuntu@DISTRO-SHORT-CODENAMES\\n\\nThe key\\'s randomart image is:\\n\\n+--[ RSA 2048]----+\\n\\n| ..|\\n\\n| o.=|\\n\\n| . **|\\n\\n| + o+=|\\n\\n| S . ...=.|\\n\\n| o . .+ .|\\n\\n| . . o o |\\n\\n| - |\\n\\n| E |\\n\\n+-----------------+\\n## Create the VM using uvt-kvm\\n\\nTo create a new virtual machine using uvtool, run the following in a terminal:\\n\\nuvt-kvm create firsttest\\n\\nThis will create a VM named ‘firsttest’ using the current locally-available LTS cloud image. If you want to specify a\\nrelease to be used to create the VM, you need to use the release= filter, and the short codename of the release, e.g.\\n“jammy”:\\n\\nuvt-kvm create secondtest release=DISTRO-SHORT-CODENAME\\n\\nThe uvt-kvm wait command can be used to wait until the creation of the VM has completed:\\n\\nuvt-kvm wait secondttest\\n\\n**Connect to the running VM**\\n\\nOnce the virtual machine creation is completed, you can connect to it using SSH:\\n\\nuvt-kvm ssh secondtest\\n\\nYou can also connect to your VM using a regular SSH session using the IP address of the VM. The address can be\\nqueried using the following command:\\n\\n$ uvt-kvm ip secondtest\\n\\n192.168.122.199\\n\\n$ ssh -i ~/.ssh/id_rsa ubuntu@192.168.122.199\\n\\n[...]\\n\\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\\n\\nSee \"man sudo_root\" for details.\\n\\nubuntu@secondtest:~$\\n\\n209\\n\\n\\n-----\\n\\n## **Get the list of running VMs**\\n\\nYou can get the list of VMs running on your system with the uvt-kvm list command.\\n## **Destroy your VM**\\n\\nOnce you are finished with your VM, you can destroy it with:\\n\\nuvt-kvm destroy secondtest\\n\\n**Note** :\\nUnlike libvirt’s destroy or undefine actions, this will (by default) also remove the associated virtual storage\\nfiles.',\n",
       " '## More uvt-kvm options\\n\\nThe following options can be used to change some of the characteristics of the VM that you are creating:\\n\\n - --memory : Amount of RAM in megabytes. Default: 512.\\n\\n - --disk : Size of the OS disk in gigabytes. Default: 8.\\n\\n - --cpu : Number of CPU cores. Default: 1.\\n\\nSome other parameters will have an impact on the cloud-init configuration:\\n\\n - --password <password> : Allows logging into the VM using the Ubuntu account and this provided password.\\n\\n - --run-script-once <script_file> : Run script_file as root on the VM the first time it is booted, but never\\nagain.\\n\\n - --packages <package_list> : Install the comma-separated packages specified in package_list on first boot.\\n\\nA complete description of all available modifiers is available in the uvt-kvm [manpages.](https://manpages.ubuntu.com/manpages/lunar/en/man1/uvt-kvm.1.html)\\n## **Resources**\\n\\nIf you are interested in learning more, have questions or suggestions, please contact the Ubuntu Server Team at:\\n\\n - IRC: [#ubuntu-server](https://kiwiirc.com/nextclient/irc.libera.chat/ubuntu-server) on Libera\\n\\n[• Mailing list: ubuntu-server at lists.ubuntu.com](https://lists.ubuntu.com/mailman/listinfo/ubuntu-server)\\n\\n[The Virtual Machine Manager, through the](https://virt-manager.org/) virt-manager package, provides a graphical user interface (GUI) for\\nmanaging local and remote virtual machines. In addition to the virt-manager utility itself, the package also contains\\na collection of other helpful tools like virt-install, virt-clone and virt-viewer .\\n## **Install virt-manager**\\n\\nTo install virt-manager, enter:\\n\\nsudo apt install virt-manager',\n",
       " 'Since virt-manager requires a Graphical User Interface (GUI) environment we recommend installing it on a workstation\\nor test machine instead of a production server. To connect to the local libvirt service, enter:\\n\\nvirt-manager\\n\\nYou can connect to the libvirt service running on another host by entering the following in a terminal prompt:\\n\\nvirt-manager -c qemu+ssh://virtnode1.mydomain.com/system\\n\\n**Note** :\\n\\nThe above example assumes that SSH connectivity between the management system and the target system\\nhas already been configured, and uses **SSH keys** for authentication. SSH keys are needed because libvirt\\nsends the password prompt to another process. See our guide on OpenSSH for details on how to set up\\nSSH keys.\\n\\n210\\n\\n\\n-----\\n\\n## **Use virt-manager to manage guests**\\n\\n**Guest lifecycle**\\n\\nWhen using virt-manager it is always important to know the context you’re looking at. The main window initially\\nlists only the currently-defined guests. You’ll see their **name**, **state** (e.g., ‘Shutoff’ or ‘Running’) and a small chart\\nshowing the **CPU usage** .\\n\\nvirt-manager-gui-start\\n\\nIn this context, there isn’t much to do except start/stop a guest. However, by double-clicking on a guest or by clicking\\nthe **Open** button at the top of the window, you can see the guest itself. For a running guest that includes the guest’s\\nmain-console/virtual-screen output.\\n\\n[virt-manager-gui-showoutput](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/7/7859f9fc0c79ef6866fc436bb775a28f1b342cde.png)\\n\\nvirt-manager-gui-showoutput1594×894 364 KB\\n\\nIf you are deeper in the guest configuration, clicking on “Show the graphical console” in the top left of the guest’s',\n",
       " 'window will get you back to this output.\\n\\n**Guest modification**\\n\\nvirt-manager provides a handy, GUI-assisted way to edit guest definitions. To do so, the per-guest context view will\\nhave “Show virtual hardware details” at the top of the guest window. Here, you can edit the virtual hardware of the\\n[guest, which will alter the guest representation behind the scenes.](https://libvirt.org/formatdomain.html)\\n\\n[virt-manager-gui-edit](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/f/f5d42f8bdcb87b3818b5b6c04f5ccec5a15fef3d.png)\\n\\nvirt-manager-gui-edit1406×706 282 KB\\n\\nThe UI edit ability is limited to the features known to (and supported by) that GUI feature. Not only does libvirt\\ngrow features faster than virt-manager can keep up – adding every feature would also overload the UI and render it\\nunusable.\\n\\nTo strike a balance between the two, there also is the XML view which can be reached via the “Edit libvirt XML”\\nbutton.\\n\\n[virt-manager-gui-XML](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/1/102df4e3030498b1768cc48685cb6f922cec0244.png)\\n\\nvirt-manager-gui-XML1406×706 340 KB\\n\\nBy default, this will be read-only and you can see what the UI-driven actions have changed. You can allow read-write\\naccess in this view via the “Preferences”. This is the same content that the virsh edit of the libvirt-client exposes.\\n## **Virtual Machine Viewer (virt-viewer)**\\n\\nThe Virtual Machine Viewer application, through virt-viewer, allows you to connect to a virtual machine’s console\\nlike virt-manager does, but reduced to the GUI functionality. virt-viewer requires a GUI to interface with the virtual\\nmachine.\\n\\n**Install virt-viewer**',\n",
       " 'If virt-viewer is not already installed, you can install it from the terminal with the following command:\\n\\nsudo apt install virt-viewer\\n\\nOnce a virtual machine is installed and running you can connect to the virtual machine’s console by using:\\n\\nvirt-viewer <guestname>\\n\\nThe UI will show a window representing the virtual screen of the guest, just like with virt-manager above, but without\\nthe extra buttons and features around it.\\n\\n[virt-viewer-gui-showoutput](https://ubuntucommunity.s3.us-east-2.amazonaws.com/original/2X/4/43f32c1efdd95a44b512cb6461bf360e64f60277.png)\\n\\nvirt-viewer-gui-showoutput1105×958 164 KB\\n\\nSimilarly to virt-manager, virt-viewer can also connect to a remote host using SSH with key authentication:\\n\\nvirt-viewer -c qemu+ssh://virtnode1.mydomain.com/system <guestname>\\n\\nBe sure to replace web_devel with the appropriate virtual machine name.\\n\\nIf configured to use a **bridged** network interface you can also set up SSH access to the virtual machine.\\n\\n211\\n\\n\\n-----\\n\\n## **virt-install**\\n\\nvirt-install is part of the virtinst package. It can help with installing classic ISO-based systems and provides a\\nCLI for the most common options needed to do so.\\n\\n**Install virt-install**\\n\\nTo install virt-install, if it is not installed already, run the following from a terminal prompt:\\n\\nsudo apt install virtinst\\n\\nThere are several options available when using virt-install . For example:\\n\\nvirt-install \\\\\\n\\n--name web_devel \\\\\\n\\n--ram 8192 \\\\\\n\\n--disk path=/home/doug/vm/web_devel.img,bus=virtio,size=50 \\\\\\n\\n--cdrom focal-desktop-amd64.iso \\\\\\n\\n--network network=default,model=virtio \\\\\\n\\n--graphics vnc,listen=0.0.0.0 --noautoconsole --hvm --vcpus=4',\n",
       " 'There are many more arguments that can be found in the [virt-install](https://manpages.ubuntu.com/manpages/jammy/man1/virt-install.1.html) manpage. However, explaining those of the\\nexample above one by one:\\n\\n - --name web_devel\\nThe name of the new virtual machine will be web_devel .\\n\\n - --ram 8192\\nSpecifies the amount of memory the virtual machine will use (in megabytes).\\n\\n - --disk path=/home/doug/vm/web_devel.img,bus=virtio,size=50\\nIndicates the path to the virtual disk which can be a file, partition, or logical volume. In this example a file\\nnamed web_devel.img in the current user’s directory, with a size of 50 gigabytes, and using virtio for the disk\\nbus. Depending on the disk path, virt-install may need to run with elevated privileges.\\n\\n - --cdrom focal-desktop-amd64.iso\\nFile to be used as a virtual CD-ROM. The file can be either an ISO file or the path to the host’s CD-ROM\\ndevice.\\n\\n - --network\\n\\nProvides details related to the VM’s network interface. Here the default network is used, and the interface model\\nis configured for virtio .\\n\\n - --graphics vnc,listen=0.0.0.0\\nExports the guest’s virtual console using VNC and on all host interfaces. Typically servers have no GUI,\\nso another GUI-based computer on the Local Area Network (LAN) can connect via VNC to complete the\\ninstallation.\\n\\n - --noautoconsole\\n\\nWill not automatically connect to the virtual machine’s console.\\n\\n - --hvm : creates a fully virtualised guest.\\n\\n - --vcpus=4 : allocate 4 virtual CPUs.\\n\\nAfter launching virt-install you can connect to the virtual machine’s console either locally using a GUI (if your\\nserver has a GUI), or via a remote VNC client from a GUI-based computer.\\n\\nvirt-clone',\n",
       " 'The virt-clone application can be used to copy one virtual machine to another. For example:\\n\\nvirt-clone --auto-clone --original focal\\n\\nOptions used:\\n\\n - --auto-clone\\n\\nTo have virt-clone create guest names and disk paths on its own.\\n\\n - --original\\nName of the virtual machine to copy.\\n\\nYou can also use the -d or --debug option to help troubleshoot problems with virt-clone .\\n\\nReplace focal with the appropriate virtual machine names for your case.\\n\\n**Warning** :\\nPlease be aware that this is a full clone, therefore any sorts of secrets, keys and for example /etc/machine-id\\n\\n212\\n\\n\\n-----\\n\\nwill be shared. This will cause issues with security and anything that needs to identify the machine like\\nDHCP. You most likely want to edit those afterwards and de-duplicate them as needed.\\n## **Resources**\\n\\n[• See the KVM home page for more details.](http://www.linux-kvm.org/)\\n\\n[• For more information on libvirt see the libvirt home page](http://libvirt.org/)\\n\\n[• The Virtual Machine Manager site has more information on](http://virt-manager.org/) virt-manager development.\\n\\n[The libvirt library is used to interface with many different virtualisation technologies. Before getting started with](https://libvirt.org/)\\n[libvirt it is best to make sure your hardware supports the necessary virtualisation extensions for Kernel-based Virtual](https://www.linux-kvm.org/page/Main_Page)\\n[Machine (KVM). To check this, enter the following from a terminal prompt:](https://www.linux-kvm.org/page/Main_Page)\\n\\nkvm-ok\\n\\nA message will be printed informing you if your CPU *does* or *does not* support hardware virtualisation.\\n\\n**Note** :',\n",
       " 'On many computers with processors supporting hardware-assisted virtualisation, it is necessary to first\\nactivate an option in the BIOS to enable it.\\n## **Virtual networking**\\n\\nThere are a few different ways to allow a virtual machine access to the external network. The default virtual network\\nconfiguration includes **bridging** and **iptables** rules implementing **usermode** [networking, which uses the SLiRP](https://en.wikipedia.org/wiki/Slirp)\\nprotocol. Traffic is NATed through the host interface to the outside network.\\n\\nTo enable external hosts to directly access services on virtual machines a different type of *bridge* than the default needs\\nto be configured. This allows the virtual interfaces to connect to the outside network through the physical interface,\\nmaking them appear as normal hosts to the rest of the network.\\n\\n[There is a great example of how to configure a bridge and combine it with libvirt so that guests will use it at the](https://netplan.readthedocs.io/en/latest/netplan-yaml/#properties-for-device-type-bridges)\\n[netplan.io documentation.](https://netplan.readthedocs.io/en/latest/)\\n## **Install libvirt**\\n\\nTo install the necessary packages, from a terminal prompt enter:\\n\\nsudo apt update\\n\\nsudo apt install qemu-kvm libvirt-daemon-system\\n\\nAfter installing libvirt-daemon-system, the user that will be used to manage virtual machines needs to be added to the\\n*libvirt* group. This is done automatically for members of the *sudo* group, but needs to be done in addition for anyone\\nelse that should access system-wide libvirt resources. Doing so will grant the user access to the advanced networking\\noptions.\\n\\nIn a terminal enter:\\n\\nsudo adduser $USER libvirt',\n",
       " '**Note** :\\n\\nIf the chosen user is the current user, you will need to log out and back in for the new group membership\\nto take effect.\\n\\nYou are now ready to install a *Guest* operating system. Installing a virtual machine follows the same process as\\ninstalling the operating system directly on the hardware.\\n\\nYou will need **one** of the following:\\n\\n  - A way to automate the installation.\\n\\n  - A keyboard and monitor attached to the physical machine.\\n\\n  - To use cloud images which are meant to self-initialise (see Multipass and UVTool).\\n\\nIn the case of virtual machines, a Graphical User Interface (GUI) is analogous to using a physical keyboard and mouse\\non a real computer. Instead of installing a GUI the virt-viewer or virt-manager application can be used to connect\\nto a virtual machine’s console using VNC. See Virtual Machine Manager / Viewer for more information.\\n\\n213\\n\\n\\n-----\\n\\n## **Virtual machine management**\\n\\nThe following section covers the command-line tools around virsh that are part of libvirt itself. But there are various\\noptions at different levels of complexities and feature-sets, like:\\n\\n  - Multipass\\n\\n - UVTool\\n\\n  - virt-* tools\\n\\n[• OpenStack](https://ubuntu.com/openstack)\\n## Manage VMs with virsh\\n\\nThere are several utilities available to manage virtual machines and libvirt. The virsh utility can be used from the\\ncommand line. Some examples:\\n\\n  - To list running virtual machines:\\n\\nvirsh list\\n\\n  - To start a virtual machine:\\n\\nvirsh start <guestname>\\n\\n  - Similarly, to start a virtual machine at boot:\\n\\nvirsh autostart <guestname>\\n\\n  - Reboot a virtual machine with:\\n\\nvirsh reboot <guestname>',\n",
       " ' - The **state** of virtual machines can be saved to a file in order to be restored later. The following will save the\\nvirtual machine state into a file named according to the date:\\n\\nvirsh save <guestname> save-my.state\\n\\nOnce saved, the virtual machine will no longer be running.\\n\\n  - A saved virtual machine can be restored using:\\n\\nvirsh restore save-my.state\\n\\n  - To shut down a virtual machine you can do:\\n\\nvirsh shutdown <guestname>\\n\\n  - A CD-ROM device can be mounted in a virtual machine by entering:\\n\\nvirsh attach-disk <guestname> /dev/cdrom /media/cdrom\\n\\n  - To change the definition of a guest, virsh exposes the domain via:\\n\\nvirsh edit <guestname>\\n\\n[This will allow you to edit the XML representation that defines the guest. When saving, it will apply format and](https://libvirt.org/formatdomain.html)\\nintegrity checks on these definitions.\\n\\nEditing the XML directly certainly is the most powerful way, but also the most complex one. Tools like Virtual\\nMachine Manager / Viewer can help inexperienced users to do most of the common tasks.\\n\\n**Note** :\\nIf virsh (or other vir* tools) connect to something other than the default qemu-kvm /system hypervisor, one\\ncan find alternatives for the --connect option using man virsh [or the libvirt docs.](http://libvirt.org/uri.html)\\n\\nsystem **and** session **scope**\\n\\nYou can pass connection strings to virsh - as well as to most other tools for managing virtualisation.\\n\\nvirsh --connect qemu:///system\\n\\nThere are two options for the connection.\\n\\n - qemu:///system  - connect locally as **root** to the daemon supervising QEMU and KVM domains\\n\\n - qemu:///session  - connect locally as a **normal user** to their own set of QEMU and KVM domains\\n\\n214',\n",
       " '-----\\n\\nThe *default* was always (and still is) qemu:///system as that is the behavior most users are accustomed to. But there\\nare a few benefits (and drawbacks) to qemu:///session to consider.\\n\\nqemu:///session is per user and can – on a multi-user system – be used to separate the people.\\nMost importantly, processes run under the permissions of the user, which means no permission struggle on the justdownloaded image in your $HOME or the just-attached USB-stick.\\n\\nOn the other hand it can’t access system resources very well, which includes network setup that is known to be hard\\nwith qemu:///session [. It falls back to SLiRP networking which is functional but slow, and makes it impossible to be](https://en.wikipedia.org/wiki/Slirp)\\nreached from other systems.\\n\\nqemu:///system is different in that it is run by the global system-wide libvirt that can arbitrate resources as needed.\\nBut you might need to mv and/or chown files to the right places and change permissions to make them usable.\\n\\nApplications will usually decide on their primary use-case. Desktop-centric applications often choose qemu:///session\\nwhile most solutions that involve an administrator anyway continue to default to qemu:///system .\\n\\n**Further reading** :\\n[There is more information about this topic in the libvirt FAQ and this blog post about the topic.](https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F)\\n## **Migration**\\n\\nThere are different types of migration available depending on the versions of libvirt and the hypervisor being used. In\\ngeneral those types are:',\n",
       " \"[• Offline migration](https://libvirt.org/migration.html#offline)\\n\\n[• Live migration](https://libvirt.org/migration.html)\\n\\n[• Postcopy migration](http://wiki.qemu.org/Features/PostCopyLiveMigration)\\n\\nThere are various options to those methods, but the entry point for all of them is virsh migrate . Read the integrated\\nhelp for more detail.\\n\\nvirsh migrate --help\\n\\n[Some useful documentation on the constraints and considerations of live migration can be found at the Ubuntu Wiki.](https://wiki.ubuntu.com/QemuKVMMigration)\\n## **Device passthrough/hotplug**\\n\\nIf, rather than the hotplugging described here, you want to always pass through a device then add the XML content\\nof the device to your static guest XML representation via virsh edit <guestname> . In that case you won’t need to\\nuse *attach/detach* . There are different kinds of passthrough. Types available to you depend on your hardware and\\nsoftware setup.\\n\\n - USB hotplug/passthrough\\n\\n - VF hotplug/Passthrough\\n\\nBoth kinds are handled in a very similar way and while there are various way to do it (e.g. also via QEMU monitor)\\ndriving such a change via libvirt is recommended. That way, libvirt can try to manage all sorts of special cases for\\nyou and also somewhat masks version differences.\\n\\nIn general when driving hotplug via libvirt you create an XML snippet that describes the device just as you would do\\n[in a static guest description. A USB device is usually identified by vendor/product ID:](https://libvirt.org/formatdomain.html)\\n\\n<hostdev mode='subsystem' type='usb' managed='yes'>\\n\\n<source>\\n\\n<vendor id='0x0b6d'/>\\n\\n<product id='0x3880'/>\\n\\n</source>\\n\\n</hostdev>\",\n",
       " \"Virtual functions are usually assigned via their PCI ID (domain, bus, slot, function).\\n\\n<hostdev mode='subsystem' type='pci' managed='yes'>\\n\\n<source>\\n\\n<address domain='0x0000' bus='0x04' slot='0x10' function='0x0'/>\\n\\n</source>\\n\\n</hostdev>\\n\\n215\\n\\n\\n-----\\n\\n**Note** :\\nTo get the virtual function in the first place is very device dependent and can therefore not be fully covered\\n[here. But in general it involves setting up an IOMMU, registering via VFIO and sometimes requesting a](https://www.kernel.org/doc/Documentation/vfio.txt)\\nnumber of VFs. Here an example on ppc64el to get 4 VFs on a device:\\n\\n$ sudo modprobe vfio-pci\\n\\n# identify device\\n\\n$ lspci -n -s 0005:01:01.3\\n\\n0005:01:01.3 0200: 10df:e228 (rev 10)\\n\\n# register and request VFs\\n\\n$ echo 10df e228 | sudo tee /sys/bus/pci/drivers/vfio-pci/new_id\\n\\n$ echo 4 | sudo tee /sys/bus/pci/devices/0005\\\\:01\\\\:00.0/sriov_numvfs\\n\\nYou then attach or detach the device via libvirt by relating the guest with the XML snippet.\\n\\nvirsh attach-device <guestname> <device-xml>\\n\\n# Use the Device in the Guest\\n\\nvirsh detach-device <guestname> <device-xml>\\n## **Access QEMU Monitor via libvirt**\\n\\n[The QEMU Monitor is the way to interact with QEMU/KVM while a guest is running. This interface has many](https://en.wikibooks.org/wiki/QEMU/Monitor)\\npowerful features for experienced users. When running under libvirt, the monitor interface is bound by libvirt itself\\nfor management purposes, but a user can still run QEMU monitor commands via libvirt. The general syntax is virsh\\n\\nqemu-monitor-command [options] [guest] 'command' .\\n\\nLibvirt covers most use cases needed, but if you ever want/need to work around libvirt or want to tweak very special\",\n",
       " \"options you can e.g. add a device as follows:\\n\\nvirsh qemu-monitor-command --hmp focal-test-log 'drive_add 0 if=none,file=/var/lib/libvirt/images/test.img,format=raw,i\\n\\nBut since the monitor is so powerful, you can do a lot – especially for debugging purposes like showing the guest\\nregisters:\\n\\nvirsh qemu-monitor-command --hmp y-ipns 'info registers'\\n\\nRAX=00ffffc000000000 RBX=ffff8f0f5d5c7e48 RCX=0000000000000000 RDX=ffffea00007571c0\\n\\nRSI=0000000000000000 RDI=ffff8f0fdd5c7e48 RBP=ffff8f0f5d5c7e18 RSP=ffff8f0f5d5c7df8\\n\\n[...]\\n## **Huge pages**\\n\\nUsing huge pages can help to reduce TLB pressure, page table overhead and speed up some further memory relate\\n[actions. Furthermore by default transparent huge pages are useful, but can be quite some overhead - so if it is clear](https://www.kernel.org/doc/Documentation/vm/transhuge.txt)\\nthat using huge pages is preferred then making them explicit usually has some gains.\\n\\nWhile huge page are admittedly harder to manage (especially later in the system’s lifetime if memory is fragmented)\\nthey provide a useful boost especially for rather large guests.\\n\\n**Bonus** :\\nWhen using device passthrough on very large guests there is an extra benefit of using huge pages as it is\\nfaster to do the initial memory clear on VFIO DMA pin.\\n\\n**Huge page allocation**\\n\\nHuge pages come in different sizes. A *normal* page is usually 4k and huge pages are either 2M or 1G, but depending\\non the architecture other options are possible.\\n\\nThe simplest yet least reliable way to allocate some huge pages is to just echo a value to sysfs :\\n\\necho 256 | sudo tee /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages\\n\\nBe sure to re-check if it worked:\",\n",
       " 'cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages\\n\\n256\\n\\nThere one of these sizes is “default huge page size” which will be used in the auto-mounted /dev/hugepages . Changing\\n[the default size requires a reboot and is set via default_hugepagesz.](https://www.kernel.org/doc/html/v5.4/admin-guide/kernel-parameters.html)\\n\\nYou can check the current default size:\\n\\n216\\n\\n\\n-----\\n\\ngrep Hugepagesize /proc/meminfo\\n\\nHugepagesize: 2048 kB\\n\\nBut there can be more than one at the same time – so it’s a good idea to check:\\n\\n$ tail /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages`\\n\\n==> /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages <==\\n\\n0\\n\\n==> /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages <==\\n\\n2\\n\\n[And even that could – on bigger systems – be further split per Numa node.](https://www.kernel.org/doc/html/v5.4/vm/numa.html)\\n\\n[One can allocate huge pages at boot or runtime, but due to fragmentation there are no guarantees it works later. The](https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt)\\n[kernel documentation lists details on both ways.](https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt)\\n\\nHuge pages need to be allocated by the kernel as mentioned above but to be consumable they also have to be mounted.\\nBy default, systemd will make /dev/hugepages available for the default huge page size.\\n\\nFeel free to add more mount points if you need different sized ones. An overview can be queried with:\\n\\nhugeadm --list-all-mounts\\n\\nMount Point Options\\n\\n/dev/hugepages rw,relatime,pagesize=2M\\n\\nA one-stop info for the overall huge page status of the system can be reported with:\\n\\nhugeadm --explain\\n\\n**Huge page usage in libvirt**',\n",
       " 'With the above in place, libvirt can map guest memory to huge pages. In a guest definition add the most simple form\\nof:\\n\\n<memoryBacking>\\n\\n<hugepages/>\\n\\n</memoryBacking>\\n\\nThat will allocate the huge pages using the default huge page size from an autodetected mount point.\\n[For more control, e.g. how memory is spread over Numa nodes or which page size to use, check out the details at the](https://www.kernel.org/doc/html/v5.4/vm/numa.html)\\n[libvirt docs.](https://libvirt.org/formatdomain.html#elementsMemoryBacking)\\n## **Controlling addressing bits**\\n\\nThis is a topic that rarely matters on a single computer with virtual machines for generic use; libvirt will automatically\\nuse the hypervisor default, which in the case of QEMU is 40 bits. This default aims for compatibility since it will be\\nthe same on all systems, which simplifies migration between them and usually is compatible even with older hardware.\\n\\nHowever, it can be very important when driving more advanced use cases. If one needs bigger guest sizes with more\\nthan a terabyte of memory then controlling the addressing bits is crucial.\\n\\n**-hpb machine types**\\n\\n[Since Ubuntu 18.04 the QEMU in Ubuntu has provided special machine-types. These have been the Ubuntu machine](https://bugs.launchpad.net/ubuntu/+source/qemu/+bug/1776189)\\ntype like pc-q35-jammy or pc-i440fx-jammy but with a -hpb suffix. The “hpb” abbreviation stands for “host-physicalbits”, which is the QEMU option that this represents.\\n\\nFor example, by using pc-q35-jammy-hpb the guest would use the number of physical bits that the Host CPU has\\navailable.',\n",
       " \"Providing the configuration that a guest should use more address bits as a machine type has the benefit that many\\nhigher level management stacks like for example openstack, are already able to control it through libvirt.\\n\\nOne can check the bits available to a given CPU via the procfs:\\n\\n$ cat /proc/cpuinfo | grep '^address sizes'\\n\\n...\\n\\n# an older server with a E5-2620\\n\\naddress sizes : 46 bits physical, 48 bits virtual\\n\\n# a laptop with an i7-8550U\\n\\n217\\n\\n\\n-----\\n\\naddress sizes : 39 bits physical, 48 bits virtual\\n\\n**maxphysaddr guest configuration**\\n\\nSince libvirt version 8.7.0 (>= Ubuntu 22.10 Lunar), maxphysaddr [can be controlled via the CPU model and topology](https://libvirt.org/formatdomain.html#cpu-model-and-topology)\\n[section of the guest configuration.](https://libvirt.org/formatdomain.html#cpu-model-and-topology)\\nIf one needs just a large guest, like before when using the -hpb types, all that is needed is the following libvirt guest\\nxml configuration:\\n\\n<maxphysaddr mode='passthrough' />\\n\\nSince libvirt 9.2.0 and 9.3.0 (>= Ubuntu 23.10 Mantic), an explicit number of emulated bits or a limit to the\\npassthrough can be specified. Combined, this pairing can be very useful for computing clusters where the CPUs have\\ndifferent hardware physical addressing bits. Without these features guests could be large, but potentially unable to\\nmigrate freely between all nodes since not all systems would support the same amount of addressing bits.\\n\\nBut now, one can either set a fix value of addressing bits:\\n\\n<maxphysaddr mode='emulate' bits='42'/>\\n\\nOr use the best available by a given hardware, without going over a certain limit to retain some compute node\\ncompatibility.\",\n",
       " \"<maxphysaddr mode='passthrough' limit='41/>\\n## **AppArmor isolation**\\n\\n[By default libvirt will spawn QEMU guests using AppArmor isolation for enhanced security. The AppArmor rules for](https://gitlab.com/apparmor/apparmor/-/wikis/Libvirt#implementation-overview)\\n[a guest will consist of multiple elements:](https://gitlab.com/apparmor/apparmor/-/wikis/Libvirt#implementation-overview)\\n\\n  - A static part that all guests share => /etc/apparmor.d/abstractions/libvirt-qemu\\n\\n  - A dynamic part created at guest start time and modified on hotplug/unplug => /etc/apparmor.d/libvirt/libvirt\\nf9533e35-6b63-45f5-96be-7cccc9696d5e.files\\n\\nOf the above, the former is provided and updated by the libvirt-daemon package and the latter is generated on guest\\nstart. Neither of the two should be manually edited. They will, by default, cover the vast majority of use cases and\\nwork fine. But there are certain cases where users either want to:\\n\\n  - Further lock down the guest, e.g. by explicitly denying access that usually would be allowed.\\n\\n  - Open up the guest isolation. Most of the time this is needed if the setup on the local machine does not follow\\nthe commonly used paths.\\n\\nTo do so there are two files. Both are local overrides which allow you to modify them without getting them clobbered\\nor command file prompts on package upgrades.\\n\\n - /etc/apparmor.d/local/abstractions/libvirt-qemu\\nThis will be applied to every guest. Therefore it is a rather powerful (if blunt) tool. It is a quite useful place to\\n[add additional deny rules.](https://gitlab.com/apparmor/apparmor/-/wikis/FAQ#what-is-default-deny-white-listing)\\n\\n - /etc/apparmor.d/local/usr.lib.libvirt.virt-aa-helper\",\n",
       " \"The above-mentioned *dynamic part* that is individual per guest is generated by a tool called libvirt.virt-aa\\nhelper . That is under AppArmor isolation as well. This is most commonly used if you want to use uncommon\\n[paths as it allows one to have those uncommon paths in the guest XML (see](https://libvirt.org/formatdomain.html) virsh edit ) and have those paths\\nrendered to the per-guest dynamic rules.\\n## **Sharing files between Host<->Guest**\\n\\nTo be able to exchange data, the memory of the guest has to be allocated as “shared”. To do so you need to add the\\nfollowing to the guest config:\\n\\n<memoryBacking>\\n\\n<access mode='shared'/>\\n\\n</memoryBacking>\\n\\nFor performance reasons (it helps virtiofs, but also is generally wise to consider) it\\nis recommended to use huge pages which then would look like:\\n\\n<memoryBacking>\\n\\n<hugepages>\\n\\n<page size='2048' unit='KiB'/>\\n\\n</hugepages>\\n\\n<access mode='shared'/>\\n\\n218\\n\\n\\n-----\\n\\n</memoryBacking>\\n\\nIn the guest definition one then can add filesytem sections to specify host paths to share with the guest. The *target*\\n*dir* is a bit special as it isn’t really a directory – instead it is a *tag* that in the guest can be used to access this particular\\n\\nvirtiofs instance.\\n\\n<filesystem type='mount' accessmode='passthrough'>\\n\\n<driver type='virtiofs'/>\\n\\n<source dir='/var/guests/h-virtiofs'/>\\n\\n<target dir='myfs'/>\\n\\n</filesystem>\\n\\nAnd in the guest this can now be used based on the tag myfs like:\\n\\nsudo mount -t virtiofs myfs /mnt/\\n\\nCompared to other Host/Guest file sharing options – commonly Samba, NFS or 9P – virtiofs is usually much faster\\nand also more compatible with usual file system semantics. For some extra compatibility in regard to filesystem\",\n",
       " \"semantics one can add:\\n\\n<binary xattr='on'>\\n\\n<lock posix='on' flock='on'/>\\n\\n</binary>\\n\\n[See the libvirt domain/filesytem documentation for further details on these.](https://libvirt.org/formatdomain.html#filesystems)\\n\\n**Note** :\\nWhile virtiofs works with >=20.10 (Groovy), with >=21.04 (Hirsute) it became more comfortable, especially in small environments (no hard requirement to specify guest Numa topology, no hard requirement to\\n[use huge pages). If needed to set up on 20.10 or just interested in those details - the libvirt knowledge-base](https://libvirt.org/kbase/virtiofs.html)\\n[about virtiofs holds more details about these.](https://libvirt.org/kbase/virtiofs.html)\\n## **Resources**\\n\\n[• See the KVM home page for more details.](http://www.linux-kvm.org/)\\n\\n[• For more information on libvirt see the libvirt home page.](http://libvirt.org/)\\n\\n**–** [XML configuration of domains and storage are the most often used libvirt reference.](https://libvirt.org/formatdomain.html)\\n\\n[• Another good resource is the Ubuntu Wiki KVM page.](https://help.ubuntu.com/community/KVM)\\n\\n[• For basics on how to assign VT-d devices to QEMU/KVM, please see the linux-kvm page.](http://www.linux-kvm.org/page/How_to_assign_devices_with_VT-d_in_KVM#Assigning_the_device)\\n\\nContainers are a lightweight virtualization technology. They are more akin to an enhanced chroot than to full virtualization like Qemu or VMware, both because they do not emulate hardware and because containers share the same\\noperating system as the host. Containers are similar to Solaris zones or BSD jails. Linux-vserver and OpenVZ are two\\npre-existing, independently developed implementations of containers-like functionality for Linux.\",\n",
       " 'In fact, containers\\ncame about as a result of the work to upstream the vserver and OpenVZ functionality.\\n\\nThere are two user-space implementations of containers, each exploiting the same kernel features. Libvirt allows the\\nuse of containers through the LXC driver by connecting to lxc:/// . This can be very convenient as it supports the\\nsame usage as its other drivers. The other implementation, called simply ‘LXC’, is not compatible with libvirt, but is\\nmore flexible with more userspace tools. It is possible to switch between the two, though there are peculiarities which\\ncan cause confusion.\\n\\nIn this document we will mainly describe the lxc package. Use of libvirt-lxc is not generally recommended due to a\\nlack of Apparmor protection for libvirt-lxc containers.\\n\\nIn this document, a container name will be shown as CN, C1, or C2.\\n## **Installation**\\n\\nThe lxc package can be installed using\\n\\nsudo apt install lxc\\n\\nThis will pull in the required and recommended dependencies, as well as set up a network bridge for containers to use.\\nIf you wish to use unprivileged containers, you will need to ensure that users have sufficient allocated subuids and\\nsubgids, and will likely want to allow users to connect containers to a bridge (see *Basic unprivileged usage* below).\\n\\n219\\n\\n\\n-----\\n\\n## **Basic usage**\\n\\nLXC can be used in two distinct ways - privileged, by running the lxc commands as the root user; or unprivileged, by\\nrunning the lxc commands as a non-root user. (The starting of unprivileged containers by the root user is possible,\\nbut not described here.) Unprivileged containers are more limited, for instance being unable to create device nodes\\nor mount block-backed filesystems.',\n",
       " 'However they are less dangerous to the host, as the root UID in the container is\\nmapped to a non-root UID on the host.\\n\\n**Basic privileged usage**\\n\\nTo create a privileged container, you can simply do:\\n\\nsudo lxc-create --template download --name u1\\n\\nor, abbreviated\\n\\nsudo lxc-create -t download -n u1\\n\\nThis will interactively ask for a container root filesystem type to download – in particular the distribution, release,\\nand architecture. To create the container non-interactively, you can specify these values on the command line:\\n\\nsudo lxc-create -t download -n u1 -- --dist ubuntu --release DISTRO-SHORT-CODENAME --arch amd64\\n\\nor\\n\\nsudo lxc-create -t download -n u1 -- -d ubuntu -r DISTRO-SHORT-CODENAME -a amd64\\n\\nYou can now use lxc-ls to list containers, lxc-info to obtain detailed container information, lxc-start to start and\\n\\nlxc-stop to stop the container. lxc-attach and lxc-console allow you to enter a container, if ssh is not an option.\\n\\nlxc-destroy removes the container, including its rootfs. See the manual pages for more information on each command.\\nAn example session might look like:\\n\\nsudo lxc-ls --fancy\\n\\nsudo lxc-start --name u1 --daemon\\n\\nsudo lxc-info --name u1\\n\\nsudo lxc-stop --name u1\\n\\nsudo lxc-destroy --name u1\\n\\n**User namespaces**\\n\\nUnprivileged containers allow users to create and administer containers without having any root privilege. The\\nfeature underpinning this is called user namespaces. User namespaces are hierarchical, with privileged tasks in a\\nparent namespace being able to map its ids into child namespaces. By default every task on the host runs in the\\ninitial user namespace, where the full range of ids is mapped onto the full range.',\n",
       " 'This can be seen by looking at\\n\\n/proc/self/uid_map and /proc/self/gid_map, which both will show 0 0 4294967295 when read from the initial user\\nnamespace. As of Ubuntu 14.04, when new users are created they are by default offered a range of UIDs. The list of\\nassigned ids can be seen in the files /etc/subuid and /etc/subgid See their respective manpages for more information.\\nSubuids and subgids are by convention started at id 100000 to avoid conflicting with system users.\\n\\nIf a user was created on an earlier release, it can be granted a range of ids using usermod, as follows:\\n\\nsudo usermod -v 100000-200000 -w 100000-200000 user1\\n\\nThe programs newuidmap and newgidmap are setuid-root programs in the uidmap package, which are used internally by\\nlxc to map subuids and subgids from the host into the unprivileged container. They ensure that the user only maps\\nids which are authorized by the host configuration.\\n\\n**Basic unprivileged usage**\\n\\nTo create unprivileged containers, a few first steps are needed. You will need to create a default container configuration\\nfile, specifying your desired id mappings and network setup, as well as configure the host to allow the unprivileged\\nuser to hook into the host network. The example below assumes that your mapped user and group id ranges are\\n100000–165536. Check your actual user and group id ranges and modify the example accordingly:\\n\\ngrep $USER /etc/subuid\\n\\ngrep $USER /etc/subgid\\n\\nmkdir -p ~/.config/lxc\\n\\necho \"lxc.id_map = u 0 100000 65536\" > ~/.config/lxc/default.conf\\n\\necho \"lxc.id_map = g 0 100000 65536\" >> ~/.config/lxc/default.conf\\n\\necho \"lxc.network.type = veth\" >> ~/.config/lxc/default.conf\\n\\n220\\n\\n\\n-----',\n",
       " 'echo \"lxc.network.link = lxcbr0\" >> ~/.config/lxc/default.conf\\n\\necho \"$USER veth lxcbr0 2\" | sudo tee -a /etc/lxc/lxc-usernet\\n\\nAfter this, you can create unprivileged containers the same way as privileged ones, simply without using sudo.\\n\\nlxc-create -t download -n u1 -- -d ubuntu -r DISTRO-SHORT-CODENAME -a amd64\\n\\nlxc-start -n u1 -d\\n\\nlxc-attach -n u1\\n\\nlxc-stop -n u1\\n\\nlxc-destroy -n u1\\n\\n**Nesting**\\n\\nIn order to run containers inside containers - referred to as nested containers - two lines must be present in the parent\\ncontainer configuration file:\\n\\nlxc.mount.auto = cgroup\\n\\nlxc.aa_profile = lxc-container-default-with-nesting\\n\\nThe first will cause the cgroup manager socket to be bound into the container, so that lxc inside the container is able\\nto administer cgroups for its nested containers. The second causes the container to run in a looser Apparmor policy\\nwhich allows the container to do the mounting required for starting containers. Note that this policy, when used with\\na privileged container, is much less safe than the regular policy or an unprivileged container. See the *Apparmor* section\\nfor more information.\\n## **Global configuration**\\n\\nThe following configuration files are consulted by LXC. For privileged use, they are found under /etc/lxc, while for\\nunprivileged use they are under ~/.config/lxc .\\n\\n - lxc.conf may optionally specify alternate values for several lxc settings, including the lxcpath, the default\\nconfiguration, cgroups to use, a cgroup creation pattern, and storage backend settings for lvm and zfs.\\n\\n - default.conf specifies configuration which every newly created container should contain. This usually contains',\n",
       " 'at least a network section, and, for unprivileged users, an id mapping section\\n\\n - lxc-usernet.conf specifies how unprivileged users may connect their containers to the host-owned network.\\n\\nlxc.conf and default.conf are both under /etc/lxc and $HOME/.config/lxc, while lxc-usernet.conf is only host-wide.\\n\\nBy default, containers are located under /var/lib/lxc for the root user.\\n## **Networking**\\n\\nBy default LXC creates a private network namespace for each container, which includes a layer 2 networking stack.\\nContainers usually connect to the outside world by either having a physical NIC or a veth tunnel endpoint passed into\\nthe container. LXC creates a NATed bridge, lxcbr0, at host startup. Containers created using the default configuration\\nwill have one veth NIC with the remote end plugged into the lxcbr0 bridge. A NIC can only exist in one namespace\\nat a time, so a physical NIC passed into the container is not usable on the host.\\n\\nIt is possible to create a container without a private network namespace. In this case, the container will have access\\nto the host networking like any other application. Note that this is particularly dangerous if the container is running\\na distribution with upstart, like Ubuntu, since programs which talk to init, like shutdown, will talk over the abstract\\nUnix domain socket to the host’s upstart, and shut down the host.\\n\\nTo give containers on lxcbr0 a persistent ip address based on domain name, you can write entries to\\n\\n/etc/lxc/dnsmasq.conf like:\\n\\ndhcp-host=lxcmail,10.0.3.100\\n\\ndhcp-host=ttrss,10.0.3.101\\n\\nIf it is desirable for the container to be publicly accessible, there are a few ways to go about it. One is to use iptables',\n",
       " 'to forward host ports to the container, for instance\\n\\niptables -t nat -A PREROUTING -p tcp -i eth0 --dport 587 -j DNAT \\\\\\n\\n--to-destination 10.0.3.100:587\\n\\nThen, specify the host’s bridge in the container configuration file in place of lxcbr0, for instance\\n\\nlxc.network.type = veth\\n\\nlxc.network.link = br0\\n\\n221\\n\\n\\n-----\\n\\nFinally, you can ask LXC to use macvlan for the container’s NIC. Note that this has limitations and depending on\\nconfiguration may not allow the container to talk to the host itself. Therefore the other two options are preferred and\\nmore commonly used.\\n\\nThere are several ways to determine the ip address for a container. First, you can use lxc-ls --fancy which will print\\nthe ip addresses for all running containers, or lxc-info -i -H -n C1 which will print C1’s ip address. If dnsmasq is\\ninstalled on the host, you can also add an entry to /etc/dnsmasq.conf as follows\\n\\nserver=/lxc/10.0.3.1\\n\\nafter which dnsmasq will resolve C1.lxc locally, so that you can do:\\n\\nping C1\\n\\nssh C1\\n\\nFor more information, see the lxc.conf(5) manpage as well as the example network configurations under\\n\\n/usr/share/doc/lxc/examples/ .\\n## **LXC startup**\\n\\nLXC does not have a long-running daemon. However it does have three upstart jobs.\\n\\n - /etc/init/lxc-net.conf: is an optional job which only runs if /etc/default/lxc-net specifies USE_LXC_BRIDGE\\n(true by default). It sets up a NATed bridge for containers to use.\\n\\n - /etc/init/lxc.conf loads the lxc apparmor profiles and optionally starts any autostart containers. The autostart\\ncontainers will be ignored if LXC_AUTO (true by default) is set to true in /etc/default/lxc .',\n",
       " 'See the lxcautostart manual page for more information on autostarted containers.\\n\\n - /etc/init/lxc-instance.conf is used by /etc/init/lxc.conf to autostart a container.\\n## **Backing Stores**\\n\\nLXC supports several backing stores for container root filesystems. The default is a simple directory backing store,\\nbecause it requires no prior host customization, so long as the underlying filesystem is large enough. It also requires no\\nroot privilege to create the backing store, so that it is seamless for unprivileged use. The rootfs for a privileged directory\\nbacked container is located (by default) under /var/lib/lxc/C1/rootfs, while the rootfs for an unprivileged container\\nis under ~/.local/share/lxc/C1/rootfs . If a custom lxcpath is specified in lxc.system.com, then the container rootfs\\nwill be under $lxcpath/C1/rootfs .\\n\\nA snapshot clone C2 of a directory backed container C1 becomes an overlayfs backed container, with a rootfs called\\n\\noverlayfs:/var/lib/lxc/C1/rootfs:/var/lib/lxc/C2/delta0 . Other backing store types include loop, btrfs, LVM and\\nzfs.\\n\\nA btrfs backed container mostly looks like a directory backed container, with its root filesystem in the same location.\\nHowever, the root filesystem comprises a subvolume, so that a snapshot clone is created using a subvolume snapshot.\\n\\nThe root filesystem for an LVM backed container can be any separate LV. The default VG name can be specified in\\nlxc.conf. The filesystem type and size are configurable per-container using lxc-create.\\n\\nThe rootfs for a zfs backed container is a separate zfs filesystem, mounted under the traditional /var/lib/lxc/C1/rootfs\\nlocation.',\n",
       " 'The zfsroot can be specified at lxc-create, and a default can be specified in lxc.system.conf.\\n\\nMore information on creating containers with the various backing stores can be found in the lxc-create manual page.\\n## **Templates**\\n\\nCreating a container generally involves creating a root filesystem for the container. lxc-create delegates this\\nwork to *templates*, which are generally per-distribution. The lxc templates shipped with lxc can be found under\\n\\n/usr/share/lxc/templates, and include templates to create Ubuntu, Debian, Fedora, Oracle, centos, and gentoo\\ncontainers among others.\\n\\nCreating distribution images in most cases requires the ability to create device nodes, often requires tools which are\\nnot available in other distributions, and usually is quite time-consuming. Therefore lxc comes with a special *download*\\ntemplate, which downloads pre-built container images from a central lxc server. The most important use case is to allow\\nsimple creation of unprivileged containers by non-root users, who could not for instance easily run the debootstrap\\ncommand.\\n\\nWhen running lxc-create, all options which come after *–* are passed to the template. In the following command,\\n*–name*, *–template* and *–bdev* are passed to lxc-create, while *–release* is passed to the template:\\n\\n222\\n\\n\\n-----\\n\\nlxc-create --template ubuntu --name c1 --bdev loop -- --release DISTRO-SHORT-CODENAME\\n\\nYou can obtain help for the options supported by any particular container by passing *–help* and the template name to\\n\\nlxc-create . For instance, for help with the download template,\\n\\nlxc-create --template download --help\\n## **Autostart**\\n\\nLXC supports marking containers to be started at system boot.',\n",
       " 'Prior to Ubuntu 14.04, this was done using symbolic\\nlinks under the directory /etc/lxc/auto . Starting with Ubuntu 14.04, it is done through the container configuration\\nfiles. An entry\\n\\nlxc.start.auto = 1\\n\\nlxc.start.delay = 5\\n\\nwould mean that the container should be started at boot, and the system should wait 5 seconds before starting the\\nnext container. LXC also supports ordering and grouping of containers, as well as reboot and shutdown by autostart\\ngroups. See the manual pages for lxc-autostart and lxc.container.conf for more information.\\n## **Apparmor**\\n\\nLXC ships with a default Apparmor profile intended to protect the host from accidental misuses of privilege inside\\nthe container. For instance, the container will not be able to write to /proc/sysrq-trigger or to most /sys files.\\n\\nThe usr.bin.lxc-start profile is entered by running lxc-start . This profile mainly prevents lxc-start from mounting\\nnew filesystems outside of the container’s root filesystem. Before executing the container’s init, LXC requests a\\nswitch to the container’s profile. By default, this profile is the lxc-container-default policy which is defined in\\n\\n/etc/apparmor.d/lxc/lxc-default . This profile prevents the container from accessing many dangerous paths, and\\nfrom mounting most filesystems.\\n\\nPrograms in a container cannot be further confined - for instance, MySQL runs under the container profile (protecting\\nthe host) but will not be able to enter the MySQL profile (to protect the container).\\n\\nlxc-execute does not enter an Apparmor profile, but the container it spawns will be confined.\\n\\n**Customizing container policies**',\n",
       " 'If you find that lxc-start is failing due to a legitimate access which is being denied by its Apparmor policy, you can\\ndisable the lxc-start profile by doing:\\n\\nsudo apparmor_parser -R /etc/apparmor.d/usr.bin.lxc-start\\n\\nsudo ln -s /etc/apparmor.d/usr.bin.lxc-start /etc/apparmor.d/disabled/\\n\\nThis will make lxc-start run unconfined, but continue to confine the container itself. If you also wish to disable\\nconfinement of the container, then in addition to disabling the usr.bin.lxc-start profile, you must add:\\n\\nlxc.aa_profile = unconfined\\n\\nto the container’s configuration file.\\n\\nLXC ships with a few alternate policies for containers. If you wish to run containers inside containers (nesting), then\\nyou can use the lxc-container-default-with-nesting profile by adding the following line to the container configuration\\nfile\\n\\nlxc.aa_profile = lxc-container-default-with-nesting\\n\\nIf you wish to use libvirt inside containers, then you will need to edit that policy (which is defined in\\n\\n/etc/apparmor.d/lxc/lxc-default-with-nesting ) by uncommenting the following line:\\n\\nmount fstype=cgroup -> /sys/fs/cgroup/**,\\n\\nand re-load the policy.\\n\\nNote that the nesting policy with privileged containers is far less safe than the default policy, as it allows containers\\nto re-mount /sys and /proc in nonstandard locations, bypassing apparmor protections. Unprivileged containers do\\nnot have this drawback since the container root cannot write to root-owned proc and sys files.\\n\\nAnother profile shipped with lxc allows containers to mount block filesystem types like ext4. This can be useful in',\n",
       " 'some cases like maas provisioning, but is deemed generally unsafe since the superblock handlers in the kernel have not\\nbeen audited for safe handling of untrusted input.\\n\\n223\\n\\n\\n-----\\n\\nIf you need to run a container in a custom profile, you can create a new profile under /etc/apparmor.d/lxc/ . Its name\\nmust start with lxc- in order for lxc-start to be allowed to transition to that profile. The lxc-default profile includes\\nthe re-usable abstractions file /etc/apparmor.d/abstractions/lxc/container-base . An easy way to start a new profile\\ntherefore is to do the same, then add extra permissions at the bottom of your policy.\\n\\nAfter creating the policy, load it using:\\n\\nsudo apparmor_parser -r /etc/apparmor.d/lxc-containers\\n\\nThe profile will automatically be loaded after a reboot, because it is sourced by the file /etc/apparmor.d/lxc-containers .\\nFinally, to make container CN use this new lxc-CN-profile, add the following line to its configuration file:\\n\\nlxc.aa_profile = lxc-CN-profile\\n## **Control Groups**\\n\\nControl groups (cgroups) are a kernel feature providing hierarchical task grouping and per-cgroup resource accounting\\nand limits. They are used in containers to limit block and character device access and to freeze (suspend) containers.\\nThey can be further used to limit memory use and block i/o, guarantee minimum cpu shares, and to lock containers\\nto specific cpus.\\n\\nBy default, a privileged container CN will be assigned to a cgroup called /lxc/CN . In the case of name conflicts (which\\ncan occur when using custom lxcpaths) a suffix “-n”, where n is an integer starting at 0, will be appended to the\\n\\ncgroup name.',\n",
       " 'By default, a privileged container CN will be assigned to a cgroup called CN under the cgroup of the task which started\\nthe container, for instance /usr/1000.user/1.session/CN . The container root will be given group ownership of the\\ndirectory (but not all files) so that it is allowed to create new child cgroups.\\n\\nAs of Ubuntu 14.04, LXC uses the cgroup manager (cgmanager) to administer cgroups. The cgroup manager receives\\nD-Bus requests over the Unix socket /sys/fs/cgroup/cgmanager/sock . To facilitate safe nested containers, the line\\n\\nlxc.mount.auto = cgroup\\n\\ncan be added to the container configuration causing the /sys/fs/cgroup/cgmanager directory to be bindmounted into the container. The container in turn should start the cgroup management proxy (done by\\ndefault if the cgmanager package is installed in the container) which will move the /sys/fs/cgroup/cgmanager\\ndirectory to /sys/fs/cgroup/cgmanager.lower, then start listening for requests to proxy on its own socket\\n\\n/sys/fs/cgroup/cgmanager/sock . The host cgmanager will ensure that nested containers cannot escape their\\nassigned cgroups or make requests for which they are not authorized.\\n## **Cloning**\\n\\nFor rapid provisioning, you may wish to customize a canonical container according to your needs and then make\\nmultiple copies of it. This can be done with the lxc-clone program.\\n\\nClones are either snapshots or copies of another container. A copy is a new container copied from the original, and\\ntakes as much space on the host as the original. A snapshot exploits the underlying backing store’s snapshotting\\nability to make a copy-on-write container referencing the first. Snapshots can be created from btrfs, LVM, zfs, and',\n",
       " 'directory backed containers. Each backing store has its own peculiarities - for instance, LVM containers which are not\\nthinpool-provisioned cannot support snapshots of snapshots; zfs containers with snapshots cannot be removed until all\\nsnapshots are released; LVM containers must be more carefully planned as the underlying filesystem may not support\\ngrowing; btrfs does not suffer any of these shortcomings, but suffers from reduced fsync performance causing dpkg\\nand apt to be slower.\\n\\nSnapshots of directory-packed containers are created using the overlay filesystem. For instance, a privileged directorybacked container C1 will have its root filesystem under /var/lib/lxc/C1/rootfs . A snapshot clone of C1 called C2 will\\nbe started with C1’s rootfs mounted readonly under /var/lib/lxc/C2/delta0 . Importantly, in this case C1 should not\\nbe allowed to run or be removed while C2 is running. It is advised instead to consider C1 a *canonical* base container,\\nand to only use its snapshots.\\n\\nGiven an existing container called C1, a copy can be created using:\\n\\nsudo lxc-clone -o C1 -n C2\\n\\nA snapshot can be created using:\\n\\nsudo lxc-clone -s -o C1 -n C2\\n\\nSee the lxc-clone manpage for more information.\\n\\n224\\n\\n\\n-----\\n\\n**Snapshots**\\n\\nTo more easily support the use of snapshot clones for iterative container development, LXC supports *snapshots* . When\\nworking on a container C1, before making a potentially dangerous or hard-to-revert change, you can create a snapshot\\n\\nsudo lxc-snapshot -n C1\\n\\nwhich is a snapshot-clone called ‘snap0’ under /var/lib/lxcsnaps or $HOME/.local/share/lxcsnaps. The next snapshot\\nwill be called ‘snap1’, etc.',\n",
       " 'Existing snapshots can be listed using lxc-snapshot -L -n C1, and a snapshot can be restored\\n\\n- erasing the current C1 container - using lxc-snapshot -r snap1 -n C1 . After the restore command, the snap1 snapshot\\ncontinues to exist, and the previous C1 is erased and replaced with the snap1 snapshot.\\n\\nSnapshots are supported for btrfs, lvm, zfs, and overlayfs containers. If lxc-snapshot is called on a directory-backed\\ncontainer, an error will be logged and the snapshot will be created as a copy-clone. The reason for this is that if the\\nuser creates an overlayfs snapshot of a directory-backed container and then makes changes to the directory-backed\\ncontainer, then the original container changes will be partially reflected in the snapshot. If snapshots of a directory\\nbacked container C1 are desired, then an overlayfs clone of C1 should be created, C1 should not be touched again,\\nand the overlayfs clone can be edited and snapshotted at will, as such\\n\\nlxc-clone -s -o C1 -n C2\\n\\nlxc-start -n C2 -d # make some changes\\n\\nlxc-stop -n C2\\n\\nlxc-snapshot -n C2\\n\\nlxc-start -n C2 # etc\\n\\n**Ephemeral Containers**\\n\\nWhile snapshots are useful for longer-term incremental development of images, ephemeral containers utilize snapshots\\nfor quick, single-use throwaway containers. Given a base container C1, you can start an ephemeral container using\\n\\nlxc-start-ephemeral -o C1\\n\\nThe container begins as a snapshot of C1. Instructions for logging into the container will be printed to the console.\\nAfter shutdown, the ephemeral container will be destroyed. See the lxc-start-ephemeral manual page for more options.\\n## **Lifecycle management hooks**',\n",
       " 'Beginning with Ubuntu 12.10, it is possible to define hooks to be executed at specific points in a container’s lifetime:\\n\\n  - Pre-start hooks are run in the host’s namespace before the container ttys, consoles, or mounts are up. If any\\nmounts are done in this hook, they should be cleaned up in the post-stop hook.\\n\\n  - Pre-mount hooks are run in the container’s namespaces, but before the root filesystem has been mounted. Mounts\\ndone in this hook will be automatically cleaned up when the container shuts down.\\n\\n  - Mount hooks are run after the container filesystems have been mounted, but before the container has called\\n\\npivot_root to change its root filesystem.\\n\\n  - Start hooks are run immediately before executing the container’s init. Since these are executed after pivoting\\ninto the container’s filesystem, the command to be executed must be copied into the container’s filesystem.\\n\\n  - Post-stop hooks are executed after the container has been shut down.\\n\\nIf any hook returns an error, the container’s run will be aborted. Any *post-stop* hook will still be executed. Any\\noutput generated by the script will be logged at the debug priority.\\n\\nPlease see the lxc.container.conf(5) manual page for the configuration file format with which to specify hooks. Some\\nsample hooks are shipped with the lxc package to serve as an example of how to write and use such hooks.\\n## **Consoles**\\n\\nContainers have a configurable number of consoles. One always exists on the container’s /dev/console . This is shown\\non the terminal from which you ran lxc-start, unless the *-d* option is specified. The output on /dev/console can\\nbe redirected to a file using the *-c console-file* option to lxc-start .',\n",
       " 'The number of extra consoles is specified by the\\n\\nlxc.tty variable, and is usually set to 4. Those consoles are shown on /dev/ttyN (for 1 <= N <= 4). To log into\\nconsole 3 from the host, use:\\n\\nsudo lxc-console -n container -t 3\\n\\nor if the -t N option is not specified, an unused console will be automatically chosen. To exit the console, use the escape\\nsequence Ctrl-a q . Note that the escape sequence does not work in the console resulting from lxc-start without the\\n\\n-d option.\\n\\n225\\n\\n\\n-----\\n\\nEach container console is actually a Unix98 pty in the host’s (not the guest’s) pty mount, bind-mounted over the\\nguest’s /dev/ttyN and /dev/console . Therefore, if the guest unmounts those or otherwise tries to access the actual\\ncharacter device 4:N, it will not be serving getty to the LXC consoles. (With the default settings, the container will\\nnot be able to access that character device and getty will therefore fail.) This can easily happen when a boot script\\nblindly mounts a new /dev .\\n## **Troubleshooting**\\n\\n**Logging**\\n\\nIf something goes wrong when starting a container, the first step should be to get full logging from LXC:\\n\\nsudo lxc-start -n C1 -l trace -o debug.out\\n\\nThis will cause lxc to log at the most verbose level, trace, and to output log information to a file called ‘debug.out’.\\nIf the file debug.out already exists, the new log information will be appended.\\n\\n**Monitoring container status**\\n\\nTwo commands are available to monitor container state changes. lxc-monitor monitors one or more containers for any\\nstate changes. It takes a container name as usual with the *-n* option, but in this case the container name can be a',\n",
       " \"posix regular expression to allow monitoring desirable sets of containers. lxc-monitor continues running as it prints\\ncontainer changes. lxc-wait waits for a specific state change and then exits. For instance,\\n\\nsudo lxc-monitor -n cont[0-5]*\\n\\nwould print all state changes to any containers matching the listed regular expression, whereas\\n\\nsudo lxc-wait -n cont1 -s 'STOPPED|FROZEN'\\n\\nwill wait until container cont1 enters state STOPPED or state FROZEN and then exit.\\n\\n**Attach**\\n\\nAs of Ubuntu 14.04, it is possible to attach to a container’s namespaces. The simplest case is to simply do\\n\\nsudo lxc-attach -n C1\\n\\nwhich will start a shell attached to C1’s namespaces, or, effectively inside the container. The attach functionality is\\nvery flexible, allowing attaching to a subset of the container’s namespaces and security context. See the manual page\\nfor more information.\\n\\n**Container init verbosity**\\n\\nIf LXC completes the container startup, but the container init fails to complete (for instance, no login prompt is\\nshown), it can be useful to request additional verbosity from the init process. For an upstart container, this might be:\\n\\nsudo lxc-start -n C1 /sbin/init loglevel=debug\\n\\nYou can also start an entirely different program in place of init, for instance\\n\\nsudo lxc-start -n C1 /bin/bash\\n\\nsudo lxc-start -n C1 /bin/sleep 100\\n\\nsudo lxc-start -n C1 /bin/cat /proc/1/status\\n## **LXC API**\\n\\nMost of the LXC functionality can now be accessed through an API exported by liblxc for which bindings are available\\nin several languages, including Python, lua, ruby, and go.\\n\\nBelow is an example using the python bindings (which are available in the python3-lxc package) which creates and\",\n",
       " 'starts a container, then waits until it has been shut down:\\n\\n# sudo python3\\n\\nPython 3.2.3 (default, Aug 28 2012, 08:26:03)\\n\\n[GCC 4.7.1 20120814 (prerelease)] on linux2\\n\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n\\n>>> import lxc\\n\\n__main__:1: Warning: The python-lxc API isn\\'t yet stable and may change at any point in the future.\\n\\n>>> c=lxc.Container(\"C1\")\\n\\n>>> c.create(\"ubuntu\")\\n\\n226\\n\\n\\n-----\\n\\nTrue\\n\\n>>> c.start()\\n\\nTrue\\n\\n>>> c.wait(\"STOPPED\")\\n\\nTrue\\n## **Security**\\n\\nA namespace maps ids to resources. By not providing a container any id with which to reference a resource, the resource\\ncan be protected. This is the basis of some of the security afforded to container users. For instance, IPC namespaces\\nare completely isolated. Other namespaces, however, have various *leaks* which allow privilege to be inappropriately\\nexerted from a container into another container or to the host.\\n\\nBy default, LXC containers are started under a Apparmor policy to restrict some actions. The details of AppArmor\\nintegration with lxc are in section *Apparmor* . Unprivileged containers go further by mapping root in the container\\nto an unprivileged host UID. This prevents access to /proc and /sys files representing host resources, as well as any\\nother files owned by root on the host.\\n\\n**Exploitable system calls**\\n\\nIt is a core container feature that containers share a kernel with the host. Therefore if the kernel contains any\\nexploitable system calls the container can exploit these as well. Once the container controls the kernel it can fully\\ncontrol any resource known to the host.\\n\\nIn general to run a full distribution container a large number of system calls will be needed.',\n",
       " 'However for application\\ncontainers it may be possible to reduce the number of available system calls to only a few. Even for system containers\\nrunning a full distribution security gains may be had, for instance by removing the 32-bit compatibility system calls in\\na 64-bit container. See the lxc.container.conf manual page for details of how to configure a container to use seccomp.\\nBy default, no seccomp policy is loaded.\\n## **Resources**\\n\\n[• The DeveloperWorks article LXC: Linux container tools was an early introduction to the use of containers.](https://developer.ibm.com/tutorials/l-lxc-containers/)\\n\\n[• The Secure Containers Cookbook demonstrated the use of security modules to make containers more secure.](http://www.ibm.com/developerworks/linux/library/l-lxc-security/index.html)\\n\\n[• The upstream LXC project is hosted at linuxcontainers.org.](http://linuxcontainers.org)\\n\\n[LXD (pronounced lex-dee) is the lightervisor, or lightweight container hypervisor. LXC (lex-see) is a program which](https://ubuntu.com/lxd)\\ncreates and administers “containers” on a local system. It also provides an API to allow higher level managers, such\\nas LXD, to administer containers. In a sense, one could compare LXC to QEMU, while comparing LXD to libvirt.\\n\\nThe LXC API deals with a ‘container’. The LXD API deals with ‘remotes’, which serve images and containers. This\\nextends the LXC functionality over the network, and allows concise management of tasks like container migration and\\ncontainer image publishing.\\n\\nLXD uses LXC under the covers for some container management tasks.',\n",
       " 'However, it keeps its own container configuration information and has its own conventions, so that it is best not to use classic LXC commands by hand with\\nLXD containers. This document will focus on how to configure and administer LXD on Ubuntu systems.\\n## **Online Resources**\\n\\n[There is excellent documentation for getting started with LXD. Stephane Graber also has an excellent blog series on](https://documentation.ubuntu.com/lxd/en/latest/getting_started/)\\n[LXD 2.0. Finally, there is great documentation on how to drive LXD using Juju.](https://docs.jujucharms.com/devel/en/clouds-lxd)\\n\\nThis document will offer an Ubuntu Server-specific view of LXD, focusing on administration.\\n## **Installation**\\n\\nLXD is pre-installed on Ubuntu Server cloud images. On other systems, the lxd package can be installed using:\\n\\nsudo snap install lxd\\n\\nThis will install the self-contained LXD snap package.\\n## **Kernel preparation**\\n\\nIn general, Ubuntu should have all the desired features enabled by default. One exception to this is that in order\\nto enable swap accounting the boot argument swapaccount=1 must be set. This can be done by appending it to the\\n\\nGRUB_CMDLINE_LINUX_DEFAULT= variable in /etc/default/grub, then running ‘update-grub’ as root and rebooting.\\n\\n227\\n\\n\\n-----\\n\\n## **Configuration**\\n\\nIn order to use LXD, some basic settings need to be configured first. This is done by running lxd init, which will\\nallow you to choose:\\n\\n[• Directory or ZFS container backend. If you choose ZFS, you can choose which block devices to use, or the size](http://open-zfs.org)\\nof a file to use as backing store.\\n\\n  - Availability over the network.',\n",
       " '  - A ‘trust password’ used by remote clients to vouch for their client certificate.\\n\\nYou must run ‘lxd init’ as root. ‘lxc’ commands can be run as any user who is a member of group lxd. If user joe is\\nnot a member of group ‘lxd’, you may run:\\n\\nadduser joe lxd\\n\\nas root to change it. The new membership will take effect on the next login, or after running newgrp lxd from an\\nexisting login.\\n\\n[See How to initialize LXD in the LXD documentation for more information on the configuration settings. Also, refer](https://documentation.ubuntu.com/lxd/en/latest/howto/initialize/)\\nto the definitive configuration provided with the source code for the server, container, profile, and device configuration.\\n## **Creating your first container**\\n\\nThis section will describe the simplest container tasks.\\n\\n**Creating a container**\\n\\nEvery new container is created based on either an image, an existing container, or a container snapshot. At install\\ntime, LXD is configured with the following image servers:\\n\\n - ubuntu : this serves official Ubuntu server cloud image releases.\\n\\n - ubuntu-daily : this serves official Ubuntu server cloud images of the daily development releases.\\n\\n - images [: this is a default-installed alias for images.linuxcontainers.org. This is serves classical lxc images built](http://images.linuxcontainers.org)\\nusing the same images which the LXC ‘download’ template uses. This includes various distributions and minimal\\ncustom-made Ubuntu images. This is not the recommended server for Ubuntu images.\\n\\nThe command to create and start a container is\\n\\nlxc launch remote:image containername\\n\\nImages are identified by their hash, but are also aliased.',\n",
       " 'The ubuntu remote knows many aliases such as 18.04 and\\n\\nbionic . A list of all images available from the Ubuntu Server can be seen using:\\n\\nlxc image list ubuntu:\\n\\nTo see more information about a particular image, including all the aliases it is known by, you can use:\\n\\nlxc image info ubuntu:bionic\\n\\nYou can generally refer to an Ubuntu image using the release name ( bionic ) or the release number ( 18.04 ). In addition,\\n\\nlts is an alias for the latest supported LTS release. To choose a different architecture, you can specify the desired\\narchitecture:\\n\\nlxc image info ubuntu:lts/arm64\\n\\nNow, let’s start our first container:\\n\\nlxc launch ubuntu:bionic b1\\n\\nThis will download the official current Bionic cloud image for your current architecture, then create a container named\\n\\nb1 using that image, and finally start it. Once the command returns, you can see it using:\\n\\nlxc list\\n\\nlxc info b1\\n\\nand open a shell in it using:\\n\\nlxc exec b1 -- bash\\n\\nA convenient alias for the command above is:\\n\\nlxc shell b1\\n\\n228\\n\\n\\n-----\\n\\nThe try-it page mentioned above gives a full synopsis of the commands you can use to administer containers.\\n\\nNow that the bionic image has been downloaded, it will be kept in sync until no new containers have been created\\nbased on it for (by default) 10 days. After that, it will be deleted.\\n## **LXD Server Configuration**\\n\\nBy default, LXD is socket activated and configured to listen only on a local UNIX socket. While LXD may not be\\nrunning when you first look at the process listing, any LXC command will start it up. For instance:\\n\\nlxc list\\n\\nThis will create your client certificate and contact the LXD server for a list of containers. To make the server accessible',\n",
       " 'over the network you can set the http port using:\\n\\nlxc config set core.https_address :8443\\n\\nThis will tell LXD to listen to port 8443 on all addresses.\\n\\n**Authentication**\\n\\nBy default, LXD will allow all members of group lxd to talk to it over the UNIX socket. Communication over the\\nnetwork is authorized using server and client certificates.\\n\\nBefore client c1 wishes to use remote r1, r1 must be registered using:\\n\\nlxc remote add r1 r1.example.com:8443\\n\\nThe fingerprint of r1’s certificate will be shown, to allow the user at c1 to reject a false certificate. The server in turn\\nwill verify that c1 may be trusted in one of two ways. The first is to register it in advance from any already-registered\\nclient, using:\\n\\nlxc config trust add r1 certfile.crt\\n\\nNow when the client adds r1 as a known remote, it will not need to provide a password as it is already trusted by the\\n\\nserver.\\n\\nThe other step is to configure a ‘trust password’ with r1, either at initial configuration using lxd init, or after the\\nfact using:\\n\\nlxc config set core.trust_password PASSWORD\\n\\nThe password can then be provided when the client registers r1 as a known remote.\\n\\n**Backing store**\\n\\nLXD supports several backing stores. The recommended and the default backing store is zfs . If you already have a\\nZFS pool configured, you can tell LXD to use it during the lxd init procedure, otherwise a file-backed zpool will be\\ncreated automatically. With ZFS, launching a new container is fast because the filesystem starts as a copy on write\\nclone of the images’ filesystem. Note that unless the container is privileged (see below) LXD will need to change',\n",
       " 'ownership of all files before the container can start, however this is fast and change very little of the actual filesystem\\ndata.\\n\\n[The other supported backing stores are described in detail in the Storage configuration section of the LXD documen-](https://documentation.ubuntu.com/lxd/en/latest/explanation/storage/)\\n\\ntation.\\n## **Container configuration**\\n\\nContainers are configured according to a set of profiles, described in the next section, and a set of container-specific\\nconfiguration. Profiles are applied first, so that container specific configuration can override profile configuration.\\n\\nContainer configuration includes properties like the architecture, limits on resources such as CPU and RAM, security\\ndetails including apparmor restriction overrides, and devices to apply to the container.\\n\\nDevices can be of several types, including UNIX character, UNIX block, network interface, or disk. In order to insert\\na host mount into a container, a ‘disk’ device type would be used. For instance, to mount /opt in container c1 at /opt,\\nyou could use:\\n\\nlxc config device add c1 opt disk source=/opt path=opt\\n\\nSee:\\n\\nlxc help config\\n\\n229\\n\\n\\n-----\\n\\nfor more information about editing container configurations. You may also use:\\n\\nlxc config edit c1\\n\\nto edit the whole of c1 ’s configuration. Comments at the top of the configuration will show examples of correct syntax\\nto help administrators hit the ground running. If the edited configuration is not valid when the editor is exited, then\\nthe editor will be restarted.\\n## **Profiles**\\n\\nProfiles are named collections of configurations which may be applied to more than one container. For instance, all',\n",
       " 'containers created with lxc launch, by default, include the default profile, which provides a network interface eth0 .\\n\\nTo mask a device which would be inherited from a profile but which should not be in the final container, define a\\ndevice by the same name but of type ‘none’:\\n\\nlxc config device add c1 eth1 none\\n## **Nesting**\\n\\nContainers all share the same host kernel. This means that there is always an inherent trade-off between features\\nexposed to the container and host security from malicious containers. Containers by default are therefore restricted\\nfrom features needed to nest child containers. In order to run lxc or lxd containers under a lxd container, the\\n\\nsecurity.nesting feature must be set to true:\\n\\nlxc config set container1 security.nesting true\\n\\nOnce this is done, container1 will be able to start sub-containers.\\n\\nIn order to run unprivileged (the default in LXD) containers nested under an unprivileged container, you will need to\\nensure a wide enough UID mapping. Please see the ‘UID mapping’ section below.\\n## **Limits**\\n\\nLXD supports flexible constraints on the resources which containers can consume. The limits come in the following\\ncategories:\\n\\n  - CPU: limit cpu available to the container in several ways.\\n\\n  - Disk: configure the priority of I/O requests under load\\n\\n  - RAM: configure memory and swap availability\\n\\n  - Network: configure the network priority under load\\n\\n  - Processes: limit the number of concurrent processes in the container.\\n\\n[For a full list of limits known to LXD, see the configuration documentation.](https://documentation.ubuntu.com/lxd/en/latest/reference/instance_options/)\\n## **UID mappings and Privileged containers**',\n",
       " 'By default, LXD creates unprivileged containers. This means that root in the container is a non-root UID on the host.\\nIt is privileged against the resources owned by the container, but unprivileged with respect to the host, making root\\nin a container roughly equivalent to an unprivileged user on the host. (The main exception is the increased attack\\nsurface exposed through the system call interface)\\n\\nBriefly, in an unprivileged container, 65536 UIDs are ‘shifted’ into the container. For instance, UID 0 in the container\\nmay be 100000 on the host, UID 1 in the container is 100001, etc, up to 165535. The starting value for UIDs and\\nGIDs, respectively, is determined by the ‘root’ entry the /etc/subuid and /etc/subgid [files. (See the subuid(5) man](http://manpages.ubuntu.com/manpages/xenial/en/man5/subuid.5.html)\\npage.)\\n\\nIt is possible to request a container to run without a UID mapping by setting the security.privileged flag to true:\\n\\nlxc config set c1 security.privileged true\\n\\nNote however that in this case the root user in the container is the root user on the host.\\n\\n230\\n\\n\\n-----\\n\\n## **Apparmor**\\n\\nLXD confines containers by default with an apparmor profile which protects containers from each other and the host\\nfrom containers. For instance this will prevent root in one container from signaling root in another container, even\\nthough they have the same uid mapping. It also prevents writing to dangerous, un-namespaced files such as many\\nsysctls and /proc/sysrq-trigger .\\n\\nIf the apparmor policy for a container needs to be modified for a container c1, specific apparmor policy lines can be\\nadded in the raw.apparmor configuration key.\\n## **Seccomp**',\n",
       " 'All containers are confined by a default seccomp policy. This policy prevents some dangerous actions such as forced\\numounts, kernel module loading and unloading, kexec, and the open_by_handle_at system call. The seccomp configuration cannot be modified, however a completely different seccomp policy – or none – can be requested using raw.lxc\\n(see below).\\n## **Raw LXC configuration**\\n\\nLXD configures containers for the best balance of host safety and container usability. Whenever possible it is highly\\nrecommended to use the defaults, and use the LXD configuration keys to request LXD to modify as needed. Sometimes, however, it may be necessary to talk to the underlying lxc driver itself. This can be done by specifying\\n[LXC configuration items in the ‘raw.lxc’ LXD configuration key. These must be valid items as documented in the](http://manpages.ubuntu.com/manpages/focal/en/man5/lxc.container.conf.5.html)\\n[lxc.container.conf(5) manual page.](http://manpages.ubuntu.com/manpages/focal/en/man5/lxc.container.conf.5.html)\\n\\n**Snapshots**\\n\\nContainers can be renamed and live-migrated using the lxc move command:\\n\\nlxc move c1 final-beta\\n\\nThey can also be snapshotted:\\n\\nlxc snapshot c1 YYYY-MM-DD\\n\\nLater changes to c1 can then be reverted by restoring the snapshot:\\n\\nlxc restore u1 YYYY-MM-DD\\n\\nNew containers can also be created by copying a container or snapshot:\\n\\nlxc copy u1/YYYY-MM-DD testcontainer\\n\\n**Publishing images**\\n\\nWhen a container or container snapshot is ready for consumption by others, it can be published as a new image using;\\n\\nlxc publish u1/YYYY-MM-DD --alias foo-2.0',\n",
       " 'The published image will be private by default, meaning that LXD will not allow clients without a trusted certificate\\nto see them. If the image is safe for public viewing (i.e. contains no private information), then the ‘public’ flag can be\\nset, either at publish time using\\n\\nlxc publish u1/YYYY-MM-DD --alias foo-2.0 public=true\\n\\nor after the fact using\\n\\nlxc image edit foo-2.0\\n\\nand changing the value of the public field.\\n\\n**Image export and import**\\n\\nImage can be exported as, and imported from, tarballs:\\n\\nlxc image export foo-2.0 foo-2.0.tar.gz\\n\\nlxc image import foo-2.0.tar.gz --alias foo-2.0 --public\\n\\n231\\n\\n\\n-----\\n\\n## **Troubleshooting**\\n\\nTo view debug information about LXD itself, on a systemd based host use\\n\\njournalctl -u lxd\\n\\nContainer logfiles for container c1 may be seen using:\\n\\nlxc info c1 --show-log\\n\\nThe configuration file which was used may be found under /var/log/lxd/c1/lxc.conf while apparmor profiles can be\\nfound in /var/lib/lxd/security/apparmor/profiles/c1 and seccomp profiles in /var/lib/lxd/security/seccomp/c1 .\\n\\nContainers are widely used across multiple server workloads (databases and web servers, for instance), and understanding how to properly set up your server to run them is becoming more important for systems administrators. In\\nthis explanatory page, we are going to discuss some of the most important factors a system administrator needs to\\nconsider when setting up the environment to run Docker containers.\\n\\nUnderstanding the options available to run Docker containers is key to optimising the use of computational resources\\nin a given scenario/workload, which might have specific requirements. Some aspects that are important for system',\n",
       " 'administrators are: **storage**, **networking** and **logging** . We are going to discuss each of these in the subsequent\\nsections, presenting how to configure them and interact with the Docker command line interface (CLI).\\n## **Storage**\\n\\nThe first thing we need to keep in mind is that containers are ephemeral, and, unless configured otherwise, so are their\\ndata. Docker images are composed of one or more layers which are read-only, and once you run a container based on\\nan image a new writable layer is created on top of the topmost image layer; the container can manage any type of\\ndata there. The content changes in the writable container layer are not persisted anywhere, and once the container is\\ngone all the changes disappear. This behavior presents some challenges to us: How can the data be persisted? How\\ncan it be shared among containers? How can it be shared between the host and the containers?\\n\\nThere are some important concepts in the Docker world that are the answer for some of those problems: they are\\n**volumes**, **bind mounts** and **tmpfs** . Another question is how all those layers that form Docker images and containers\\nwill be stored, and for that we are going to talk about **storage drivers** (more on that later).\\n\\nWhen we want to persist data we have two options:\\n\\n  - Volumes are the preferred way to persist data generated and used by Docker containers if your workload will\\ngenerate a high volume of data, such as a database.\\n\\n  - Bind mounts are another option if you need to access files from the host, for example system files.\\n\\nIf what you want is to store some sensitive data in memory, like credentials, and do not want to persist it in either',\n",
       " 'the host or the container layer, we can use tmpfs mounts.\\n\\n**Volumes**\\n\\nThe recommended way to persist data to and from Docker containers is by using volumes. Docker itself manages them,\\nthey are not OS-dependent and they can provide some interesting features for system administrators:\\n\\n  - Easier to back up and migrate when compared to bind mounts;\\n\\n  - Managed by the Docker CLI or API;\\n\\n  - Safely shared among containers;\\n\\n  - Volume drivers allow one to store data in remote hosts or in public cloud providers (also encrypting the data).\\n\\nMoreover, volumes are a better choice than persisting data in the container layer, because volumes do not increase\\nthe size of the container, which can affect the life-cycle management performance.\\n\\nVolumes can be created before or at the container creation time. There are two CLI options you can use to mount a\\nvolume in the container during its creation ( docker run or docker create ):\\n\\n - --mount : it accepts multiple key-value pairs ( <key>=<value> ). This is the preferred option to use.\\n\\n**–** type : for volumes it will always be volume ;\\n**–** source or src : the name of the volume, if the volume is anonymous (no name) this can be omitted;\\n**–** destination, dst or target : the path inside the container where the volume will be mounted;\\n**–** readonly or ro (optional): whether the volume should be mounted as read-only inside the container;\\n**–** volume-opt (optional): a comma separated list of options in the format you would pass to the mount\\ncommand.\\n\\n - -v or --volume : it accepts 3 parameters separated by colon ( : ):\\n\\n**–** First, the name of the volume.',\n",
       " 'For the default local driver, the name should use only: letters in upper and\\nlower case, numbers, ., _ and      - ;\\n\\n232\\n\\n\\n-----\\n\\n**–**\\nSecond, the path inside the container where the volume will be mounted;\\n**–** Third (optional), a comma-separated list of options in the format you would pass to the mount command,\\nsuch as rw .\\n\\nHere are a few examples of how to manage a volume using the Docker CLI:\\n\\n# create a volume\\n\\n$ docker volume create my-vol\\n\\nmy-vol\\n\\n# list volumes\\n\\n$ docker volume ls\\n\\nDRIVER VOLUME NAME\\n\\nlocal my-vol\\n\\n# inspect volume\\n\\n$ docker volume inspect my-vol\\n\\n[\\n\\n{\\n\\n\"CreatedAt\": \"2023-10-25T00:53:24Z\",\\n\\n\"Driver\": \"local\",\\n\\n\"Labels\": null,\\n\\n\"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\\n\\n\"Name\": \"my-vol\",\\n\\n\"Options\": null,\\n\\n\"Scope\": \"local\"\\n\\n}\\n\\n]\\n\\n# remove a volume\\n\\n$ docker volume rm my-vol\\n\\nmy-vol\\n\\nRunning a container and mounting a volume:\\n\\n$ docker run –name web-server -d \\\\\\n\\n--mount source=my-vol,target=/app \\\\\\n\\nubuntu/apache2\\n\\n0709c1b632801fddd767deddda0d273289ba423e9228cc1d77b2194989e0a882\\n\\nAfter that, you can inspect your container to make sure the volume is mounted correctly:\\n\\n$ docker inspect web-server --format \\'{{ json .Mounts }}\\' | jq .\\n\\n[\\n\\n{\\n\\n\"Type\": \"volume\",\\n\\n\"Name\": \"my-vol\",\\n\\n\"Source\": \"/var/lib/docker/volumes/my-vol/_data\",\\n\\n\"Destination\": \"/app\",\\n\\n\"Driver\": \"local\",\\n\\n\"Mode\": \"z\",\\n\\n\"RW\": true,\\n\\n\"Propagation\": \"\"\\n\\n}\\n\\n]\\n\\nBy default, all your volumes will be stored in /var/lib/docker/volumes .\\n\\n**Bind mounts**\\n\\nBind mounts are another option for persisting data, however, they have some limitations compared to volumes. Bind\\nmounts are tightly associated with the directory structure and with the OS, but performance-wise they are similar to',\n",
       " 'volumes in Linux systems.\\n\\nIn a scenario where a container needs to have access to any host system’s file or directory, bind mounts are probably\\nthe best solution. Some monitoring tools make use of bind mounts when executed as Docker containers.\\n\\nBind mounts can be managed via the Docker CLI, and as with volumes there are two options you can use:\\n\\n - --mount : it accepts multiple key-value pairs ( <key>=<value> ). This is the preferred option to use.\\n\\n**–** type : for bind mounts it will always be bind ;\\n\\n233\\n\\n\\n-----\\n\\n**–** source or src : path of the file or directory on the host;\\n**–** destination, dst or target : container’s directory to be mounted;\\n**–** readonly or ro (optional): the bind mount is mounted in the container as read-only;\\n**–** volume-opt (optional): it accepts any mount command option;\\n**–** bind-propagation (optional): it changes the bind propagation. It can be rprivate, private, rshared, shared,\\n\\nrslave, slave .\\n\\n - -v or --volume : it accepts 3 parameters separated by colon ( : ):\\n\\n**–**\\nFirst, path of the file or directory on the host;\\n\\n**–**\\nSecond, path of the container where the volume will be mounted;\\n**–** Third (optional), a comma separated of option in the format you would pass to mount command, such as\\n\\nrw .\\n\\nAn example of how you can create a Docker container and bind mount a host directory:\\n\\n$ docker run -d \\\\\\n\\n--name web-server \\\\\\n\\n--mount type=bind,source=\"$(pwd)\",target=/app \\\\\\n\\nubuntu/apache2\\n\\n6f5378e34d6c6811702e16d047a5a80f18adbd9d8a14b11050ae3c3353bf8d2a\\n\\nAfter that, you can inspect your container to check for the bind mount:\\n\\n$ docker inspect web-server --format \\'{{ json .Mounts }}\\' | jq .\\n\\n[\\n\\n{\\n\\n\"Type\": \"bind\",\\n\\n\"Source\": \"/root\",',\n",
       " '\"Destination\": \"/app\",\\n\\n\"Mode\": \"\",\\n\\n\"RW\": true,\\n\\n\"Propagation\": \"rprivate\"\\n\\n}\\n\\n]\\n\\n**Tmpfs**\\n\\nTmpfs mounts allow users to store data temporarily in RAM memory, not in the host’s storage (via bind mount or\\nvolume) or in the container’s writable layer (with the help of storage drivers). When the container stops, the tmpfs\\nmount will be removed and the data will not be persisted in any storage.\\n\\nThis is ideal for accessing credentials or security-sensitive information. The downside is that a tmpfs mount cannot\\nbe shared with multiple containers.\\n\\nTmpfs mounts can be managed via the Docker CLI with the following two options:\\n\\n - --mount : it accepts multiple key-value pairs ( <key>=<value> ). This is the preferred option to use.\\n\\n**–** type : for volumes it will always be tmpfs ;\\n**–** destination, dst or target : container’s directory to be mounted;\\n**–** tmpfs-size and tmpfs-mode [options (optional). For a full list see the Docker documentation.](https://docs.docker.com/storage/tmpfs/#specify-tmpfs-options)\\n\\n - --tmpfs : it accepts no configurable options, just mount the tmpfs for a standalone container.\\n\\nAn example of how you can create a Docker container and mount a tmpfs:\\n\\n$ docker run --name web-server -d \\\\\\n\\n--mount type=tmpfs,target=/app \\\\\\n\\nubuntu/apache2\\n\\n03483cc28166fc5c56317e4ee71904941ec5942071e7c936524f74d732b6a24c\\n\\nAfter that, you can inspect your container to check for the tmpfs mount:\\n\\n$ docker inspect web-server --format \\'{{ json .Mounts }}\\' | jq .\\n\\n[\\n\\n{\\n\\n\"Type\": \"tmpfs\",\\n\\n\"Source\": \"\",\\n\\n\"Destination\": \"/app\",\\n\\n\"Mode\": \"\",\\n\\n\"RW\": true,\\n\\n\"Propagation\": \"\"\\n\\n234\\n\\n\\n-----\\n\\n}\\n\\n]\\n\\n**Storage drivers**',\n",
       " 'Storage drivers are used to store image layers and to store data in the writable layer of a container. In general, storage\\ndrivers are implemented trying to optimise the use of space, but write speed might be lower than native filesystem\\nperformance depending on the driver in use. To better understand the options and make informed decisions, take a\\n[look at the Docker documentation on how layers, images and containers work.](https://docs.docker.com/storage/storagedriver/#images-and-layers)\\n\\nThe default storage driver is the overlay2 which is backed by OverlayFS . This driver is recommended by upstream for\\nuse in production systems. The following storage drivers are available and are supported in Ubuntu (as at the time of\\nwriting):\\n\\n - **OverlayFS** : it is a modern union filesystem. The Linux kernel driver is called OverlayFS and the Docker storage\\ndriver is called overlay2 . **This is the recommended driver** .\\n\\n - **ZFS** : it is a next generation filesystem that supports many advanced storage technologies such as volume\\nmanagement, snapshots, checksumming, compression and deduplication, replication and more. The Docker\\nstorage driver is called zfs .\\n\\n - **Btrfs** : it is a copy-on-write filesystem included in the Linux kernel mainline. The Docker storage driver is called\\n\\nbtrfs .\\n\\n - **Device Mapper** : it is a kernel-based framework that underpins many advanced volume management technologies on Linux. The Docker storage driver is called devicemapper .\\n\\n - **VFS** : it is not a union filesystem, instead, each layer is a directory on disk, and there is no copy-on-write support.\\nTo create a new layer, a “deep copy” is done of the previous layer.',\n",
       " 'This driver does not perform well compared\\nto the others, however, it is robust, stable and works in any environment. The Docker storage driver is called\\n\\nvfs .\\n\\nIf you want to use a different storage driver based on your specific requirements, you can add it to /etc/docker/daemon.json\\nlike in the following example:\\n\\n{\\n\\n\"storage-driver\": \"vfs\"\\n\\n}\\n\\nThe storage drivers accept some options via storage-opts [, check the storage driver documentation for more information.](https://docs.docker.com/storage/storagedriver/)\\nKeep in mind that this is a JSON file and all lines should end with a comma (, ) except the last one.\\n\\nBefore changing the configuration above and restarting the daemon, make sure that the specified filesystem (zfs, btrfs,\\ndevice mapper) is mounted in /var/lib/docker . Otherwise, if you configure the Docker daemon to use a storage\\ndriver different from the filesystem backing /var/lib/docker a failure will happen. The Docker daemon expects that\\n\\n/var/lib/docker is correctly set up when it starts.\\n## **Networking**\\n\\nNetworking in the context of containers refers to the ability of containers to communicate with each other and with\\nnon-Docker workloads. The Docker networking subsystem was implemented in a pluggable way, and we have different\\nnetwork drivers available to be used in different scenarios:\\n\\n - **Bridge** : This is the default network driver. This is widely used when containers need to communicate among\\nthemselves in the same host.\\n\\n - **Overlay** : It is used to make containers managed by different docker daemons (different hosts) communicate\\namong themselves.',\n",
       " ' - **Host** : It is used when the networking isolation between the container and the host is not desired, the container\\nwill use the host’s networking capabilities directly.\\n\\n - **IPvlan** : It is used to provide full control over the both IPv4 and IPv6 addressing.\\n\\n - **Macvlan** : It is used to allow the assignment of Mac addresses to containers, making them appear as a physical\\ndevice in the network.\\n\\n - **None** : It is used to make the container completely isolated from the host.\\n\\nThis is how you can create a user-defined network using the Docker CLI:\\n\\n# create network\\n\\n$ docker network create --driver bridge my-net\\n\\nD84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\\n\\n# list networks\\n\\n$ docker network ls\\n\\nNETWORK ID NAME DRIVER SCOPE\\n\\n235\\n\\n\\n-----\\n\\n1f55a8891c4a bridge bridge local\\n\\n9ca94be2c1a0 host host local\\n\\nd84efaca11d6 my-net bridge local\\n\\n5d300e6a07b1 none null local\\n\\n# inspect the network we created\\n\\n$ docker network inspect my-net\\n\\n[\\n\\n{\\n\\n\"Name\": \"my-net\",\\n\\n\"Id\": \"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\",\\n\\n\"Created\": \"2023-10-25T22:18:52.972569338Z\",\\n\\n\"Scope\": \"local\",\\n\\n\"Driver\": \"bridge\",\\n\\n\"EnableIPv6\": false,\\n\\n\"IPAM\": {\\n\\n\"Driver\": \"default\",\\n\\n\"Options\": {},\\n\\n\"Config\": [\\n\\n{\\n\\n\"Subnet\": \"172.18.0.0/16\",\\n\\n\"Gateway\": \"172.18.0.1\"\\n\\n}\\n\\n]\\n\\n},\\n\\n\"Internal\": false,\\n\\n\"Attachable\": false,\\n\\n\"Ingress\": false,\\n\\n\"ConfigFrom\": {\\n\\n\"Network\": \"\"\\n\\n},\\n\\n\"ConfigOnly\": false,\\n\\n\"Containers\": {},\\n\\n\"Options\": {},\\n\\n\"Labels\": {}\\n\\n}\\n\\n]\\n\\nContainers can connect to a defined network when they are created (via docker run ) or can be connected to it at any\\ntime of its lifecycle:\\n\\n$ docker run -d --name c1 --network my-net ubuntu/apache2',\n",
       " 'C7aa78f45ce3474a276ca3e64023177d5984b3df921aadf97e221da8a29a891e\\n\\n$ docker inspect c1 --format \\'{{ json .NetworkSettings }}\\' | jq .\\n\\n{\\n\\n\"Bridge\": \"\",\\n\\n\"SandboxID\": \"ee1cc10093fdfdf5d4a30c056cef47abbfa564e770272e1e5f681525fdd85555\",\\n\\n\"HairpinMode\": false,\\n\\n\"LinkLocalIPv6Address\": \"\",\\n\\n\"LinkLocalIPv6PrefixLen\": 0,\\n\\n\"Ports\": {\\n\\n\"80/tcp\": null\\n\\n},\\n\\n\"SandboxKey\": \"/var/run/docker/netns/ee1cc10093fd\",\\n\\n\"SecondaryIPAddresses\": null,\\n\\n\"SecondaryIPv6Addresses\": null,\\n\\n\"EndpointID\": \"\",\\n\\n\"Gateway\": \"\",\\n\\n\"GlobalIPv6Address\": \"\",\\n\\n\"GlobalIPv6PrefixLen\": 0,\\n\\n\"IPAddress\": \"\",\\n\\n\"IPPrefixLen\": 0,\\n\\n\"IPv6Gateway\": \"\",\\n\\n\"MacAddress\": \"\",\\n\\n\"Networks\": {\\n\\n236\\n\\n\\n-----\\n\\n\"my-net\": {\\n\\n\"IPAMConfig\": null,\\n\\n\"Links\": null,\\n\\n\"Aliases\": [\\n\\n\"c7aa78f45ce3\"\\n\\n],\\n\\n\"NetworkID\": \"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\",\\n\\n\"EndpointID\": \"1cb76d44a484d302137bb4b042c8142db8e931e0c63f44175a1aa75ae8af9cb5\",\\n\\n\"Gateway\": \"172.18.0.1\",\\n\\n\"IPAddress\": \"172.18.0.2\",\\n\\n\"IPPrefixLen\": 16,\\n\\n\"IPv6Gateway\": \"\",\\n\\n\"GlobalIPv6Address\": \"\",\\n\\n\"GlobalIPv6PrefixLen\": 0,\\n\\n\"MacAddress\": \"02:42:ac:12:00:02\",\\n\\n\"DriverOpts\": null\\n\\n}\\n\\n}\\n\\n}\\n\\n# make a running container connect to the network\\n\\n$ docker run -d --name c2 ubuntu/nginx\\n\\nFea22fbb6e3685eae28815f3ad8c8a655340ebcd6a0c13f3aad0b45d71a20935\\n\\n$ docker network connect my-net c2\\n\\n$ docker inspect c2 --format \\'{{ json .NetworkSettings }}\\' | jq .\\n\\n{\\n\\n\"Bridge\": \"\",\\n\\n\"SandboxID\": \"82a7ea6efd679dffcc3e4392e0e5da61a8ccef33dd78eb5381c9792a4c01f366\",\\n\\n\"HairpinMode\": false,\\n\\n\"LinkLocalIPv6Address\": \"\",\\n\\n\"LinkLocalIPv6PrefixLen\": 0,\\n\\n\"Ports\": {\\n\\n\"80/tcp\": null\\n\\n},\\n\\n\"SandboxKey\": \"/var/run/docker/netns/82a7ea6efd67\",\\n\\n\"SecondaryIPAddresses\": null,',\n",
       " '\"SecondaryIPv6Addresses\": null,\\n\\n\"EndpointID\": \"490c15cf3bcb149dd8649e3ac96f71addd13f660b4ec826dc39e266184b3f65b\",\\n\\n\"Gateway\": \"172.17.0.1\",\\n\\n\"GlobalIPv6Address\": \"\",\\n\\n\"GlobalIPv6PrefixLen\": 0,\\n\\n\"IPAddress\": \"172.17.0.3\",\\n\\n\"IPPrefixLen\": 16,\\n\\n\"IPv6Gateway\": \"\",\\n\\n\"MacAddress\": \"02:42:ac:11:00:03\",\\n\\n\"Networks\": {\\n\\n\"bridge\": {\\n\\n\"IPAMConfig\": null,\\n\\n\"Links\": null,\\n\\n\"Aliases\": null,\\n\\n\"NetworkID\": \"1f55a8891c4a523a288aca8881dae0061f9586d5d91c69b3a74e1ef3ad1bfcf4\",\\n\\n\"EndpointID\": \"490c15cf3bcb149dd8649e3ac96f71addd13f660b4ec826dc39e266184b3f65b\",\\n\\n\"Gateway\": \"172.17.0.1\",\\n\\n\"IPAddress\": \"172.17.0.3\",\\n\\n\"IPPrefixLen\": 16,\\n\\n\"IPv6Gateway\": \"\",\\n\\n\"GlobalIPv6Address\": \"\",\\n\\n\"GlobalIPv6PrefixLen\": 0,\\n\\n\"MacAddress\": \"02:42:ac:11:00:03\",\\n\\n\"DriverOpts\": null\\n\\n},\\n\\n\"my-net\": {\\n\\n\"IPAMConfig\": {},\\n\\n\"Links\": null,\\n\\n237\\n\\n\\n-----\\n\\n\"Aliases\": [\\n\\n\"fea22fbb6e36\"\\n\\n],\\n\\n\"NetworkID\": \"d84efaca11d6f643394de31ad8789391e3ddf29d46faecf0661849f5ead239f7\",\\n\\n\"EndpointID\": \"17856b7f6902db39ff6ab418f127d75d8da597fdb8af0a6798f35a94be0cb805\",\\n\\n\"Gateway\": \"172.18.0.1\",\\n\\n\"IPAddress\": \"172.18.0.3\",\\n\\n\"IPPrefixLen\": 16,\\n\\n\"IPv6Gateway\": \"\",\\n\\n\"GlobalIPv6Address\": \"\",\\n\\n\"GlobalIPv6PrefixLen\": 0,\\n\\n\"MacAddress\": \"02:42:ac:12:00:03\",\\n\\n\"DriverOpts\": {}\\n\\n}\\n\\n}\\n\\n}\\n\\nThe default network created by the Docker daemon is called bridge using the bridge network driver. A system\\nadministrator can configure this network by editing /etc/docker/daemon.json :\\n\\n{\\n\\n\"bip\": \"192.168.1.1/24\",\\n\\n\"fixed-cidr\": \"192.168.1.0/25\",\\n\\n\"fixed-cidr-v6\": \"2001:db8::/64\",\\n\\n\"mtu\": 1500,\\n\\n\"default-gateway\": \"192.168.1.254\",\\n\\n\"default-gateway-v6\": \"2001:db8:abcd::89\",\\n\\n\"dns\": [\"10.20.1.2\",\"10.20.1.3\"]\\n\\n}',\n",
       " 'After deciding how you are going to manage the network and selecting the most appropriate driver, there are some\\nspecific deployment details that a system administrator has to bear in mind when running containers.\\n\\nExposing ports of any system is always a concern, since it increases the surface for malicious attacks. For containers,\\nwe also need to be careful, analysing whether we really need to publish ports to the host. For instance, if the goal is to\\nallow containers to access a specific port from another container, there is no need to publish any port to the host. This\\ncan be solved by connecting all the containers to the same network. You should publish ports of a container to the\\nhost only if you want to make it available to non-Docker workloads. When a container is created no port is published\\nto the host, the option --publish (or -p ) should be passed to docker run or docker create listing which port will be\\nexposed and how.\\n\\nThe --publish option of Docker CLI accepts the following options:\\n\\n  - First, the host port that will be used to publish the container’s port. It can also contain the IP address of the\\nhost. For example, 0.0.0.0:8080 .\\n\\n  - Second, the container’s port to be published. For example, 80 .\\n\\n  - Third (optional), the type of port that will be published which can be TCP or UDP. For example, 80/tcp or\\n\\n80/udp .\\n\\nAn example of how to publish port 80 of a container to port 8080 of the host:\\n\\n$ docker run -d --name web-server --publish 8080:80 ubuntu/nginx\\n\\nf451aa1990db7d2c9b065c6158e2315997a56a764b36a846a19b1b96ce1f3910\\n\\n$ docker inspect web-server --format \\'{{ json .NetworkSettings.Ports }}\\' | jq .\\n\\n{\\n\\n\"80/tcp\": [\\n\\n{\\n\\n\"HostIp\": \"0.0.0.0\",',\n",
       " '\"HostPort\": \"8080\"\\n\\n},\\n\\n{\\n\\n\"HostIp\": \"::\",\\n\\n\"HostPort\": \"8080\"\\n\\n}\\n\\n]\\n\\n}\\n\\n238\\n\\n\\n-----\\n\\nThe HostIp values are 0.0.0.0 (IPv4) and :: (IPv6), and the service running in the container is accessible to everyone\\nin the network (reaching the host), if you want to publish the port from the container and let the service be available\\njust to the host you can use --publish 127.0.0.1:8080:80 instead. The published port can be TCP or UDP and one\\ncan specify that passing --publish 8080:80/tcp or --publish 8080:80/udp .\\n\\nThe system administrator might also want to manually set the IP address or the hostname of the container. To achieve\\nthis, one can use the --ip (IPv4), --ip6 (IPv6), and --hostname options of the docker network connect command to\\nspecify the desired values.\\n\\nAnother important aspect of networking with containers is the DNS service. By default containers will use the DNS\\nsetting of the host, defined in /etc/resolv.conf . Therefore, if a container is created and connected to the default\\n\\nbridge network it will get a copy of host’s /etc/resolv.conf . If the container is connected to a user-defined network,\\nthen it will use Docker’s embedded DNS server. The embedded DNS server forwards external DNS lookups to the\\nDNS servers configured on the host. In case the system administrator wants to configure the DNS service, the docker\\n\\nrun and docker create commands have options to allow that, such as --dns (IP address of a DNS server) and --dns\\nopt (key-value pair representing a DNS option and its value). For more information, check the manpages of those\\ncommands.\\n## **Logging**',\n",
       " 'Monitoring what is happening in the system is a crucial part of systems administration, and with Docker containers\\nit is no different. Docker provides the logging subsystem (which is pluggable) and there are many drivers that can\\nforward container logs to a file, an external host, a database, or another logging back-end. The logs are basically\\neverything written to STDOUT and STDERR . When building a Docker image, the relevant data should be forwarded to\\nthose I/O stream devices.\\n\\nThe following storage drivers are available (at the time of writing):\\n\\n - **json-file** : it is the default logging driver. It writes logs in a file in JSON format.\\n\\n - **local** : write logs to an internal storage that is optimised for performance and disk use.\\n\\n - **journald** : send logs to systemd journal.\\n\\n - **syslog** : send logs to a syslog server.\\n\\n - **logentries** [: send container logs to the Logentries server.](https://logentries.com/)\\n\\n - **gelf** [: write logs in a Graylog Extended Format which is understood by many tools, such as Graylog, Logstash,](https://www.graylog.org/)\\n[and Fluentd.](https://www.fluentd.org)\\n\\n - **awslogs** [: send container logs to Amazon CloudWatch Logs.](https://aws.amazon.com/cloudwatch/details/#log-monitoring)\\n\\n - **etwlogs** : forward container logs as ETW events. ETW stands for Event Tracing in Windows, and is the common\\nframework for tracing applications in Windows. Not supported in Ubuntu systems.\\n\\n - **fluentd** [: send container logs to the Fluentd collector as structured log data.](https://www.fluentd.org)\\n\\n - **gcplogs** [: send container logs to Google Cloud Logging Logging.](https://cloud.google.com/logging/docs/)',\n",
       " ' - **splunk** [: sends container logs to HTTP Event Collector in Splunk Enterprise and Splunk Cloud.](https://dev.splunk.com/enterprise/docs/devtools/httpeventcollector/)\\n\\nThe default logging driver is json-file, and the system administrator can change it by editing the /etc/docker/daemon.json :\\n\\n{\\n\\n\"log-driver\": \"journald\"\\n\\n}\\n\\nAnother option is specifying the logging driver during container creation time:\\n\\n$ docker run -d --name web-server --log-driver=journald ubuntu/nginx\\n\\n1c08b667f32d8b834f0d9d6320721e07de5f22168cfc8a024d6e388daf486dfa\\n\\n$ docker inspect web-server --format \\'{{ json .HostConfig.LogConfig }}\\' | jq .\\n\\n{\\n\\n\"Type\": \"journald\",\\n\\n\"Config\": {}\\n\\n}\\n\\n$ docker logs web-server\\n\\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\\n\\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\\n\\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\\n\\n/docker-entrypoint.sh: Configuration complete; ready for start up\\n\\nDepending on the driver you might also want to pass some options. You can do that via the CLI, passing --log-opt\\nor in the daemon config file adding the key log-opts . For more information check the logging driver documentation.\\n\\nDocker CLI also provides the docker logs and docker service logs commands which allows one to check for the logs\\nproduced by a given container or service (set of containers) in the host. However, those two commands are functional\\n\\n239\\n\\n\\n-----\\n\\nonly if the logging driver for the containers is json-file, local or journald . They are useful for debugging in general,\\nbut there is the downside of increasing the storage needed in the host.',\n",
       " 'The remote logging drivers are useful to store data in an external service/host, and they also avoid spending more\\ndisk space in the host to store log files. Nonetheless, sometimes, for debugging purposes, it is important to have log\\nfiles locally. Considering that, Docker has a feature called “dual logging”, which is enabled by default, and even if\\nthe system administrator configures a logging driver different from json-file, local and journald, the logs will be\\navailable locally to be accessed via the Docker CLI. If this is not the desired behavior, the feature can be disabled in\\nthe /etc/docker/daemon.json file:\\n\\n{\\n\\n\"log-driver\": \"syslog\",\\n\\n\"log-opts\": {\\n\\n“cache-disabled”: “true”,\\n\\n\"syslog-address\": \"udp://1.2.3.4:1111\"\\n\\n}\\n\\n}\\n\\nThe option cache-disabled is used to disable the “dual logging” feature. If you try to run docker logs with that\\nconfiguration you will get the following error:\\n\\n$ docker logs web-server\\n\\nError response from daemon: configured logging driver does not support reading\\n\\nIn this guide we show how to install and configure isc-kea in Ubuntu 23.04\\n[or greater. Kea is the DHCP server developed by ISC to replace](https://www.isc.org/kea/) isc-dhcp . It is newer and designed for more modern\\nnetwork environments.\\n\\nFor isc-dhcp-server instructions, refer to this guide instead.\\n## **Install isc-kea**\\n\\nAt a terminal prompt, enter the following command to install isc-kea :\\n\\nsudo apt install kea\\n\\nThis will also install a few binary packages, including\\n\\n - kea-dhcp4-server : The IPv4 DHCP server (the one we will configure in this guide).\\n\\n - kea-dhcp6-server : The IPv6 DHCP server.\\n\\n - kea-ctrl-agent : A REST API service for Kea.',\n",
       " ' - kea-dhcp-ddns-server : A Dynamic DNS service to update DNS based on DHCP lease events.\\n\\nSince the kea-ctrl-agent service has some administrative rights to the Kea\\nservices, we need to ensure regular users are not allowed to use the API\\nwithout permissions. Ubuntu does it by requiring user authentication to access\\nthe kea-ctrl-agent [API service (LP: #2007312 has more details on this).](https://bugs.launchpad.net/ubuntu/+source/isc-kea/+bug/2007312)\\n\\nTherefore, the installation process described above will get a debconf “high”\\npriority prompt with 3 options:\\n\\n  - no action (default);\\n\\n  - configure with a random password; or\\n\\n  - configure with a given password.\\n\\nIf there is no password, the kea-ctrl-agent will **not** start.\\n\\nThe password is expected to be in /etc/kea/kea-api-password, with ownership\\n\\nroot:_kea and permissions 0640 . To change it, run dpkg-reconfigure kea-ctrl-agent\\n(which will present the same 3 options from above again), or just edit the file\\nmanually.\\n## **Configure kea-dhcp4**\\n\\nThe kea-dhcp4 service can be configured by editing /etc/kea/kea-dhcp4.conf .\\n\\nMost commonly, what you want to do is let Kea assign an IP address from a\\npre-configured IP address pool. This can be done with settings as follows:\\n\\n{\\n\\n\"Dhcp4\": {\\n\\n240\\n\\n\\n-----\\n\\n\"interfaces-config\": {\\n\\n\"interfaces\": [ \"eth4\" ]\\n\\n},\\n\\n\"control-socket\": {\\n\\n\"socket-type\": \"unix\",\\n\\n\"socket-name\": \"/run/kea/kea4-ctrl-socket\"\\n\\n},\\n\\n\"lease-database\": {\\n\\n\"type\": \"memfile\",\\n\\n\"lfc-interval\": 3600\\n\\n},\\n\\n\"valid-lifetime\": 600,\\n\\n\"max-valid-lifetime\": 7200,\\n\\n\"subnet4\": [\\n\\n{\\n\\n\"id\": 1,\\n\\n\"subnet\": \"192.168.1.0/24\",\\n\\n\"pools\": [\\n\\n{\\n\\n\"pool\": \"192.168.1.150 - 192.168.1.200\"\\n\\n}\\n\\n],\\n\\n\"option-data\": [\\n\\n{\\n\\n\"name\": \"routers\",',\n",
       " '\"data\": \"192.168.1.254\"\\n\\n},\\n\\n{\\n\\n\"name\": \"domain-name-servers\",\\n\\n\"data\": \"192.168.1.1, 192.168.1.2\"\\n\\n},\\n\\n{\\n\\n\"name\": \"domain-name\",\\n\\n\"data\": \"mydomain.example\"\\n\\n}\\n\\n]\\n\\n}\\n\\n]\\n\\n}\\n\\n}\\n\\nThis will result in the DHCP server listening on interface “eth4”, giving clients an IP address from the range\\n\\n192.168.1.150 - 192.168.1.200 . It will lease an IP address for 600 seconds if the client doesn’t ask for a specific\\ntime frame. Otherwise the maximum (allowed) lease will be 7200 seconds. The server will also “advise” the client to\\nuse 192.168.1.254 as the default-gateway and 192.168.1.1 and 192.168.1.2 as its DNS servers.\\n\\nAfter changing the config file you can reload the server configuration through kea-shell with the following command\\n(considering you have the kea-ctrl-agent running as described above):\\n\\nkea-shell --host 127.0.0.1 --port 8000 --auth-user kea-api --auth-password $(cat /etc/kea/kea-api-password) \\n-service dhcp4 config-reload\\n\\nThen, press ctrl-d. The server should respond with:\\n\\n[ { \"result\": 0, \"text\": \"Configuration successful.\" } ]\\n\\nmeaning your configuration was received by the server.\\n\\nThe kea-dhcp4-server service logs should contain an entry similar to:\\n\\nDHCP4_DYNAMIC_RECONFIGURATION_SUCCESS dynamic server reconfiguration succeeded with file: /etc/kea/kea\\ndhcp4.conf\\n\\nsignaling that the server was successfully reconfigured.\\n\\nYou can read kea-dhcp4-server service logs with journalctl :\\n\\njournalctl -u kea-dhcp4-server\\n\\n241\\n\\n\\n-----\\n\\nAlternatively, instead of reloading the DHCP4 server configuration through\\n\\nkea-shell, you can restart the kea-dhcp4-service with:\\n\\nsystemctl restart kea-dhcp4-server\\n## **Further reading**',\n",
       " '[• ISC Kea Documentation](https://kb.isc.org/docs/kea-administrator-reference-manual)\\n\\n**Note** :\\n\\nAlthough Ubuntu still supports isc-dhcp-server [, this software is no longer supported by its vendor. It has](https://www.isc.org/blogs/isc-dhcp-eol/)\\n[been replaced by Kea.](https://www.isc.org/kea/)\\n\\nIn this guide we show how to install and configure isc-dhcp-server, which installs the dynamic host configuration\\nprotocol daemon, dhcpd . For isc-kea instructions, refer to this guide instead.\\n## **Install isc-dhcp-server**\\n\\nAt a terminal prompt, enter the following command to install isc-dhcp-server :\\n\\nsudo apt install isc-dhcp-server\\n\\n**Note** :\\nYou can find diagnostic messages from dhcpd in syslog .\\n## **Configure isc-dhcp-server**\\n\\nYou will probably need to change the default configuration by editing /etc/dhcp/dhcpd.conf to suit your needs and\\nparticular configuration.\\n\\nMost commonly, what you want to do is assign an IP address randomly. This can be done with /etc/dhcp/dhcpd.conf\\nsettings as follows:\\n\\n# minimal sample /etc/dhcp/dhcpd.conf\\n\\ndefault-lease-time 600;\\n\\nmax-lease-time 7200;\\n\\nsubnet 192.168.1.0 netmask 255.255.255.0 {\\n\\nrange 192.168.1.150 192.168.1.200;\\n\\noption routers 192.168.1.254;\\n\\noption domain-name-servers 192.168.1.1, 192.168.1.2;\\n\\noption domain-name \"mydomain.example\";\\n\\n}\\n\\nThis will result in the DHCP server giving clients an IP address from the range 192.168.1.150 - 192.168.1.200 . It will\\nlease an IP address for 600 seconds if the client doesn’t ask for a specific time frame. Otherwise the maximum (allowed)\\nlease will be 7200 seconds. The server will also “advise” the client to use 192.168.1.254 as the default-gateway and',\n",
       " '192.168.1.1 and 192.168.1.2 as its DNS servers.\\n\\nYou also may need to edit /etc/default/isc-dhcp-server to specify the interfaces dhcpd should listen to.\\n\\nINTERFACESv4=\"eth4\"\\n\\nAfter changing the config files you need to restart the dhcpd service:\\n\\nsudo systemctl restart isc-dhcp-server.service\\n## **Further reading**\\n\\n[• The isc-dhcp-server Ubuntu Wiki page has more information.](https://help.ubuntu.com/community/isc-dhcp-server)\\n\\n - For more /etc/dhcp/dhcpd.conf [options see the dhcpd.conf man page.](https://manpages.ubuntu.com/manpages/focal/en/man5/dhcpd.conf.5.html)\\n\\n[• ISC dhcp-server](https://www.isc.org/software/dhcp)\\n\\nUbuntu uses timedatectl and timesyncd for synchronising time, and they are installed by default as part of systemd .\\nYou can optionally use chrony to serve the Network Time Protocol.\\n\\nIn this guide, we will show you how to configure these services.\\n\\n242\\n\\n\\n-----\\n\\n**Note** :\\n\\nIf chrony is installed, timedatectl steps back to let chrony handle timekeeping. This ensures that no two\\ntime-syncing services will be in conflict.\\n## Check status of timedatectl\\n\\nThe current status of time and time configuration via timedatectl and timesyncd can be checked with the timedatectl\\n\\nstatus command, which will produce output like this:\\n\\nLocal time: Wed 2023-06-14 12:05:11 BST\\n\\nUniversal time: Wed 2023-06-14 11:05:11 UTC\\n\\nRTC time: Wed 2023-06-14 11:05:11\\n\\nTime zone: Europe/Isle_of_Man (BST, +0100)\\n\\nSystem clock synchronized: yes\\n\\nNTP service: active\\n\\nRTC in local TZ: no\\n\\nIf chrony is running, it will automatically switch to:\\n\\n[...]\\n\\nsystemd-timesyncd.service active: no\\n\\n**Configure** timedatectl',\n",
       " 'By using timedatectl, an admin can control the timezone, how the system clock should relate to the hwclock and\\nwhether permanent synchronisation should be enabled. See man timedatectl for more details.\\n## Check status of timesyncd\\n\\ntimesyncd itself is a normal service, so you can check its status in more detail using:\\n\\nsystemctl status systemd-timesyncd\\n\\nThe output produced will look something like this:\\n\\nsystemd-timesyncd.service - Network Time Synchronization\\n\\nLoaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled)\\n\\nActive: active (running) since Fri 2018-02-23 08:55:46 UTC; 10s ago\\n\\nDocs: man:systemd-timesyncd.service(8)\\n\\nMain PID: 3744 (systemd-timesyn)\\n\\nStatus: \"Synchronized to time server 91.189.89.198:123 (ntp.ubuntu.com).\"\\n\\nTasks: 2 (limit: 4915)\\n\\nCGroup: /system.slice/systemd-timesyncd.service\\n\\n|-3744 /lib/systemd/systemd-timesyncd\\n\\nFeb 23 08:55:46 bionic-test systemd[1]: Starting Network Time Synchronization...\\n\\nFeb 23 08:55:46 bionic-test systemd[1]: Started Network Time Synchronization.\\n\\nFeb 23 08:55:46 bionic-test systemd-timesyncd[3744]: Synchronized to time server 91.189.89.198:123 (ntp.ubuntu.com).\\n\\n**Configure** timesyncd\\n\\nThe server from which to fetch time for timedatectl and timesyncd can be specified in /etc/systemd/timesyncd.conf .\\nAdditional config files can be stored in /etc/systemd/timesyncd.conf.d/ . The entries for NTP= and FallbackNTP= are\\nspace-separated lists. See man timesyncd.conf for more details.\\n## **Next steps**\\n\\nIf you would now like to serve the Network Time Protocol via crony, this guide will walk you through how to install\\nand configure your setup.\\n## **References**',\n",
       " '[• Freedesktop.org info on timedatectl](https://www.freedesktop.org/software/systemd/man/timedatectl.html)\\n\\n[• Freedesktop.org info on systemd-timesyncd service](https://www.freedesktop.org/software/systemd/man/systemd-timesyncd.service.html)\\n\\n[• See the Ubuntu Time wiki page for more information.](https://help.ubuntu.com/community/UbuntuTime)\\n\\n243\\n\\n\\n-----\\n\\ntimesyncd and timedatectl will generally do the right thing in keeping your time in sync. However, if you also want\\nto serve NTP information then you need an NTP server.\\n\\nBetween chrony, the now-deprecated ntpd, and open-ntp, there are plenty of options. The solution we recommend is\\n\\nchrony .\\n\\nThe NTP daemon chronyd calculates the drift and offset of your system clock and continuously adjusts it, so there\\nare no large corrections that could lead to inconsistent logs, for instance. The cost is a little processing power and\\nmemory, but for a modern server this is usually negligible.\\n## Install chronyd\\n\\nTo install chrony, run the following command from a terminal prompt:\\n\\nsudo apt install chrony\\n\\nThis will provide two binaries:\\n\\n - chronyd  - the actual daemon to sync and serve via the Network Time Protocol\\n\\n - chronyc  - command-line interface for the chrony daemon\\n## Configure chronyd\\n\\nFirstly, edit /etc/chrony/chrony.conf to add/remove server lines. By default these servers are configured:\\n\\n# Use servers from the NTP Pool Project. Approved by Ubuntu Technical Board\\n\\n# on 2011-02-08 (LP: #104525). See http://www.pool.ntp.org/join.html for\\n\\n# more information.\\n\\npool 0.ubuntu.pool.ntp.org iburst\\n\\npool 1.ubuntu.pool.ntp.org iburst\\n\\npool 2.ubuntu.pool.ntp.org iburst\\n\\npool 3.ubuntu.pool.ntp.org iburst',\n",
       " 'See man chrony.conf for more details on the configuration options available. After changing any part of the config file\\nyou need to restart chrony, as follows:\\n\\nsudo systemctl restart chrony.service\\n\\nOf the pool, 2.ubuntu.pool.ntp.org and ntp.ubuntu.com also support IPv6, if needed. If you need to force IPv6, there\\nis also ipv6.ntp.ubuntu.com which is not configured by default.\\n## **Enable serving the Network Time Protocol**\\n\\nYou can install chrony (above) and configure special Hardware (below) for a local synchronisation\\nand as-installed that is the default to stay on the secure and conservative side. But if you want to *serve* NTP you\\nneed adapt your configuration.\\n\\nTo enable serving NTP you’ll need to at least set the allow rule. This controls which clients/networks you want chrony\\nto serve NTP to.\\n\\nAn example would be:\\n\\nallow 1.2.3.4\\n\\n[See the section “NTP server” in the man page for more details on how you can control and restrict access to your](http://manpages.ubuntu.com/manpages/jammy/man5/chrony.conf.5.html)\\nNTP server.\\n## View chrony status\\n\\nYou can use chronyc to see query the status of the chrony daemon. For example, to get an overview of the currently\\navailable and selected time sources, run chronyc sources, which provides output like this:\\n\\nMS Name/IP address Stratum Poll Reach LastRx Last sample\\n\\n===============================================================================\\n\\n^+ gamma.rueckgr.at 2 8 377 135 -1048us[-1048us] +/- 29ms\\n\\n^- 2b.ncomputers.org 2 8 377 204 -1141us[-1124us] +/- 50ms\\n\\n^+ www.kashra.com 2 8 377 139 +3483us[+3483us] +/- 18ms\\n\\n^+ stratum2-4.NTP.TechFak.U> 2 8 377 143 -2090us[-2073us] +/- 19ms',\n",
       " '^- zepto.mcl.gg 2 7 377 9 -774us[ -774us] +/- 29ms\\n\\n^- mirrorhost.pw 2 7 377 78 -660us[ -660us] +/- 53ms\\n\\n244\\n\\n\\n-----\\n\\n^- atto.mcl.gg 2 7 377 8 -823us[ -823us] +/- 50ms\\n\\n^- static.140.107.46.78.cli> 2 8 377 9 -1503us[-1503us] +/- 45ms\\n\\n^- 4.53.160.75 2 8 377 137 -11ms[ -11ms] +/- 117ms\\n\\n^- 37.44.185.42 3 7 377 10 -3274us[-3274us] +/- 70ms\\n\\n^- bagnikita.com 2 7 377 74 +3131us[+3131us] +/- 71ms\\n\\n^- europa.ellipse.net 2 8 377 204 -790us[ -773us] +/- 97ms\\n\\n^- tethys.hot-chilli.net 2 8 377 141 -797us[ -797us] +/- 59ms\\n\\n^- 66-232-97-8.static.hvvc.> 2 7 377 206 +1669us[+1686us] +/- 133ms\\n\\n^+ 85.199.214.102 1 8 377 205 +175us[ +192us] +/- 12ms\\n\\n^* 46-243-26-34.tangos.nl 1 8 377 141 -123us[ -106us] +/- 10ms\\n\\n^- pugot.canonical.com 2 8 377 21 -95us[ -95us] +/- 57ms\\n\\n^- alphyn.canonical.com 2 6 377 23 -1569us[-1569us] +/- 79ms\\n\\n^- golem.canonical.com 2 7 377 92 -1018us[-1018us] +/- 31ms\\n\\n^- chilipepper.canonical.com 2 8 377 21 -1106us[-1106us] +/- 27ms\\n\\nYou can also make use of the chronyc sourcestats command, which produces output like this:\\n\\n210 Number of sources = 20\\n\\nName/IP Address NP NR Span Frequency Freq Skew Offset Std Dev\\n\\n==============================================================================\\n\\ngamma.rueckgr.at 25 15 32m -0.007 0.142 -878us 106us\\n\\n2b.ncomputers.org 26 16 35m -0.132 0.283 -1169us 256us\\n\\nwww.kashra.com 25 15 32m -0.092 0.259 +3426us 195us\\n\\nstratum2-4.NTP.TechFak.U> 25 14 32m -0.018 0.130 -2056us 96us\\n\\nzepto.mcl.gg 13 11 21m +0.148 0.196 -683us 66us\\n\\nmirrorhost.pw 6 5 645 +0.117 0.445 -591us 19us\\n\\natto.mcl.gg 21 13 25m -0.069 0.199 -904us 103us\\n\\nstatic.140.107.46.78.cli> 25 18 34m -0.005 0.094 -1526us 78us\\n\\n4.53.160.75 25 10 32m +0.412 0.110 -11ms 84us',\n",
       " '37.44.185.42 24 12 30m -0.983 0.173 -3718us 122us\\n\\nbagnikita.com 17 7 31m -0.132 0.217 +3527us 139us\\n\\neuropa.ellipse.net 26 15 35m +0.038 0.553 -473us 424us\\n\\ntethys.hot-chilli.net 25 11 32m -0.094 0.110 -864us 88us\\n\\n66-232-97-8.static.hvvc.> 20 11 35m -0.116 0.165 +1561us 109us\\n\\n85.199.214.102 26 11 35m -0.054 0.390 +129us 343us\\n\\n46-243-26-34.tangos.nl 25 16 32m +0.129 0.297 -307us 198us\\n\\npugot.canonical.com 25 14 34m -0.271 0.176 -143us 135us\\n\\nalphyn.canonical.com 17 11 1100 -0.087 0.360 -1749us 114us\\n\\ngolem.canonical.com 23 12 30m +0.057 0.370 -988us 229us\\n\\nchilipepper.canonical.com 25 18 34m -0.084 0.224 -1116us 169us\\n\\nCertain chronyc commands are privileged and cannot be run via the network without explicitly allowing them. See\\nthe **Command and monitoring access** section in man chrony.conf for more details. A local admin can use sudo\\nsince this will grant access to the local admin socket /var/run/chrony/chronyd.sock .\\n## **Pulse-Per-Second (PPS) support**\\n\\nChrony supports various PPS types natively. It can use kernel PPS API as well as Precision Time Protocol (PTP)\\nhardware clocks. Most general GPS receivers can be leveraged via GPSD. The latter (and potentially more) can be\\naccessed via **SHM** or via a **socket** (recommended). All of the above can be used to augment chrony with additional\\nhigh quality time sources for better accuracy, jitter, drift, and longer- or shorter-term accuracy. Usually, each kind of\\nclock type is good at one of those, but non-perfect at the others. For more details on configuration see some of the\\nexternal PPS/GPSD resources listed below.\\n\\n**Note** :',\n",
       " 'As of the release of 20.04, there was a bug which - until fixed - you might want to [add this\\ncontent] (https://bugs.launchpad.net/ubuntu/+source/gpsd/+bug/1872175/comments/21) to your\\n/etc/apparmor.d/local/usr.sbin.gpsd‘.\\n\\n**Example configuration for GPSD to feed** chrony\\n\\nFor the installation and setup you will first need to run the following command in your terminal window:\\n\\nsudo apt install gpsd chrony\\n\\nHowever, since you will want to test/debug your setup (especially the GPS reception), you should also install:\\n\\n245\\n\\n\\n-----\\n\\nsudo apt install pps-tools gpsd-clients\\n\\nGPS devices usually communicate via serial interfaces. The most common type these days are USB GPS devices,\\nwhich have a serial converter behind USB. If you want to use one of these devices for PPS then please be aware that\\n[the majority do not signal PPS via USB. Check the GPSD hardware list for details. The examples below were run](https://gpsd.gitlab.io/gpsd/hardware.html)\\nwith a Navisys GR701-W.\\n\\nWhen plugging in such a device (or at boot time) dmesg should report a serial connection of some sort, as in this\\nexample:\\n\\n[ 52.442199] usb 1-1.1: new full-speed USB device number 3 using xhci_hcd\\n\\n[ 52.546639] usb 1-1.1: New USB device found, idVendor=067b, idProduct=2303, bcdDevice= 4.00\\n\\n[ 52.546654] usb 1-1.1: New USB device strings: Mfr=1, Product=2, SerialNumber=0\\n\\n[ 52.546665] usb 1-1.1: Product: USB-Serial Controller D\\n\\n[ 52.546675] usb 1-1.1: Manufacturer: Prolific Technology Inc.\\n\\n[ 52.602103] usbcore: registered new interface driver usbserial_generic\\n\\n[ 52.602244] usbserial: USB Serial support registered for generic\\n\\n[ 52.609471] usbcore: registered new interface driver pl2303',\n",
       " '[ 52.609503] usbserial: USB Serial support registered for pl2303\\n\\n[ 52.609564] pl2303 1-1.1:1.0: pl2303 converter detected\\n\\n[ 52.618366] usb 1-1.1: pl2303 converter now attached to ttyUSB0\\n\\nWe see in this example that the device appeared as ttyUSB0 . So that chrony later accepts being fed time information\\nby this device, we have to set it up in /etc/chrony/chrony.conf (please replace USB0 with whatever applies to your\\nsetup):\\n\\nrefclock SHM 0 refid GPS precision 1e-1 offset 0.9999 delay 0.2\\n\\nrefclock SOCK /var/run/chrony.ttyUSB0.sock refid PPS\\n\\nNext, we need to restart chrony to make the socket available and have it waiting.\\n\\nsudo systemctl restart chrony\\n\\nWe then need to tell gpsd which device to manage. Therefore, in /etc/default/gpsd we set:\\n\\nDEVICES=\"/dev/ttyUSB0\"\\n\\nIt should be noted that since the *default* use-case of gpsd is, well, for *gps position tracking*, it will normally not consume\\nany CPU since it is just waiting on a **socket** for clients. Furthermore, the client will tell gpsd what it requests, and\\n\\ngpsd will only provide what is asked for.\\n\\nFor the use case of gpsd as a PPS-providing-daemon, you want to set the option to:\\n\\n  - Immediately start (even without a client connected). This can be set in GPSD_OPTIONS of /etc/default/gpsd :\\n\\n**–** GPSD_OPTIONS=\"-n\"\\n\\n  - Enable the service itself and not wait for a client to reach the socket in the future:\\n\\n**–**\\nsudo systemctl enable /lib/systemd/system/gpsd.service\\n\\nRestarting gpsd will now initialize the PPS from GPS and in dmesg you will see:\\n\\npps_ldisc: PPS line discipline registered\\n\\npps pps0: new PPS source usbserial0\\n\\npps pps0: source \"/dev/ttyUSB0\" added',\n",
       " 'If you have multiple PPS sources, the tool ppsfind may be useful to help identify which PPS belongs to which GPS.\\nIn our example, the command sudo ppsfind /dev/ttyUSB0 would return the following:\\n\\npps0: name=usbserial0 path=/dev/ttyUSB0\\n\\nNow we have completed the basic setup. To proceed, we now need our GPS to get a lock. Tools like cgps or gpsmon\\nneed to report a 3D “fix” in order to provide accurate data. Let’s run the command cgps, which in our case returns:\\n\\n...\\n\\n│Status: 3D FIX (7 secs) ...\\n\\nYou would then want to use ppstest in order to check that you are really receiving PPS data. So, let us run the\\ncommand sudo ppstest /dev/pps0, which will produce an output like this:\\n\\ntrying PPS source \"/dev/pps0\"\\n\\nfound PPS source \"/dev/pps0\"\\n\\nok, found 1 source(s), now start fetching data...\\n\\n246\\n\\n\\n-----\\n\\nsource 0 - assert 1588140739.099526246, sequence: 69 - clear 1588140739.999663721, sequence: 70\\n\\nsource 0 - assert 1588140740.099661485, sequence: 70 - clear 1588140739.999663721, sequence: 70\\n\\nsource 0 - assert 1588140740.099661485, sequence: 70 - clear 1588140740.999786664, sequence: 71\\n\\nsource 0 - assert 1588140741.099792447, sequence: 71 - clear 1588140740.999786664, sequence: 71\\n\\nOk, gpsd is now running, the GPS reception has found a fix, and it has fed this into chrony . Let’s check on that from\\nthe point of view of chrony .\\n\\nInitially, before gpsd has started or before it has a lock, these sources will be new and “untrusted” - they will be\\nmarked with a “?” as shown in the example below. If your devices remain in the “?” state (even after some time)\\nthen gpsd is not feeding any data to chrony and you will need to debug why.\\n\\nchronyc> sources\\n\\n210 Number of sources = 10',\n",
       " 'MS Name/IP address Stratum Poll Reach LastRx Last sample\\n\\n===============================================================================\\n\\n#? GPS 0 4 0 - +0ns[ +0ns] +/- 0ns\\n\\n#? PPS 0 4 0 - +0ns[ +0ns] +/- 0ns\\n\\nOver time, chrony will classify all of the unknown sources as “good” or “bad”.\\nIn the example below, the raw GPS had too much deviation (± 200ms) but the PPS is good (± 63us).\\n\\nchronyc> sources\\n\\n210 Number of sources = 10\\n\\nMS Name/IP address Stratum Poll Reach LastRx Last sample\\n\\n===============================================================================\\n\\n#x GPS 0 4 177 24 -876ms[ -876ms] +/- 200ms\\n\\n#- PPS 0 4 177 21 +916us[ +916us] +/- 63us\\n\\n^- chilipepper.canonical.com 2 6 37 53 +33us[ +33us] +/- 33ms\\n\\nFinally, after a while it used the hardware PPS input (as it was better):\\n\\nchronyc> sources\\n\\n210 Number of sources = 10\\n\\nMS Name/IP address Stratum Poll Reach LastRx Last sample\\n\\n===============================================================================\\n\\n#x GPS 0 4 377 20 -884ms[ -884ms] +/- 200ms\\n\\n#* PPS 0 4 377 18 +6677ns[ +52us] +/- 58us\\n\\n^- alphyn.canonical.com 2 6 377 20 -1303us[-1258us] +/- 114ms\\n\\nThe PPS might also be OK – but used in a combined way with the selected server, for example. See man chronyc for\\nmore details about how these combinations can look:\\n\\nchronyc> sources\\n\\n210 Number of sources = 11\\n\\nMS Name/IP address Stratum Poll Reach LastRx Last sample\\n\\n===============================================================================\\n\\n#? GPS 0 4 0 - +0ns[ +0ns] +/- 0ns\\n\\n#+ PPS 0 4 377 22 +154us[ +154us] +/- 8561us\\n\\n^* chilipepper.canonical.com 2 6 377 50 -353us[ -300us] +/- 44ms',\n",
       " 'If you’re wondering if your SHM-based GPS data is any good, you can check on that as well. chrony will not only tell\\nyou if the data is classified as good or bad – using sourcestats you can also check the details:\\n\\nchronyc> sourcestats\\n\\n210 Number of sources = 10\\n\\nName/IP Address NP NR Span Frequency Freq Skew Offset Std Dev\\n\\n==============================================================================\\n\\nGPS 20 9 302 +1.993 11.501 -868ms 1208us\\n\\nPPS 6 3 78 +0.324 5.009 +3365ns 41us\\n\\ngolem.canonical.com 15 10 783 +0.859 0.509 -750us 108us\\n\\nYou can also track the raw data that gpsd or other ntpd -compliant reference clocks are sending via shared memory by\\nusing ntpshmmon . Let us run the command sudo ntpshmmon -o, which should provide the following output:\\n\\nntpshmmon: version 3.20\\n\\n# Name Offset Clock Real L Prc\\n\\nsample NTP1 0.000223854 1588265805.000223854 1588265805.000000000 0 -10\\n\\nsample NTP0 0.125691783 1588265805.125999851 1588265805.000308068 0 -20\\n\\n247\\n\\n\\n-----\\n\\nsample NTP1 0.000349341 1588265806.000349341 1588265806.000000000 0 -10\\n\\nsample NTP0 0.130326636 1588265806.130634945 1588265806.000308309 0 -20\\n\\nsample NTP1 0.000485216 1588265807.000485216 1588265807.000000000 0 -10\\n## **NTS Support**\\n\\n[In Chrony 4.0 (which first appeared in Ubuntu 21.04 Hirsute) support for Network Time Security “NTS” was added.](https://www.networktimesecurity.org/)\\n\\n**NTS server**\\n\\nTo set up your server with NTS you’ll need certificates so that the server can authenticate itself and, based on that,\\nallow the encryption and verification of NTP traffic.\\n\\nIn addition to the allow statement that any chrony (while working as an NTP server) needs there are two mandatory',\n",
       " 'config entries that will be needed. Example certificates for those entries would look like:\\n\\nntsservercert /etc/chrony/fullchain.pem\\n\\nntsserverkey /etc/chrony/privkey.pem\\n\\nIt is important to note that for isolation reasons chrony, by default, runs as user and group _chrony . Therefore you\\nneed to grant access to the certificates for that user, by running the following command:.\\n\\nsudo chown _chrony:_chrony /etc/chrony/*.pem\\n\\nThen restart chrony with systemctl restart chrony and it will be ready to provide NTS-based time services.\\n\\nA running chrony server measures various statistics. One of them counts the number of NTS connections that were\\nestablished (or dropped) – we can check this by running sudo chronyc -N serverstats, which shows us the statistics:\\n\\nNTP packets received : 213\\n\\nNTP packets dropped : 0\\n\\nCommand packets received : 117\\n\\nCommand packets dropped : 0\\n\\nClient log records dropped : 0\\n\\nNTS-KE connections accepted: 2\\n\\nNTS-KE connections dropped : 0\\n\\nAuthenticated NTP packets : 197\\n\\nThere is also a per-client statistic which can be enabled by the -p option of the clients command.\\n\\nsudo chronyc -N clients -k\\n\\nThis provides output in the following form:\\n\\nHostname NTP Drop Int IntL Last NTS-KE Drop Int Last\\n\\n===============================================================================\\n\\n10.172.196.173 197 0 10   - 595 2 0 5 48h\\n\\n...\\n\\n[For more complex scenarios there are many more advanced options for configuring NTS. These are documented in the](https://manpages.ubuntu.com/manpages/impish/man5/chrony.conf.5.html)\\n\\nchrony [man page.](https://manpages.ubuntu.com/manpages/impish/man5/chrony.conf.5.html)\\n\\n**Note** : *About certificate placement*',\n",
       " 'Chrony, by default, is isolated via AppArmor and uses a number of protect* features of systemd . Due\\nto that, there are not many paths chrony can access for the certificates. But /etc/chrony/* is allowed as\\nread-only and that is enough.\\nCheck /etc/apparmor.d/usr.sbin.chronyd if you want other paths or allow custom paths in /etc/apparmor.d/local/usr.sbin\\n\\n**NTS client**\\n\\nThe client needs to specify server as usual ( pool directives do not work with NTS). Afterwards, the server address\\noptions can be listed and it is there that nts can be added. For example:\\n\\nserver <server-fqdn-or-IP> iburst nts\\n\\nOne can check the authdata of the connections established by the client using sudo chronyc -N authdata, which will\\nprovide the following information:\\n\\nName/IP address Mode KeyID Type KLen Last Atmp NAK Cook CLen\\n\\n=========================================================================\\n\\n<server-fqdn-or-ip> NTS 1 15 256 48h 0 0 8 100\\n\\n248\\n\\n\\n-----\\n\\n[Again, there are more advanced options documented in the man page. Common use cases are specifying an explicit](https://manpages.ubuntu.com/manpages/impish/man5/chrony.conf.5.html)\\ntrusted certificate.\\n\\n**Bad Clocks and secure time syncing - “A Chicken and Egg” problem**\\nThere is one problem with systems that have very bad clocks. NTS is based on TLS, and TLS needs\\na reasonably correct clock. Due to that, an NTS-based sync might fail. On hardware affected by this\\nproblem, one can consider using the nocerttimecheck option which allows the user to set the number of\\ntimes that the time can be synced without checking validation and expiration.\\n## **References**\\n\\n[• Chrony FAQ](https://chrony.tuxfamily.org/faq.html)',\n",
       " '[• ntp.org: home of the Network Time Protocol project](http://www.ntp.org/)\\n\\n[• pool.ntp.org: project of virtual cluster of timeservers](http://www.pool.ntp.org/)\\n\\n[• Freedesktop.org info on timedatectl](https://www.freedesktop.org/software/systemd/man/timedatectl.html)\\n\\n[• Freedesktop.org info on systemd-timesyncd service](https://www.freedesktop.org/software/systemd/man/systemd-timesyncd.service.html)\\n\\n[• Feeding chrony from GPSD](https://gpsd.gitlab.io/gpsd/gpsd-time-service-howto.html#_feeding_chrony_from_gpsd)\\n\\n[• See the Ubuntu Time wiki page for more information.](https://help.ubuntu.com/community/UbuntuTime)\\n\\nSince DPDK is *[just](https://ubuntu.com/server/docs/network-dpdk)* a library, it doesn’t do a lot on its own so it depends on emerging projects making use of it. One\\nconsumer of the library that is already part of Ubuntu is Open vSwitch with DPDK (OvS-DPDK) support in the\\npackage openvswitch-switch-dpdk .\\n\\nHere is a brief example of how to install and configure a basic Open vSwitch using DPDK for later use via libvirt / qemu\\nkvm .\\n\\nsudo apt-get install openvswitch-switch-dpdk\\n\\nsudo update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk\\n\\novs-vsctl set Open_vSwitch . \"other_config:dpdk-init=true\"\\n\\n# run on core 0 only\\n\\novs-vsctl set Open_vSwitch . \"other_config:dpdk-lcore-mask=0x1\"\\n\\n# Allocate 2G huge pages (not Numa node aware)\\n\\novs-vsctl set Open_vSwitch . \"other_config:dpdk-alloc-mem=2048\"\\n\\n# limit to one whitelisted device\\n\\novs-vsctl set Open_vSwitch . \"other_config:dpdk-extra=--pci-whitelist=0000:04:00.0\"\\n\\nsudo service openvswitch-switch restart\\n\\n**Remember** :',\n",
       " 'You need to assign devices to DPDK-compatible drivers before restarting – see the DPDK section on\\n[unassigning the default kernel drivers.](https://ubuntu.com/server/docs/network-dpdk/#heading--unassign-default-kernel-drivers)\\n\\nPlease note that the section _dpdk-alloc-mem=2048_ in the above example is the most basic non-uniform memory access\\n(NUMA) setup for a single socket system. If you have multiple sockets you may want to define how the memory should\\n[be split among them. More details about these options are outlined in Open vSwitch setup.](http://docs.openvswitch.org/en/latest/intro/install/dpdk/#setup-ovs)\\n## **Attach DPDK ports to Open vSwitch**\\n\\nThe Open vSwitch you started above supports all the same port types as Open vSwitch usually does, *plus* DPDK port\\ntypes. The following example shows how to create a bridge and – instead of a normal external port – add an external\\nDPDK port to it. When doing so you can specify the associated device.\\n\\novs-vsctl add-br ovsdpdkbr0 -- set bridge ovsdpdkbr0 datapath_type=netdev\\n\\novs-vsctl add-port ovsdpdkbr0 dpdk0 -- set Interface dpdk0 type=dpdk \"options:dpdk-devargs=${OVSDEV_PCIID}\"\\n\\nYou can tune this further by setting options:\\n\\novs-vsctl set Interface dpdk0 \"options:n_rxq=2\"\\n## **Open vSwitch DPDK to KVM guests**\\n\\nIf you are not building some sort of software-defined networking (SDN) switch or NFV on top of DPDK, it is very likely\\nthat you want to forward traffic to KVM guests. The good news is; with the new qemu / libvirt / dpdk / openvswitch\\nversions in Ubuntu this is no longer about manually appending a command line string. This section demonstrates a\\nbasic setup to connect a KVM guest to an Open vSwitch DPDK instance.',\n",
       " '249\\n\\n\\n-----\\n\\nThe recommended way to get to a KVM guest is using vhost_user_client . This will cause OvS-DPDK to connect\\nto a socket created by QEMU. In this way, we can avoid old issues like “guest failures on OvS restart”. Here is an\\nexample of how to add such a port to the bridge you created above.\\n\\novs-vsctl add-port ovsdpdkbr0 vhost-user-1 -- set Interface vhost-user-1 type=dpdkvhostuserclient \"options:vhost\\nserver-path=/var/run/vhostuserclient/vhost-user-client-1\"\\n\\nThis will connect to the specified path that has to be created by a guest listening for it.\\n\\nTo let libvirt / kvm consume this socket and create a guest VirtIO network device for it, add the following snippet to\\nyour guest definition as the network definition.\\n\\n<interface type=\\'vhostuser\\'>\\n\\n<source type=\\'unix\\'\\n\\npath=\\'/var/run/vhostuserclient/vhost-user-client-1\\'\\n\\nmode=\\'server\\'/>\\n\\n<model type=\\'virtio\\'/>\\n\\n</interface>\\n## **Tuning Open vSwitch-DPDK**\\n\\nDPDK has plenty of options – in combination with Open vSwitch-DPDK the two most commonly used are:\\n\\novs-vsctl set Open_vSwitch . other_config:n-dpdk-rxqs=2\\n\\novs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x6\\n\\nThe first line selects how many Rx Queues are to be used for each DPDK interface, while the second controls how\\nmany poll mode driver (PMD) threads to run (and where to run them). The example above will use two Rx Queues,\\nand run PMD threads on CPU 1 and 2.\\n\\n**See also** :\\n\\nCheck the links to “EAL Command-line Options” and “Open vSwitch DPDK installation” at the end of\\nthis document for more information.\\n\\nAs usual with tunings, you need to know your system and workload really well - so please verify any tunings with',\n",
       " 'workloads matching your real use case.\\n## **Support and troubleshooting**\\n\\nDPDK is a fast-evolving project. In any search for support and/or further guides, we highly recommended first\\nchecking to see if they apply to the current version.\\n\\nYou can check if your issues is known on:\\n\\n[• DPDK Mailing Lists](http://dpdk.org/ml)\\n\\n[• For OpenVswitch-DPDK OpenStack Mailing Lists](http://openvswitch.org/mlists)\\n\\n[• Known issues in DPDK Launchpad Area](https://bugs.launchpad.net/ubuntu/+source/dpdk)\\n\\n  - Join the IRC channels #DPDK or #openvswitch on freenode.\\n\\nIssues are often due to missing small details in the general setup. Later on, these missing details cause problems which\\ncan be hard to track down to their root cause.\\n\\nA common case seems to be the “could not open network device dpdk0 (No such device)” issue. This occurs rather\\nlate when setting up a port in Open vSwitch with DPDK, but the root cause (most of the time) is very early in the\\nsetup and initialisation. Here is an example of how proper initialiasation of a device looks - this can be found in the\\n\\nsyslog/journal when starting Open vSwitch with DPDK enabled.\\n\\novs-ctl[3560]: EAL: PCI device 0000:04:00.1 on NUMA socket 0\\n\\novs-ctl[3560]: EAL: probe driver: 8086:1528 rte_ixgbe_pmd\\n\\novs-ctl[3560]: EAL: PCI memory mapped at 0x7f2140000000\\n\\novs-ctl[3560]: EAL: PCI memory mapped at 0x7f2140200000\\n\\nIf this is missing, either by ignored cards, failed initialisation or other reasons, later on there will be no DPDK device\\nto refer to. Unfortunately, the logging is spread across syslog/journal and the openvswitch log. To enable some',\n",
       " 'cross-checking, here is an example of what can be found in these logs, relative to the entered command.\\n\\n#Note: This log was taken with dpdk 2.2 and openvswitch 2.5 but still looks quite similar (a bit extended) these days\\n\\nCaptions:\\n\\nCMD: that you enter\\n\\nSYSLOG: (Inlcuding EAL and OVS Messages)\\n\\nOVS-LOG: (Openvswitch messages)\\n\\n250\\n\\n\\n-----\\n\\n#PREPARATION\\n\\nBind an interface to DPDK UIO drivers, make Hugepages available, enable DPDK on OVS\\n\\nCMD: sudo service openvswitch-switch restart\\n\\nSYSLOG:\\n\\n2016-01-22T08:58:31.372Z|00003|daemon_unix(monitor)|INFO|pid 3329 died, killed (Terminated), exiting\\n\\n2016-01-22T08:58:33.377Z|00002|vlog|INFO|opened log file /var/log/openvswitch/ovs-vswitchd.log\\n\\n2016-01-22T08:58:33.381Z|00003|ovs_numa|INFO|Discovered 12 CPU cores on NUMA node 0\\n\\n2016-01-22T08:58:33.381Z|00004|ovs_numa|INFO|Discovered 1 NUMA nodes and 12 CPU cores\\n\\n2016-01-22T08:58:33.381Z|00005|reconnect|INFO|unix:/var/run/openvswitch/db.sock: connecting...\\n\\n2016-01-22T08:58:33.383Z|00006|reconnect|INFO|unix:/var/run/openvswitch/db.sock: connected\\n\\n2016-01-22T08:58:33.386Z|00007|bridge|INFO|ovs-vswitchd (Open vSwitch) 2.5.0\\n\\nOVS-LOG:\\n\\nsystemd[1]: Stopping Open vSwitch...\\n\\nsystemd[1]: Stopped Open vSwitch.\\n\\nsystemd[1]: Stopping Open vSwitch Internal Unit...\\n\\novs-ctl[3541]: * Killing ovs-vswitchd (3329)\\n\\novs-ctl[3541]: * Killing ovsdb-server (3318)\\n\\nsystemd[1]: Stopped Open vSwitch Internal Unit.\\n\\nsystemd[1]: Starting Open vSwitch Internal Unit...\\n\\novs-ctl[3560]: * Starting ovsdb-server\\n\\novs-vsctl: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait -- init -- set Open_vSwitch . db-version=7.12.1',\n",
       " 'ovs-vsctl: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait set Open_vSwitch . ovs-version=2.5.0 \"external\\nids:system-id=\\\\\"e7c5ba80-bb14-45c1-b8eb-628f3ad03903\\\\\"\" \"system-type=\\\\\"Ubuntu\\\\\"\" \"system-version=\\\\\"16.04\\nxenial\\\\\"\"\\n\\novs-ctl[3560]: * Configuring Open vSwitch system IDs\\n\\novs-ctl[3560]: 2016-01-22T08:58:31Z|00001|dpdk|INFO|No -vhost_sock_dir provided - defaulting to /var/run/openvswitch\\n\\novs-vswitchd: ovs|00001|dpdk|INFO|No -vhost_sock_dir provided - defaulting to /var/run/openvswitch\\n\\novs-ctl[3560]: EAL: Detected lcore 0 as core 0 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 1 as core 1 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 2 as core 2 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 3 as core 3 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 4 as core 4 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 5 as core 5 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 6 as core 0 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 7 as core 1 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 8 as core 2 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 9 as core 3 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 10 as core 4 on socket 0\\n\\novs-ctl[3560]: EAL: Detected lcore 11 as core 5 on socket 0\\n\\novs-ctl[3560]: EAL: Support maximum 128 logical core(s) by configuration.\\n\\novs-ctl[3560]: EAL: Detected 12 lcore(s)\\n\\novs-ctl[3560]: EAL: VFIO modules not all loaded, skip VFIO support...\\n\\novs-ctl[3560]: EAL: Setting up physically contiguous memory...\\n\\novs-ctl[3560]: EAL: Ask a virtual area of 0x100000000 bytes\\n\\novs-ctl[3560]: EAL: Virtual area found at 0x7f2040000000 (size = 0x100000000)\\n\\novs-ctl[3560]: EAL: Requesting 4 pages of size 1024MB from socket 0',\n",
       " 'ovs-ctl[3560]: EAL: TSC frequency is ~2397202 KHz\\n\\novs-vswitchd[3592]: EAL: TSC frequency is ~2397202 KHz\\n\\novs-vswitchd[3592]: EAL: Master lcore 0 is ready (tid=fc6cbb00;cpuset=[0])\\n\\novs-vswitchd[3592]: EAL: PCI device 0000:04:00.0 on NUMA socket 0\\n\\novs-vswitchd[3592]: EAL: probe driver: 8086:1528 rte_ixgbe_pmd\\n\\novs-vswitchd[3592]: EAL: Not managed by a supported kernel driver, skipped\\n\\novs-vswitchd[3592]: EAL: PCI device 0000:04:00.1 on NUMA socket 0\\n\\novs-vswitchd[3592]: EAL: probe driver: 8086:1528 rte_ixgbe_pmd\\n\\novs-vswitchd[3592]: EAL: PCI memory mapped at 0x7f2140000000\\n\\novs-vswitchd[3592]: EAL: PCI memory mapped at 0x7f2140200000\\n\\novs-ctl[3560]: EAL: Master lcore 0 is ready (tid=fc6cbb00;cpuset=[0])\\n\\novs-ctl[3560]: EAL: PCI device 0000:04:00.0 on NUMA socket 0\\n\\novs-ctl[3560]: EAL: probe driver: 8086:1528 rte_ixgbe_pmd\\n\\n251\\n\\n\\n-----\\n\\novs-ctl[3560]: EAL: Not managed by a supported kernel driver, skipped\\n\\novs-ctl[3560]: EAL: PCI device 0000:04:00.1 on NUMA socket 0\\n\\novs-ctl[3560]: EAL: probe driver: 8086:1528 rte_ixgbe_pmd\\n\\novs-ctl[3560]: EAL: PCI memory mapped at 0x7f2140000000\\n\\novs-ctl[3560]: EAL: PCI memory mapped at 0x7f2140200000\\n\\novs-vswitchd[3592]: PMD: eth_ixgbe_dev_init(): MAC: 4, PHY: 3\\n\\novs-vswitchd[3592]: PMD: eth_ixgbe_dev_init(): port 0 vendorID=0x8086 deviceID=0x1528\\n\\novs-ctl[3560]: PMD: eth_ixgbe_dev_init(): MAC: 4, PHY: 3\\n\\novs-ctl[3560]: PMD: eth_ixgbe_dev_init(): port 0 vendorID=0x8086 deviceID=0x1528\\n\\novs-ctl[3560]: Zone 0: name:<RG_MP_log_history>, phys:0x83fffdec0, len:0x2080, virt:0x7f213fffdec0, socket_id:0, flags:0\\n\\novs-ctl[3560]: Zone 1: name:<MP_log_history>, phys:0x83fd73d40, len:0x28a0c0, virt:0x7f213fd73d40, socket_id:0, flags:0',\n",
       " 'ovs-ctl[3560]: Zone 2: name:<rte_eth_dev_data>, phys:0x83fd43380, len:0x2f700, virt:0x7f213fd43380, socket_id:0, flags:0\\n\\novs-ctl[3560]: * Starting ovs-vswitchd\\n\\novs-ctl[3560]: * Enabling remote OVSDB managers\\n\\nsystemd[1]: Started Open vSwitch Internal Unit.\\n\\nsystemd[1]: Starting Open vSwitch...\\n\\nsystemd[1]: Started Open vSwitch.\\n\\nCMD: sudo ovs-vsctl add-br ovsdpdkbr0 -- set bridge ovsdpdkbr0 datapath_type=netdev\\n\\nSYSLOG:\\n\\n2016-01-22T08:58:56.344Z|00008|memory|INFO|37256 kB peak resident set size after 24.5 seconds\\n\\n2016-01-22T08:58:56.346Z|00009|ofproto_dpif|INFO|netdev@ovs-netdev: Datapath supports recirculation\\n\\n2016-01-22T08:58:56.346Z|00010|ofproto_dpif|INFO|netdev@ovs-netdev: MPLS label stack length probed as 3\\n\\n2016-01-22T08:58:56.346Z|00011|ofproto_dpif|INFO|netdev@ovs-netdev: Datapath supports unique flow ids\\n\\n2016-01-22T08:58:56.346Z|00012|ofproto_dpif|INFO|netdev@ovs-netdev: Datapath does not support ct_state\\n\\n2016-01-22T08:58:56.346Z|00013|ofproto_dpif|INFO|netdev@ovs-netdev: Datapath does not support ct_zone\\n\\n2016-01-22T08:58:56.346Z|00014|ofproto_dpif|INFO|netdev@ovs-netdev: Datapath does not support ct_mark\\n\\n2016-01-22T08:58:56.346Z|00015|ofproto_dpif|INFO|netdev@ovs-netdev: Datapath does not support ct_label\\n\\n2016-01-22T08:58:56.360Z|00016|bridge|INFO|bridge ovsdpdkbr0: added interface ovsdpdkbr0 on port 65534\\n\\n2016-01-22T08:58:56.361Z|00017|bridge|INFO|bridge ovsdpdkbr0: using datapath ID 00005a4a1ed0a14d\\n\\n2016-01-22T08:58:56.361Z|00018|connmgr|INFO|ovsdpdkbr0: added service controller \"punix:/var/run/openvswitch/ovsdpdkbr0\\n\\nOVS-LOG:\\n\\novs-vsctl: ovs|00001|vsctl|INFO|Called as ovs-vsctl add-br ovsdpdkbr0 -- set bridge ovsdpdkbr0 datapath_type=netdev',\n",
       " \"systemd-udevd[3607]: Could not generate persistent MAC address for ovs-netdev: No such file or directory\\n\\nkernel: [50165.886554] device ovs-netdev entered promiscuous mode\\n\\nkernel: [50165.901261] device ovsdpdkbr0 entered promiscuous mode\\n\\nCMD: sudo ovs-vsctl add-port ovsdpdkbr0 dpdk0 -- set Interface dpdk0 type=dpdk\\n\\nSYSLOG:\\n\\n2016-01-22T08:59:06.369Z|00019|memory|INFO|peak resident set size grew 155% in last 10.0 seconds, from 37256 kB to 95008 k\\n\\n2016-01-22T08:59:06.369Z|00020|memory|INFO|handlers:4 ports:1 revalidators:2 rules:5\\n\\n2016-01-22T08:59:30.989Z|00021|dpdk|INFO|Port 0: 8c:dc:d4:b3:6d:e9\\n\\n2016-01-22T08:59:31.520Z|00022|dpdk|INFO|Port 0: 8c:dc:d4:b3:6d:e9\\n\\n2016-01-22T08:59:31.521Z|00023|dpif_netdev|INFO|Created 1 pmd threads on numa node 0\\n\\n2016-01-22T08:59:31.522Z|00001|dpif_netdev(pmd16)|INFO|Core 0 processing port 'dpdk0'\\n\\n2016-01-22T08:59:31.522Z|00024|bridge|INFO|bridge ovsdpdkbr0: added interface dpdk0 on port 1\\n\\n2016-01-22T08:59:31.522Z|00025|bridge|INFO|bridge ovsdpdkbr0: using datapath ID 00008cdcd4b36de9\\n\\n2016-01-22T08:59:31.523Z|00002|dpif_netdev(pmd16)|INFO|Core 0 processing port 'dpdk0'\\n\\nOVS-LOG:\\n\\novs-vsctl: ovs|00001|vsctl|INFO|Called as ovs-vsctl add-port ovsdpdkbr0 dpdk0 -- set Interface dpdk0 type=dpdk\\n\\novs-vswitchd[3595]: PMD: ixgbe_dev_tx_queue_setup(): sw_ring=0x7f211a79ebc0 hw_ring=0x7f211a7a6c00 dma_addr=0x81a7a6c00\\n\\novs-vswitchd[3595]: PMD: ixgbe_set_tx_function(): Using simple tx code path\\n\\novs-vswitchd[3595]: PMD: ixgbe_set_tx_function(): Vector tx enabled.\\n\\novs-vswitchd[3595]: PMD: ixgbe_dev_rx_queue_setup(): sw_ring=0x7f211a78a6c0 sw_sc_ring=0x7f211a786580 hw_ring=0x7f211a7\",\n",
       " \"ovs-vswitchd[3595]: PMD: ixgbe_set_rx_function(): Vector rx enabled, please make sure RX burst size no less than 4 (port=0\\n\\novs-vswitchd[3595]: PMD: ixgbe_dev_tx_queue_setup(): sw_ring=0x7f211a79ebc0 hw_ring=0x7f211a7a6c00 dma_addr=0x81a7a6c00\\n\\n...\\n\\n252\\n\\n\\n-----\\n\\nCMD: sudo ovs-vsctl add-port ovsdpdkbr0 vhost-user-1 -- set Interface vhost-user-1 type=dpdkvhostuser\\n\\nOVS-LOG:\\n\\n2016-01-22T09:00:35.145Z|00026|dpdk|INFO|Socket /var/run/openvswitch/vhost-user-1 created for vhost\\nuser port vhost-user-1\\n\\n2016-01-22T09:00:35.145Z|00003|dpif_netdev(pmd16)|INFO|Core 0 processing port 'dpdk0'\\n\\n2016-01-22T09:00:35.145Z|00004|dpif_netdev(pmd16)|INFO|Core 0 processing port 'vhost-user-1'\\n\\n2016-01-22T09:00:35.145Z|00027|bridge|INFO|bridge ovsdpdkbr0: added interface vhost-user-1 on port 2\\n\\nSYSLOG:\\n\\novs-vsctl: ovs|00001|vsctl|INFO|Called as ovs-vsctl add-port ovsdpdkbr0 vhost-user-1 -- set Interface vhost\\nuser-1 type=dpdkvhostuser\\n\\novs-vswitchd[3595]: VHOST_CONFIG: socket created, fd:46\\n\\novs-vswitchd[3595]: VHOST_CONFIG: bind to /var/run/openvswitch/vhost-user-1\\n\\nEventually we can see the poll thread in top\\n\\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND\\n\\n3595 root 10 -10 4975344 103936 9916 S 100.0 0.3 33:13.56 ovs-vswitchd\\n## **Resources**\\n\\n[• DPDK documentation](http://dpdk.org/doc)\\n\\n[• Release Notes matching the version packages in Ubuntu 16.04](http://dpdk.org/doc/guides/rel_notes/release_2_2.html)\\n\\n[• Linux DPDK user getting started](http://dpdk.org/doc/guides/linux_gsg/index.html)\\n\\n[• EAL command-line options](http://dpdk.org/doc/guides/testpmd_app_ug/run_app.html)\\n\\n[• DPDK API documentation](http://dpdk.org/doc/api/)\",\n",
       " '[• Open Vswitch DPDK installation](https://github.com/openvswitch/ovs/blob/branch-2.5/INSTALL.DPDK.md)\\n\\n[• Wikipedia’s definition of DPDK](https://en.wikipedia.org/wiki/Data_Plane_Development_Kit)\\n\\nA Samba server needs to join the Active Directory (AD) domain before it can serve files and printers to Active Directory\\nusers. This is different from Network User Authentication with SSSD, where we integrate the AD users and groups\\ninto the local Ubuntu system as if they were local.\\n\\nFor Samba to authenticate these users via Server Message Block (SMB) authentication protocols, we need both for\\nthe remote users to be “seen”, and for Samba itself to be aware of the domain. In this scenario, Samba is called a\\nMember Server or Domain Member.\\n\\n**See also** :\\n\\nSamba itself has the necessary tooling to join an Active Directory domain. It requires a sequence of manual\\n[steps and configuration file editing, which is thoroughly documented on the Samba wiki. It’s useful to read](https://wiki.samba.org/index.php/Setting_up_Samba_as_a_Domain_Member)\\nthat documentation to get an idea of the steps necessary, and the decisions you will need to make.\\n## Use realmd to join the Active Directory domain\\n\\nFor this guide, though, we are going to use the realmd package and instruct it to use the Samba tooling for joining\\nthe AD domain. This package will make certain decisions for us which will work for most cases, but more complex\\nsetups involving multiple or very large domains might require additional tweaking.\\n\\n**Install** realmd\\n\\nFirst, let’s install the necessary packages:\\n\\nsudo apt install realmd samba\\n\\nIn order to have the joined machine registered in the AD DNS, it needs to have an FQDN set.',\n",
       " 'You might have that\\nalready, if running the hostname -f command returns a full hostname with domain. If it doesn’t, then set the hostname\\nas follows:\\n\\nsudo hostnamectl hostname <yourfqdn>\\n\\nFor this guide, we will be using j1.internal.example.fake, and the AD domain will be internal.example.fake .\\n\\n253\\n\\n\\n-----\\n\\n**Verify the AD server**\\n\\nNext, we need to verify that the AD server is both reachable and known by running the following command:\\n\\nsudo realm discover internal.example.fake\\n\\nThis should provide an output like this, given our setup:\\n\\ninternal.example.fake\\n\\ntype: kerberos\\n\\nrealm-name: INTERNAL.EXAMPLE.FAKE\\n\\ndomain-name: internal.example.fake\\n\\nconfigured: no\\n\\nserver-software: active-directory\\n\\nclient-software: sssd\\n\\nrequired-package: sssd-tools\\n\\nrequired-package: sssd\\n\\nrequired-package: libnss-sss\\n\\nrequired-package: libpam-sss\\n\\nrequired-package: adcli\\n\\nrequired-package: samba-common-bin\\n\\nrealm is suggesting a set of packages for the discovered domain, but we will override that and select the Samba tooling\\nfor this join, because we want Samba to become a Member Server.\\n\\n**Join the AD domain**\\n\\nLet’s join the domain in verbose mode so we can see all the steps:\\n\\nsudo realm join -v --membership-software=samba --client-software=winbind internal.example.fake\\n\\nThis should produce the following output for us:\\n\\n - Resolving: _ldap._tcp.internal.example.fake\\n\\n - Performing LDAP DSE lookup on: 10.0.16.5\\n\\n - Successfully discovered: internal.example.fake\\n\\nPassword for Administrator:\\n\\n - Unconditionally checking packages\\n\\n - Resolving required packages\\n\\n - Installing necessary packages: libnss-winbind samba-common-bin libpam-winbind winbind',\n",
       " \" - LANG=C LOGNAME=root /usr/bin/net --configfile /var/cache/realmd/realmd-smb-conf.A53NO1 -U Administrator \\n-use-kerberos=required ads join internal.example.fake\\n\\nPassword for [INTEXAMPLE\\\\Administrator]:\\n\\nUsing short domain name -- INTEXAMPLE\\n\\nJoined 'J1' to dns domain 'internal.example.fake'\\n\\n - LANG=C LOGNAME=root /usr/bin/net --configfile /var/cache/realmd/realmd-smb-conf.A53NO1 -U Administrator ads keytab cre\\n\\nPassword for [INTEXAMPLE\\\\Administrator]:\\n\\n - /usr/sbin/update-rc.d winbind enable\\n\\n - /usr/sbin/service winbind restart\\n\\n - Successfully enrolled machine in realm\\n\\n**Note** :\\n\\nThis command also installed the libpam-winbind package, **which allows AD users to authenticate to**\\n**other services on this system via PAM, like SSH or console logins** . For example, if your SSH\\nserver allows password authentication ( PasswordAuthentication yes in /etc/ssh/sshd_config ), then the\\ndomain users will be allowed to login remotely on this system via SSH.\\nIf you don’t expect or need AD users to log into this system (unless it’s via Samba or Windows), then it’s\\nsafe and probably best to remove the libpam-winbind package.\\n\\n[Until bug #1980246 is fixed, one extra step is needed:](https://bugs.launchpad.net/ubuntu/+source/realmd/+bug/1980246)\\n\\n  - Configure /etc/nsswitch.conf by adding the word winbind to the passwd and group lines as shown below:\\n\\npasswd: files systemd winbind\\n\\ngroup: files systemd winbind\\n\\nNow you will be able to query users from the AD domain. Winbind adds the short domain name as a prefix to\\ndomain users and groups:\\n\\n$ getent passwd INTEXAMPLE\\\\\\\\Administrator\\n\\n254\\n\\n\\n-----\\n\\nINTEXAMPLE\\\\administrator:*:2000500:2000513::/home/administrator@INTEXAMPLE:/bin/bash\",\n",
       " 'You can find out the short domain name in the realm output shown earlier, or inspect the workgroup parameter\\nof /etc/samba/smb.conf .\\n\\n**Common installation options**\\n\\nWhen domain users and groups are brought to the Linux world, a bit of translation needs to happen, and sometimes\\nnew values need to be created. For example, there is no concept of a “login shell” for AD users, but it exists in Linux.\\n\\nThe following are some common /etc/samba/smb.conf options you are likely to want to tweak in your installation. The\\n\\n[smb.conf(5)](https://manpages.ubuntu.com/manpages/jammy/man5/smb.conf.5.html) man page explains the % variable substitutions and other details:\\n\\n - **home directory**\\n\\ntemplate homedir = /home/%U@%D\\n(Another popular choice is /home/%D/%U )\\n\\n - **login shell**\\n\\ntemplate shell = /bin/bash\\n\\n - winbind separator = \\\\\\nThis is the \\\\ character between the short domain name and the user or group name that we saw in the getent\\n\\npasswd output above.\\n\\n - winbind use default domain\\n\\nIf this is set to yes, then the domain name will not be part of the users and groups. Setting this to yes makes the\\nsystem more friendly towards Linux users, as they won’t have to remember to include the domain name every\\ntime a user or group is referenced. However, if multiple domains are involved, such as in an AD forest or other\\nform of domain trust relationship, then leave this setting at no (default).\\n\\nTo have the home directory created automatically the first time a user logs in to the system, and if you haven’t removed\\n\\nlibpam-winbind, then enable the pam_mkhomedir module via this command:\\n\\nsudo pam-auth-update --enable mkhomedir',\n",
       " 'Note that this won’t apply to logins via Samba: this only creates the home directory for system logins like those via\\n\\nssh or the console.\\n\\n**Export shares**\\n\\nShares can be exported as usual. Since this is now a Member Server, there is no need to deal with user and group\\nmanagement. All of this is integrated with the Active Directory server we joined.\\n\\nFor example, let’s create a simple [storage] share. Add this to the /etc/samba/smb.conf file:\\n\\n[storage]\\n\\npath = /storage\\n\\ncomment = Storage share\\n\\nwritable = yes\\n\\nguest ok = no\\n\\nThen create the /storage directory. Let’s also make it 1777 so all users can use it, and then ask samba to reload its\\nconfiguration:\\n\\nsudo mkdir -m 1777 /storage\\n\\nsudo smbcontrol smbd reload-config\\n\\nWith this, users from the AD domain will be able to access this share. For example, if there is a user ubuntu the\\nfollowing command would access the share from another system, using the domain credentials:\\n\\n$ smbclient //j1.internal.example.fake/storage -U INTEXAMPLE\\\\\\\\ubuntu\\n\\nEnter INTEXAMPLE\\\\ubuntu\\'s password:\\n\\nTry \"help\" to get a list of possible commands.\\n\\nsmb: \\\\>\\n\\nAnd smbstatus on the member server will show the connected user:\\n\\n$ sudo smbstatus\\n\\nSamba version 4.15.5-Ubuntu\\n\\nPID Username Group Machine Protocol Version Encryption Signing\\n\\n255\\n\\n\\n-----\\n\\n----------------------------------------------------------------------------------------------------------\\n----------------------------\\n3631 INTEXAMPLE\\\\ubuntu INTEXAMPLE\\\\domain users 10.0.16.1 (ipv4:10.0.16.1:39534) SMB3_11 \\npartial(AES-128-CMAC)\\n\\nService pid Machine Connected at Encryption Signing\\n\\n--------------------------------------------------------------------------------------------',\n",
       " 'storage 3631 10.0.16.1 Wed Jun 29 17:42:54 2022 UTC - \\nNo locked files\\n\\nYou can also restrict access to the share as usual. Just keep in mind the syntax for the domain users. For example,\\nto restrict access to the [storage] share we just created to *only* members of the LTS Releases domain group, add the\\n\\nvalid users parameter like below:\\n\\n[storage]\\n\\npath = /storage\\n\\ncomment = Storage share\\n\\nwritable = yes\\n\\nguest ok = no\\n\\nvalid users = \"@INTEXAMPLE\\\\LTS Releases\"\\n\\n**Choose an** idmap **backend**\\n\\nrealm made some choices for us when we joined the domain. A very important one is the idmap backend, and it might\\nneed changing for more complex setups.\\n\\nUser and group identifiers on the AD side are not directly usable as identifiers on the Linux site. A *mapping* needs to\\nbe performed.\\n\\nWinbind supports several idmap backends, and each one has its own man page. The three main ones are:\\n\\n - [idmap_ad](https://manpages.ubuntu.com/manpages/jammy/man8/idmap_ad.8.html)\\n\\n - [idmap_autorid](https://manpages.ubuntu.com/manpages/jammy/man8/idmap_autorid.8.html)\\n\\n - [idmap_rid](https://manpages.ubuntu.com/manpages/jammy/man8/idmap_rid.8.html)\\n\\nChoosing the correct backend for each deployment type needs careful planing. Upstream has some guidelines at\\nChoosing an [idmap](https://wiki.samba.org/index.php/Setting_up_Samba_as_a_Domain_Member#Choosing_an_idmap_backend) backend, and each man page has more details and recommendations.\\n\\nThe realm tool selects (by default) the rid backend. This backend uses an algorithm to calculate the Unix user and\\ngroup IDs from the respective RID value on the AD side. You might need to review the idmap config settings in',\n",
       " '/etc/samba/smb.conf and make sure they can accommodate the number of users and groups that exist in the domain,\\nand that the range does not overlap with users from other sources.\\n\\nFor example, these settings:\\n\\nidmap config * : range = 10000-999999\\n\\nidmap config intexample : backend = rid\\n\\nidmap config intexample : range = 2000000-2999999\\n\\nidmap config * : backend = tdb\\n\\nWill reserve the 2,000,000 through 2,999,999 range for user and group ID allocations on the Linux side for the\\n\\nintexample domain. The default backend ( -, which acts as a “globbing” catch-all rule) is used for the BUILTIN user\\nand groups, and other domains (if they exist). It’s important that these ranges do not overlap.\\n\\nThe Administrator user we inspected before with getent passwd can give us a glimpse of how these ranges are used\\n(output format changed for clarity):\\n\\n$ id INTEXAMPLE\\\\\\\\Administrator\\n\\nuid=2000500(INTEXAMPLE\\\\administrator)\\n\\ngid=2000513(INTEXAMPLE\\\\domain users)\\n\\ngroups=2000513(INTEXAMPLE\\\\domain users),\\n\\n2000500(INTEXAMPLE\\\\administrator),\\n\\n2000572(INTEXAMPLE\\\\denied rodc password replication group),\\n\\n2000519(INTEXAMPLE\\\\enterprise admins),\\n\\n2000518(INTEXAMPLE\\\\schema admins),\\n\\n2000520(INTEXAMPLE\\\\group policy creator owners),\\n\\n2000512(INTEXAMPLE\\\\domain admins),\\n\\n10001(BUILTIN\\\\users),\\n\\n256\\n\\n\\n-----\\n\\n10000(BUILTIN\\\\administrators)\\n## **Further reading**\\n\\n[• The Samba Wiki](https://wiki.samba.org)\\n\\nOne of the most common ways to network Ubuntu and Windows computers is to configure Samba as a *file server* . It\\ncan be set up to share files with Windows clients, as we’ll see in this section.\\n\\nThe server will be configured to share files with any client on the network without prompting for a password.',\n",
       " 'If your\\nenvironment requires stricter Access Controls see Share Access Control.\\n## **Install Samba**\\n\\nThe first step is to install the samba package. From a terminal prompt enter:\\n\\nsudo apt install samba\\n\\nThat’s all there is to it; you are now ready to configure Samba to share files.\\n## **Configure Samba as a file server**\\n\\nThe main Samba configuration file is located in /etc/samba/smb.conf . The default configuration file contains a significant number of comments, which document various configuration directives.\\n\\n**Note** :\\nNot all available options are included in the default configuration file. See the smb.conf [man page or the](https://www.samba.org/samba/docs/current/man-html/smb.conf.5.html)\\n[Samba HOWTO Collection for more details.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/)\\n\\nFirst, edit the workgroup parameter in the *[global]* section of /etc/samba/smb.conf and change it to better match your\\nenvironment:\\n\\nworkgroup = EXAMPLE\\n\\nCreate a new section at the bottom of the file, or uncomment one of the examples, for the directory you want to share:\\n\\n[share]\\n\\ncomment = Ubuntu File Server Share\\n\\npath = /srv/samba/share\\n\\nbrowsable = yes\\n\\nguest ok = yes\\n\\nread only = no\\n\\ncreate mask = 0755\\n\\n - **comment**\\nA short description of the share. Adjust to fit your needs.\\n\\n - **path**\\nThe path to the directory you want to share.\\n\\n**Note** :\\n\\nThis example uses /srv/samba/sharename because, according to the *Filesystem Hierarchy Standard*\\n*(FHS)*, [/srv](http://www.pathname.com/fhs/pub/fhs-2.3.html#SRVDATAFORSERVICESPROVIDEDBYSYSTEM) is where site-specific data should be served. Technically, Samba shares can be placed',\n",
       " 'anywhere on the filesystem as long as the permissions are correct, but adhering to standards is recommended.\\n\\n - **browsable**\\n\\nEnables Windows clients to browse the shared directory using Windows Explorer.\\n\\n - **guest ok**\\nAllows clients to connect to the share without supplying a password.\\n\\n - *read only:* determines if the share is read only or if write privileges are granted. Write privileges are allowed\\nonly when the value is *no*, as is seen in this example. If the value is *yes*, then access to the share is read only.\\n\\n - **create mask**\\nDetermines the permissions that new files will have when created.\\n\\n257\\n\\n\\n-----\\n\\n**Create the directory**\\n\\nNow that Samba is configured, the directory needs to be created and the permissions changed. From a terminal, run\\nthe following commands:\\n\\nsudo mkdir -p /srv/samba/share\\n\\nsudo chown nobody:nogroup /srv/samba/share/\\n\\nThe -p switch tells mkdir to create the entire directory tree if it doesn’t already exist.\\n\\n**Enable the new configuration**\\n\\nFinally, restart the Samba services to enable the new configuration by running the following command:\\n\\nsudo systemctl restart smbd.service nmbd.service\\n\\n**Warning** :\\nOnce again, the above configuration gives full access to any client on the local network. For a more secure\\nconfiguration see Share Access Control.\\n\\nFrom a Windows client you should now be able to browse to the Ubuntu file server and see the shared directory. If\\nyour client doesn’t show your share automatically, try to access your server by its IP address, e.g. \\\\\\\\192.168.1.1, in\\na Windows Explorer window. To check that everything is working try creating a directory from Windows.',\n",
       " 'To create additional shares simply create new *[sharename]* sections in /etc/samba/smb.conf, and restart *Samba* . Just\\nmake sure that the directory you want to share actually exists and the permissions are correct.\\n\\nThe file share named *[share]* and the path /srv/samba/share used in this example can be adjusted to fit your environment. It is a good idea to name a share after a directory on the file system. Another example would be a share name\\nof *[qa]* with a path of /srv/samba/qa .\\n## **Further reading**\\n\\n[• For in-depth Samba configurations see the Samba HOWTO Collection](https://www.samba.org/samba/docs/old/Samba3-HOWTO/)\\n\\n[• The guide is also available in printed format.](http://www.amazon.com/exec/obidos/tg/detail/-/0131882228)\\n\\n[• O’Reilly’s Using Samba is another good reference.](http://www.oreilly.com/catalog/9780596007690/)\\n\\n[• The Ubuntu Wiki Samba page.](https://help.ubuntu.com/community/Samba)\\n\\nAnother common way to network Ubuntu and Windows computers is to configure Samba as a *print server* . This will\\nallow it to share printers installed on an Ubuntu server, whether locally or over the network.\\n\\n[Just as we did in using Samba as a file server, this section will configure Samba to allow any client on the local network](https://ubuntu.com/server/docs/samba-file-server)\\nto use the installed printers without prompting for a username and password.\\n\\n[If your environment requires stricter Access Controls see Share Access Control.](https://ubuntu.com/server/docs/samba-share-access-control)\\n## **Install and configure CUPS**\\n\\nBefore installing and configuring Samba as a print server, it is best to already have a working CUPS installation. See',\n",
       " '[our guide on CUPS for details.](https://ubuntu.com/server/docs/service-cups)\\n## **Install Samba**\\n\\nTo install the samba package, run the following command in your terminal:\\n\\nsudo apt install samba\\n## **Configure Samba**\\n\\nAfter installing samba, edit /etc/samba/smb.conf . Change the *workgroup* attribute to what is appropriate for your\\nnetwork:\\n\\nworkgroup = EXAMPLE\\n\\nIn the *[printers]* section, change the *guest ok* option to ‘yes’:\\n\\nbrowsable = yes\\n\\nguest ok = yes\\n\\nAfter editing smb.conf, restart Samba:\\n\\n258\\n\\n\\n-----\\n\\nsudo systemctl restart smbd.service nmbd.service\\n\\nThe default Samba configuration will automatically share any printers installed. Now all you need to do is install the\\nprinter locally on your Windows clients.\\n## **Further reading**\\n\\n[• For in-depth Samba configurations see the Samba HOWTO Collection.](http://samba.org/samba/docs/man/Samba-HOWTO-Collection/)\\n\\n[• The guide is also available in printed format.](http://www.amazon.com/exec/obidos/tg/detail/-/0131882228)\\n\\n[• O’Reilly’s Using Samba is another good reference.](http://www.oreilly.com/catalog/9780596007690/)\\n\\n[• Also, see the CUPS Website for more information on configuring CUPS.](http://www.cups.org/)\\n\\n[• The Ubuntu Wiki Samba page.](https://help.ubuntu.com/community/Samba)\\n\\nThere are several options available to control access for each individual shared directory. Using the *[share]* example,\\nthis section will cover some common options.\\n## **Groups**\\n\\n*Groups* define a collection of users who have a common level of access to particular network resources. This provides\\ngranularity in controlling access to such resources. For example, let’s consider a group called “qa” is defined to contain',\n",
       " 'the users *Freda*, *Danika*, and *Rob*, and then a group called “support” is created containing the users *Danika*, *Jeremy*,\\nand *Vincent* . Any network resources configured to allow access by the “qa” group will be available to Freda, Danika,\\nand Rob, but not Jeremy or Vincent. Danika can access resources available to both groups since she belongs to both\\nthe “qa” and “support” groups. All other users only have access to resources explicitly allowed to the group they are\\npart of.\\n\\nWhen mentioning groups in the Samba configuration file, /etc/samba/smb.conf, the recognized syntax is to preface the\\ngroup name with an “@” symbol. For example, if you wished to use a group named *sysadmin* in a certain section of\\nthe /etc/samba/smb.conf, you would do so by entering the group name as @sysadmin . If a group name has a space in\\nit, use double quotes, like \"@LTS Releases\" .\\n## **Read and write permissions**\\n\\nRead and write permissions define the explicit rights a computer or user has to a particular share. Such permissions\\nmay be defined by editing the /etc/samba/smb.conf file and specifying the explicit permissions inside a share.\\n\\nFor example, if you have defined a Samba share called *share* and wish to give read-only permissions to the group of\\nusers known as “qa”, but wanted to allow writing to the share by the group called “sysadmin” *and* the user named\\n“vincent”, then you could edit the /etc/samba/smb.conf file and add the following entries under the *[share]* entry:\\n\\nread list = @qa\\n\\nwrite list = @sysadmin, vincent\\n\\nAnother possible Samba permission is to declare *administrative* permissions to a particular shared resource. Users',\n",
       " 'having administrative permissions may read, write, or modify any information contained in the resource the user has\\nbeen given explicit administrative permissions to.\\n\\nFor example, if you wanted to give the user *Melissa* administrative permissions to the *share* example, you would edit\\nthe /etc/samba/smb.conf file, and add the following line under the *[share]* entry:\\n\\nadmin users = melissa\\n\\nAfter editing /etc/samba/smb.conf, reload Samba for the changes to take effect by running the following command:\\n\\nsudo smbcontrol smbd reload-config\\n## **Filesystem permissions**\\n\\nNow that Samba has been configured to limit which groups have access to the shared directory, the filesystem permissions need to be checked.\\n\\nTraditional Linux file permissions do not map well to Windows NT Access Control Lists (ACLs). Fortunately POSIX\\nACLs are available on Ubuntu servers, which provides more fine-grained control. For example, to enable ACLs on\\n\\n/srv in an EXT3 filesystem, edit /etc/fstab and add the *acl* option:\\n\\nUUID=66bcdd2e-8861-4fb0-b7e4-e61c569fe17d /srv ext3 noatime,relatime,acl 0 1\\n\\nThen remount the partition:\\n\\n259\\n\\n\\n-----\\n\\nsudo mount -v -o remount /srv\\n\\n**Note** :\\nThis example assumes /srv is on a separate partition. If /srv, or wherever you have configured your share\\npath, is part of the / partition then a reboot may be required.\\n\\nTo match the Samba configuration above, the “sysadmin” group will be given read, write, and execute permissions\\nto /srv/samba/share, the “qa” group will be given read and execute permissions, and the files will be owned by the\\nusername “Melissa”. Enter the following in a terminal:\\n\\nsudo chown -R melissa /srv/samba/share/',\n",
       " 'sudo chgrp -R sysadmin /srv/samba/share/\\n\\nsudo setfacl -R -m g:qa:rx /srv/samba/share/\\n\\n**Note** :\\nThe setfacl command above gives *execute* permissions to all files in the /srv/samba/share directory, which\\nyou may or may not want.\\n\\nNow from a Windows client you should notice the new file permissions are implemented. See the [acl](https://manpages.ubuntu.com/manpages/trusty/man5/acl.5.html) and [setfacl](https://manpages.ubuntu.com/manpages/trusty/man1/setfacl.1.html) man\\npages for more information on POSIX ACLs.\\n## **Further reading**\\n\\n[• For in-depth Samba configurations see the Samba HOWTO Collection.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/)\\n\\n[• The guide is also available in printed format.](http://www.amazon.com/exec/obidos/tg/detail/-/0131882228)\\n\\n[• O’Reilly’s Using Samba is also a good reference.](http://www.oreilly.com/catalog/9780596007690/)\\n\\n[• Chapter 18 of the Samba HOWTO Collection is devoted to security.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/securing-samba.html)\\n\\n[• For more information on Samba and ACLs see the Samba ACLs page.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/AccessControls.html)\\n\\n[• The Ubuntu Wiki Samba page.](https://help.ubuntu.com/community/Samba)\\n\\nUbuntu comes with the AppArmor security module, which provides mandatory access controls. The default AppArmor\\n[profile for Samba may need to be adapted to your configuration. More details on using AppArmor can be found in](https://ubuntu.com/server/docs/security-apparmor)\\n[this guide.](https://ubuntu.com/server/docs/security-apparmor)\\n\\nThere are default AppArmor profiles for /usr/sbin/smbd and /usr/sbin/nmbd, the Samba daemon binaries, as part of',\n",
       " 'the apparmor-profiles package.\\n## Install apparmor-profiles\\n\\nTo install the package, enter the following command from a terminal prompt:\\n\\nsudo apt install apparmor-profiles apparmor-utils\\n\\n**Note** :\\nThis package contains profiles for several other binaries.\\n## **AppArmor profile modes**\\n\\nBy default, the profiles for smbd and nmbd are set to ‘complain’ mode. In this mode, Samba can work without modifying\\nthe profile, and only logs errors or violations. There is no need to add exceptions for the shares, as the smbd service\\nunit takes care of doing that automatically via a helper script.\\n\\nThis is what an ALLOWED message looks like. It means that, were the profile not in complain mode, this action would\\nhave been denied instead (formatted into multiple lines here for better visibility):\\n\\nJun 30 14:41:09 ubuntu kernel: [ 621.478989] audit:\\n\\ntype=1400 audit(1656600069.123:418):\\n\\napparmor=\"ALLOWED\" operation=\"exec\" profile=\"smbd\"\\n\\nname=\"/usr/lib/x86_64-linux-gnu/samba/samba-bgqd\" pid=4122 comm=\"smbd\"\\n\\nrequested_mask=\"x\" denied_mask=\"x\" fsuid=0 ouid=0\\n\\ntarget=\"smbd//null-/usr/lib/x86_64-linux-gnu/samba/samba-bgqd\"\\n\\nThe alternative to ‘complain’ mode is ‘enforce’ mode, where any operations that violate policy are blocked. To place\\nthe profile into enforce mode and reload it, run:\\n\\nsudo aa-enforce /usr/sbin/smbd\\n\\nsudo apparmor_parser -r -W -T /etc/apparmor.d/usr.sbin.smbd\\n\\n260\\n\\n\\n-----\\n\\nIt’s advisable to monitor /var/log/syslog for audit entries that contain AppArmor DENIED messages, or\\n\\n/var/log/audit/audit.log if you are running the auditd daemon. Actions blocked by AppArmor may surface\\nas odd or unrelated errors in the application.\\n## **Further reading:**',\n",
       " '[• For more information on how to use AppArmor, including details of the profile modes, the Debian AppArmor](https://wiki.debian.org/AppArmor/HowToUse)\\n[guide may be helpful.](https://wiki.debian.org/AppArmor/HowToUse)\\n\\n**Note** :\\nThis section is flagged as *legacy* because nowadays, Samba can be deployed in full Active Directory domain\\ncontroller mode, and the old-style NT4 Primary Domain Controller is deprecated.\\n\\nA Samba server can be configured to appear as a Windows NT4-style domain controller. A major advantage of this\\nconfiguration is the ability to centralise user and machine credentials. Samba can also use multiple backends to store\\nthe user information.\\n## **Primary domain controller**\\n\\nIn this section, we’ll install and configure Samba as a Primary Domain Controller (PDC) using the default smbpasswd\\nbackend.\\n\\n**Install Samba**\\n\\nFirst, we’ll install Samba, and libpam-winbind (to sync the user accounts), by entering the following in a terminal\\nprompt:\\n\\nsudo apt install samba libpam-winbind\\n\\n**Configure Samba**\\n\\nNext, we’ll configure Samba by editing /etc/samba/smb.conf . The *security* mode should be set to *user*, and the\\n*workgroup* should relate to your organization:\\n\\nworkgroup = EXAMPLE\\n\\n...\\n\\nsecurity = user\\n\\nIn the commented “Domains” section, add or uncomment the following (the last line has been split to fit the format\\nof this document):\\n\\ndomain logons = yes\\n\\nlogon path = \\\\\\\\%N\\\\%U\\\\profile\\n\\nlogon drive = H:\\n\\nlogon home = \\\\\\\\%N\\\\%U\\n\\nlogon script = logon.cmd\\n\\nadd machine script = sudo /usr/sbin/useradd -N -g machines -c Machine -d\\n\\n/var/lib/samba -s /bin/false %u\\n\\n**Note** :',\n",
       " 'If you wish to not use *Roaming Profiles* leave the logon home and logon path options commented out.\\n\\n - domain logons\\nProvides the netlogon service, causing Samba to act as a domain controller.\\n\\n - logon path\\nPlaces the user’s Windows profile into their home directory. It is also possible to configure a *[profiles]* share\\nplacing all profiles under a single directory.\\n\\n - logon drive\\nSpecifies the home directory local path.\\n\\n - logon home\\nSpecifies the home directory location.\\n\\n - logon script\\nDetermines the script to be run locally once a user has logged in. The script needs to be placed in the *[netlogon]*\\nshare.\\n\\n261\\n\\n\\n-----\\n\\n - add machine script\\nA script that will automatically create the *Machine Trust Account* needed for a workstation to join the domain.\\n\\nIn this example the *machines* group will need to be created using the addgroup utility (see Security - Users: Adding\\nand Deleting Users for details).\\n\\n**Mapping shares**\\n\\nUncomment the *[homes]* share to allow the logon home to be mapped:\\n\\n[homes]\\n\\ncomment = Home Directories\\n\\nbrowseable = no\\n\\nread only = no\\n\\ncreate mask = 0700\\n\\ndirectory mask = 0700\\n\\nvalid users = %S\\n\\nWhen configured as a domain controller, a *[netlogon]* share needs to be configured. To enable the share, uncomment:\\n\\n[netlogon]\\n\\ncomment = Network Logon Service\\n\\npath = /srv/samba/netlogon\\n\\nguest ok = yes\\n\\nread only = yes\\n\\nshare modes = no\\n\\n**Note** :\\n\\nThe original netlogon share path is /home/samba/netlogon, but according to the Filesystem Hierarchy\\n[Standard (FHS), /srv is the correct location for site-specific data provided by the system.](http://www.pathname.com/fhs/pub/fhs-2.3.html#SRVDATAFORSERVICESPROVIDEDBYSYSTEM)',\n",
       " 'Now create the netlogon directory, and an empty (for now) logon.cmd script file:\\n\\nsudo mkdir -p /srv/samba/netlogon\\n\\nsudo touch /srv/samba/netlogon/logon.cmd\\n\\nYou can enter any normal Windows logon script commands in logon.cmd to customise the client’s environment.\\n\\nRestart Samba to enable the new domain controller, using the following command:\\n\\nsudo systemctl restart smbd.service nmbd.service\\n\\n**Final setup tasks**\\n\\nLastly, there are a few additional commands needed to set up the appropriate rights.\\n\\nSince *root* is disabled by default, a system group needs to be mapped to the Windows *Domain Admins* group in order\\nto join a workstation to the domain. Using the net utility, from a terminal enter:\\n\\nsudo net groupmap add ntgroup=\"Domain Admins\" unixgroup=sysadmin rid=512 type=d\\n\\nYou should change *sysadmin* to whichever group you prefer. Also, the user joining the domain needs to be a member\\nof the *sysadmin* group, as well as a member of the system *admin* group. The *admin* group allows sudo use.\\n\\nIf the user does not have Samba credentials yet, you can add them with the smbpasswd utility. Change the *sysadmin*\\nusername appropriately:\\n\\nsudo smbpasswd -a sysadmin\\n\\nAlso, rights need to be explicitly provided to the *Domain Admins* group to allow the *add machine script* (and other\\nadmin functions) to work. This is achieved by executing:\\n\\nnet rpc rights grant -U sysadmin \"EXAMPLE\\\\Domain Admins\" SeMachineAccountPrivilege \\\\\\n\\nSePrintOperatorPrivilege SeAddUsersPrivilege SeDiskOperatorPrivilege \\\\\\n\\nSeRemoteShutdownPrivilege\\n\\nYou should now be able to join Windows clients to the Domain in the same manner as joining them to an NT4 domain\\nrunning on a Windows server.\\n\\n262\\n\\n\\n-----',\n",
       " '## **Backup domain controller**\\n\\nWith a Primary Domain Controller (PDC) on the network it is best to have a Backup Domain Controller (BDC) as\\nwell. This will allow clients to authenticate in case the PDC becomes unavailable.\\n\\nWhen configuring Samba as a BDC you need a way to sync account information with the PDC. There are multiple\\nways of accomplishing this; secure copy protocol (SCP), rsync, or by using LDAP as the passdb backend.\\n\\nUsing LDAP is the most robust way to sync account information, because both domain controllers can use the same\\ninformation in real time. However, setting up an LDAP server may be overly complicated for a small number of user\\nand computer accounts. See Samba - OpenLDAP Backend for details.\\n\\nFirst, install samba and libpam-winbind . From a terminal enter:\\n\\nsudo apt install samba libpam-winbind\\n\\nNow, edit /etc/samba/smb.conf and uncomment the following in the *[global]* :\\n\\nworkgroup = EXAMPLE\\n\\n...\\n\\nsecurity = user\\n\\nIn the commented *Domains* uncomment or add:\\n\\ndomain logons = yes\\n\\ndomain master = no\\n\\nMake sure a user has rights to read the files in /var/lib/samba . For example, to allow users in the *admin* group to\\nSCP the files, enter:\\n\\nsudo chgrp -R admin /var/lib/samba\\n\\nNext, sync the user accounts, using SCP to copy the /var/lib/samba directory from the PDC:\\n\\nsudo scp -r username@pdc:/var/lib/samba /var/lib\\n\\nYou can replace *username* with a valid username and *pdc* with the hostname or IP address of your actual PDC.\\n\\nFinally, restart samba:\\n\\nsudo systemctl restart smbd.service nmbd.service\\n\\nYou can test that your Backup Domain Controller is working by first stopping the Samba daemon on the PDC – then',\n",
       " 'try to log in to a Windows client joined to the domain.\\n\\nAnother thing to keep in mind is if you have configured the logon home option as a directory on the PDC, and the PDC\\nbecomes unavailable, access to the user’s *Home* drive will also be unavailable. For this reason it is best to configure\\nthe logon home to reside on a separate file server from the PDC and BDC.\\n## **Further reading**\\n\\n[• For in depth Samba configurations see the Samba HOWTO Collection.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/)\\n\\n[• The guide is also available in printed format.](http://www.amazon.com/exec/obidos/tg/detail/-/0131882228)\\n\\n[• O’Reilly’s Using Samba is also a good reference.](http://www.oreilly.com/catalog/9780596007690/)\\n\\n[• Chapter 4 of the Samba HOWTO Collection explains setting up a Primary Domain Controller.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/samba-pdc.html)\\n\\n[• Chapter 5 of the Samba HOWTO Collection explains setting up a Backup Domain Controller.](https://www.samba.org/samba/docs/old/Samba3-HOWTO/samba-bdc.html)\\n\\n[• The Ubuntu Wiki Samba page.](https://help.ubuntu.com/community/Samba)\\n\\n**Note** :\\nThis section is flagged as *legacy* because nowadays, Samba 4 is best integrated with its own LDAP server\\nin Active Directory mode. Integrating Samba with LDAP as described here covers the NT4 mode, which\\nhas been deprecated for many years.\\n\\nThis section covers the integration of Samba with LDAP. The Samba server’s role will be that of a “standalone” server\\nand the LDAP directory will provide the authentication layer in addition to containing the user, group, and machine\\naccount information that Samba requires in order to function (in any of its 3 possible roles).',\n",
       " 'The pre-requisite is an\\nOpenLDAP server configured with a directory that can accept authentication requests. See Install LDAP and LDAP\\nwith Transport Layer Security for details on fulfilling this requirement. Once those steps are completed, you will need\\nto decide what specifically you want Samba to do for you and then configure it accordingly.\\n\\n263\\n\\n\\n-----\\n\\nThis guide will assume that the LDAP and Samba services are running on the same server and therefore use SASL\\nEXTERNAL authentication whenever changing something under *cn=config* . If that is not your scenario, you will have\\nto run those LDAP commands on the LDAP server.\\n## **Install the software**\\n\\nThere are two packages needed when integrating Samba with LDAP: samba and smbldap-tools .\\n\\nStrictly speaking, the smbldap-tools package isn’t needed, but unless you have some other way to manage the various\\nSamba entities (users, groups, computers) in an LDAP context then you should install it.\\n\\nInstall these packages now:\\n\\nsudo apt install samba smbldap-tools\\n## **Configure LDAP**\\n\\nWe will now configure the LDAP server so that it can accommodate Samba data. We will perform three tasks in this\\n\\nsection:\\n\\n  - Import a schema\\n\\n  - Index some entries\\n\\n  - Add objects\\n\\n**Samba schema**\\n\\nIn order for OpenLDAP to be used as a backend for Samba, the DIT will need to use attributes that can properly\\ndescribe Samba data. Such attributes can be obtained by introducing a Samba LDAP schema. Let’s do this now.\\n\\nThe schema is found in the now-installed samba package and is already in the LDIF format. We can import it with\\none simple command:\\n\\nsudo ldapadd -Q -Y EXTERNAL -H ldapi:/// -f /usr/share/doc/samba/examples/LDAP/samba.ldif',\n",
       " \"To query and view this new schema:\\n\\nsudo ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=schema,cn=config 'cn=*samba*'\\n\\n**Samba indices**\\n\\nNow that slapd knows about the Samba attributes, we can set up some indices based on them. Indexing entries is a\\nway to improve performance when a client performs a filtered search on the DIT.\\n\\nCreate the file samba_indices.ldif with the following contents:\\n\\ndn: olcDatabase={1}mdb,cn=config\\n\\nchangetype: modify\\n\\nreplace: olcDbIndex\\n\\nolcDbIndex: objectClass eq\\n\\nolcDbIndex: uidNumber,gidNumber eq\\n\\nolcDbIndex: loginShell eq\\n\\nolcDbIndex: uid,cn eq,sub\\n\\nolcDbIndex: memberUid eq,sub\\n\\nolcDbIndex: member,uniqueMember eq\\n\\nolcDbIndex: sambaSID eq\\n\\nolcDbIndex: sambaPrimaryGroupSID eq\\n\\nolcDbIndex: sambaGroupType eq\\n\\nolcDbIndex: sambaSIDList eq\\n\\nolcDbIndex: sambaDomainName eq\\n\\nolcDbIndex: default sub,eq\\n\\nUsing the ldapmodify utility load the new indices:\\n\\nsudo ldapmodify -Q -Y EXTERNAL -H ldapi:/// -f samba_indices.ldif\\n\\nIf all went well you should see the new indices when using ldapsearch :\\n\\nsudo ldapsearch -Q -LLL -Y EXTERNAL -H \\\\\\n\\nldapi:/// -b cn=config olcDatabase={1}mdb olcDbIndex\\n\\n264\\n\\n\\n-----\\n\\n**Adding Samba LDAP objects**\\n\\nNext, configure the smbldap-tools package to match your environment. The package comes with a configuration helper\\nscript called smbldap-config . Before running it, though, you should decide on two important configuration settings in\\n\\n/etc/samba/smb.conf :\\n\\n - **netbios name**\\n\\nHow this server will be known. The default value is derived from the server’s hostname, but truncated at 15\\ncharacters.\\n\\n - **workgroup**\\nThe workgroup name for this server, or, if you later decide to make it a domain controller, this will be the\",\n",
       " \"domain.\\n\\nIt’s important to make these choices now because smbldap-config will use them to generate the config that will be later\\nstored in the LDAP directory. If you run smbldap-config now and later change these values in /etc/samba/smb.conf\\nthere will be an inconsistency.\\n\\nOnce you are happy with netbios name and workgroup, proceed to generate the smbldap-tools configuration by running\\nthe configuration script which will ask you some questions:\\n\\nsudo smbldap-config\\n\\nSome of the more important ones:\\n\\n - **workgroup name**\\nHas to match what you will configure in /etc/samba/smb.conf later on.\\n\\n - **ldap suffix**\\nHas to match the LDAP suffix you chose when you configured the LDAP server.\\n\\n - **other ldap suffixes**\\nThey are all relative to ldap suffix above. For example, for ldap user suffix you should use ou=People, and\\nfor computer/machines, use ou=Computers .\\n\\n - **ldap master bind dn** and **bind password**\\nUse the Root DN credentials.\\n\\nThe smbldap-populate script will then add the LDAP objects required for Samba. It will ask you for a password for\\nthe “domain root” user, which is also the “root” user stored in LDAP:\\n\\nsudo smbldap-populate -g 10000 -u 10000 -r 10000\\n\\nThe -g, -u and -r parameters tell smbldap-tools where to start the numeric uid and gid allocation for the LDAP users.\\nYou should pick a range start that does not overlap with your local /etc/passwd users.\\n\\nYou can create a LDIF file containing the new Samba objects by executing sudo smbldap-populate -e samba.ldif .\\nThis allows you to look over the changes making sure everything is correct. If it is, rerun the script without the '-e'\\nswitch.\",\n",
       " 'Alternatively, you can take the LDIF file and import its data as per usual.\\n\\nYour LDAP directory now has the necessary information to authenticate Samba users.\\n## **Samba configuration**\\n\\nTo configure Samba to use LDAP, edit its configuration file /etc/samba/smb.conf commenting out the default passdb\\n\\nbackend parameter and adding some LDAP-related ones. Make sure to use the same values you used when running\\n\\nsmbldap-populate :\\n\\n# passdb backend = tdbsam\\n\\nworkgroup = EXAMPLE\\n\\n# LDAP Settings\\n\\npassdb backend = ldapsam:ldap://ldap01.example.com\\n\\nldap suffix = dc=example,dc=com\\n\\nldap user suffix = ou=People\\n\\nldap group suffix = ou=Groups\\n\\nldap machine suffix = ou=Computers\\n\\nldap idmap suffix = ou=Idmap\\n\\nldap admin dn = cn=admin,dc=example,dc=com\\n\\nldap ssl = start tls\\n\\nldap passwd sync = yes\\n\\n265\\n\\n\\n-----\\n\\nChange the values to match your environment.\\n\\n**Note** :\\nThe smb.conf as shipped by the package is quite long and has many configuration examples. An easy way\\nto visualise it without any comments is to run testparm -s .\\n\\nNow inform Samba about the Root DN user’s password (the one set during the installation of the slapd package):\\n\\nsudo smbpasswd -W\\n\\nAs a final step to have your LDAP users be able to connect to Samba and authenticate, we need these users to also\\nshow up in the system as “Unix” users. Use SSSD for that as detailed in Network User Authentication with SSSD.\\n\\nInstall sssd-ldap :\\n\\nsudo apt install sssd-ldap\\n\\nConfigure /etc/sssd/sssd.conf :\\n\\n[sssd]\\n\\nconfig_file_version = 2\\n\\ndomains = example.com\\n\\n[domain/example.com]\\n\\nid_provider = ldap\\n\\nauth_provider = ldap\\n\\nldap_uri = ldap://ldap01.example.com\\n\\ncache_credentials = True\\n\\nldap_search_base = dc=example,dc=com',\n",
       " 'Adjust permissions and start the service:\\n\\nsudo chmod 0600 /etc/sssd/sssd.conf\\n\\nsudo chown root:root /etc/sssd/sssd.conf\\n\\nsudo systemctl start sssd\\n\\nRestart the Samba services:\\n\\nsudo systemctl restart smbd.service nmbd.service\\n\\nTo quickly test the setup, see if getent can list the Samba groups:\\n\\n$ getent group Replicators\\n\\nReplicators:*:552:\\n\\n**Note** :\\n\\nThe names are case sensitive!\\n\\nIf you have existing LDAP users that you want to include in your new LDAP-backed Samba they will, of course, also\\nneed to be given some of the extra Samba specific attributes. The smbpasswd utility can do this for you:\\n\\nsudo smbpasswd -a username\\n\\nYou will be prompted to enter a password. It will be considered as the new password for that user. Making it the\\nsame as before is reasonable. Note that this command cannot be used to create a new user from scratch in LDAP\\n(unless you are using ldapsam:trusted and ldapsam:editposix, which are not covered in this guide).\\n\\nTo manage user, group, and machine accounts use the utilities provided by the smbldap-tools package. Here are some\\nexamples:\\n\\n  - To add a new user with a home directory:\\n\\nsudo smbldap-useradd -a -P -m username\\n\\nThe -a option adds the Samba attributes, and the -P option calls the smbldap-passwd utility after the user is\\ncreated allowing you to enter a password for the user. Finally, -m creates a local home directory. Test with the\\n\\ngetent command:\\n\\ngetent passwd username\\n\\n  - To remove a user:\\n\\nsudo smbldap-userdel username\\n\\nIn the above command, use the -r option to remove the user’s home directory.\\n\\n266\\n\\n\\n-----\\n\\n  - To add a group:\\n\\nsudo smbldap-groupadd -a groupname\\n\\nAs for *smbldap-useradd*, the *-a* adds the Samba attributes.',\n",
       " '  - To make an existing user a member of a group:\\n\\nsudo smbldap-groupmod -m username groupname\\n\\nThe -m option can add more than one user at a time by listing them in comma-separated format.\\n\\n  - To remove a user from a group:\\n\\nsudo smbldap-groupmod -x username groupname\\n\\n  - To add a Samba machine account:\\n\\nsudo smbldap-useradd -t 0 -w username\\n\\nReplace username with the name of the workstation. The -t 0 option creates the machine account without a\\ndelay, while the -w option specifies the user as a machine account.\\n## **Resources**\\n\\n[• Upstream documentation collection](https://www.samba.org/samba/docs/)\\n\\n[• Upstream samba wiki](https://wiki.samba.org/index.php/Main_Page)\\n\\n**Note** :\\nThis guide does not cover setting up Postfix *Virtual Domains* . For information on Virtual Domains and\\nother advanced configurations see References.\\n## **Install Postfix**\\n\\n[To install Postfix run the following command:](https://www.postfix.org/)\\n\\nsudo apt install postfix\\n\\nIt is OK to accept defaults initially by pressing return for each question. Some of the configuration options will be\\ninvestigated in greater detail in the configuration stage.\\n\\n**Deprecation warning:**\\nThe mail-stack-delivery metapackage has been deprecated in Focal. The package still exists for compatibility reasons, but won’t setup a working email system.\\n## **Configure Postfix**\\n\\nThere are four things you should decide before configuring:\\n\\n  - The <Domain> for which you’ll accept email (we’ll use mail.example.com in our example)\\n\\n  - The network and class range of your mail server (we’ll use 192.168.0.0/24 )\\n\\n  - The username (we’re using steve )',\n",
       " \"  - Type of mailbox format ( mbox is the default, but we’ll use the alternative, Maildir )\\n\\nTo configure postfix, run the following command:\\n\\nsudo dpkg-reconfigure postfix\\n\\nThe user interface will be displayed. On each screen, select the following values:\\n\\n  - Internet Site\\n\\n - mail.example.com\\n\\n - steve\\n\\n - mail.example.com, localhost.localdomain, localhost\\n\\n - No\\n\\n - 127.0.0.0/8 \\\\[::ffff:127.0.0.0\\\\]/104 \\\\[::1\\\\]/128 192.168.0.0/24\\n\\n - 0\\n\\n - +\\n\\n  - all\\n\\nTo set the mailbox format, you can either edit the configuration file directly, or use the postconf command. In either\\ncase, the configuration parameters will be stored in /etc/postfix/main.cf file. Later if you wish to re-configure a\\nparticular parameter, you can either run the command or change it manually in the file.\\n\\n267\\n\\n\\n-----\\n\\n**Configure mailbox format**\\n\\nTo configure the mailbox format for Maildir :\\n\\nsudo postconf -e 'home_mailbox = Maildir/'\\n\\nThis will place new mail in /home/<username>/Maildir so you will need to configure your Mail Delivery Agent (MDA)\\nto use the same path.\\n## **SMTP authentication**\\n\\nSMTP-AUTH allows a client to identify itself through the Simple Authentication and Security Layer (SASL) authentication mechanism, using Transport Layer Security (TLS) to encrypt the authentication process. Once it has been\\nauthenticated, the SMTP server will allow the client to relay mail.\\n\\n**Configure SMTP authentication**\\n\\nTo configure Postfix for SMTP-AUTH using SASL (Dovecot SASL), run these commands at a terminal prompt:\\n\\nsudo postconf -e 'smtpd_sasl_type = dovecot'\\n\\nsudo postconf -e 'smtpd_sasl_path = private/auth'\\n\\nsudo postconf -e 'smtpd_sasl_local_domain ='\",\n",
       " \"sudo postconf -e 'smtpd_sasl_security_options = noanonymous,noplaintext'\\n\\nsudo postconf -e 'smtpd_sasl_tls_security_options = noanonymous'\\n\\nsudo postconf -e 'broken_sasl_auth_clients = yes'\\n\\nsudo postconf -e 'smtpd_sasl_auth_enable = yes'\\n\\nsudo postconf -e 'smtpd_recipient_restrictions = \\\\\\n\\npermit_sasl_authenticated,permit_mynetworks,reject_unauth_destination'\\n\\n**Note** :\\nThe smtpd_sasl_path config parameter is a path relative to the Postfix queue directory.\\n\\nThere are several SASL mechanism properties worth evaluating to improve the security of your deployment. The\\noptions “noanonymous,noplaintext” prevent the use of mechanisms that permit anonymous authentication or that\\ntransmit credentials unencrypted.\\n\\n**Configure TLS**\\n\\nNext, generate or obtain a digital certificate for TLS. MUAs connecting to your mail server via TLS will need to\\nrecognise the certificate used for TLS. This can either be done using a certificate from Let’s Encrypt, from a commercial\\nCA or with a self-signed certificate that users manually install/accept.\\n\\nFor MTA-to-MTA, TLS certificates are never validated without prior agreement from the affected organisations. For\\nMTA-to-MTA TLS, there is no reason not to use a self-signed certificate unless local policy requires it. See our guide\\non security certificates for details about generating digital certificates and setting up your own Certificate Authority\\n(CA).\\n\\nOnce you have a certificate, configure Postfix to provide TLS encryption for both incoming and outgoing mail:\\n\\nsudo postconf -e 'smtp_tls_security_level = may'\\n\\nsudo postconf -e 'smtpd_tls_security_level = may'\\n\\nsudo postconf -e 'smtp_tls_note_starttls_offer = yes'\",\n",
       " 'sudo postconf -e \\'smtpd_tls_key_file = /etc/ssl/private/server.key\\'\\n\\nsudo postconf -e \\'smtpd_tls_cert_file = /etc/ssl/certs/server.crt\\'\\n\\nsudo postconf -e \\'smtpd_tls_loglevel = 1\\'\\n\\nsudo postconf -e \\'smtpd_tls_received_header = yes\\'\\n\\nsudo postconf -e \\'myhostname = mail.example.com\\'\\n\\nIf you are using your own Certificate Authority to sign the certificate, enter:\\n\\nsudo postconf -e \\'smtpd_tls_CAfile = /etc/ssl/certs/cacert.pem\\'\\n\\nAgain, for more details about certificates see our security certificates guide.\\n\\n**Outcome of initial configuration**\\n\\nAfter running all the above commands, Postfix will be configured for SMTP-AUTH with a self-signed certificate for\\nTLS encryption.\\n\\nNow, the file /etc/postfix/main.cf should look like this:\\n\\n268\\n\\n\\n-----\\n\\n# See /usr/share/postfix/main.cf.dist for a commented, more complete\\n\\n# version\\n\\nsmtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)\\n\\nbiff = no\\n\\n# appending .domain is the MUA\\'s job.\\n\\nappend_dot_mydomain = no\\n\\n# Uncomment the next line to generate \"delayed mail\" warnings\\n\\n#delay_warning_time = 4h\\n\\nmyhostname = server1.example.com\\n\\nalias_maps = hash:/etc/aliases\\n\\nalias_database = hash:/etc/aliases\\n\\nmyorigin = /etc/mailname\\n\\nmydestination = server1.example.com, localhost.example.com, localhost\\n\\nrelayhost =\\n\\nmynetworks = 127.0.0.0/8\\n\\nmailbox_command = procmail -a \"$EXTENSION\"\\n\\nmailbox_size_limit = 0\\n\\nrecipient_delimiter = +\\n\\ninet_interfaces = all\\n\\nsmtpd_sasl_local_domain =\\n\\nsmtpd_sasl_auth_enable = yes\\n\\nsmtpd_sasl_security_options = noanonymous\\n\\nbroken_sasl_auth_clients = yes\\n\\nsmtpd_recipient_restrictions =\\n\\npermit_sasl_authenticated,permit_mynetworks,reject _unauth_destination\\n\\nsmtpd_tls_auth_only = no\\n\\nsmtp_tls_security_level = may',\n",
       " \"smtpd_tls_security_level = may\\n\\nsmtp_tls_note_starttls_offer = yes\\n\\nsmtpd_tls_key_file = /etc/ssl/private/smtpd.key\\n\\nsmtpd_tls_cert_file = /etc/ssl/certs/smtpd.crt\\n\\nsmtpd_tls_CAfile = /etc/ssl/certs/cacert.pem\\n\\nsmtpd_tls_loglevel = 1\\n\\nsmtpd_tls_received_header = yes\\n\\nsmtpd_tls_session_cache_timeout = 3600s\\n\\ntls_random_source = dev:/dev/urandom\\n\\nThe Postfix initial configuration is now complete. Run the following command to restart the Postfix daemon:\\n\\nsudo systemctl restart postfix.service\\n## **SASL**\\n\\n[Postfix supports SMTP-AUTH as defined in RFC2554. It is based on SASL. However it is still necessary to set up](http://www.ietf.org/rfc/rfc2554.txt)\\nSASL authentication before you can use SMTP-AUTH.\\n\\nWhen using IPv6, the mynetworks parameter may need to be modified to allow IPv6 addresses, for example:\\n\\nmynetworks = 127.0.0.0/8, [::1]/128\\n\\n**Configure SASL**\\n\\nPostfix supports two SASL implementations: **Cyrus SASL** and **Dovecot SASL** .\\n\\nTo enable Dovecot SASL the dovecot-core package will need to be installed:\\n\\nsudo apt install dovecot-core\\n\\nNext, edit /etc/dovecot/conf.d/10-master.conf and change the following:\\n\\nservice auth {\\n\\n# auth_socket_path points to this userdb socket by default. It's typically\\n\\n# used by dovecot-lda, doveadm, possibly imap process, etc. Its default\\n\\n269\\n\\n\\n-----\\n\\n# permissions make it readable only by root, but you may need to relax these\\n\\n# permissions. Users that have access to this socket are able to get a list\\n\\n# of all usernames and get results of everyone's userdb lookups.\\n\\nunix_listener auth-userdb {\\n\\n#mode = 0600\\n\\n#user =\\n\\n#group =\\n\\n}\\n\\n# Postfix smtp-auth\\n\\nunix_listener /var/spool/postfix/private/auth {\\n\\nmode = 0660\\n\\nuser = postfix\",\n",
       " 'group = postfix\\n\\n}\\n\\n}\\n\\nTo permit use of SMTP-AUTH by Outlook clients, change the following line in the **authentication mechanisms**\\nsection of /etc/dovecot/conf.d/10-auth.conf from:\\n\\nauth_mechanisms = plain\\n\\nto this:\\n\\nauth_mechanisms = plain login\\n\\nOnce you have configured Dovecot, restart it with:\\n\\nsudo systemctl restart dovecot.service\\n## **Test your setup**\\n\\nSMTP-AUTH configuration is complete – now it is time to test the setup. To see if SMTP-AUTH and TLS work\\nproperly, run the following command:\\n\\ntelnet mail.example.com 25\\n\\nAfter you have established the connection to the Postfix mail server, type:\\n\\nehlo mail.example.com\\n\\nIf you see the following in the output, then everything is working perfectly. Type quit to exit.\\n\\n250-STARTTLS\\n\\n250-AUTH LOGIN PLAIN\\n\\n250-AUTH=LOGIN PLAIN\\n\\n250 8BITMIME\\n## **Troubleshooting**\\n\\nWhen problems arise, there are a few common ways to diagnose the cause.\\n\\n**Escaping** chroot\\n\\nThe Ubuntu Postfix package will, by default, install into a chroot environment for security reasons. This can add\\ngreater complexity when troubleshooting problems.\\n\\nTo turn off the chroot usage, locate the following line in the /etc/postfix/master.cf configuration file:\\n\\nsmtp inet n - - - - smtpd\\n\\nModify it as follows:\\n\\nsmtp inet n - n - - smtpd\\n\\nYou will then need to restart Postfix to use the new configuration. From a terminal prompt enter:\\n\\nsudo service postfix restart\\n\\n270\\n\\n\\n-----\\n\\n**SMTPS**\\n\\nIf you need secure SMTP, edit /etc/postfix/master.cf and uncomment the following line:\\n\\nsmtps inet n - - - - smtpd\\n\\n-o smtpd_tls_wrappermode=yes\\n\\n-o smtpd_sasl_auth_enable=yes\\n\\n-o smtpd_client_restrictions=permit_sasl_authenticated,reject',\n",
       " \"-o milter_macro_daemon_name=ORIGINATING\\n\\n**Log viewing**\\n\\nPostfix sends all log messages to /var/log/mail.log . However, error and warning messages can sometimes get lost in\\nthe normal log output so they are also logged to /var/log/mail.err and /var/log/mail.warn respectively.\\n\\nTo see messages entered into the logs in real time you can use the tail -f command:\\n\\ntail -f /var/log/mail.err\\n\\n**Increase logging detail**\\n\\nThe amount of detail recorded in the logs can be increased via the configuration options. For example, to increase\\nTLS activity logging set the smtpd_tls_loglevel option to a value from 1 to 4.\\n\\nsudo postconf -e 'smtpd_tls_loglevel = 4'\\n\\nReload the service after any configuration change, to activate the new config:\\n\\nsudo systemctl reload postfix.service\\n\\n**Logging mail delivery**\\n\\nIf you are having trouble sending or receiving mail from a specific domain you can add the domain to the de\\nbug_peer_list parameter.\\n\\nsudo postconf -e 'debug_peer_list = problem.domain'\\n\\nsudo systemctl reload postfix.service\\n\\n**Increase daemon verbosity**\\n\\nYou can increase the verbosity of any Postfix daemon process by editing the /etc/postfix/master.cf and adding a -v\\nafter the entry. For example, edit the smtp entry:\\n\\nsmtp unix - - - - - smtp -v\\n\\nThen, reload the service as usual:\\n\\nsudo systemctl reload postfix.service\\n\\n**Log SASL debug info**\\n\\nTo increase the amount of information logged when troubleshooting SASL issues you can set the following options in\\n\\n/etc/dovecot/conf.d/10-logging.conf\\n\\nauth_debug=yes\\n\\nauth_debug_passwords=yes\\n\\nAs with Postfix, if you change a Dovecot configuration the process will need to be reloaded:\\n\\nsudo systemctl reload dovecot.service\\n\\n**Note** :\",\n",
       " 'Some of the options above can drastically increase the amount of information sent to the log files. Remember\\nto return the log level back to normal after you have corrected the problem – then reload the appropriate\\ndaemon for the new configuration to take effect.\\n\\n**References**\\n\\nAdministering a Postfix server can be a very complicated task. At some point you may need to turn to the Ubuntu\\ncommunity for more experienced help.\\n\\n[• The Postfix website documents all available configuration options.](http://www.postfix.org/documentation.html)\\n\\n271\\n\\n\\n-----\\n\\n[• O’Reilly’s Postfix: The Definitive Guide is rather dated but provides deep background information about con-](http://shop.oreilly.com/product/9780596002121.do)\\nfiguration options.\\n\\n[• The Ubuntu Wiki Postfix page has more information from an Ubuntu context.](https://help.ubuntu.com/community/Postfix)\\n\\n[• There is also a Debian Wiki Postfix page that’s a bit more up to date; they also have a set of Postfix Tutorials](https://wiki.debian.org/Postfix)\\n\\nfor different Debian versions.\\n\\n[• Info on how to set up mailman3 with postfix.](https://mailman.readthedocs.io/en/latest/src/mailman/docs/mta.html#postfix)\\n## **Install Dovecot**\\n\\nTo install a basic Dovecot server with common POP3 and IMAP functions, run the following command:\\n\\nsudo apt install dovecot-imapd dovecot-pop3d\\n\\nThere are various other Dovecot modules including dovecot-sieve (mail filtering), dovecot-solr (full text search),\\n\\ndovecot-antispam (spam filter training), dovecot-ldap (user directory).\\n## **Configure Dovecot**\\n\\nTo configure Dovecot, edit the file /etc/dovecot/dovecot.conf and its included config files in /etc/dovecot/conf.d/ .',\n",
       " 'By default, all installed protocols will be enabled via an *include* directive in /etc/dovecot/dovecot.conf .\\n\\n!include_try /usr/share/dovecot/protocols.d/*.protocol\\n\\nIMAPS and POP3S are more secure because they use SSL encryption to connect. A basic self-signed SSL certificate\\nis automatically set up by package ssl-cert and used by Dovecot in /etc/dovecot/conf.d/10-ssl.conf .\\n\\nMbox format is configured by default, but you can also use Maildir if required. More details can be found in the\\ncomments in /etc/dovecot/conf.d/10-mail.conf [. Also see the Dovecot web site to learn about further benefits and](https://doc.dovecot.org/admin_manual/mailbox_formats/)\\ndetails.\\n\\nMake sure to also configure your chosen Mail Transport Agent (MTA) to transfer the incoming mail to the selected\\ntype of mailbox.\\n\\n**Restart the Dovecot daemon**\\n\\nOnce you have configured Dovecot, restart its daemon in order to test your setup using the following command:\\n\\nsudo service dovecot restart\\n\\nTry to log in with the commands telnet localhost pop3 (for POP3) or telnet localhost imap2 (for IMAP). You\\nshould see something like the following:\\n\\nbhuvan@rainbow:~$ telnet localhost pop3\\n\\nTrying 127.0.0.1...\\n\\nConnected to localhost.localdomain.\\n\\nEscape character is \\'^]\\'.\\n\\n+OK Dovecot ready.\\n## **Dovecot SSL configuration**\\n\\nBy default, Dovecot is configured to use SSL automatically using the package ssl-cert which provides a self signed\\ncertificate.\\n\\nYou can instead generate your own custom certificate for Dovecot using openssh, for example:\\n\\nsudo openssl req -new -x509 -days 1000 -nodes -out \"/etc/dovecot/dovecot.pem\" \\\\\\n\\n-keyout \"/etc/dovecot/private/dovecot.pem\"',\n",
       " 'Next, edit /etc/dovecot/conf.d/10-ssl.conf and amend following lines to specify that Dovecot should use these custom\\ncertificates :\\n\\nssl_cert = </etc/dovecot/private/dovecot.pem\\n\\nssl_key = </etc/dovecot/private/dovecot.key\\n\\nYou can get the SSL certificate from a Certificate Issuing Authority or you can create self-signed one. Once you create\\nthe certificate, you will have a key file and a certificate file that you want to make known in the config shown above.\\n\\n**Further reading** :\\nFor more details on creating custom certificates, see our guide on security certificates.\\n\\n272\\n\\n\\n-----\\n\\n## **Configure a firewall for an email server**\\n\\nTo access your mail server from another computer, you must configure your firewall to allow connections to the server\\non the necessary ports.\\n\\n - IMAP - 143\\n\\n - IMAPS - 993\\n\\n - POP3 - 110\\n\\n - POP3S - 995\\n## **References**\\n\\n[• The Dovecot website has more general information about Dovecot.](http://www.dovecot.org/)\\n\\n[• The Dovecot manual provides full documentation for Dovecot use.](https://doc.dovecot.org)\\n\\n[• The Dovecot Ubuntu Wiki page has more details on configuration.](https://help.ubuntu.com/community/Dovecot)\\n## **Install Exim4**\\n\\n[To install Exim4, run the following command:](https://www.exim.org/)\\n\\nsudo apt install exim4\\n## **Configure Exim4**\\n\\nTo configure Exim4, run the following command:\\n\\nsudo dpkg-reconfigure exim4-config\\n\\nThis displays a “wizard” user interface for configuring the software. For example, in Exim4 the configuration files are\\nsplit amongst multiple files by default; if you wish to have them in one file you can configure this via the user interface.',\n",
       " 'All configurable parameters from the user interface are stored in the /etc/exim4/update-exim4.conf.conf file. To\\nre-configure the software you can either re-run the wizard, or manually edit this file using your preferred editor.\\n\\nOnce you are finished, you can run the following command to generate the master configuration file:\\n\\nsudo update-exim4.conf\\n\\nThe master configuration file is stored in /var/lib/exim4/config.autogenerated .\\n\\n**Warning** :\\nYou should never manually edit the master configuration file, /var/lib/exim4/config.autogenerated, because it is updated automatically every time you run update-exim4.conf . Any changes you make to this\\nfile will be lost during future updates.\\n## **Start the Exim4 daemon**\\n\\nThe following command will start the Exim4 daemon:\\n\\nsudo service exim4 start\\n## **SMTP authentication**\\n\\nExim4 can be configured to use SMTP-AUTH with Transport Layer Security (TLS) and Simple Authentication and\\nSecurity Layer (SASL).\\n\\nFirst, enter the following into a terminal prompt to create a certificate for use with TLS:\\n\\nsudo /usr/share/doc/exim4-base/examples/exim-gencert\\n\\nConfigure Exim4 for TLS by editing the /etc/exim4/conf.d/main/03_exim4-config_tlsoptions file and adding the\\nfollowing:\\n\\nMAIN_TLS_ENABLE = yes\\n\\nNext, configure Exim4 to use the saslauthd daemon for authentication by editing /etc/exim4/conf.d/auth/30_exim4\\nconfig_examples – uncomment the plain_saslauthd_server and login_saslauthd_server sections:\\n\\n273\\n\\n\\n-----\\n\\nplain_saslauthd_server:\\n\\ndriver = plaintext\\n\\npublic_name = PLAIN\\n\\nserver_condition = ${if saslauthd{{$auth2}{$auth3}}{1}{0}}\\n\\nserver_set_id = $auth2\\n\\nserver_prompts = :\\n\\n.ifndef AUTH_SERVER_ALLOW_NOTLS_PASSWORDS',\n",
       " 'server_advertise_condition = ${if eq{$tls_cipher}{}{}{*}}\\n\\n.endif\\n\\nlogin_saslauthd_server:\\n\\ndriver = plaintext\\n\\npublic_name = LOGIN\\n\\nserver_prompts = \"Username:: : Password::\"\\n\\n# don\\'t send system passwords over unencrypted connections\\n\\nserver_condition = ${if saslauthd{{$auth1}{$auth2}}{1}{0}}\\n\\nserver_set_id = $auth1\\n\\n.ifndef AUTH_SERVER_ALLOW_NOTLS_PASSWORDS\\n\\nserver_advertise_condition = ${if eq{$tls_cipher}{}{}{*}}\\n\\n.endif\\n\\nTo enable outside mail clients to connect to the new server, a new user needs to be added into Exim4 by using the\\nfollowing commands:\\n\\nsudo /usr/share/doc/exim4-base/examples/exim-adduser\\n\\nProtect the new password files with the following commands:\\n\\nsudo chown root:Debian-exim /etc/exim4/passwd\\n\\nsudo chmod 640 /etc/exim4/passwd\\n\\nFinally, update the Exim4 configuration and restart the service:\\n\\nsudo update-exim4.conf\\n\\nsudo systemctl restart exim4.service\\n## **Configure SASL**\\n\\nTo configure saslauthd to provide authentication for Exim4, first install the sasl2-bin package by running this command at a terminal prompt:\\n\\nsudo apt install sasl2-bin\\n\\nTo configure saslauthd, edit the /etc/default/saslauthd configuration file and set:\\n\\nSTART=yes\\n\\nNext, to make Exim4 use the saslauthd service, the *Debian-exim* user needs to be part of the *sasl* group:\\n\\nsudo adduser Debian-exim sasl\\n\\nFinally, start the saslauthd service:\\n\\nsudo service saslauthd start\\n\\nExim4 is now configured with SMTP-AUTH using TLS and SASL authentication.\\n## **References**\\n\\n[• See exim.org for more information.](http://www.exim.org/)\\n\\n[• Another resource is the Exim4 Ubuntu Wiki page.](https://help.ubuntu.com/community/Exim4)',\n",
       " '[• Further resources to set up mailman3 with Exim4.](https://mailman.readthedocs.io/en/latest/src/mailman/docs/mta.html#exim)\\n\\n[Bacula is a backup management tool that enables you to backup, restore, and verify data across your network. There](http://www.bacula.org/)\\nare Bacula clients for Linux, Windows, and Mac OS X – making it a cross-platform and network-wide solution.\\n## **Bacula components**\\n\\nBacula is made up of several components and services that are used to manage backup files and locations:\\n\\n274\\n\\n\\n-----\\n\\n - **Bacula Director** : A service that controls all backup, restore, verify, and archive operations.\\n\\n - **Bacula Console** : An application that allows communication with the Director. There are three versions of the\\nConsole:\\n\\n**–** Text-based command line.\\n\\n**–**\\nGnome-based GTK+ Graphical User Interface (GUI) interface.\\n\\n**–**\\nwxWidgets GUI interface.\\n\\n - **Bacula File** : Also known as the Bacula Client program. This application is installed on machines to be backed\\nup, and is responsible for handling data requested by the Director.\\n\\n - **Bacula Storage** : The program that performs the storage of data onto, and recovery of data from, the physical\\nmedia.\\n\\n - **Bacula Catalog** : Responsible for maintaining the file indices and volume databases for all backed-up files. This\\nenables rapid location and restoration of archived files. The Catalog supports three different databases: MySQL,\\nPostgreSQL, and SQLite.\\n\\n - **Bacula Monitor** : Monitors the Director, File daemons, and Storage daemons. Currently the Monitor is only\\navailable as a GTK+ GUI application.',\n",
       " 'These services and applications can be run on multiple servers and clients, or they can be installed on one machine if\\nbacking up a single disk or volume.\\n## **Install Bacula**\\n\\n**Note** :\\n\\nIf using MySQL or PostgreSQL as your database, you should already have the services available. Bacula\\nwill not install them for you. For more information, take a look at MySQL databases and PostgreSQL\\ndatabases.\\n\\nThere are multiple packages containing the different Bacula components. To install bacula, from a terminal prompt\\n\\nenter:\\n\\nsudo apt install bacula\\n\\nBy default, installing the bacula package will use a PostgreSQL database for the Catalog. If you want to use SQLite\\nor MySQL for the Catalog instead, install bacula-director-sqlite3 or bacula-director-mysql respectively.\\n\\nDuring the install process you will be asked to supply a password for the *database owner* of the *bacula database* .\\n## **Configure Bacula**\\n\\nBacula configuration files are formatted based on **resources** composed of **directives** surrounded by curly “{}” braces.\\nEach Bacula component has an individual file in the /etc/bacula directory.\\n\\nThe various Bacula components must authorise themselves to each other. This is accomplished using the **password**\\ndirective. For example, the Storage resource password in the /etc/bacula/bacula-dir.conf file must match the Director\\nresource password in /etc/bacula/bacula-sd.conf .\\n\\nBy default, the backup job named BackupClient1 is configured to archive the Bacula Catalog. If you plan on using\\nthe server to back up more than one client you should change the name of this job to something more descriptive. To\\nchange the name, edit /etc/bacula/bacula-dir.conf :\\n\\n#',\n",
       " '# Define the main nightly save backup job\\n\\n# By default, this job will back up to disk in\\n\\nJob {\\n\\nName = \"BackupServer\"\\n\\nJobDefs = \"DefaultJob\"\\n\\nWrite Bootstrap = \"/var/lib/bacula/Client1.bsr\"\\n\\n}\\n\\n**Note** :\\n\\nThe example above changes the job name to “BackupServer”, matching the machine’s host name. Replace\\n“BackupServer” with your own hostname, or other descriptive name.\\n\\nThe Console can be used to query the Director about jobs, but to use the Console with a *non-root* user, the user needs\\nto be in the **Bacula group** . To add a user to the Bacula group, run the following command from a terminal:\\n\\n275\\n\\n\\n-----\\n\\nsudo adduser $username bacula\\n\\n**Note** :\\n\\nReplace $username with the actual username. Also, if you are adding the current user to the group you\\nshould log out and back in for the new permissions to take effect.\\n## **Localhost backup**\\n\\nThis section shows how to back up specific directories on a single host to a local tape drive.\\n\\n  - First, the Storage device needs to be configured. Edit /etc/bacula/bacula-sd.conf and add:\\n\\nDevice {\\n\\nName = \"Tape Drive\"\\n\\nDevice Type = tape\\n\\nMedia Type = DDS-4\\n\\nArchive Device = /dev/st0\\n\\nHardware end of medium = No;\\n\\nAutomaticMount = yes; # when device opened, read it\\n\\nAlwaysOpen = Yes;\\n\\nRemovableMedia = yes;\\n\\nRandomAccess = no;\\n\\nAlert Command = \"sh -c \\'tapeinfo -f %c | grep TapeAlert\\'\"\\n\\n}\\n\\nThe example is for a DDS-4 tape drive. Adjust the “Media Type” and “Archive Device” to match your hardware.\\nAlternatively, you could also uncomment one of the other examples in the file.\\n\\n  - After editing /etc/bacula/bacula-sd.conf, the Storage daemon will need to be restarted:\\n\\nsudo systemctl restart bacula-sd.service',\n",
       " '  - Now add a Storage resource in /etc/bacula/bacula-dir.conf to use the new Device:\\n\\n# Definition of \"Tape Drive\" storage device\\n\\nStorage {\\n\\nName = TapeDrive\\n\\n# Do not use \"localhost\" here\\n\\nAddress = backupserver # N.B. Use a fully qualified name here\\n\\nSDPort = 9103\\n\\nPassword = \"Cv70F6pf1t6pBopT4vQOnigDrR0v3LT3Cgkiyjc\"\\n\\nDevice = \"Tape Drive\"\\n\\nMedia Type = tape\\n\\n}\\n\\nNote:\\n\\n**–** The **Address** directive needs to be the Fully Qualified Domain Name (FQDN) of the server.\\n**–** Change backupserver to the actual host name.\\n**–** Make sure the **Password** directive matches the password string in /etc/bacula/bacula-sd.conf .\\n\\n - Create a new **FileSet** – this will define which directories to backup – by adding:\\n\\n# LocalhostBacup FileSet.\\n\\nFileSet {\\n\\nName = \"LocalhostFiles\"\\n\\nInclude {\\n\\nOptions {\\n\\nsignature = MD5\\n\\ncompression=GZIP\\n\\n}\\n\\nFile = /etc\\n\\nFile = /home\\n\\n}\\n\\n}\\n\\nThis FileSet will backup the /etc and /home directories. The **Options** resource directives configure the FileSet\\nto create an MD5 signature for each file backed up, and to compress the files using GZIP.\\n\\n  - Next, create a new **Schedule** for the backup job:\\n\\n276\\n\\n\\n-----\\n\\n# LocalhostBackup Schedule -- Daily.\\n\\nSchedule {\\n\\nName = \"LocalhostDaily\"\\n\\nRun = Full daily at 00:01\\n\\n}\\n\\nThe job will run every day at 00:01 or 12:01 am. There are many other scheduling options available.\\n\\n  - Finally, create the **Job** :\\n\\n# Localhost backup.\\n\\nJob {\\n\\nName = \"LocalhostBackup\"\\n\\nJobDefs = \"DefaultJob\"\\n\\nEnabled = yes\\n\\nLevel = Full\\n\\nFileSet = \"LocalhostFiles\"\\n\\nSchedule = \"LocalhostDaily\"\\n\\nStorage = TapeDrive\\n\\nWrite Bootstrap = \"/var/lib/bacula/LocalhostBackup.bsr\"\\n\\n}\\n\\nThe Job will do a **Full** backup every day to the tape drive.',\n",
       " '  - Each tape used will need to have a **Label** . If the current tape does not have a Label, Bacula will send an email\\nletting you know. To label a tape using the Console enter the following command from a terminal:\\n\\nbconsole\\n\\n  - At the Bacula Console prompt enter:\\n\\nlabel\\n\\n  - You will then be prompted for the Storage resource:\\n\\nAutomatically selected Catalog: MyCatalog\\n\\nUsing Catalog \"MyCatalog\"\\n\\nThe defined Storage resources are:\\n\\n1: File\\n\\n2: TapeDrive\\n\\nSelect Storage resource (1-2):2\\n\\n - Enter the new **Volume** name:\\n\\nEnter new Volume name: Sunday\\n\\nDefined Pools:\\n\\n1: Default\\n\\n2: Scratch\\n\\nReplace “Sunday” with the desired label.\\n\\n  - Now, select the **Pool** :\\n\\nSelect the Pool (1-2): 1\\n\\nConnecting to Storage daemon TapeDrive at backupserver:9103 ...\\n\\nSending label command for Volume \"Sunday\" Slot 0 ...\\n\\nCongratulations, you have now configured Bacula to backup the localhost to an attached tape drive.\\n## **Further reading**\\n\\n[• For more Bacula configuration options, refer to the Bacula documentation.](https://www.bacula.org/documentation/documentation/)\\n\\n[• The Bacula home page contains the latest Bacula news and developments.](http://www.bacula.org/)\\n\\n[• Also, see the Bacula Ubuntu Wiki page.](https://help.ubuntu.com/community/Bacula)\\n\\n[rsnapshot is an rsync-based filesystem snapshot utility. It can take incremental backups of local and remote filesystems](https://rsnapshot.org/)\\nfor any number of machines. rsnapshot makes extensive use of hard links, so disk space is only used when absolutely\\nnecessary. It leverages the power of rsync to create scheduled, incremental backups.\\n\\n277\\n\\n\\n-----\\n\\n## **Install rsnapshot**\\n\\nTo install rsnapshot open a terminal shell and run:',\n",
       " 'sudo apt-get install rsnapshot\\n\\nIf you want to backup a remote filesystem, the rsnapshot server needs to be able to access the target machine over SSH\\nwithout password. For more information on how to enable this please see OpenSSH documentation. If the backup\\ntarget is a local filesystem there is no need to set up OpenSSH.\\n## **Configure rsnapshot**\\n\\nThe rsnapshot configuration resides in /etc/rsnapshot.conf . Below you can find some of the options available there.\\n\\nThe root directory where all snapshots will be stored is found at:\\n\\nsnapshot_root /var/cache/rsnapshot/\\n\\n**Number of backups to keep**\\n\\nSince rsnapshot uses incremental backups, we can afford to keep older backups for a while before removing them. You\\nset these up under the BACKUP LEVELS / INTERVALS section. You can tell rsnapshot to retain a specific number of\\nbackups of each kind of interval.\\n\\nretain daily 6\\n\\nretain weekly 7\\n\\nretain monthly 4\\n\\nIn this example we will keep 6 snapshots of our daily strategy, 7 snapshots of our weekly strategy, and 4 snapshots of\\nour monthly strategy. These data will guide the rotation made by rsnapshot .\\n\\n**Remote machine access**\\n\\nIf you are accessing a remote machine over SSH and the port to bind is not the default (port 22 ), you need to set the\\nfollowing variable with the port number:\\n\\nssh_args -p 22222\\n\\n**What to backup**\\n\\nNow the most important part; you need to decide what you would like to backup.\\n\\nIf you are backing up locally to the same machine, this is as easy as specifying the directories that you want to save\\nand following it with localhost/ which will be a sub-directory in the snapshot_root that you set up earlier.\\n\\nbackup /home/ localhost/',\n",
       " 'backup /etc/ localhost/\\n\\nbackup /usr/local/ localhost/\\n\\nIf you are backing up a remote machine you just need to tell rsnapshot where the server is and which directories you\\nwould like to back up.\\n\\nbackup root@example.com:/home/ example.com/ +rsync_long_args=--bwlimit=16,exclude=core\\n\\nbackup root@example.com:/etc/ example.com/ exclude=mtab,exclude=core\\n\\nAs you can see, you can pass extra rsync parameters (the + appends the parameter to the default list – if you remove\\nthe + sign you override it) and also exclude directories.\\n\\nYou can check the comments in /etc/rsnapshot.conf [and the rsnapshot man page for more options.](http://manpages.ubuntu.com/manpages/focal/man1/rsnapshot.1.html)\\n## **Test configuration**\\n\\nAfter modifying the configuration file, it is good practice to check if the syntax is OK:\\n\\nsudo rsnapshot configtest\\n\\nYou can also test your backup levels with the following command:\\n\\nsudo rsnapshot -t daily\\n\\nIf you are happy with the output and want to see it in action you can run:\\n\\nsudo rsnapshot daily\\n\\n278\\n\\n\\n-----\\n\\n## **Scheduling backups**\\n\\nWith rsnapshot working correctly with the current configuration, the only thing left to do is schedule it to run at certain\\nintervals. We will use cron to make this happen since rsnapshot includes a default cron file in /etc/cron.d/rsnapshot .\\nIf you open this file there are some entries commented out as reference.\\n\\n0 4 - * * root /usr/bin/rsnapshot daily\\n\\n0 3 - * 1 root /usr/bin/rsnapshot weekly\\n\\n0 2 1 * * root /usr/bin/rsnapshot monthly\\n\\nThe settings above added to /etc/cron.d/rsnapshot run:\\n\\n - The **daily snapshot** everyday at 4:00 am\\n\\n - The **weekly snapshot** every Monday at 3:00 am',\n",
       " ' - The **monthly snapshot** on the first of every month at 2:00 am\\n\\nFor more information on how to schedule a backup using cron please take a look at the Executing with cron section\\nin Backups - Shell Scripts.\\n\\n**Further reading**\\n\\n[• rsnapshot offical web page](https://rsnapshot.org/)\\n\\n[• rsnapshot man page](http://manpages.ubuntu.com/manpages/focal/man1/rsnapshot.1.html)\\n\\n[• rsync man page](http://manpages.ubuntu.com/manpages/focal/man1/rsync.1.html)\\n\\nIn general, a shell script configures which directories to backup, and passes those directories as arguments to the tar\\nutility, which creates an archive file. The archive file can then be moved or copied to another location. The archive\\ncan also be created on a remote file system such as a Network File System (NFS) mount.\\n\\nThe tar utility creates one archive file out of many files or directories. tar can also filter the files through compression\\nutilities, thus reducing the size of the archive file.\\n\\nIn this guide, we will walk through how to use a shell script for backing up files, and how to restore files from the\\narchive we create.\\n## **The shell script**\\n\\nThe following shell script uses the basic backup shell script from our Reference section. It uses tar to create an\\narchive file on a remotely mounted NFS file system. The archive filename is determined using additional command\\nline utilities. For more details about the script, check out the example Reference page.\\n\\n#!/bin/bash\\n\\n####################################\\n\\n#\\n\\n# Backup to NFS mount script.\\n\\n#\\n\\n####################################\\n\\n# What to backup.\\n\\nbackup_files=\"/home /var/spool/mail /etc /root /boot /opt\"\\n\\n# Where to backup to.\\n\\ndest=\"/mnt/backup\"',\n",
       " '# Create archive filename.\\n\\nday=$(date +%A)\\n\\nhostname=$(hostname -s)\\n\\narchive_file=\"$hostname-$day.tgz\"\\n\\n# Print start status message.\\n\\necho \"Backing up $backup_files to $dest/$archive_file\"\\n\\ndate\\n\\necho\\n\\n# Backup the files using tar.\\n\\ntar czf $dest/$archive_file $backup_files\\n\\n# Print end status message.\\n\\n279\\n\\n\\n-----\\n\\necho\\n\\necho \"Backup finished\"\\n\\ndate\\n\\n# Long listing of files in $dest to check file sizes.\\n\\nls -lh $dest\\n## **Running the script**\\n\\n**Run from a terminal**\\n\\nThe simplest way to use the above backup script is to copy and paste the contents into a file (called backup.sh, for\\nexample). The file must be made executable:\\n\\nchmod u+x backup.sh\\n\\nThen from a terminal prompt, run the following command:\\n\\nsudo ./backup.sh\\n\\nThis is a great way to test the script to make sure everything works as expected.\\n\\n**Run with** cron\\n\\nThe cron utility can be used to automate use of the script. The cron daemon allows scripts, or commands, to be run\\nat a specified time and date.\\n\\ncron is configured through entries in a crontab file. crontab files are separated into fields:\\n\\n# m h dom mon dow command\\n\\nWhere:\\n\\n - m : The minute the command executes on, between 0 and 59.\\n\\n - h : The hour the command executes on, between 0 and 23.\\n\\n - dom : The day of the month the command executes on.\\n\\n - mon : The month the command executes on, between 1 and 12.\\n\\n - dow : The day of the week the command executes on, between 0 and 7. Sunday may be specified by using 0 or 7,\\nboth values are valid.\\n\\n - command : The command to run.\\n\\nTo add or change entries in a crontab file the crontab -e command should be used. Also note the contents of a crontab\\nfile can be viewed using the crontab -l command.',\n",
       " 'To run the backup.sh script listed above using cron, enter the following from a terminal prompt:\\n\\nsudo crontab -e\\n\\n**Note** :\\n\\nUsing sudo with the crontab -e command edits the *root* user’s crontab . This is necessary if you are backing\\nup directories only the root user has access to.\\n\\nAs an example, if we add the following entry to the crontab file:\\n\\n# m h dom mon dow command\\n\\n0 0 * * * bash /usr/local/bin/backup.sh\\n\\nThe backup.sh script would be run every day at 12:00 pm.\\n\\n**Note** :\\n\\nThe backup.sh script will need to be copied to the /usr/local/bin/ directory in order for this entry to run\\nproperly. The script can reside anywhere on the file system, simply change the script path appropriately.\\n## **Restoring from the archive**\\n\\nOnce an archive has been created, it is important to test the archive. The archive can be tested by listing the files it\\ncontains, but the best test is to **restore** a file from the archive.\\n\\n  - To see a listing of the archive contents, run the following command from a terminal:\\n\\ntar -tzvf /mnt/backup/host-Monday.tgz\\n\\n280\\n\\n\\n-----\\n\\n  - To restore a file from the archive back to a different directory, enter:\\n\\ntar -xzvf /mnt/backup/host-Monday.tgz -C /tmp etc/hosts\\n\\nThe -C option to tar redirects the extracted files to the specified directory. The above example will extract the\\n\\n/etc/hosts file to /tmp/etc/hosts . tar recreates the directory structure that it contains. Also, notice the leading\\n“ / ” is left off the path of the file to restore.\\n\\n  - To restore all files in the archive enter the following:\\n\\ncd /\\n\\nsudo tar -xzvf /mnt/backup/host-Monday.tgz\\n\\n**Note** :\\nThis will overwrite the files currently on the file system.\\n## **Further reading**',\n",
       " '[• For more information on shell scripting see the Advanced Bash-Scripting Guide](http://tldp.org/LDP/abs/html/)\\n\\n[• The CronHowto Wiki Page contains details on advanced cron options.](https://help.ubuntu.com/community/CronHowto)\\n\\n[• See the GNU tar Manual for more tar options.](http://www.gnu.org/software/tar/manual/index.html)\\n\\n[• The Wikipedia Backup Rotation Scheme article contains information on other backup rotation schemes.](http://en.wikipedia.org/wiki/Backup_rotation_scheme)\\n\\n  - The shell script uses tar to create the archive, but there many other command line utilities that can be used.\\nFor example:\\n\\n**–**\\n[cpio: used to copy files to and from archives.](http://www.gnu.org/software/cpio/)\\n\\n**–** [dd: part of the coreutils package. A low level utility that can copy data from one format to another.](http://www.gnu.org/software/coreutils/)\\n\\n**–** [rsnapshot: a file system snapshot utility used to create copies of an entire file system. Also check the Tools](http://www.rsnapshot.org/)\\n\\n     - rsnapshot for some information.\\n\\n**–** [rsync: a flexible utility used to create incremental copies of files.](http://manpages.ubuntu.com/manpages/focal/man1/rsync.1.html)\\n\\nSquid is a filtering and caching mechanism for web servers that can optimise bandwidth and performance. For more\\ninformation about Squid proxy servers, refer to this guide.\\n## **Install Squid**\\n\\nAt a terminal prompt, enter the following command to install the Squid server:\\n\\nsudo apt install squid\\n## **Configure Squid**\\n\\nSquid is configured by editing directives in the /etc/squid/squid.conf configuration file. The following examples',\n",
       " 'illustrate a sample of directives that can be modified to configure the Squid server’s behavior. For more in-depth\\nconfiguration details, see the links at the bottom of the page.\\n\\n**Protect the original config file**\\n\\nBefore editing the configuration file, you should make a copy of the original and protect it from writing. You will then\\nhave the original settings as a reference, and can reuse it when needed. Run the following commands to make a copy\\nof the original configuration file and protect it from being written to:\\n\\nsudo cp /etc/squid/squid.conf /etc/squid/squid.conf.original\\n\\nsudo chmod a-w /etc/squid/squid.conf.original\\n\\n**Change TCP port**\\n\\nTo set your Squid server to listen on TCP port 8888 instead of the default TCP port 3128, change the **http_port**\\ndirective as such:\\n\\nhttp_port 8888\\n\\n281\\n\\n\\n-----\\n\\n**Set the hostname**\\n\\nChange the **visible_hostname** directive to give the Squid server a specific hostname. This hostname does not need\\nto be the same as the computer’s hostname. In this example it is set to weezie :\\n\\nvisible_hostname weezie\\n\\n**Configure on-disk cache**\\n\\nThe default setting is to use on-memory cache. By changing the **cache_dir** directive you can configure use of an\\non-disk cache. The cache_dir directive takes the following arguments:\\n\\ncache_dir <Type> <Directory-Name> <Fs-specific-data> [options]\\n\\nIn the config file you can find the default cache_dir directive commented out:\\n\\n# Uncomment and adjust the following to add a disk cache directory.\\n\\n#cache_dir ufs /var/spool/squid 100 16 256\\n\\nYou can use the default option but you can also customise your cache directory, by changing the <Type> of this directory.\\nIt can be one of the following options:',\n",
       " ' - ufs : This is the common Squid storage format.\\n\\n - aufs : Uses the same storage format as ufs, using POSIX-threads to avoid blocking the main Squid process on\\ndisk-I/O. This was formerly known in Squid as async-io .\\n\\n - diskd : Uses the same storage format as ufs, using a separate process to avoid blocking the main Squid process\\non disk-I/O.\\n\\n - rock : This is a database-style storage. All cached entries are stored in a “database” file, using fixed-size slots. A\\nsingle entry occupies one or more slots.\\n\\nIf you want to use a different directory type please take a look at their different options.\\n\\n**Access control**\\n\\nUsing Squid’s access control, you can configure use of Squid-proxied Internet services to be available only to users\\nwith certain Internet Protocol (IP) addresses. For example, we will illustrate access by users of the 192.168.42.0/24\\nsubnetwork only:\\n\\n  - Add the following to the **bottom** of the ACL section of your /etc/squid/squid.conf file:\\n\\nacl fortytwo_network src 192.168.42.0/24\\n\\n  - Then, add the following to the **top** of the http_access section of your /etc/squid/squid.conf file:\\n\\nhttp_access allow fortytwo_network\\n\\nUsing Squid’s access control features, you can configure Squid-proxied Internet services to only be available during\\nnormal business hours. As an example, we’ll illustrate access by employees of a business which is operating between\\n9:00AM and 5:00PM, Monday through Friday, and which uses the 10.1.42.0/24 subnetwork:\\n\\n  - Add the following to the **bottom** of the ACL section of your /etc/squid/squid.conf file:\\n\\nacl biz_network src 10.1.42.0/24\\n\\nacl biz_hours time M T W T F 9:00-17:00',\n",
       " '  - Then, add the following to the **top** of the http_access section of your /etc/squid/squid.conf file:\\n\\nhttp_access allow biz_network biz_hours\\n## **Restart the Squid server**\\n\\nAfter making any changes to the /etc/squid/squid.conf file, you will need to save the file and restart the squid server\\napplication. You can restart the server using the following command:\\n\\nsudo systemctl restart squid.service\\n\\n**Note** :\\n\\nIf a formerly customised squid3 was used to set up the spool at /var/log/squid3 to be a mountpoint, but\\notherwise kept the default configuration, the upgrade will fail. The upgrade tries to rename/move files\\nas needed, but it can’t do so for an active mountpoint. In that case you will need to adapt either the\\nmountpoint or the config in /etc/squid/squid.conf so that they match.\\nThe same applies if the **include** config statement was used to pull in more files from the old path at\\n\\n/etc/squid3/ . In those cases you should move and adapt your configuration accordingly.\\n\\n282\\n\\n\\n-----\\n\\n## **Further reading**\\n\\n[• The Squid Website](http://www.squid-cache.org/)\\n\\n[• Ubuntu Wiki page on Squid.](https://help.ubuntu.com/community/Squid)\\n## **Overview**\\n\\nLAMP installations (Linux + Apache + MySQL + PHP/Perl/Python) are a popular setup for Ubuntu servers.\\nThere are a plethora of Open Source applications written using the LAMP application stack. Some popular LAMP\\napplications include **wikis**, **management software** such as phpMyAdmin, and **Content Management Systems**\\n**(CMSs)** like WordPress.\\n\\nOne advantage of LAMP is the substantial flexibility for different database, web server, and scripting languages.\\nPopular substitutes for MySQL include PostgreSQL and SQLite.',\n",
       " 'Python, Perl, and Ruby are also frequently used\\ninstead of PHP. While Nginx, Cherokee and Lighttpd can replace Apache.\\n## **Quickstart**\\n\\nThe fastest way to get started is to install LAMP using tasksel . Tasksel is a Debian/Ubuntu tool that installs multiple\\nrelated packages as a co-ordinated “task” onto your system.\\n\\nAt a terminal prompt enter the following commands:\\n\\nsudo apt-get update\\n\\nsudo apt-get install -y tasksel\\n\\nsudo tasksel install lamp-server\\n\\n**LAMP application install process**\\n\\nAfter installing LAMP you’ll be able to install most LAMP applications in this way:\\n\\n  - Download an archive containing the application source files.\\n\\n  - Unpack the archive, usually in a directory accessible to a web server.\\n\\n  - Depending on where the source was extracted, configure a web server to serve the files.\\n\\n  - Configure the application to connect to the database.\\n\\n  - Run a script, or browse to a page of the application, to install the database needed by the application.\\n\\n  - Once the steps above, or similar steps, are completed you are ready to begin using the application.\\n\\nA disadvantage of using this approach is that the application files are not placed in the file system in a standard way,\\nwhich can cause confusion as to where the application is installed.\\n\\n**Update LAMP applications**\\n\\nWhen a new LAMP application version is released, follow the same installation process to apply updates to the\\napplication.\\n\\nFortunately, a number of LAMP applications are already packaged for Ubuntu, and are available for installation in the\\nsame way as non-LAMP applications (some applications may need extra configuration and setup steps). Two popular',\n",
       " 'examples are phpMyAdmin and WordPress.\\n\\nRefer to our guides on how to install phpMyAdmin and how to install WordPress for more information on those\\napplications.\\n\\n[The Apache HTTP Server (“httpd”) is the most commonly used web server on Linux systems, and is often used as](https://httpd.apache.org/)\\npart of the LAMP configuration. In this guide, we will show you how to install and configure Apache2, which is the\\ncurrent release of “httpd”.\\n## Install apache2\\n\\nTo install Apache2, enter the following command at the terminal prompt:\\n\\nsudo apt install apache2\\n\\n283\\n\\n\\n-----\\n\\n## Configure apache2\\n\\nApache2 is configured by placing **directives** in plain text configuration files in /etc/apache2/ . These *directives* are\\nseparated between the following files and directories:\\n\\n**Files**\\n\\n - apache2.conf\\nThe main Apache2 configuration file. Contains settings that are **global** to Apache2.\\n\\n**Note** : Historically, the main Apache2 configuration file was httpd.conf, named after the “httpd”\\ndaemon. In other distributions (or older versions of Ubuntu), the file might be present. In modern\\nreleases of Ubuntu, all configuration options have been moved to apache2.conf and the below referenced\\ndirectories and httpd.conf no longer exists.\\n\\n - envvars\\n\\nFile where Apache2 **environment** variables are set.\\n\\n - magic\\nInstructions for determining MIME type based on the first few bytes of a file.\\n\\n - ports.conf\\nHouses the directives that determine which TCP ports Apache2 is listening on.\\n\\nIn addition, other configuration files may be added using the **Include** directive, and wildcards can be used to include\\nmany configuration files.',\n",
       " 'Any directive may be placed in any of these configuration files. Changes to the main\\nconfiguration files are only recognized by Apache2 when it is started or restarted.\\n\\nThe server also reads a file containing MIME document types; the filename is set by the **TypesConfig** directive, typically via /etc/apache2/mods-available/mime.conf, which might also include additions and overrides, and\\nis /etc/mime.types by default.\\n\\n**Directories**\\n\\n - conf-available\\nThis directory contains available configuration files. All files that were previously in /etc/apache2/conf.d should\\nbe moved to /etc/apache2/conf-available .\\n\\n - conf-enabled\\nHolds **symlinks** to the files in /etc/apache2/conf-available . When a configuration file is symlinked, it will be\\nenabled the next time Apache2 is restarted.\\n\\n - mods-available\\nThis directory contains configuration files to both load **modules** and configure them. Not all modules will have\\nspecific configuration files, however.\\n\\n - mods-enabled\\nHolds symlinks to the files in /etc/apache2/mods-available . When a module configuration file is symlinked it\\nwill be enabled the next time Apache2 is restarted.\\n\\n - sites-available\\nThis directory has configuration files for Apache2 **Virtual Hosts** . Virtual Hosts allow Apache2 to be configured\\nfor multiple sites that have separate configurations.\\n\\n - sites-enabled\\n\\nLike mods-enabled, sites-enabled contains symlinks to the /etc/apache2/sites-available directory. Similarly,\\nwhen a configuration file in sites-available is symlinked, the site configured by it will be active once Apache2\\nis restarted.\\n## **Detailed configuration**',\n",
       " 'For more detailed information on configuring Apache2, check out our follow-up guides.\\n\\n  - Part 2: Apache2 configuration settings\\n\\n  - Part 3: how to extend Apache2 with modules.\\n## **Further reading**\\n\\n[• Apache2 Documentation contains in depth information on Apache2 configuration directives.](https://httpd.apache.org/docs/2.4/) Also, see the\\n\\napache2-doc package for the official Apache2 docs.\\n\\n284\\n\\n\\n-----\\n\\n[• O’Reilly’s Apache Cookbook is a good resource for accomplishing specific Apache2 configurations.](http://shop.oreilly.com/product/9780596529949.do)\\n\\n  - For Ubuntu-specific Apache2 questions, ask in the *#ubuntu-server* [IRC channel on libera.chat.](https://libera.chat/)\\n\\nAfter you have installed Apache2, you will likely need to configure it. In this explanatory guide, we will explain the\\nApache2 server essential configuration parameters.\\n## **Basic directives**\\n\\nApache2 ships with a “virtual-host-friendly” default configuration – it is configured with a single default virtual host\\n(using the **VirtualHost** directive) which can be modified or used as-is if you have a single site, or used as a template\\nfor additional virtual hosts if you have multiple sites.\\n\\nIf left alone, the default virtual host will serve as your default site, or the site users will see if the URL they enter\\ndoes not match the **ServerName** directive of any of your custom sites. To modify the default virtual host, edit the\\nfile /etc/apache2/sites-available/000-default.conf .\\n\\n**Note** :\\n\\nThe directives set for a virtual host only apply to that particular virtual host. If a directive is set serverwide and not defined in the virtual host settings, the default setting is used.',\n",
       " 'For example, you can define\\na Webmaster email address and not define individual email addresses for each virtual host.\\n\\nIf you want to configure a new virtual host or site, copy the 000-default.conf file into the same directory with a name\\nyou choose. For example:\\n\\nsudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/mynewsite.conf\\n\\nEdit the new file to configure the new site using some of the directives described below:\\n\\n**The ServerAdmin directive**\\n\\nFound in /etc/apache2/sites-available\\n\\nSpecifies the email address to be advertised for the server’s administrator. The default value is webmaster@localhost .\\nThis should be changed to an email address that is delivered to you (if you are the server’s administrator). If your\\nwebsite has a problem, Apache2 will display an error message containing this email address to report the problem to.\\n\\n**The Listen directive**\\n\\nFound in /etc/apache2/ports.conf\\n\\nSpecifies the port, and optionally the IP address, Apache2 should listen on. If the IP address is not specified, Apache2\\nwill listen on all IP addresses assigned to the machine it runs on. The default value for the **Listen** directive is 80 .\\nChange this to:\\n\\n - 127.0.0.1:80 to make Apache2 listen only on your loopback interface so that it will not be available to the\\nInternet,\\n\\n  - to e.g. 81 to change the port that it listens on,\\n\\n  - or leave it as is for normal operation.\\n\\n**The ServerName directive (optional)**\\n\\nSpecifies what FQDN your site should answer to. The default virtual host has no **ServerName** directive specified,\\nso it will respond to all requests that do not match a ServerName directive in another virtual host. If you have just',\n",
       " 'acquired the domain name mynewsite.com and wish to host it on your Ubuntu server, the value of the ServerName\\ndirective in your virtual host configuration file should be mynewsite.com .\\n\\nAdd this directive to the new virtual host file you created earlier ( /etc/apache2/sites-available/mynewsite.conf ).\\n\\n**The ServerAlias directive**\\n\\nYou may also want your site to respond to www.mynewsite.com, since many users will assume the www prefix is\\nappropriate – use the *ServerAlias* directive for this. You may also use wildcards in the ServerAlias directive.\\n\\nFor example, the following configuration will cause your site to respond to any domain request ending in\\n*.mynewsite.com* .\\n\\nServerAlias *.mynewsite.com\\n\\n285\\n\\n\\n-----\\n\\n**The DocumentRoot directive**\\n\\nSpecifies where Apache2 should look for the files that make up the site. The default value is /var/www/html, as specified\\nin /etc/apache2/sites-available/000-default.conf . If desired, change this value in your site’s virtual host file, and\\nremember to create that directory if necessary!\\n\\nEnable the new *VirtualHost* using the a2ensite utility and restart Apache2:\\n\\nsudo a2ensite mynewsite\\n\\nsudo systemctl restart apache2.service\\n\\n**Note** :\\n\\nBe sure to replace mynewsite with a more descriptive name for the VirtualHost. One method is to name\\nthe file after the **ServerName** directive of the VirtualHost.\\n\\nSimilarly, use the a2dissite utility to disable sites. This is can be useful when troubleshooting configuration problems\\nwith multiple virtual hosts:\\n\\nsudo a2dissite mynewsite\\n\\nsudo systemctl restart apache2.service\\n## **Apache2 server default settings**\\n\\nThis section explains configuration of the Apache2 server default settings.',\n",
       " 'For example, if you add a virtual host, the\\nsettings you configure for the virtual host take precedence for that virtual host. For a directive not defined within the\\nvirtual host settings, the default value is used.\\n\\n**The DirectoryIndex**\\n\\nThe **DirectoryIndex** is the default page served by the server when a user requests an index of a directory by specifying\\na forward slash (/) at the end of the directory name.\\n\\nFor example, when a user requests the page http://www.example.com/this_directory/, they will get either the DirectoryIndex page (if it exists), a server-generated directory list (if it does not and the Indexes option is specified), or a\\nPermission Denied page if neither is true.\\n\\nThe server will try to find one of the files listed in the DirectoryIndex directive and will return the first one it finds. If\\nit does not find any of these files and if **Options Indexes** is set for that directory, the server will generate and return\\na list, in HTML format, of the subdirectories and files in the directory. The default value, found in /etc/apache2/mods\\navailable/dir.conf is “index.html index.cgi index.pl index.php index.xhtml index.htm”. Thus, if Apache2 finds a file\\nin a requested directory matching any of these names, the first will be displayed.\\n\\n**The ErrorDocument**\\n\\nThe **ErrorDocument** directive allows you to specify a file for Apache2 to use for specific error events. For example,\\nif a user requests a resource that does not exist, a 404 error will occur.\\n\\nBy default, Apache2 will return a HTTP 404 Return code. Read /etc/apache2/conf-available/localized-error\\npages.conf for detailed instructions on using ErrorDocument, including locations of example files.',\n",
       " '**CustomLog and ErrorLog**\\n\\nBy default, the server writes the transfer log to the file /var/log/apache2/access.log . You can change this on a per-site\\nbasis in your virtual host configuration files with the **CustomLog** directive, or omit it to accept the default, specified\\nin /etc/apache2/conf-available/other-vhosts-access-log.conf .\\n\\nYou can also specify the file to which errors are logged, via the **ErrorLog** directive, whose default is\\n\\n/var/log/apache2/error.log . These are kept separate from the transfer logs to aid in troubleshooting problems with your Apache2 server. You may also specify the **LogLevel** (the default value is “warn”) and the\\n**LogFormat** (see /etc/apache2/apache2.conf for the default value).\\n\\n**The Options directive**\\n\\nSome options are specified on a per-directory basis rather than per-server. **Options** is one of these directives. A\\nDirectory stanza is enclosed in XML-like tags, like so:\\n\\n<Directory /var/www/html/mynewsite>\\n\\n...\\n\\n</Directory>\\n\\n286\\n\\n\\n-----\\n\\nThe Options directive within a Directory stanza accepts one or more of the following values (among others), separated\\nby spaces:\\n\\n - **ExecCGI**\\n\\nAllow CGI scripts to be run. CGI scripts are not run if this option is not chosen.\\n\\n**Caution**\\nMost files should not be run as CGI scripts. This would be very dangerous. CGI scripts should kept in\\na directory separate from and outside your **DocumentRoot**, and only this directory should have the\\nExecCGI option set. This is the default, and the default location for CGI scripts is /usr/lib/cgi-bin .\\n\\n - **Includes**\\nAllow **server-side includes** . Server-side includes allow an HTML file to *include* [other files.',\n",
       " 'See Apache SSI](https://help.ubuntu.com/community/ServerSideIncludes)\\n[documentation (Ubuntu community) for more information.](https://help.ubuntu.com/community/ServerSideIncludes)\\n\\n - **IncludesNOEXEC**\\n\\nAllow server-side includes, but disable the #exec and #include commands in CGI scripts.\\n\\n - **Indexes**\\nDisplay a formatted list of the directory’s contents, if no DirectoryIndex (such as index.html ) exists in the\\nrequested directory.\\n\\n**Caution**\\n\\nFor security reasons, this should usually not be set, and certainly should not be set on your DocumentRoot directory. Enable this option carefully on a per-directory basis **only** if you are certain you want\\nusers to see the entire contents of the directory.\\n\\n - **Multiview**\\n\\n[Support content-negotiated multiviews; this option is disabled by default for security reasons. See the Apache2](https://httpd.apache.org/docs/2.4/mod/mod_negotiation.html#multiviews)\\n[documentation on this option.](https://httpd.apache.org/docs/2.4/mod/mod_negotiation.html#multiviews)\\n\\n - **SymLinksIfOwnerMatch**\\nOnly follow symbolic links if the target file or directory has the same owner as the link.\\n\\n**Apache2 daemon settings**\\n\\nThis section briefly explains some basic Apache2 daemon configuration settings.\\n\\n - **LockFile**\\nThe **LockFile** directive sets the path to the lockfile used when the server is compiled with either\\n\\nUSE_FCNTL_SERIALIZED_ACCEPT or USE_FLOCK_SERIALIZED_ACCEPT . It must be stored on the local disk. It\\nshould be left to the default value unless the logs directory is located on an NFS share. If this is the case, the',\n",
       " 'default value should be changed to a location on the local disk and to a directory that is readable only by root.\\n\\n - **PidFile**\\nThe **PidFile** directive sets the file in which the server records its process ID (pid). This file should only be\\nreadable by root. In most cases, it should be left to the default value.\\n\\n - **User**\\n\\nThe **User** directive sets the userid used by the server to answer requests. This setting determines the server’s\\naccess. Any files inaccessible to this user will also be inaccessible to your website’s visitors. The default value\\nfor User is “www-data”.\\n\\n**Warning**\\nUnless you know exactly what you are doing, do not set the User directive to root. Using root as the\\nUser will create large security holes for your Web server.\\n\\n - **Group**\\nThe **Group** directive is similar to the User directive. Group sets the group under which the server will answer\\nrequests. The default group is also “www-data”.\\n## **Extending Apache2**\\n\\nNow that you know how to configure Apache2, you may also want to know how to extend Apache2 with modules.\\n## **Further reading**\\n\\n[• The Apache2 Documentation contains in depth information on Apache2 configuration directives. Also, see the](https://httpd.apache.org/docs/2.4/)\\n\\napache2-doc package for the official Apache2 docs.\\n\\n[• O’Reilly’s Apache Cookbook is a good resource for accomplishing specific Apache2 configurations.](http://shop.oreilly.com/product/9780596529949.do)\\n\\n287\\n\\n\\n-----\\n\\n  - For Ubuntu specific Apache2 questions, ask in the #ubuntu-server [IRC channel on libera.chat.](https://libera.chat/)\\n\\nApache2 is a modular server. This implies that only the most basic functionality is included in the core server.',\n",
       " 'Extended features are available through modules which can be loaded into Apache2.\\n\\nBy default, a base set of modules is included in the server at compile-time. If the server is compiled to use dynamically\\nloaded modules, then modules can be compiled separately, and added at any time using the **LoadModule** directive.\\nOtherwise, Apache2 must be recompiled to add or remove modules.\\n\\nUbuntu compiles Apache2 to allow the dynamic loading of modules. Configuration directives may be conditionally\\nincluded on the presence of a particular module by enclosing them in an <IfModule> block.\\n## **Installing and handling modules**\\n\\nYou can install additional Apache2 modules and use them with your web server. For example, run the following\\ncommand at a terminal prompt to install the Python 3 WSGI module:\\n\\nsudo apt install libapache2-mod-wsgi-py3\\n\\nThe installation will enable the module automatically, but we can disable it with a2dismod :\\n\\nsudo a2dismod wsgi\\n\\nsudo systemctl restart apache2.service\\n\\nAnd then use the a2enmod utility to re-enable it:\\n\\nsudo a2enmod wsgi\\n\\nsudo systemctl restart apache2.service\\n\\nSee the /etc/apache2/mods-available directory for additional modules already available on your system.\\n## **Configure Apache2 for HTTPS**\\n\\nThe mod_ssl module adds an important feature to the Apache2 server - the ability to encrypt communications. Thus,\\nwhen your browser is communicating using SSL, the https:// prefix is used at the beginning of the Uniform Resource\\nLocator (URL) in the browser navigation bar.\\n\\nThe mod_ssl module is available in the apache2-common package. Run the following command at a terminal prompt to\\nenable the mod_ssl module:\\n\\nsudo a2enmod ssl',\n",
       " 'There is a default HTTPS configuration file in /etc/apache2/sites-available/default-ssl.conf . In order for Apache2\\nto provide HTTPS, a **certificate** and **key** file are also needed. The default HTTPS configuration will use a certificate\\nand key generated by the ssl-cert package. They are good for testing, but the auto-generated certificate and key\\nshould be replaced by a certificate specific to the site or server.\\n\\n**Note** :\\nFor more information on generating a key and obtaining a certificate see Certificates.\\n\\nTo configure Apache2 for HTTPS, enter the following:\\n\\nsudo a2ensite default-ssl\\n\\n**Note** :\\n\\nThe directories /etc/ssl/certs and /etc/ssl/private are the default locations. If you install the certificate and key in another directory make sure to change *SSLCertificateFile* and *SSLCertificateKeyFile*\\nappropriately.\\n\\nWith Apache2 now configured for HTTPS, restart the service to enable the new settings:\\n\\nsudo systemctl restart apache2.service\\n\\nNote that depending on how you obtained your certificate, you may need to enter a passphrase when Apache2 restarts.\\n\\nYou can access the secure server pages by typing https://your_hostname/url/ in your browser address bar.\\n## **Sharing write permission**\\n\\nFor more than one user to be able to write to the same directory you will need to grant write permission to a group they\\nshare in common. The following example grants shared write permission to /var/www/html to the group “webmasters”.\\n\\nsudo chgrp -R webmasters /var/www/html\\n\\nsudo chmod -R g=rwX /var/www/html/\\n\\n288\\n\\n\\n-----\\n\\nThese commands recursively set the group permission on all files and directories in /var/www/html to allow reading,\\nwriting and searching of directories.',\n",
       " 'Many admins find this useful for allowing multiple users to edit files in a directory\\n\\ntree.\\n\\n**Warning** :\\nThe apache2 daemon will run as the www-data user, which has a corresponding www-data group. These\\n**should not** be granted write access to the document root, as this would mean that vulnerabilities in\\nApache or the applications it is serving would allow attackers to overwrite the served content.\\n## **Further reading**\\n\\n[• The Apache2 Documentation contains in depth information on Apache2 configuration directives. Also, see the](https://httpd.apache.org/docs/2.4/)\\napache2-doc package for the official Apache2 docs.\\n\\n[• O’Reilly’s Apache Cookbook is a good resource for accomplishing specific Apache2 configurations.](http://shop.oreilly.com/product/9780596529949.do)\\n\\n  - For Ubuntu specific Apache2 questions, ask in the #ubuntu-server [IRC channel on libera.chat.](https://libera.chat/)\\n\\nThe nginx HTTP server is a powerful alternative to Apache. In this guide, we will demonstrate how to install and use\\nnginx for web services.\\n## **Install nginx**\\n\\nTo install nginx, enter the following command at the terminal prompt:\\n\\n$ sudo apt update\\n\\n$ sudo apt install nginx\\n\\nThis will also install any required dependency packages, and some common mods for your server, and then start the\\nnginx web server.\\n\\n**Verify nginx is running**\\n\\nYou can verify that nginx is running via this command:\\n\\n$ sudo systemctl status nginx\\n\\n - nginx.service - A high performance web server and a reverse proxy server\\n\\nLoaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)\\n\\nActive: active (running) since Sun 2023-08-20 01:04:22 UTC; 53s ago\\n\\nDocs: man:nginx(8)',\n",
       " 'Process: 28210 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SU\\\\\\n\\nCCESS)\\n\\nProcess: 28211 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS)\\n\\nMain PID: 28312 (nginx)\\n\\nTasks: 13 (limit: 76969)\\n\\nMemory: 13.1M\\n\\nCPU: 105ms\\n\\nCGroup: /system.slice/nginx.service\\n\\n├─28312 \"nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\"\\n\\n├─28314 \"nginx: worker process\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\\n\\n...\\n\\n**Restarting nginx**\\n\\nTo restart nginx, run:\\n\\n$ sudo systemctl restart nginx\\n\\n**Enable/disable nginx manually**\\n\\nBy default, Nginx will automatically start at boot time. To disable this behaviour so that you can start it up manually,\\nyou can disable it:\\n\\n$ sudo systemctl disable nginx\\n\\nThen, to re-enable it, run:\\n\\n$ sudo systemctl enable nginx\\n\\n289\\n\\n\\n-----\\n\\n**The default nginx homepage**\\n\\nA default nginx home page is also set up during the installation process. You can load this page in your web browser\\nusing your web server’s IP address; http:// *your_server_ip* .\\n\\nThe default home page should look similar to:\\n\\nWelcome to nginx!\\n\\nWelcome to nginx!\\n\\nIf you see this page, the nginx web server is successfully installed and working. Further configuration is\\n\\nrequired.\\n\\nFor online documentation and support please refer to nginx.org.\\n\\nCommercial support is available at nginx.com.\\n\\nThank you for using nginx.\\n\\n**Setting up nginx**\\n\\nFor more information on customising nginx for your needs, see these follow-up guides:\\n\\n  - Part 2: How to configure nginx\\n\\n  - Part 3: How to use nginx modules\\n\\n**Further reading**',\n",
       " '[• The nginx documentation provides detailed explanations of configuration directives.](https://nginx.org/en/docs/)\\n\\n  - O’Reilly’s nginx cookbook provides guidance on solving specific needs\\n\\n  - For Ubuntu-specific nginx questions, ask in the #ubuntu-server IRC channel on libera.chat.\\n\\nOnce you have installed nginx, you can customise it for your use with the configuration options explained in this guide.\\n## **Server blocks**\\n\\nnginx organises sets of site-specific configuration details into **server blocks**, and by default comes pre-configured for\\nsingle-site operation. This can either be used “as-is”, or as a starting template\\nfor serving multiple sites.\\n\\nThe single-site configuration serves files out of /var/www/html, as defined by the server block and as provided by\\n\\n/etc/nginx/sites-enabled/default :\\n\\nserver {\\n\\nlisten 80 default_server;\\n\\nlisten [::]:80 default_server;\\n\\nroot /var/www/html;\\n\\n# Add index.php to the list if you are using PHP\\n\\nindex index.html index.htm index.nginx-debian.html;\\n\\nserver_name _;\\n\\nlocation / {\\n\\n# First attempt to serve request as file, then\\n\\n# as directory, then fall back to displaying a 404.\\n\\ntry_files $uri $uri/ =404;\\n\\n}\\n\\n}\\n\\nEven for a single-site configuration, while you can place your website at /var/www/html, you may want to place the\\nwebsite’s files at a different location in your filesystem. For example, if you were hosting www.my-site.org from\\n\\n/srv/my-site/html you might edit the above file to look like this:\\n\\nserver {\\n\\nlisten 80;\\n\\nroot /srv/my-site/html;\\n\\n290\\n\\n\\n-----\\n\\nindex index.html;\\n\\nserver_name my-site.org www.my-site.org;\\n\\nlocation / {\\n\\ntry_files $uri $uri/ =404;\\n\\n}\\n\\n}\\n\\nMake sure to create your web root directory structure:',\n",
       " '$ sudo mkdir -p /srv/my-site/html\\n\\n$ sudo chmod -R 755 /srv/my-site/html\\n\\n$ echo \"<html><body><h1>My Site!</h1></body></html>\" > /srv/my-site/html/index.html\\n\\nThen, to make nginx reload its configuration, run:\\n\\n$ sudo systemctl reload nginx\\n\\nCheck that the settings have taken effect using your web browser:\\n\\n$ www-browser www.my-site.org\\n## **Multi-site hosting**\\n\\nSimilar to Apache, nginx uses the sites-available and sites-enabled directories for the configurations of multiple\\nwebsites. Unlike with Apache, you’ll need to handle the enablement manually.\\n\\nTo do that, first create a new server block in a configuration file as above, and save it to /etc/nginx/sites\\navailable/<your-domain> . Make sure to give each site a unique server_name and a different listen port number.\\n\\nNext, enable the site by creating a symlink to it from the sites-enabled directory:\\n\\n$ sudo ln -s /etc/nginx/sites-available/<your-domain> /etc/nginx/sites-enabled/\\n\\nTo disable a website, you can delete the symlink in sites-enabled . For example, once you have your new site(s)\\nconfigured and no longer need the default site configuration:\\n\\n$ sudo rm /etc/nginx/sites-available/default\\n## **SSL and HTTPS**\\n\\nWhile establishing an HTTP website on port 80 is a good starting point (and perhaps adequate for static content),\\nproduction systems will want HTTPS, such as serving on port 443 with SSL enabled via cert files. A server block with\\nsuch a configuration might look like this, with HTTP-to-HTTPS redirection handled in the first block, and HTTPS\\nin the second block:\\n\\nserver {\\n\\nlisten 80;\\n\\nserver_name our-site.org www.our-site.org;\\n\\nreturn 301 https://$host$request_url;\\n\\n}\\n\\nserver {\\n\\nlisten 443 ssl;',\n",
       " 'root /srv/our-site/html;\\n\\nindex index.html;\\n\\nserver_name our-site.org www.our-site.org;\\n\\nssl_certificate our-site.org.crt;\\n\\nssl_certificate_key our-site.org.key;\\n\\nssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;\\n\\nssl_ciphers HIGH:!aNULL:!MD5;\\n\\nssl_session_timeout 15m;\\n\\nlocation / {\\n\\ntry_files $uri $uri/ =404;\\n\\n}\\n\\n291\\n\\n\\n-----\\n\\n}\\n\\nThanks to the return 301 line in the above configuration, anyone visiting the site on port 80 via an HTTP URL will\\nget automatically redirected to the equivalent secure HTTPS URL.\\n\\nRefer to the GnuTLS section explains how to configure different SSL protocol versions and their associated ciphers.\\n\\nFor example, to generate a self-signed certificate, you might run a set of commands similar to these:\\n\\n$ sudo openssl genrsa -out our-site.org.key 2048\\n\\n$ openssl req -nodes -new -key our-site.org.key -out ca.csr\\n\\n$ openssl x509 -req -days 365 -in our-site.org.csr -signkey our-site.org.key -out our-site.org.crt\\n\\n$ mkdir /etc/apache2/ssl\\n\\n$ cp our-site.org.crt our-site.org.key our-site.org.csr /etc/apache2/ssl/\\n## **Setting up nginx**\\n\\nBeyond the settings outlined above, nginx can be further customised through the use of modules. Please see the next\\nguide in this series for details of how to do that.\\n\\n  - Part 3: How to use nginx modules\\n## **Further reading**\\n\\n[• nginx’s beginner’s guide covers use cases such as proxy servers, FastCGI for use with PHP and other frameworks,](https://nginx.org/en/docs/beginners_guide.html)\\nand optimising the handling of static content.\\n\\n[• The nginx documentation describes HTTPS server configuration in greater detail, including certificate chains,](https://nginx.org/en/docs/http/configuring_https_servers.html)',\n",
       " \"disambiguating various multi-site certificate situations, performance optimisations and compatibility issues.\\n\\n  - For Ubuntu-specific nginx questions, ask in the #ubuntu-server IRC channel on libera.chat.\\n\\nLike other web servers, nginx supports dynamically loaded modules to provide in-server support for programming\\nlanguages, security mechanisms, and so on. Ubuntu provides a number of these modules as separate packages that\\nare either installed simultaneously with nginx, or can be installed separately.\\n## **Available modules**\\n\\nnginx will report the modules it has been built with via its -V option. A quick and dirty way to list the available\\nmodules is thus:\\n\\n$ nginx -V 2>&1 | tr -- - '\\\\n' | grep _module\\n\\nhttp_ssl_module\\n\\nhttp_stub_status_module\\n\\nhttp_realip_module\\n\\n...\\n\\nhttp_image_filter_module=dynamic\\n\\nhttp_perl_module=dynamic\\n\\nhttp_xslt_module=dynamic\\n\\nstream_geoip_module=dynamic\\n\\nMany of these modules are built-in and thus are always available with nginx, but some exist as separate packages\\nwhose installation status can be checked via apt . For example:\\n\\n$ apt policy libnginx-mod-http-image-filter\\n\\nlibnginx-mod-http-image-filter:\\n\\nInstalled: (none)\\n\\nCandidate: 1.24.0-1ubuntu1\\n\\nVersion table:\\n\\n1.24.0-1ubuntu1 500\\n\\n500 http://archive.ubuntu.com/ubuntu mantic/main amd64 Packages\\n\\napt can also be used to install the desired dynamic module:\\n\\n$ sudo apt install libnginx-mod-http-image-filter\\n\\n...\\n\\nThe following NEW packages will be installed:\\n\\nlibnginx-mod-http-image-filter\\n\\n0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\\n\\n...\\n\\nTriggering nginx reload\\n\\n292\\n\\n\\n-----\\n\\n...\\n## **Enabling and disabling dynamic modules**\",\n",
       " 'Dynamic modules are automatically enabled and get reloaded by nginx on installation. If you need to manually disable\\nan installed module, remove its file from the /etc/nginx/modules-enabled directory, for example:\\n\\n$ ls /etc/nginx/modules-*\\n\\n/etc/nginx/modules-available:\\n\\n/etc/nginx/modules-enabled:\\n\\n50-mod-http-image-filter.conf\\n\\n$ sudo mv /etc/nginx/modules-enabled/50-mod-http-image-filter.conf /etc/nginx/modules-available/\\n\\n$ service nginx restart\\n\\nNote that built-in modules cannot be disabled/enabled.\\n## **Configuring modules**\\n\\nThe installed configuration file for an nginx module mainly consists of the dynamically-loaded binary library:\\n\\n## /etc/nginx/modules-enabled/50-mod-http-image-filter.conf\\n\\nload_module modules/ngx_http_image_filter_module.so;\\n\\nNote that you can also use the load_module parameter in your /etc/nginx/nginx.conf at the top level, if preferred for\\n\\nsome reason.\\n\\nTo use a module for your website, its settings are specified in your server block. For example:\\n\\nlocation /img/ {\\n\\nimage_filter resize 240 360;\\n\\nimage_filter rotate 180;\\n\\nimage_filter_buffer 16M;\\n\\nerror_page 415 = /415.html;\\n\\n}\\n\\n**Further reading**\\n\\nYou’ve completed the nginx guide! See the following resources for more in-depth information on further extending\\nnginx’s capabilities:\\n\\n[• The nginx documentation provides detailed explanations of configuration directives.](https://nginx.org/en/docs/)\\n\\n  - O’Reilly’s nginx cookbook provides guidance on solving specific needs.\\n\\n  - For Ubuntu-specific nginx questions, ask in the #ubuntu-server IRC channel on libera.chat.',\n",
       " '[PHP is a general-purpose scripting language well-suited for Web development since PHP scripts can be embedded into](https://www.php.net/)\\nHTML. This guide explains how to install and configure PHP in an Ubuntu System with Apache2 and MySQL.\\n## **Prerequisites**\\n\\nBefore installing PHP you should install Apache (or a preferred web server) and a database service such as MySQL.\\n\\n  - To install the Apache package, please refer to our Apache guide.\\n\\n  - To install and configure a MySQL database service, refer to our MySQL guide.\\n## **Install PHP**\\n\\nPHP is available on Ubuntu Linux, but unlike Python (which comes pre-installed), must be manually installed.\\n\\nTo install PHP – and the Apache PHP module – you can enter the following command into a terminal prompt:\\n\\nsudo apt install php libapache2-mod-php\\n\\n293\\n\\n\\n-----\\n\\n## **Install optional packages**\\n\\nThe following packages are optional, and can be installed if you need them for your setup.\\n\\n - **PHP-CLI**\\nYou can run PHP scripts via the Command Line Interface (CLI). To do this, you must first install the php-cli\\npackage. You can install it by running the following command:\\n\\nsudo apt install php-cli\\n\\n - **PHP-CGI**\\n\\nYou can also execute PHP scripts without installing the Apache PHP module. To accomplish this, you should\\ninstall the php-cgi package via this command:\\n\\nsudo apt install php-cgi\\n\\n - **PHP-MySQL**\\nTo use MySQL with PHP you should install the php-mysql package, like so:\\n\\nsudo apt install php-mysql\\n\\n - **PHP-PgSQL**\\nSimilarly, to use PostgreSQL with PHP you should install the php-pgsql package:\\n\\nsudo apt install php-pgsql\\n## **Configure PHP**',\n",
       " 'If you have installed the libapache2-mod-php or php-cgi packages, you can run PHP scripts from your web browser. If\\nyou have installed the php-cli package, you can run PHP scripts at a terminal prompt.\\n\\nBy default, when libapache2-mod-php is installed, the Apache2 web server is configured to run PHP scripts using this\\nmodule. First, verify if the files /etc/apache2/mods-enabled/php8.*.conf and /etc/apache2/mods-enabled/php8.*.load\\nexist. If they do not exist, you can enable the module using the a2enmod command.\\n\\nOnce you have installed the PHP-related packages and enabled the Apache PHP module, you should restart the\\nApache2 web server to run PHP scripts, by running the following command:\\n\\nsudo systemctl restart apache2.service\\n## **Test your setup**\\n\\nTo verify your installation, you can run the following PHP phpinfo script:\\n\\n<?php\\n\\nphpinfo();\\n\\n?>\\n\\nYou can save the content in a file – phpinfo.php for example – and place it under the DocumentRoot directory of the\\nApache2 web server. Pointing your browser to http://hostname/phpinfo.php will display the values of various PHP\\nconfiguration parameters.\\n## **Further reading**\\n\\n[• For more in depth information see the php.net documentation.](http://www.php.net/docs.php)\\n\\n[• There are a plethora of books on PHP 7 and PHP 8. A good book from O’Reilly is Learning PHP, which](http://oreilly.com/catalog/0636920043034/)\\nincludes an exploration of PHP 7’s enhancements to the language.\\n\\n[• Also, see the Apache MySQL PHP Ubuntu Wiki page for more information.](https://help.ubuntu.com/community/ApacheMySQLPHP)\\n\\n[Ruby on Rails is an open source web framework for developing database-backed web applications.',\n",
       " 'It is optimised for](https://rubyonrails.org/)\\nsustainable productivity of the programmer since it lets the programmer to write code by favouring convention over\\nconfiguration. This guide explains how to install and configure Ruby on Rails for an Ubuntu system with Apache2\\nand MySQL.\\n## **Prerequisites**\\n\\nBefore installing Rails you should install Apache (or a preferred web server) and a database service such as MySQL.\\n\\n  - To install the Apache package, please refer to our Apache guide.\\n\\n  - To install and configure a MySQL database service, refer to our MySQL guide.\\n\\n294\\n\\n\\n-----\\n\\n## Install rails\\n\\nOnce you have a web server and a database service installed and configured, you are ready to install the Ruby on\\nRails package, rails, by entering the following in the terminal prompt.\\n\\nsudo apt install rails\\n\\nThis will install both the Ruby base packages, and Ruby on Rails.\\n## **Configure the web server**\\n\\nYou will need to modify the /etc/apache2/sites-available/000-default.conf configuration file to set up your domains.\\n\\nThe first thing to change is the DocumentRoot directive:\\n\\nDocumentRoot /path/to/rails/application/public\\n\\nNext, change the <Directory \"/path/to/rails/application/public\"> directive:\\n\\n<Directory \"/path/to/rails/application/public\">\\n\\nOptions Indexes FollowSymLinks MultiViews ExecCGI\\n\\nAllowOverride All\\n\\nOrder allow,deny\\n\\nallow from all\\n\\nAddHandler cgi-script .cgi\\n\\n</Directory>\\n\\nYou should also enable the mod_rewrite module for Apache. To enable the mod_rewrite module, enter the following\\ncommand into a terminal prompt:\\n\\nsudo a2enmod rewrite',\n",
       " 'Finally, you will need to change the ownership of the /path/to/rails/application/public and /path/to/rails/application/tmp\\ndirectories to the user that will be used to run the Apache process:\\n\\nsudo chown -R www-data:www-data /path/to/rails/application/public\\n\\nsudo chown -R www-data:www-data /path/to/rails/application/tmp\\n\\nIf you need to compile your application assets run the following command in\\nyour application directory:\\n\\nRAILS_ENV=production rake assets:precompile\\n## **Configure the database**\\n\\nWith your database service in place, you need to make sure your app database configuration is also correct. For\\nexample, if you are using MySQL the your config/database.yml should look like this:\\n\\n# Mysql\\n\\nproduction:\\n\\nadapter: mysql2\\n\\nusername: user\\n\\npassword: password\\n\\nhost: 127.0.0.1\\n\\ndatabase: app\\n\\nTo finally create your application database and apply its migrations you can run the following commands from your\\napp directory:\\n\\nRAILS_ENV=production rake db:create\\n\\nRAILS_ENV=production rake db:migrate\\n\\nThat’s it! Now your Server is ready for your Ruby on Rails application. You can daemonize your application as you\\n\\nwant.\\n## **Further reading**\\n\\n[• See the Ruby on Rails website for more information.](http://rubyonrails.org/)\\n\\n[• Agile Development with Rails is also a great resource.](https://pragprog.com/book/rails4/agile-web-development-with-rails-4)\\n\\n[phpMyAdmin is a LAMP application specifically written for administering MySQL servers. Written in PHP, and](https://www.phpmyadmin.net/)\\naccessed through a web browser, phpMyAdmin provides a graphical interface for database administration tasks.\\n\\n295\\n\\n\\n-----\\n\\n## **Prerequisites**',\n",
       " \"Before you can install phpMyAdmin, you will need access to a MySQL database – either on the same host as phpMyAdmin will be installed on, or on a host accessible over the network. For instructions on how to install a MySQL\\ndatabase service, see our MySQL guide.\\n\\nYou will also need a web server. In this guide we use Apache2, although you can use another if you prefer. If you\\nwould like instructions on how to install Apache2, see our Apache guide.\\n## Install phpmyadmin\\n\\nOnce your MySQL database is set up, you can install phpmyadmin via the terminal:\\n\\nsudo apt install phpmyadmin\\n\\nAt the prompt, choose which web server to configure for phpMyAdmin. Here, we are using Apache2 for the web server.\\n\\nIn a browser, go to http://servername/phpmyadmin (replace servername with the server’s actual hostname).\\n\\nAt the login, page enter **root** for the username. Or, if you have a MySQL user already set up, enter the MySQL user’s\\npassword.\\n\\nOnce logged in, you can reset the root password if needed, create users, create or destroy databases and tables, etc.\\n## Configure phpmyadmin\\n\\nThe configuration files for phpMyAdmin are located in /etc/phpmyadmin . The main configuration file is\\n\\n/etc/phpmyadmin/config.inc.php . This file contains configuration options that apply globally to phpMyAdmin.\\n\\nTo use phpMyAdmin to administer a MySQL database hosted on another server, adjust the following in\\n\\n/etc/phpmyadmin/config.inc.php :\\n\\n$cfg['Servers'][$i]['host'] = 'db_server';\\n\\n**Note** :\\n\\nReplace db_server with the actual remote database server name or IP address. Also, be sure that the\\nphpMyAdmin host has permissions to access the remote database.\",\n",
       " 'Once configured, log out of phpMyAdmin then back in again, and you should be accessing the new server.\\n\\n**Configuration files**\\n\\nThe config.header.inc.php and config.footer.inc.php files in the /etc/phpmyadmin directory are used to add a HTML\\nheader and footer, respectively, to phpMyAdmin.\\n\\nAnother important configuration file is /etc/phpmyadmin/apache.conf . This file is symlinked to /etc/apache2/conf\\navailable/phpmyadmin.conf, and once enabled, is used to configure Apache2 to serve the phpMyAdmin site. The file\\ncontains directives for loading PHP, directory permissions, etc. From a terminal type:\\n\\nsudo ln -s /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf\\n\\nsudo a2enconf phpmyadmin.conf\\n\\nsudo systemctl reload apache2.service\\n## **Further reading**\\n\\n  - The phpMyAdmin documentation comes installed with the package and can be accessed from the **phpMyAdmin**\\n**Documentation** link (a question mark with a box around it) under the phpMyAdmin logo. The official docs\\n[can also be access on the phpMyAdmin website.](http://www.phpmyadmin.net/home_page/docs.php)\\n\\n[• Another resource is the phpMyAdmin Ubuntu Wiki page.](https://help.ubuntu.com/community/phpMyAdmin)\\n\\n  - If you need more information on configuring Apache2, refer to our guide on Apache2.\\n\\n[Wordpress is a blog tool, publishing platform and content management system (CMS) implemented in PHP and](https://wordpress.com/)\\n[licensed under the GNU General Public License (GPL) v2 or later.](https://en-gb.wordpress.org/about/license/)\\n\\nIn this guide, we show you how to install and configure WordPress in an Ubuntu system with Apache2 and MySQL.\\n\\n296\\n\\n\\n-----\\n\\n## **Prerequisites**',\n",
       " 'Before installing WordPress you should install Apache2 (or a preferred web server) and a database service such as\\nMySQL.\\n\\n  - To install the Apache package, refer to our Apache guide.\\n\\n  - To install and configure a MySQL database service, refer to our MySQL guide.\\n## **Install WordPress**\\n\\nTo install WordPress, run the following comand in the command prompt:\\n\\nsudo apt install wordpress\\n## **Configure WordPress**\\n\\nTo configure your first WordPress application, you need to configure your Apache web server. To do this, open\\n\\n/etc/apache2/sites-available/wordpress.conf and write the following lines:\\n\\nAlias /blog /usr/share/wordpress\\n\\n<Directory /usr/share/wordpress>\\n\\nOptions FollowSymLinks\\n\\nAllowOverride Limit Options FileInfo\\n\\nDirectoryIndex index.php\\n\\nOrder allow,deny\\n\\nAllow from all\\n\\n</Directory>\\n\\n<Directory /usr/share/wordpress/wp-content>\\n\\nOptions FollowSymLinks\\n\\nOrder allow,deny\\n\\nAllow from all\\n\\n</Directory>\\n\\nNow you can enable this new WordPress site:\\n\\nsudo a2ensite wordpress\\n\\nOnce you configure the Apache2 web server (and make it ready for your WordPress application), you will need to\\nrestart it. You can run the following command to restart the Apache2 web server:\\n\\nsudo systemctl reload apache2.service\\n\\n**The configuration file**\\n\\nTo facilitate having multiple WordPress installations, the name of the configuration file is based on the **Host header**\\nof the HTTP request.\\n\\nThis means you can have a configuration per **Virtual Host** by matching the hostname portion of this configuration with your Apache Virtual Host, e.g. /etc/wordpress/config-10.211.55.50.php, /etc/wordpress/config\\nhostalias1.php, etc.',\n",
       " \"These instructions assume you can access Apache via the **localhost** hostname (perhaps by using an SSH tunnel) if\\nnot, replace /etc/wordpress/config-localhost.php with /etc/wordpress/config-NAME_OF_YOUR_VIRTUAL_HOST.php .\\n\\nOnce the configuration file is written, it is up to you to choose a convention for username and password to MySQL\\nfor each WordPress database instance. This documentation shows only one, localhost, to act as an example.\\n\\n**Configure the MySQL database**\\n\\nNow we need to configure WordPress to use a MySQL database. Open the /etc/wordpress/config-localhost.php file\\nand write the following lines:\\n\\n<?php\\n\\ndefine('DB_NAME', 'wordpress');\\n\\ndefine('DB_USER', 'wordpress');\\n\\ndefine('DB_PASSWORD', 'yourpasswordhere');\\n\\ndefine('DB_HOST', 'localhost');\\n\\ndefine('WP_CONTENT_DIR', '/usr/share/wordpress/wp-content');\\n\\n?>\\n\\n297\\n\\n\\n-----\\n\\n## **Create the MySQL database**\\n\\nNow create the mySQL database you’ve just configured. Open a temporary file with MySQL command wordpress.sql\\nand write the following lines:\\n\\nCREATE DATABASE wordpress;\\n\\nCREATE USER 'wordpress'@'localhost'\\n\\nIDENTIFIED BY 'yourpasswordhere';\\n\\nGRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER\\n\\nON wordpress.*\\n\\nTO wordpress@localhost;\\n\\nFLUSH PRIVILEGES;\\n\\nThen, run the following commands:\\n\\ncat wordpress.sql | sudo mysql --defaults-extra-file=/etc/mysql/debian.cnf\\n\\nYour new WordPress installation can now be configured by visiting http://localhost/blog/wp-admin/install.php\\n(or http://NAME_OF_YOUR_VIRTUAL_HOST/blog/wp-admin/install.php if your server has no GUI and you are completing\\nWordPress configuration via a web browser running on another computer). Fill out the Site Title, username, password,\",\n",
       " 'and E-mail and click “Install WordPress”.\\n\\nNote the generated password (if applicable) and click the login password. Your WordPress is now ready for use!\\n## **Further reading**\\n\\n[• WordPress.org Codex](https://codex.wordpress.org/)\\n\\n[• Ubuntu Wiki WordPress](https://help.ubuntu.com/community/WordPress)\\n\\nThis page shows how to install the NVIDIA drivers from the command line, using either the ubuntu-drivers tool\\n(recommended), or APT.\\n## **NVIDIA drivers releases**\\n\\nWe package two types of NVIDIA drivers:\\n\\n1. **Unified Driver Architecture (UDA)** drivers - which are recommended for the generic desktop use, and which\\n[you can also find on the NVIDIA website.](https://www.nvidia.com/en-us/drivers/unix/)\\n\\n2. **Enterprise Ready Drivers (ERD)**  - which are recommended on servers and for computing tasks. Their\\npackages can be recognised by the -server suffix. [You can read more about these drivers in the NVIDIA](https://docs.nvidia.com/datacenter/tesla/index.html)\\n[documentation.](https://docs.nvidia.com/datacenter/tesla/index.html)\\n\\nAdditionally, we package the **NVIDIA Fabric Manager** and the **NVIDIA Switch Configuration and Query**\\n**(NSCQ) Library**, which you will only need if you have NVswitch hardware. The Fabric Manager and NSCQ library\\nare only available with the ERDs or -server driver versions.\\n## **Check driver versions**\\n\\nTo check the version of your currently running driver:\\n\\ncat /proc/driver/nvidia/version\\n## **The recommended way (ubuntu-drivers tool)**\\n\\nThe ubuntu-drivers tool relies on the same logic as the “Additional Drivers” graphical tool, and allows more flexibility\\non desktops and on servers.',\n",
       " 'The ubuntu-drivers tool is recommended if your computer uses Secure Boot, since it always tries to install signed\\ndrivers which are known to work with Secure Boot.\\n\\n**Check the available drivers for your hardware**\\n\\nFor desktop:\\n\\nsudo ubuntu-drivers list\\n\\nor, for servers:\\n\\nsudo ubuntu-drivers list --gpgpu\\n\\n298\\n\\n\\n-----\\n\\nYou should see a list such as the following:\\n\\nnvidia-driver-418-server\\n\\nnvidia-driver-515-server\\n\\nnvidia-driver-525-server\\n\\nnvidia-driver-450-server\\n\\nnvidia-driver-515\\n\\nnvidia-driver-525\\n\\n**Installing the drivers for generic use (e.g. desktop and gaming)**\\n\\nYou can either rely on automatic detection, which will install the driver that is considered the best match for your\\nhardware:\\n\\nsudo ubuntu-drivers install\\n\\nOr you can tell the ubuntu-drivers tool which driver you would like installed. If this is the case, you will have to use\\nthe driver version (such as 525 ) that you saw when you used the ubuntu-drivers list command.\\n\\nLet’s assume we want to install the 525 driver:\\n\\nsudo ubuntu-drivers install nvidia:525\\n\\n**Installing the drivers on servers and/or for computing purposes**\\n\\nYou can either rely on automatic detection, which will install the driver that is considered the best match for your\\nhardware:\\n\\nsudo ubuntu-drivers install --gpgpu\\n\\nOr you can tell the ubuntu-drivers tool which driver you would like installed. If this is the case, you will have to use\\nthe driver version (such as 525 ) and the -server suffix that you saw when you used the ubuntu-drivers list --gpgpu\\ncommand.\\n\\nLet’s assume we want to install the 525-server driver (listed as nvidia-driver-525-server ):\\n\\nsudo ubuntu-drivers install --gpgpu nvidia:525-server',\n",
       " 'You will also want to install the following additional components:\\n\\nsudo apt install nvidia-utils-525-server\\n\\n**Optional step**\\n\\nIf your system comes with NVswitch hardware, then you will want to install Fabric Manager and the NVSwitch\\nConfiguration and Query library. You can do so by running the following:\\n\\nsudo apt install nvidia-fabricmanager-525 libnvidia-nscq-525\\n\\n**Note** :\\n\\nWhile nvidia-fabricmanager and libnvidia-nscq do not have the same -server label in their name, they are\\nreally meant to match the -server drivers in the Ubuntu archive. For example, nvidia-fabricmanager-525\\nwill match the nvidia-driver-525-server package version (not the nvidia-driver-525 package ).\\n## **Manual driver installation (using APT)**\\n\\nInstalling the NVIDIA driver manually means installing the correct kernel modules first, then installing the metapackage for the driver series.\\n\\n**Installing the kernel modules**\\n\\nIf your system uses Secure Boot (as most x86 modern systems do), your kernel will require the kernel modules to be\\nsigned. There are two (mutually exclusive) ways to achieve this.\\n\\n**Installing the pre-compiled NVIDIA modules for your kernel**\\n\\nInstall the metapackage for your kernel flavour (e.g. generic, lowlatency, etc) which is specific to the driver branch\\n(e.g. 525 ) that you want to install, and whether you want the compute vs. general display driver (e.g. -server or not):\\n\\nsudo apt install linux-modules-nvidia-${DRIVER_BRANCH}${SERVER}-${LINUX_FLAVOUR}\\n\\n299\\n\\n\\n-----\\n\\n(e.g. linux-modules-nvidia-525-generic )\\n\\nCheck that the modules for your specific kernel/ABI were installed by the metapackage:',\n",
       " 'sudo apt-cache policy linux-modules-nvidia-${DRIVER_BRANCH}${SERVER}-$(uname -r)\\n\\n(e.g. sudo apt-cache policy linux-modules-nvidia-525-$(uname -r) )\\n\\nIf the modules were not installed for your current running kernel, upgrade to the latest kernel or install them by\\nspecifying the running kernel version:\\n\\nsudo apt install linux-modules-nvidia-${DRIVER_BRANCH}${SERVER}-$(uname -r)\\n\\n(e.g. sudo apt install linux-modules-nvidia-525-$(uname -r) )\\n\\n**Building your own kernel modules using the NVIDIA DKMS package**\\n\\nInstall the relevant NVIDIA DKMS package and linux-headers to build the kernel modules, and enroll your own key\\nto sign the modules.\\n\\nInstall the linux-headers metapackage for your kernel flavour (e.g. generic, lowlatency, etc):\\n\\nsudo apt install linux-headers-${LINUX_FLAVOUR}\\n\\nCheck that the headers for your specific kernel were installed by the metapackage:\\n\\nsudo apt-cache policy linux-headers-$(uname -r)\\n\\nIf the headers for your current running kernel were not installed, install them by specifying the running kernel version:\\n\\nsudo apt install linux-headers-$(uname -r)\\n\\nFinally, install the NVIDIA DKMS package for your desired driver series (this may automatically guide you through\\ncreating and enrolling a new key for Secure Boot):\\n\\nsudo apt install nvidia-dkms-${DRIVER_BRANCH}${SERVER}\\n\\n**Installing the user-space drivers and the driver libraries**\\n\\nAfter installing the correct kernel modules (see the relevant section of this document), install the correct driver\\nmetapackage:\\n\\nsudo apt install nvidia-driver-${DRIVER_BRANCH}${SERVER}\\n\\n**(Optional) Installing Fabric Manager and the NSCQ library**',\n",
       " \"If your system comes with NVswitch hardware, then you will want to install Fabric Manager and the NVSwitch\\nConfiguration and Query library. You can do so by running the following:\\n\\nsudo apt install nvidia-fabricmanager-${DRIVER_BRANCH} libnvidia-nscq-${DRIVER_BRANCH}\\n\\n**Note** :\\n\\nWhile nvidia-fabricmanager and libnvidia-nscq do not have the same -server label in their name, they are\\nreally meant to match the -server drivers in the Ubuntu archive. For example, nvidia-fabricmanager-525\\nwill match the nvidia-driver-525-server package version (not the nvidia-driver-525 package).\\n## **Switching between pre-compiled and DKMS modules**\\n\\n1. Uninstalling the NVIDIA drivers (below)\\n\\n2. Manual driver installation using APT\\n\\n**Uninstalling the NVIDIA drivers**\\n\\nRemove any NVIDIA packages from your system:\\n\\nsudo apt --purge remove '*nvidia*${DRIVER_BRANCH}*'\\n\\nRemove any additional packages that may have been installed as a dependency (e.g. the i386 libraries on amd64\\nsystems) and which were not caught by the previous command:\\n\\nsudo apt autoremove\\n\\n300\\n\\n\\n-----\\n\\n## **Transitional packages to new driver branches**\\n\\nWhen NVIDIA stops support on a driver branch, then Canonical will transition you to the next supported driver\\nbranch automatically if you try to install that driver branch.\\n\\n[See NVIDIA’s current support matrix in their documentation.](https://docs.nvidia.com/datacenter/tesla/drivers/index.html#branches)\\n## **Graphics**\\n\\nGraphics for QEMU/KVM always comes in two pieces: a front end and a back end.\\n\\n - frontend : Controlled via the -vga argument, which is provided to the guest. Usually one of cirrus, std, qxl, or\\n\\nvirtio .\",\n",
       " 'The default these days is qxl which strikes a good balance between guest compatibility and performance.\\nThe guest needs a driver for whichever option is selected – this is the most common reason to not use the default\\n(e.g., on very old Windows versions).\\n\\n - backend : Controlled via the -display argument. This is what the host uses to actually display the graphical\\ncontent, which can be an application window via gtk or a vnc .\\n\\n  - In addition, one can enable the -spice back end (which can be done in addition to vnc ). This can be faster and\\nprovides more authentication methods than vnc .\\n\\n  - If you want no graphical output at all, you can save some memory and CPU cycles by setting -nographic .\\n\\nIf you run with spice or vnc you can use native vnc tools or virtualization-focused tools like virt-viewer . You can\\nread more about these in the libvirt section.\\n\\nAll these options are considered basic usage of graphics, but there are also advanced options for more specific use-cases.\\n[Those cases usually differ in their ease-of-use and capability, such as:](https://cpaelzer.github.io/blogs/006-mediated-device-to-pass-parts-of-your-gpu-to-a-guest/)\\n\\n - *Need 3D acceleration* : Use -vga virtio with a local display having a GL context -display gtk,gl=on . This will\\n[use virgil3d on the host, and guest drivers are needed (which are common in Linux since Kernels >= 4.4 but can](https://virgil3d.github.io/)\\nbe hard to come by for other cases). While not as fast as the next two options, the major benefit is that it can\\nbe used without additional hardware and without a proper input-output memory management unit (IOMMU)',\n",
       " '[set up for device passthrough.](https://www.kernel.org/doc/Documentation/vfio-mediated-device.txt)\\n\\n - *Need native performance* : Use PCI passthrough of additional GPUs in the system. You’ll need an IOMMU set\\nup, and you’ll need to unbind the cards from the host before you can pass it through, like so:\\n\\n-device vfio-pci,host=05:00.0,bus=1,addr=00.0,multifunction=on,x-vga=on -device vfio-pci,host=05:00.1,bus=1,addr=\\n\\n - *Need native performance, but multiple guests per card* : Like with PCI passthrough, but using mediated devices\\nto shard a card on the host into multiple devices, then passing those:\\n\\n-display gtk,gl=on -device vfio-pci,sysfsdev=/sys/bus/pci/devices/0000:00:02.0/4dd511f6-ec08-11e8-b839\\n2f163ddee3b3,display=on,rombar=0\\n\\n[You can read more about vGPU at kraxel and Ubuntu GPU mdev evaluation. The sharding of the cards is](https://www.kraxel.org/blog/2018/04/vgpu-display-support-finally-merged-upstream/)\\n[driver-specific and therefore will differ per manufacturer – Intel, Nvidia, or AMD.](https://github.com/intel/gvt-linux/wiki/GVTg_Setup_Guide)\\n\\nThe advanced cases in particular can get pretty complex – it is recommended to use QEMU through libvirt for those\\ncases. libvirt will take care of all but the host kernel/BIOS tasks of such configurations. Below are the common basic\\nactions needed for faster options (i.e., passthrough and mediated devices passthrough).\\n\\nThe initial step for both options is the same; you want to ensure your system has its IOMMU enabled and the device\\nto pass should be in a group of its own. Enabling the VT-d and IOMMU is usually a BIOS action and thereby\\nmanufacturer dependent.',\n",
       " '## **Preparing the input-output memory management unit (IOMMU)**\\n\\n[On the kernel side, there are various options you can enable/configure for the IOMMU feature. In recent Ubuntu](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html?highlight=iommu)\\nkernels (>=5.4 => Focal or Bionic-HWE kernels) everything usually works by default, unless your hardware setup\\nmakes you need any of those tuning options.\\n\\n**Note** :\\nThe card used in all examples below e.g. when filtering for or assigning PCI IDs, is an NVIDIA V100 on\\nPCI ID 41.00.0\\n$ lspci | grep 3D\\n41:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] (rev a1)\\n\\nYou can check your boot-up kernel messages for IOMMU/DMAR messages or even filter it for a particular PCI ID.\\n\\nTo list all:\\n\\n301\\n\\n\\n-----\\n\\ndmesg | grep -i -e DMAR -e IOMMU\\n\\nWhich produces an output like this:\\n\\n[ 3.509232] iommu: Default domain type: Translated\\n\\n...\\n\\n[ 4.516995] pci 0000:00:01.0: Adding to iommu group 0\\n\\n...\\n\\n[ 4.702729] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank).\\n\\nTo filter for the installed 3D card:\\n\\ndmesg | grep -i -e DMAR -e IOMMU | grep $(lspci | awk \\'/ 3D / {print $1}\\' )\\n\\nWhich shows the following output:\\n\\n[ 4.598150] pci 0000:41:00.0: Adding to iommu group 66\\n\\nIf you have a particular device and want to check for its group you can do that via sysfs . If you have multiple cards\\nor want the full list you can traverse the same sysfs paths for that.\\n\\nFor example, to find the group for our example card:\\n\\nfind /sys/kernel/iommu_groups/ -name \"*$(lspci | awk \\'/ 3D / {print $1}\\')*\"\\n\\nWhich it tells us is found here:\\n\\n/sys/kernel/iommu_groups/66/devices/0000:41:00.0',\n",
       " 'We can also check if there are other devices in this group:\\n\\nll /sys/kernel/iommu_groups/66/devices/\\n\\nlrwxrwxrwx 1 root root 0 Jan 3 06:57 0000:41:00.0 -> ../../../../devices/pci0000:40/0000:40:03.1/0000:41:00.0/\\n\\nAnother useful tool for this stage (although the details are beyond the scope of this article) can be virsh node*,\\nespecially virsh nodedev-list --tree and virsh nodedev-dumpxml <pcidev> .\\n\\n**Note** :\\n\\nSome older or non-server boards tend to group devices in one IOMMU group, which isn’t very useful as it\\nmeans you’ll need to pass “all or none of them” to the same guest.\\n## **Preparations for PCI and mediated devices pass-through – block host drivers**\\n\\nFor both, you’ll want to ensure the normal driver isn’t loaded. In some cases you can do that at runtime via virsh\\n\\nnodedev-detach <pcidevice> . libvirt will even do that automatically if, on the passthrough configuration, you have\\nset <hostdev mode=\\'subsystem\\' type=\\'pci\\' managed=\\'yes\\'> .\\n\\nThis usually works fine for e.g. network cards, but some other devices like GPUs do not like to be unassigned, so there\\nthe required step usually is block loading the drivers you do not want to be loaded. In our GPU example the nouveau\\ndriver would load and that has to be blocked. To do so you can create a modprobe blacklist.\\n\\necho \"blacklist nouveau\" | sudo tee /etc/modprobe.d/blacklist-nouveau.conf\\n\\necho \"options nouveau modeset=0\" | sudo tee -a /etc/modprobe.d/blacklist-nouveau.conf\\n\\nsudo update-initramfs -u\\n\\nsudo reboot\\n\\nYou can check which kernel modules are loaded and available via lspci -v :\\n\\nlspci -v | grep -A 10 \" 3D \"\\n\\nWhich in our example shows:',\n",
       " '41:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] (rev a1)\\n\\n...\\n\\nKernel modules: nvidiafb, nouveau\\n\\nIf the configuration did not work instead it would show:\\n\\nKernel driver in use: nouveau\\n\\n302\\n\\n\\n-----\\n\\n## **Preparations for mediated devices pass-through - driver**\\n\\nFor PCI passthrough, the above steps would be all the preparation needed, but for mediated devices one also needs\\n[to install and set up the host driver. The example here continues with our NVIDIA V100 which is supported and](https://docs.nvidia.com/grid/latest/product-support-matrix/index.html#abstract__ubuntu)\\n[available from Nvidia.](https://docs.nvidia.com/grid/latest/product-support-matrix/index.html#abstract__ubuntu)\\n\\n[There is also an Nvidia document about the same steps available on installation and configuration of vGPU on Ubuntu.](https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#ubuntu-install-configure-vgpu)\\n\\nOnce you have the drivers from Nvidia, like nvidia-vgpu-ubuntu-470_470.68_amd64.deb, then install them and check\\n(as above) that that driver is loaded. The one you need to see is nvidia_vgpu_vfio :\\n\\nlsmod | grep nvidia\\n\\nWhich we can see in the output:\\n\\nnvidia_vgpu_vfio 53248 38\\n\\nnvidia 35282944 586 nvidia_vgpu_vfio\\n\\nmdev 24576 2 vfio_mdev,nvidia_vgpu_vfio\\n\\ndrm 491520 6 drm_kms_helper,drm_vram_helper,nvidia\\n\\n**Note** :\\n[While it works without a vGPU manager, to get the full capabilities you’ll need to configure the vGPU](https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#install-vgpu-package-ubuntu)',\n",
       " '[manager (that came with above package) and a license server so that each guest can get a license for](https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#install-vgpu-package-ubuntu)\\n[the vGPU provided to it. Please see Nvidia’s documentation for the license server. While not officially](https://docs.nvidia.com/grid/ls/latest/grid-license-server-user-guide/index.html)\\nsupported on Linux (as of Q1 2022), it’s worthwhile to note that it runs fine on Ubuntu with sudo apt\\n\\ninstall unzip default-jre tomcat9 liblog4j2-java libslf4j-java using /var/lib/tomcat9 as the server\\npath in the license server installer.\\n\\n[It’s also worth mentioning that the Nvidia license server went EOL on 31 July 2023. At that time, it was](https://docs.nvidia.com/grid/news/vgpu-software-license-server-eol-notice/index.html)\\n[replaced by the NVIDIA License System.](https://docs.nvidia.com/license-system/latest/nvidia-license-system-quick-start-guide/index.html)\\n\\nHere is an example of those when running fine:\\n\\n# general status\\n\\n$ systemctl status nvidia-vgpu-mgr\\n\\nLoaded: loaded (/lib/systemd/system/nvidia-vgpu-mgr.service; enabled; vendor preset: enabled)\\n\\nActive: active (running) since Tue 2021-09-14 07:30:19 UTC; 3min 58s ago\\n\\nProcess: 1559 ExecStart=/usr/bin/nvidia-vgpu-mgr (code=exited, status=0/SUCCESS)\\n\\nMain PID: 1564 (nvidia-vgpu-mgr)\\n\\nTasks: 1 (limit: 309020)\\n\\nMemory: 1.1M\\n\\nCGroup: /system.slice/nvidia-vgpu-mgr.service\\n\\n└─1564 /usr/bin/nvidia-vgpu-mgr\\n\\nSep 14 07:30:19 node-watt systemd[1]: Starting NVIDIA vGPU Manager Daemon...\\n\\nSep 14 07:30:19 node-watt systemd[1]: Started NVIDIA vGPU Manager Daemon.',\n",
       " 'Sep 14 07:30:20 node-watt nvidia-vgpu-mgr[1564]: notice: vmiop_env_log: nvidia-vgpu-mgr daemon started\\n\\n# Entries when a guest gets a vGPU passed\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): gpu-pci-id : 0x4100\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): vgpu_type : Quadro\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): Framebuffer: 0x1dc000000\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): Virtual Device Id: 0x1db4:0x1252\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): FRL Value: 60 FPS\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: ######## vGPU Manager Information: ########\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: Driver Version: 470.68\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): vGPU supported range: (0x70001, 0xb0001)\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): Init frame copy engine: syncing...\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: (0x0): vGPU migration enabled\\n\\nSep 14 08:29:50 node-watt nvidia-vgpu-mgr[2866]: notice: vmiop_log: display_init inst: 0 successful\\n\\n# Entries when a guest grabs a license\\n\\nSep 15 06:55:50 node-watt nvidia-vgpu-mgr[4260]: notice: vmiop_log: (0x0): vGPU license state: Unlicensed (Unrestricted)\\n\\nSep 15 06:55:52 node-watt nvidia-vgpu-mgr[4260]: notice: vmiop_log: (0x0): vGPU license state: Licensed\\n\\n# In the guest the card is then fully recognized and enabled\\n\\n$ nvidia-smi -a | grep -A 2 \"Licensed Product\"\\n\\nvGPU Software Licensed Product\\n\\n303\\n\\n\\n-----',\n",
       " 'Product Name : NVIDIA RTX Virtual Workstation\\n\\nLicense Status : Licensed\\n\\n[A mediated device is essentially partitioning of a hardware device using firmware and host driver features. This brings](https://www.kernel.org/doc/html/latest/driver-api/vfio-mediated-device.html)\\na lot of flexibility and options; in our example we can split our 16G GPU into 2x8G, 4x4G, 8x2G or 16x1G just as we\\nneed it. The following gives an example of how to split it into two 8G cards for a compute profile and pass those to\\nguests.\\n\\n[Please refer to the NVIDIA documentation for advanced tunings and different card profiles.](https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#ubuntu-install-configure-vgpu)\\n\\nThe tool for listing and configuring these mediated devices is mdevctl :\\n\\nsudo mdevctl types\\n\\nWhich will list the available types:\\n\\n...\\n\\nnvidia-300\\n\\nAvailable instances: 0\\n\\nDevice API: vfio-pci\\n\\nName: GRID V100-8C\\n\\nDescription: num_heads=1, frl_config=60, framebuffer=8192M, max_resolution=4096x2160, max_instance=2\\n\\nKnowing the PCI ID ( 0000:41:00.0 ) and the mediated device type we want ( nvidia-300 ) we can now create those\\nmediated devices:\\n\\n$ sudo mdevctl define --parent 0000:41:00.0 --type nvidia-300\\n\\nbc127e23-aaaa-4d06-a7aa-88db2dd538e0\\n\\n$ sudo mdevctl define --parent 0000:41:00.0 --type nvidia-300\\n\\n1360ce4b-2ed2-4f63-abb6-8cdb92100085\\n\\n$ sudo mdevctl start --parent 0000:41:00.0 --uuid bc127e23-aaaa-4d06-a7aa-88db2dd538e0\\n\\n$ sudo mdevctl start --parent 0000:41:00.0 --uuid 1360ce4b-2ed2-4f63-abb6-8cdb92100085\\n\\nAfter that, you can check the UUID of your ready mediated devices:\\n\\n$ sudo mdevctl list -d',\n",
       " \"bc127e23-aaaa-4d06-a7aa-88db2dd538e0 0000:41:00.0 nvidia-108 manual (active)\\n\\n1360ce4b-2ed2-4f63-abb6-8cdb92100085 0000:41:00.0 nvidia-108 manual (active)\\n\\nThose UUIDs can then be used to pass the mediated devices to the guest - which from here is rather similar to the\\npass through of a full PCI device.\\n## **Passing through PCI or mediated devices**\\n\\nAfter the above setup is ready one can pass through those devices, in libvirt for a PCI passthrough that looks like:\\n\\n<hostdev mode='subsystem' type='pci' managed='yes'>\\n\\n<source>\\n\\n<address domain='0x0000' bus='0x41' slot='0x00' function='0x0'/>\\n\\n</source>\\n\\n</hostdev>\\n\\nAnd for mediated devices it is quite similar, but using the UUID.\\n\\n<hostdev mode='subsystem' type='mdev' managed='no' model='vfio-pci' display='on'>\\n\\n<source>\\n\\n<address uuid='634fc146-50a3-4960-ac30-f09e5cedc674'/>\\n\\n</source>\\n\\n</hostdev>\\n\\n[Those sections can be part of the guest definition itself, to be added on guest startup and freed on guest shutdown.](https://libvirt.org/formatdomain.html#usb-pci-scsi-devices)\\nOr they can be in a file and used by for hot-add remove if the hardware device and its drivers support it virsh\\n\\nattach-device .\\n\\n**Note** :\\n\\nThis works great on Focal, but type='none' as well as display='off' weren’t available on Bionic. If this\\n[level of control is required one would need to consider using the Ubuntu Cloud Archive or Server-Backports](https://wiki.ubuntu.com/OpenStack/CloudArchive)\\nfor a newer stack of the virtualisation components.\\n\\nAnd finally, it might be worth noting that while mediated devices are becoming more common and known for vGPU\",\n",
       " '[handling, they are a general infrastructure also used (for example) for s390x vfio-ccw.](https://www.kernel.org/doc/html/latest/s390/vfio-ccw.html)\\n\\n304\\n\\n\\n-----\\n\\nOur explanatory and conceptual guides are written to provide a better understanding of how Ubuntu Server works\\nand how it can be used and configured. They enable you to expand your knowledge, making the operating system\\n\\neasier to use.\\n## **Explanation guides**\\n\\nSoftware\\n\\nAbout apt upgrade and phased updates\\nChanging package files\\nNetwork\\n\\nIntroduction\\nConfiguration\\nDHCP\\n\\nNTP\\n\\nDPDK\\n\\nOpenVswitch-DPDK\\nCryptography\\n\\nIntroduction to crypto libraries\\nOpenSSL\\nGnuTLS\\nNetwork Security Services (NSS)\\nJava cryptography configuration\\nBIND 9 DNSSEC cryptography selection\\nOpenSSH crypto configuration\\nTroubleshooting TLS/SSL\\n## **Virtualisation and containers**\\n\\nAbout virtual machines\\n\\nOverview of VM tools in Ubuntu\\n\\nUsing virtual machines\\n\\nUsing QEMU for microVMs\\nUpgrading the machine type of your VM\\n\\nAbout containers\\n\\nOverview of container tools in Ubuntu\\n\\nOther virtualization tools\\n\\nAbout OpenStack\\n## **Web servers**\\n\\nDetails and key concepts…\\n\\nAbout web servers\\n\\nAbout Squid proxy servers\\n## **Introduction to…**\\n\\nIf you’re not sure how or where to get started with a topic, these introductory pages will give you a high-level overview\\nand relevant links (with context!) to help you navigate easily to the guides and other materials of most interest to\\n\\nyou.\\n\\nVirtualization\\n\\nIntroduction to virtualization\\n\\nNetworks\\n\\n305\\n\\n\\n-----\\n\\nIntroduction to networking\\nIntroduction to Samba\\n\\nUseful services\\n\\nIntroduction to mail servers\\n\\nIntroduction to web servers\\n\\nIntroduction to backups',\n",
       " 'You may have noticed recently that updating your system with apt upgrade sometimes produces a weird message\\nabout packages being kept back…like this one:\\n\\nReading package lists... Done\\n\\nBuilding dependency tree... Done\\n\\nReading state information... Done\\n\\nCalculating upgrade... Done\\n\\nThe following packages have been kept back:\\n\\n(Names of <X> held back packages listed here)\\n\\n0 upgraded, 0 newly installed, 0 to remove and <X> not upgraded.\\n\\nIf you’ve ever used combinations of packages from different releases or third party repos, you may be familiar with this\\nmessage already. However, it has become a much more common occurrence due to something called “phased updates”.\\n## **What are phased updates?**\\n\\nPhased updates are software updates that are rolled out in stages, rather than being provided to everyone at the\\nsame time. Initially, the update is provided only to a small subset of Ubuntu machines. As the update proves to be\\nstable, it is provided to an ever-increasing number of users until everyone has received it (i.e., when the update is\\n“fully phased”).\\n\\nThe good news is, you don’t need to do anything about the “packages kept back” message – you can safely ignore it.\\nOnce the update has been deemed safe for release, you will receive the update automatically.\\n## **Why is Ubuntu doing this?**\\n\\nAlthough updates are thoroughly tested before they get released at all, sometimes bugs can be hidden well enough to\\nescape our attention and make it into a release – especially in highly specific use cases that we didn’t know we needed\\nto test. This can obviously cause problems for our users, and used to be the norm before we phased updates through\\n\\napt .',\n",
       " 'Update phasing makes it much easier for us to detect serious breakages early on – before they have a chance to cause\\nproblems for the majority of our users. It gives us the opportunity to hold back the update until the bugs are fixed.\\n\\nIn other words, it directly benefits our users by increasing the safety, stability and reliability of Ubuntu.\\n\\nThe phasing system makes it so that different sets of users are chosen to be the first to get the updates, so that there\\nisn’t one group of unlucky people who always get potentially broken updates soon after release.\\n\\n**Note** :\\n\\nIt should be mentioned here that security updates are *never* phased.\\n## **Can I turn off phased updates?**\\n\\nThat depends on how stable you need your system to be. If you just want to avoid any notices about packages being\\nheld back during apt updates, and you’re willing to be one of the first people to get updates whenever they’re released,\\nyou can turn off phased updates. Be warned, though – if an update *is* broken, you will almost always be in the first\\nset of people to get it (i.e., you’re basically volunteering yourself as a guinea pig for the early update releases!). It will\\nget rid of the “held back packages” in apt message, though.\\n\\nIf that doesn’t sound like something you want, leave phased updates on (this is the default). You will still temporarily\\nget the “held back packages” message, but your machine will be more protected from updates that might otherwise\\nbreak it – and once the packages are ready to be safely installed on your system, they will no longer be held back.\\n\\n306\\n\\n\\n-----\\n\\n## Can I apt upgrade the individual packages? (and should I?)',\n",
       " 'While you can *technically* get around phased updates by running apt install on individual held back packages, it’s\\nnot recommended. You’re unlikely to break your machine by doing this – as long as the package is being held back\\ndue to update phasing.\\n\\nIf you want to apt upgrade a package, you should first carefully examine the proposed changes that apt would make\\nbefore you proceed. If the package update was kept back for a reason unrelated to phasing, apt may be forced to\\nremove packages in order to complete your request, which could then cause problems elsewhere.\\n## **How do I turn off phased updates?**\\n\\nIf you’re sure that you want to disable phased updates, reverting to the old behaviour, you can change apt ’s configuration by creating a file in /etc/apt/apt.conf.d called 99-Phased-Updates (if /etc/apt/apt.conf.d/99-Phased-Updates\\ndoesn’t already exist). In the file, simply add the following lines:\\n\\nUpdate-Manager::Always-Include-Phased-Updates true;\\n\\nAPT::Get::Always-Include-Phased-Updates true;\\n\\nAgain, please only do this if you really know what you’re doing and are absolutely sure you need to do it (for instance,\\nif you are intentionally installing all the latest packages to help test them – and don’t mind if your system breaks).\\nWe definitely don’t recommend turning off phased updates if you’re a newer user.\\n## **Why is this suddenly happening now?**\\n\\nPhased updates have been part of the update-manager on Ubuntu Desktop for quite a while (since 13.04, in fact!),\\n[but were implemented in APT in 21.04. It now works on all versions of Ubuntu (including Ubuntu Server, Raspberry](https://discourse.ubuntu.com/t/phased-updates-in-apt-in-21-04/20345)\\nPi, and containers).',\n",
       " 'Since this includes the 22.04 LTS, it’s now getting a lot more attention as a result!\\n# **How does it actually work?**\\n\\nPhased updates depend on a value derived from your machine’s “Machine ID”, as well as the package name and\\npackage version. The neat thing about this is that phasing is determined completely at the client end; no identifiable\\ninformation (or indeed any new information at all) is ever sent to the server to achieve update phasing.\\n\\nWhen the software update is released, the initial subset of machines to receive the update first is chosen at random.\\nOnly if there are no problems detected by the first set of users will the update be made available to everyone.\\n\\n[For more detailed information, including about how changes to phasing are timed, you can check the Ubuntu wiki](https://wiki.ubuntu.com/PhasedUpdates)\\n[page on phased updates.](https://wiki.ubuntu.com/PhasedUpdates)\\n## **How can I find out more information about currently phased packages?**\\n\\nYou can find out the phasing details of a package by using the apt policy command:\\n\\napt policy <package>\\n\\nFor example, at the time of writing, the package libglapi-mesa has a phased update. Running apt policy libglapi\\nmesa then produces an output like this:\\n\\nlibglapi-mesa:\\n\\nInstalled: 22.0.5-0ubuntu0.3\\n\\nCandidate: 22.2.5-0ubuntu0.1~22.04.1\\n\\nVersion table:\\n\\n22.2.5-0ubuntu0.1~22.04.1 500 (phased 20%)\\n\\n500 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages\\n\\n*** 22.0.5-0ubuntu0.3 100\\n\\n100 /var/lib/dpkg/status\\n\\n22.0.1-1ubuntu2 500\\n\\n500 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages\\n\\nIn this output you can see that this package is 20% phased.',\n",
       " '[You can see the status of all packages currently being phased in Ubuntu at https://people.canonical.com/~ubuntu-](https://people.canonical.com/~ubuntu-archive/phased-updates.html)\\n[archive/phased-updates.html](https://people.canonical.com/~ubuntu-archive/phased-updates.html)\\n\\n**Note** :\\n\\nThere is a bug report currently active about the fact that the “kept back” message is not as informative as\\nit could be, and the issue is on our radar. If this issue also affects you, you have an Ubuntu Single Sign-On\\n\\n307\\n\\n\\n-----\\n\\n(SSO) account and can log into Launchpad, you can click on the link near the top of the page that says\\n“This bug affects 85 people. Does this bug affect you?”. If you then click on “Yes, it affects me”, it will\\nincrease the bug heat rating, making it more significant.\\n## **Further reading**\\n\\n[• The details in this page are based on this excellent post on AskUbuntu by AskUbuntu user](https://askubuntu.com/questions/1431940/what-are-phased-updates-and-why-does-ubuntu-use-them) *ArrayBolt3* . This\\npage is therefore licensed under Creative Commons Attribution-ShareAlike license, distributed under the terms\\n[of CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)\\n\\n[• You can check on the progress of the current phasing Ubuntu Stable Release Updates.](https://people.canonical.com/~ubuntu-archive/phased-updates.html)\\n\\n[• There is also more detail on how phased updates work in the Ubuntu wiki, the Error Tracker, and the](https://wiki.ubuntu.com/PhasedUpdates) [apt](https://manpages.ubuntu.com/manpages/jammy/man5/apt_preferences.5.html)\\n[preferences manpage.](https://manpages.ubuntu.com/manpages/jammy/man5/apt_preferences.5.html)',\n",
       " 'Many packages in Ubuntu will create extra files when installed. These files can contain metadata, configurations, rules\\nfor operating system interaction, and so on. In many cases, these files will be fully managed by updates to a package,\\nleading to issues when they are modified manually. This page goes over some methods for changing the behavior of a\\npackage without causing conflicts in maintained files.\\n## **Configuration files**\\n\\nConfiguration files are often provided by packages. They come in many forms, but the majority can be found in the\\n\\n/etc/ directory with either a .conf or .cnf extention. Most of the time, these files are managed by the package and\\nediting them could lead to a conflict when updating. To get around this, packages will check in additional <config>.d/\\ndirectories where you can place personal changes.\\n\\nFor example, if you would like mysql-server to run on port 3307 instead of 3306, you can open the file\\n\\n/etc/mysql/mysql.conf.d/mysqld.cnf, and edit the port option.\\n\\n[mysqld]\\n\\n#\\n\\n# * Basic Settings\\n\\n#\\n\\nuser = mysql\\n\\n# pid-file = /var/run/mysqld/mysqld.pid\\n\\n# socket = /var/run/mysqld/mysqld.sock\\n\\nport = 3307\\n\\n**Note** :\\nSome packages do not automatically create files for you to edit in their .d directories. In these cases it\\nis often acceptable to just create an additional config file by any name there. When in doubt, check the\\npackage’s documentation to confirm.\\n\\nAfter saving the file, restart the service.\\n\\nsystemctl restart mysql\\n\\nThe netstat command shows that this was successful:\\n\\nnetstat -tunpevaW | grep -i 3307\\n\\ntcp 0 0 127.0.0.1:3307 0.0.0.0:* LISTEN 106 416022 1730/mysqld\\n## **Systemd files**',\n",
       " '[Many packages ship service unit files for interacting with Systemd. Unit files allow packages to define background tasks,](https://systemd.io/)\\ninitialization behavior, and interactions with the operating system. The files, or symlinks of them, will automatically\\nbe placed in the /lib/systemd/system/ directory. Likewise, the files can also show up in /etc/systemd/system . If these\\nare edited manually they can cause major issues when updating or even running in general.\\n\\nInstead, if you would like to modify a unit file, do so through Systemd. It provides the command systemctl edit\\n\\n<service> which creates an override file and brings up a text editor for you to edit it.\\n\\nFor example, if you want to edit Apache2 such that it restarts after a failure instead of just when it aborts, you can\\nrun the following:\\n\\nsudo systemctl edit apache2\\n\\nThis will open a text editor containing:\\n\\n308\\n\\n\\n-----\\n\\n### Editing /etc/systemd/system/apache2.service.d/override.conf\\n\\n### Anything between here and the comment below will become the new contents of the file\\n\\n### Lines below this comment will be discarded\\n\\n### /lib/systemd/system/apache2.service\\n\\n# [Unit]\\n\\n# Description=The Apache HTTP Server\\n\\n# After=network.target remote-fs.target nss-lookup.target\\n\\n# Documentation=https://httpd.apache.org/docs/2.4/\\n\\n#\\n\\n# [Service]\\n\\n# Type=forking\\n\\n# Environment=APACHE_STARTED_BY_SYSTEMD=true\\n\\n# ExecStart=/usr/sbin/apachectl start\\n\\n# ExecStop=/usr/sbin/apachectl graceful-stop\\n\\n# ExecReload=/usr/sbin/apachectl graceful\\n\\n# KillMode=mixed\\n\\n# PrivateTmp=true\\n\\n# Restart=on-abort\\n\\n...\\n\\nOverride the on-abort option by adding a new line in the designated edit location.',\n",
       " '### Editing /etc/systemd/system/apache2.service.d/override.conf\\n\\n### Anything between here and the comment below will become the new contents of the file\\n\\n[Service]\\n\\nRestart=on-failure\\n\\n### Lines below this comment will be discarded\\n\\n...\\n\\n**Note** :\\n\\nSome options, such as ExecStart are additive. If you would like to fully override them add an extra line that\\nclears it (e.g. ExecStart= [) before providing new options. See Systemd’s man page for more information.](https://www.freedesktop.org/software/systemd/man/systemd.service.html)\\n\\nOnce the changes are saved, the override file will be created in /etc/systemd/system/apache2.service.d/override.conf .\\nTo apply changes, run\\n\\nsudo systemctl daemon-reload\\n\\nTo verify the change was successful, you can run the status command.\\n\\nsystemctl status apache2\\n\\n- apache2.service - The Apache HTTP Server\\n\\nLoaded: loaded (/lib/systemd/system/apache2.service; enabled; preset: enabled)\\n\\nDrop-In: /etc/systemd/system/apache2.service.d\\n\\n└─override.conf\\n\\n/run/systemd/system/service.d\\n\\n└─zzz-lxc-service.conf\\n\\nActive: active (running) since Fri 2023-02-17 16:39:22 UTC; 27min ago\\n\\nDocs: https://httpd.apache.org/docs/2.4/\\n\\nMain PID: 4735 (apache2)\\n\\nTasks: 55 (limit: 76934)\\n\\nMemory: 6.5M\\n\\nCPU: 65ms\\n\\nCGroup: /system.slice/apache2.service\\n\\n├─4735 /usr/sbin/apache2 -k start\\n\\n├─4736 /usr/sbin/apache2 -k start\\n\\n└─4737 /usr/sbin/apache2 -k start\\n\\n...\\n\\n309\\n\\n\\n-----\\n\\n## **AppArmor**\\n\\n[Packages that use AppArmor will install AppArmor profiles in the](https://ubuntu.com/server/docs/security-apparmor) /etc/apparmor.d/ directory. These files are often\\nnamed after the process being protected, such as usr.bin.firefox and usr.sbin.libvirtd .',\n",
       " \"When these files are modified manually, it can lead to a conflict during updates. This will show up in apt with\\nsomething like:\\n\\nConfiguration file '/etc/apparmor.d/usr.bin.swtpm'\\n\\n==> Modified (by you or by a script) since installation.\\n\\n==> Package distributor has shipped an updated version.\\n\\nWhat would you like to do about it ? Your options are:\\n\\nY or I : install the package maintainer's version\\n\\nN or O : keep your currently-installed version\\n\\nD : show the differences between the versions\\n\\nZ : start a shell to examine the situation\\n\\nThe default action is to keep your current version.\\n\\n*** usr.bin.swtpm (Y/I/N/O/D/Z) [default=N] ?\\n\\nUpdating to the maintainer’s version will override your changes, which could cause problems with your setup. However,\\nusing your version could cause security issues.\\n\\nIf you would like to modify these rules to provide the application with additional permissions, you can instead update\\nthe local profile, most often found in /etc/apparmor.d/local/ .\\n\\nFor example, if you would like swtpm to access a custom directory called /var/customtpm, you can append the following\\nline to /etc/apparmor.d/local/usr.bin.swtpm :\\n\\n/var/customtpm/** rwk,\\n\\n[This method will work for all AppArmor syntax.](https://ubuntu.com/tutorials/beginning-apparmor-profile-development)\\n\\n**Note** :\\nAlthough most local profiles have the same name as the maintainer’s, you can often check what file is\\nincluded based on the main profile’s contents. In swtpm ’s case, /etc/apparmor.d/usr.bin.swtpm contains\\nthe lines:\\n\\n# Site-specific additions and overrides. See local/README for details.\\n\\n#include <local/usr.bin.swtpm>\",\n",
       " 'showing that the local profile is located at /etc/apparmor.d/local/usr.bin.swtpm\\n## **Restoring configuration files**\\n\\nSince config files are meant to be intentional changes by the user/admin, they are not overwritten by updates or even\\nre-installs of the package. However, it’s possible you might change it by accident or may just want to go back to step\\none of a trial-and-error phase that you are in. In those situations you can use apt to restore the original config files.\\nNote that while we call apt, it is dpkg that actually handles the restoration.\\n\\nIf you have a particular config file, like in the example /etc/rsyslog.conf, you first want to find out which package\\nowns that config file:\\n\\n$ dpkg -S /etc/rsyslog.conf\\n\\nrsyslog: /etc/rsyslog.conf\\n\\nSo we now know that the package rsyslog owns the config file /etc/rsyslog.conf .\\nThis command just queries package metadata and works even if the file has been deleted.\\n\\n$ rm /etc/rsyslog.conf\\n\\n$ dpkg -S /etc/rsyslog.conf\\n\\nrsyslog: /etc/rsyslog.con\\n\\nTo restore that file you can re-install the package, telling dpdk to bring any missing files back.\\nTo do so you pass dpkg options through apt using -o Dpkg::Options::=\" and then set --force-... depending on what\\naction you want. For example:\\n\\n$ sudo apt install --reinstall -o Dpkg::Options::=\"--force-confmiss\" rsyslog\\n\\n...\\n\\nPreparing to unpack .../rsyslog_8.2302.0-1ubuntu3_amd64.deb ...\\n\\nUnpacking rsyslog (8.2302.0-1ubuntu3) over (8.2302.0-1ubuntu3) ...\\n\\n310\\n\\n\\n-----\\n\\nSetting up rsyslog (8.2302.0-1ubuntu3) ...\\n\\nConfiguration file \\'/etc/rsyslog.conf\\', does not exist on system.\\n\\nInstalling new config file as you requested.',\n",
       " 'More details on these options can be found in the dpkg [man page, but the most common and important ones are:](https://manpages.ubuntu.com/manpages/jammy/en/man1/dpkg.1.html)\\n\\n - confmiss\\nAlways install the missing conffile without prompting.\\n\\n - confnew\\nIf a conffile has been modified and the version in the package changed, always install the new version without\\nprompting.\\n\\n - confold\\nIf a conffile has been modified and the version in the package changed, always keep the old version without\\nprompting.\\n\\n - confdef\\nIf a conffile has been modified and the version in the package changed, always choose the default action without\\nprompting.\\n\\n - confask\\nIf a conffile has been modified, always offer to replace it with the version in the package, even if the version in\\nthe package did not change.\\n\\nSo in the case of an accidental bad config entry, if you want to go back to the package default you could use --force\\nconfask to check the difference and consider restoring the content.\\n\\n$ echo badentry >> /etc/rsyslog.conf\\n\\n$ sudo apt install --reinstall -o Dpkg::Options::=\"--force-confask\" rsyslog\\n\\n...\\n\\nPreparing to unpack .../rsyslog_8.2302.0-1ubuntu3_amd64.deb ...\\n\\nUnpacking rsyslog (8.2302.0-1ubuntu3) over (8.2302.0-1ubuntu3) ...\\n\\nSetting up rsyslog (8.2302.0-1ubuntu3) ...\\n\\nConfiguration file \\'/etc/rsyslog.conf\\'\\n\\n==> Modified (by you or by a script) since installation.\\n\\nVersion in package is the same as at last installation.\\n\\nWhat would you like to do about it ? Your options are:\\n\\nY or I : install the package maintainer\\'s version\\n\\nN or O : keep your currently-installed version\\n\\nD : show the differences between the versions\\n\\nZ : start a shell to examine the situation',\n",
       " 'The default action is to keep your current version.\\n\\n*** rsyslog.conf (Y/I/N/O/D/Z) [default=N] ? D\\n\\n--- /etc/rsyslog.conf 2023-04-18 07:11:50.427040350 +0000\\n\\n+++ /etc/rsyslog.conf.dpkg-new 2023-02-23 16:58:03.000000000 +0000\\n\\n@@ -51,4 +51,3 @@\\n\\n# Include all config files in /etc/rsyslog.d/\\n\\n#\\n\\n$IncludeConfig /etc/rsyslog.d/*.conf\\n\\n-badentry\\n\\nConfiguration file \\'/etc/rsyslog.conf\\'\\n\\n==> Modified (by you or by a script) since installation.\\n\\nVersion in package is the same as at last installation.\\n\\nWhat would you like to do about it ? Your options are:\\n\\nY or I : install the package maintainer\\'s version\\n\\nN or O : keep your currently-installed version\\n\\nD : show the differences between the versions\\n\\nZ : start a shell to examine the situation\\n\\nThe default action is to keep your current version.\\n\\n*** rsyslog.conf (Y/I/N/O/D/Z) [default=N] ? y\\n\\nInstalling new version of config file /etc/rsyslog.conf ...\\n\\nThe same can be used if you removed a whole directory by accident, to detect and re-install all related packages config\\n\\n311\\n\\n\\n-----\\n\\nfiles.\\n\\n$ rm -rf /etc/vim\\n\\n$ dpkg -S /etc/vim\\n\\nvim-common, vim-tiny: /etc/vim\\n\\n$ sudo apt install --reinstall -o Dpkg::Options::=\"--force-confmiss\" vim-common vim-tiny\\n\\n...\\n\\nConfiguration file \\'/etc/vim/vimrc\\', does not exist on system.\\n\\nInstalling new config file as you requested.\\n\\n...\\n\\nConfiguration file \\'/etc/vim/vimrc.tiny\\', does not exist on system.\\n\\nInstalling new config file as you requested.\\n\\nThis section provides high-level information pertaining to networking, including an overview of network concepts and\\ndetailed discussion of popular network protocols.\\n## **The Transmission Control Protocol and Internet Protocol (TCP/IP)**',\n",
       " 'The Transmission Control Protocol and Internet Protocol is a standard set of protocols developed in the late 1970s\\nby the Defense Advanced Research Projects Agency (DARPA) as a means of communication between different types\\nof computers and computer networks. TCP/IP is the driving force of the Internet, and thus it is the most popular set\\nof network protocols on Earth.\\n\\n**TCP/IP overview**\\n\\nThe two protocol components of TCP/IP deal with different aspects of computer networking.\\n\\n - **Internet Protocol** – the “IP” of TCP/IP – is a connectionless protocol that deals only with network packet\\nrouting using the *IP Datagram* as the basic unit of networking information. The IP Datagram consists of a\\nheader followed by a message.\\n\\n - **Transmission Control Protocol** – the “TCP” of TCP/IP – enables network hosts to establish connections\\nthat may be used to exchange data streams. TCP also guarantees that data sent between connections is delivered,\\nand that it arrives at one network host in the same order as sent from another network host.\\n\\n**TCP/IP configuration**\\n\\nThe TCP/IP protocol configuration consists of several elements that must be set by editing the appropriate configuration files, or by deploying solutions such as the Dynamic Host Configuration Protocol (DHCP) server which can,\\nin turn, be configured to provide the proper TCP/IP configuration settings to network clients automatically. These\\nconfiguration values must be set correctly in order to facilitate the proper network operation of your Ubuntu system.\\n\\nThe common configuration elements of TCP/IP and their purposes are as follows:',\n",
       " ' - **IP address** : The IP address is a unique identifying string expressed as four decimal numbers ranging from zero\\n(0) to two-hundred and fifty-five (255), separated by periods, with each of the four numbers representing eight\\n(8) bits of the address for a total length of thirty-two (32) bits for the whole address. This format is called *dotted*\\n*quad notation* .\\n\\n - **Netmask** : The subnet mask (or simply, *netmask* ) is a local bit mask, or set of flags, which separate the portions\\nof an IP address significant to the network from the bits significant to the *subnetwork* . For example, in a Class\\nC network, the standard netmask is 255.255.255.0 which masks the first three bytes of the IP address and allows\\nthe last byte of the IP address to remain available for specifying hosts on the subnetwork.\\n\\n - **Network address** : The network address represents the bytes comprising the network portion of an IP address.\\nFor example, the host 12.128.1.2 in a Class A network would use 12.0.0.0 as the network address, where twelve\\n(12) represents the first byte of the IP address, (the network part) and zeroes (0) in all of the remaining three\\nbytes to represent the potential host values. A network host using the private IP address 192.168.1.100 would in\\nturn use a network address of 192.168.1.0, which specifies the first three bytes of the Class C 192.168.1 network\\nand a zero (0) for all the possible hosts on the network.\\n\\n - **Broadcast address** : The broadcast address is an IP address that allows network data to be sent simultaneously\\nto all hosts on a given subnetwork, rather than specifying a particular host. The standard general broadcast',\n",
       " 'address for IP networks is 255.255.255.255, but this broadcast address cannot be used to send a broadcast\\nmessage to every host on the Internet because routers block it. A more appropriate broadcast address is set\\nto match a specific subnetwork. For example, on the private Class C IP network, 192.168.1.0, the broadcast\\naddress is 192.168.1.255. Broadcast messages are typically produced by network protocols such as the Address\\nResolution Protocol (ARP) and the Routing Information Protocol (RIP).\\n\\n312\\n\\n\\n-----\\n\\n - **Gateway address** : A gateway address is the IP address through which a particular network, or host on a\\nnetwork, may be reached. If one network host wishes to communicate with another network host, and that host\\nis not located on the same network, then a *gateway* must be used. In many cases, the gateway address will\\nbe that of a router on the same network, which will in turn pass traffic on to other networks or hosts, such as\\nInternet hosts. The value of the Gateway Address setting must be correct, or your system will not be able to\\nreach any hosts beyond those on the same network.\\n\\n - **Nameserver address** : Nameserver addresses represent the IP addresses of Domain Name Service (DNS) systems, which resolve network hostnames into IP addresses. There are three levels of nameserver addresses, which\\nmay be specified in order of precedence: The *primary* nameserver, the *secondary* nameserver, and the *tertiary*\\nnameserver. So that your system can resolve network hostnames into their corresponding IP addresses, you\\nmust specify valid nameserver addresses that you are authorized to use in your system’s TCP/IP configuration.',\n",
       " 'In many cases, these addresses can and will be provided by your network service provider, but many free and\\npublicly accessible nameservers are available for use, such as the Level3 (Verizon) servers with IP addresses from\\n4.2.2.1 to 4.2.2.6.\\n\\n**Tip**\\nThe IP address, netmask, network address, broadcast address, gateway address, and nameserver addresses\\nare typically specified via the appropriate directives in the file /etc/network/interfaces . For more information, view the system manual page for interfaces, with the following command typed at a terminal\\nprompt:\\n\\nman interfaces\\n\\n**IP routing**\\n\\nIP routing is a way of specifying and discovering paths in a TCP/IP network that network data can be sent along.\\nRouting uses a set of *routing tables* to direct the forwarding of network data packets from their source to the destination,\\noften via many intermediary network nodes known as *routers* . There are two primary forms of IP routing: *static routing*\\nand *dynamic routing.*\\n\\nStatic routing involves manually adding IP routes to the system’s routing table, and this is usually done by manipulating the routing table with the route command. Static routing enjoys many advantages over dynamic routing, such\\nas simplicity of implementation on smaller networks, predictability (the routing table is always computed in advance,\\nand thus the route is precisely the same each time it is used), and low overhead on other routers and network links\\ndue to the lack of a dynamic routing protocol. However, static routing does present some disadvantages as well. For\\nexample, static routing is limited to small networks and does not scale well. Static routing also fails completely to',\n",
       " 'adapt to network outages and failures along the route due to the fixed nature of the route.\\n\\nDynamic routing depends on large networks with multiple possible IP routes from a source to a destination and\\nmakes use of special routing protocols, such as the Router Information Protocol (RIP), which handle the automatic\\nadjustments in routing tables that make dynamic routing possible. Dynamic routing has several advantages over static\\nrouting, such as superior scalability and the ability to adapt to failures and outages along network routes. Additionally,\\nthere is less manual configuration of the routing tables, since routers learn from one another about their existence and\\navailable routes. This trait also prevents mistakes being introduced into the routing tables via human error. Dynamic\\nrouting is not perfect, however, and presents disadvantages such as heightened complexity and additional network\\noverhead from router communications, which does not immediately benefit the end users but still consumes network\\nbandwidth.\\n\\n**About TCP and UDP**\\n\\nTCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are the most common protocols used to\\ntransfer data over networks.\\n\\nTCP is a connection-based protocol, offering error correction and guaranteed delivery of data via what is known as\\n*flow control* . Flow control determines when the flow of a data stream needs to be stopped, and previously-sent data\\npackets should be re-sent due to problems such as *collisions*, for example, thus ensuring complete and accurate delivery\\nof the data. TCP is typically used in the exchange of important information such as database transactions.',\n",
       " 'UDP on the other hand, is a *connectionless* protocol which seldom deals with the transmission of important data\\nbecause it lacks flow control or any other method to ensure reliable delivery of the data. UDP is commonly used in\\nsuch applications as audio and video streaming, where it is considerably faster than TCP due to the lack of error\\ncorrection and flow control, and where the loss of a few packets is not generally catastrophic.\\n\\n**Internet Control Messaging Protocol (ICMP)**\\n\\n[The Internet Control Messaging Protocol is an extension to the Internet Protocol (IP) as defined in the Request For](https://www.rfc-editor.org/rfc/rfc792)\\n[Comments (RFC) #792, and supports network packets containing control, error, and informational messages. ICMP](https://www.rfc-editor.org/rfc/rfc792)\\n\\n313\\n\\n\\n-----\\n\\nis used by such network applications as the ping utility, which can determine the availability of a network host or\\ndevice. Examples of some error messages returned by ICMP which are useful to both network hosts and devices such\\nas routers, include *Destination Unreachable* and *Time Exceeded* .\\n\\n**About daemons**\\n\\nDaemons are special system applications which typically execute continuously in the background and await requests\\nfor the functions they provide from other applications. Many daemons are network-centric; that is, a large number of\\ndaemons executing in the background on an Ubuntu system may provide network-related functionality. Such network\\ndaemons include the *Hyper Text Transport Protocol Daemon* ( httpd ), which provides web server functionality; the',\n",
       " '*Secure SHell Daemon* ( sshd ), which provides secure remote login shell and file transfer capabilities; and the *Internet*\\n*Message Access Protocol Daemon* ( imapd ), which provides E-Mail services.\\n\\n**Resources**\\n\\n[• There are man pages for TCP and IP that contain more useful information.](https://manpages.ubuntu.com/manpages/focal/en/man7/tcp.7.html)\\n\\n[• Also, see the TCP/IP Tutorial and Technical Overview IBM Redbook.](http://www.redbooks.ibm.com/abstracts/gg243376.html)\\n\\n[• Another resource is O’Reilly’s TCP/IP Network Administration.](http://oreilly.com/catalog/9780596002978/)\\n\\nUbuntu ships with a number of graphical utilities to configure your network devices. This document is geared toward\\nserver administrators and will focus on managing your network on the command line.\\n## **Ethernet interfaces**\\n\\nEthernet interfaces are identified by the system using predictable network interface names. These names can appear\\nas *eno1* or *enp0s25* . However, in some cases an interface may still use the kernel *eth#* style of naming.\\n\\n**Identify Ethernet interfaces**\\n\\nTo quickly identify all available Ethernet interfaces, you can use the ip command as shown below.\\n\\nip a\\n\\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\\n\\nlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\\n\\ninet 127.0.0.1/8 scope host lo\\n\\nvalid_lft forever preferred_lft forever\\n\\ninet6 ::1/128 scope host\\n\\nvalid_lft forever preferred_lft forever\\n\\n2: enp0s25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\\n\\nlink/ether 00:16:3e:e2:52:42 brd ff:ff:ff:ff:ff:ff link-netnsid 0',\n",
       " 'inet 10.102.66.200/24 brd 10.102.66.255 scope global dynamic eth0\\n\\nvalid_lft 3257sec preferred_lft 3257sec\\n\\ninet6 fe80::216:3eff:fee2:5242/64 scope link\\n\\nvalid_lft forever preferred_lft forever\\n\\nAnother application that can help identify all network interfaces available to your system is the lshw command. This\\ncommand provides greater details around the hardware capabilities of specific adapters. In the example below, lshw\\nshows a single Ethernet interface with the logical name of *eth4* along with bus information, driver details and all\\nsupported capabilities.\\n\\nsudo lshw -class network\\n\\n*-network\\n\\ndescription: Ethernet interface\\n\\nproduct: MT26448 [ConnectX EN 10GigE, PCIe 2.0 5GT/s]\\n\\nvendor: Mellanox Technologies\\n\\nphysical id: 0\\n\\nbus info: pci@0004:01:00.0\\n\\nlogical name: eth4\\n\\nversion: b0\\n\\nserial: e4:1d:2d:67:83:56\\n\\nslot: U78CB.001.WZS09KB-P1-C6-T1\\n\\nsize: 10Gbit/s\\n\\ncapacity: 10Gbit/s\\n\\nwidth: 64 bits\\n\\nclock: 33MHz\\n\\n314\\n\\n\\n-----\\n\\ncapabilities: pm vpd msix pciexpress bus_master cap_list ethernet physical fibre 10000bt-fd\\n\\nconfiguration: autonegotiation=off broadcast=yes driver=mlx4_en driverversion=4.0-0 duplex=full firmware=2.9.1326 i\\n\\nresources: iomemory:24000-23fff irq:481 memory:3fe200000000-3fe2000fffff memory:240000000000\\n240007ffffff\\n\\n**Ethernet Interface logical names**\\n\\nInterface logical names can also be configured via a Netplan configuration. If you would like control which interface\\nreceives a particular logical name use the match and set-name keys. The match key is used to find an adapter based on\\nsome criteria like MAC address, driver, etc. The set-name key can be used to change the device to the desired logical\\n\\nname.\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd',\n",
       " 'ethernets:\\n\\neth_lan0:\\n\\ndhcp4: true\\n\\nmatch:\\n\\nmacaddress: 00:11:22:33:44:55\\n\\nset-name: eth_lan0\\n\\n**Ethernet Interface settings**\\n\\nethtool is a program that displays and changes Ethernet card settings such as auto-negotiation, port speed, duplex\\nmode, and Wake-on-LAN. The following is an example of how to view the supported features and configured settings\\nof an Ethernet interface.\\n\\nsudo ethtool eth4\\n\\nSettings for eth4:\\n\\nSupported ports: [ FIBRE ]\\n\\nSupported link modes: 10000baseT/Full\\n\\nSupported pause frame use: No\\n\\nSupports auto-negotiation: No\\n\\nSupported FEC modes: Not reported\\n\\nAdvertised link modes: 10000baseT/Full\\n\\nAdvertised pause frame use: No\\n\\nAdvertised auto-negotiation: No\\n\\nAdvertised FEC modes: Not reported\\n\\nSpeed: 10000Mb/s\\n\\nDuplex: Full\\n\\nPort: FIBRE\\n\\nPHYAD: 0\\n\\nTransceiver: internal\\n\\nAuto-negotiation: off\\n\\nSupports Wake-on: d\\n\\nWake-on: d\\n\\nCurrent message level: 0x00000014 (20)\\n\\nlink ifdown\\n\\nLink detected: yes\\n## **IP addressing**\\n\\nThe following section describes the process of configuring your system’s IP address and default gateway needed for\\ncommunicating on a local area network and the Internet.\\n\\n**Temporary IP address assignment**\\n\\nFor temporary network configurations, you can use the ip command which is also found on most other GNU/Linux\\noperating systems. The ip command allows you to configure settings which take effect immediately – however they\\nare not persistent and will be lost after a reboot.\\n\\nTo temporarily configure an IP address, you can use the ip command in the following manner. Modify the IP address\\nand subnet mask to match your network requirements.\\n\\nsudo ip addr add 10.102.66.200/24 dev enp0s25\\n\\n315\\n\\n\\n-----',\n",
       " 'The ip can then be used to set the link up or down.\\n\\nip link set dev enp0s25 up\\n\\nip link set dev enp0s25 down\\n\\nTo verify the IP address configuration of enp0s25, you can use the ip command in the following manner:\\n\\nip address show dev enp0s25\\n\\n10: enp0s25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\\n\\nlink/ether 00:16:3e:e2:52:42 brd ff:ff:ff:ff:ff:ff link-netnsid 0\\n\\ninet 10.102.66.200/24 brd 10.102.66.255 scope global dynamic eth0\\n\\nvalid_lft 2857sec preferred_lft 2857sec\\n\\ninet6 fe80::216:3eff:fee2:5242/64 scope link\\n\\nvalid_lft forever preferred_lft forever6\\n\\nTo configure a default gateway, you can use the ip command in the following manner. Modify the default gateway\\naddress to match your network requirements.\\n\\nsudo ip route add default via 10.102.66.1\\n\\nYou can also use the ip command to verify your default gateway configuration, as follows:\\n\\nip route show\\n\\ndefault via 10.102.66.1 dev eth0 proto dhcp src 10.102.66.200 metric 100\\n\\n10.102.66.0/24 dev eth0 proto kernel scope link src 10.102.66.200\\n\\n10.102.66.1 dev eth0 proto dhcp scope link src 10.102.66.200 metric 100\\n\\nIf you require DNS for your temporary network configuration, you can add DNS server IP addresses in the file\\n\\n/etc/resolv.conf . In general, editing /etc/resolv.conf directly is not recommended, but this is a temporary and\\nnon-persistent configuration. The example below shows how to enter two DNS servers to /etc/resolv.conf, which\\nshould be changed to servers appropriate for your network. A more lengthy description of the proper (persistent) way\\nto do DNS client configuration is in a following section.\\n\\nnameserver 8.8.8.8\\n\\nnameserver 8.8.4.4',\n",
       " 'If you no longer need this configuration and wish to purge all IP configuration from an interface, you can use the ip\\ncommand with the flush option:\\n\\nip addr flush eth0\\n\\n**Note**\\nFlushing the IP configuration using the ip command does not clear the contents of /etc/resolv.conf . You\\nmust remove or modify those entries manually (or re-boot), which should also cause /etc/resolv.conf,\\nwhich is a symlink to /run/systemd/resolve/stub-resolv.conf, to be re-written.\\n\\n**Dynamic IP address assignment (DHCP client)**\\n\\nTo configure your server to use DHCP for dynamic address assignment, create a Netplan configuration in the file\\n\\n/etc/netplan/99_config.yaml . The following example assumes you are configuring your first Ethernet interface identified as enp3s0 .\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\nethernets:\\n\\nenp3s0:\\n\\ndhcp4: true\\n\\nThe configuration can then be applied using the netplan command:\\n\\nsudo netplan apply\\n\\n**Static IP address assignment**\\n\\nTo configure your system to use static address assignment, create a netplan configuration in the file /etc/netplan/99_config.yaml .\\nThe example below assumes you are configuring your first Ethernet interface identified as eth0 . Change the addresses,\\n\\nroutes, and nameservers values to meet the requirements of your network.\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\n316\\n\\n\\n-----\\n\\nethernets:\\n\\neth0:\\n\\naddresses:\\n\\n    - 10.10.10.2/24\\n\\nroutes:\\n\\n    - to: default\\n\\nvia: 10.10.10.1\\n\\nnameservers:\\n\\nsearch: [mydomain, otherdomain]\\n\\naddresses: [10.10.10.1, 1.1.1.1]\\n\\nThe configuration can then be applied using the netplan command.\\n\\nsudo netplan apply\\n\\n**NOTE**',\n",
       " 'netplan in Ubuntu Bionic 18.04 LTS doesn’t understand the “ to: default ” syntax to specify a default\\nroute, and should use the older gateway4: 10.10.10.1 key instead of the whole routes: block.\\n\\nThe loopback interface is identified by the system as lo and has a default IP address of 127.0.0.1. It can be viewed\\nusing the ip command.\\n\\nip address show lo\\n\\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\\n\\nlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\\n\\ninet 127.0.0.1/8 scope host lo\\n\\nvalid_lft forever preferred_lft forever\\n\\ninet6 ::1/128 scope host\\n\\nvalid_lft forever preferred_lft forever\\n## **Name resolution**\\n\\nName resolution (as it relates to IP networking) is the process of mapping hostnames to IP addresses, and vice-versa,\\nmaking it easier to identify resources on a network. The following section will explain how to properly configure your\\nsystem for name resolution using DNS and static hostname records.\\n\\n**DNS client configuration**\\n\\nTraditionally, the file /etc/resolv.conf was a static configuration file that rarely needed to be changed, or it automatically changed via DCHP client hooks. systemd-resolved handles nameserver configuration, and it should be interacted\\nwith through the systemd-resolve command. Netplan configures systemd-resolved to generate a list of nameservers\\nand domains to put in /etc/resolv.conf, which is a symlink:\\n\\n/etc/resolv.conf -> ../run/systemd/resolve/stub-resolv.conf\\n\\nTo configure the resolver, add the IP addresses of the appropriate nameservers for your network to the netplan\\nconfiguration file. You can also add optional DNS suffix search-lists to match your network domain names. The',\n",
       " 'resulting file might look like the following:\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\nethernets:\\n\\nenp0s25:\\n\\naddresses:\\n\\n    - 192.168.0.100/24\\n\\nroutes:\\n\\n    - to: default\\n\\nvia: 192.168.0.1\\n\\nnameservers:\\n\\nsearch: [mydomain, otherdomain]\\n\\naddresses: [1.1.1.1, 8.8.8.8, 4.4.4.4]\\n\\nThe *search* option can also be used with multiple domain names so that DNS queries will be appended in the order\\nin which they are entered. For example, your network may have multiple sub-domains to search; a parent domain of\\n\\nexample.com, and two sub-domains, sales.example.com and dev.example.com .\\n\\nIf you have multiple domains you wish to search, your configuration might look like the following:\\n\\n317\\n\\n\\n-----\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\nethernets:\\n\\nenp0s25:\\n\\naddresses:\\n\\n    - 192.168.0.100/24\\n\\nroutes:\\n\\n    - to: default\\n\\nvia: 192.168.0.1\\n\\nnameservers:\\n\\nsearch: [example.com, sales.example.com, dev.example.com]\\n\\naddresses: [1.1.1.1, 8.8.8.8, 4.4.4.4]\\n\\nIf you try to ping a host with the name server1, your system will automatically query DNS for its Fully Qualified\\nDomain Name (FQDN) in the following order:\\n\\n1. server1.example.com\\n\\n2. server1.sales.example.com\\n\\n3. server1.dev.example.com\\n\\nIf no matches are found, the DNS server will provide a result of *notfound* and the DNS query will fail.\\n\\n**Static hostnames**\\n\\nStatic hostnames are locally defined hostname-to-IP mappings located in the file /etc/hosts . Entries in the hosts file\\nwill have precedence over DNS by default. This means that if your system tries to resolve a hostname and it matches\\nan entry in /etc/hosts, it will not attempt to look up the record in DNS. In some configurations, especially when',\n",
       " 'Internet access is not required, servers that communicate with a limited number of resources can be conveniently set\\nto use static hostnames instead of DNS.\\n\\nThe following is an example of a hosts file where a number of local servers have been identified by simple hostnames,\\naliases and their equivalent Fully Qualified Domain Names (FQDN’s):\\n\\n127.0.0.1 localhost\\n\\n127.0.1.1 ubuntu-server\\n\\n10.0.0.11 server1 server1.example.com vpn\\n\\n10.0.0.12 server2 server2.example.com mail\\n\\n10.0.0.13 server3 server3.example.com www\\n\\n10.0.0.14 server4 server4.example.com file\\n\\n**Note**\\n\\nIn this example, notice that each of the servers were given aliases in addition to their proper names and\\nFQDN’s. *Server1* has been mapped to the name *vpn*, *server2* is referred to as *mail*, *server3* as *www*, and\\n*server4* as *file* .\\n\\n**Name Service Switch (NSS) configuration**\\n\\nThe order in which your system selects a method of resolving hostnames to IP addresses is controlled by the Name\\nService Switch (NSS) configuration file /etc/nsswitch.conf . As mentioned in the previous section, typically static\\nhostnames defined in the systems /etc/hosts file have precedence over names resolved from DNS. The following is an\\nexample of the line responsible for this order of hostname lookups in the file /etc/nsswitch.conf .\\n\\nhosts: files mdns4_minimal [NOTFOUND=return] dns mdns4\\n\\n - files first tries to resolve static hostnames located in /etc/hosts .\\n\\n - mdns4_minimal attempts to resolve the name using Multicast DNS.\\n\\n - [NOTFOUND=return] means that any response of notfound by the preceding mdns4_minimal process should be treated',\n",
       " 'as authoritative and that the system should not try to continue hunting for an answer.\\n\\n - dns represents a legacy unicast DNS query.\\n\\n - **mdns4** represents a multicast DNS query.\\n\\nTo modify the order of these name resolution methods, you can simply change the hosts: string to the value of your\\nchoosing. For example, if you prefer to use legacy unicast DNS versus multicast DNS, you can change the string in\\n\\n/etc/nsswitch.conf as shown below:\\n\\n318\\n\\n\\n-----\\n\\nhosts: files dns [NOTFOUND=return] mdns4_minimal mdns4\\n## **Bridging multiple interfaces**\\n\\nBridging is a more advanced configuration, but is very useful in multiple scenarios. One scenario is setting up a bridge\\nwith multiple network interfaces, then using a firewall to filter traffic between two network segments. Another scenario\\nis using bridge on a system with one interface to allow virtual machines direct access to the outside network. The\\nfollowing example covers the latter scenario:\\n\\nConfigure the bridge by editing your netplan configuration found in /etc/netplan/, entering the appropriate values\\nfor your physical interface and network:\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\nethernets:\\n\\nenp3s0:\\n\\ndhcp4: no\\n\\nbridges:\\n\\nbr0:\\n\\ndhcp4: yes\\n\\ninterfaces:\\n\\n    - enp3s0\\n\\nNow apply the configuration to enable the bridge:\\n\\nsudo netplan apply\\n\\nThe new bridge interface should now be up and running. The brctl provides useful information about the state of the\\nbridge, controls which interfaces are part of the bridge, etc. See man brctl for more information.\\n## **networkd-dispatcher for hook scripts**\\n\\nUsers of the former ifupdown may be familiar with using hook scripts (e.g., pre-up, post-up) in their interfaces file.',\n",
       " '[Netplan configuration does not currently support hook scripts in its configuration definition.](https://netplan.readthedocs.io/en/stable/netplan-yaml/)\\n\\nInstead, to achieve this functionality with the networkd [renderer, users can use networkd-dispatcher. The package](http://manpages.ubuntu.com/manpages/focal/man8/networkd-dispatcher.8.html)\\nprovides both users and packages with hook points when specific network states are reached, to aid in reacting to\\nnetwork state.\\n\\n**Note** :\\nIf you are on Desktop (not Ubuntu Server) the network is driven by Network Manager - in that case you\\n[need NM Dispatcher scripts instead.](https://developer.gnome.org/NetworkManager/unstable/NetworkManager.html)\\n\\n[The Netplan FAQ has a great table that compares event timings between](https://netplan.io/faq/#use-pre-up%2C-post-up%2C-etc.-hook-scripts) ifupdown / systemd-networkd / network-manager .\\n\\nIt is important to be aware that these hooks run asynchronously; i.e. they will not block transition into another state.\\n\\n[The Netplan FAQ also has an example on converting an old](https://netplan.io/faq/#example-for-an-ifupdown-legacy-hook-for-post-up%2Fpost-down-states) ifupdown hook to networkd-dispatcher .\\n## **Resources**\\n\\n[• The Ubuntu Wiki Network page has links to articles covering more advanced network configuration.](https://help.ubuntu.com/community/Network)\\n\\n[• The Netplan website has additional examples and documentation.](https://netplan.io)\\n\\n[• The Netplan man page has more information on Netplan.](https://manpages.ubuntu.com/manpages/focal/man5/netplan.5.html)',\n",
       " '[• The systemd-resolved man page has more information on systemd-resolved service.](https://manpages.ubuntu.com/manpages/focal/man8/systemd-resolved.8.html)\\n\\n  - For more information on *bridging* [see the netplan.io examples page](https://netplan.readthedocs.io/en/stable/netplan-yaml/#properties-for-device-type-bridges)\\n\\nThe Dynamic Host Configuration Protocol (DHCP) is a network service that enables host computers to be automatically assigned settings from a server as opposed to manually configuring each network host. Computers configured\\nto be DHCP clients have no control over the settings they receive from the DHCP server, and the configuration is\\ntransparent to the computer’s user.\\n\\nThe most common settings provided by a DHCP server to DHCP clients include:\\n\\n  - IP address and netmask\\n\\n  - IP address of the default-gateway to use\\n\\n319\\n\\n\\n-----\\n\\n  - IP addresses of the DNS servers to use\\n\\nHowever, a DHCP server can also supply configuration properties such as:\\n\\n - Hostname\\n\\n - Domain name\\n\\n  - Time server\\n\\n  - Print server\\n\\nThe advantage of using DHCP is that any changes to the network, such as a change in the DNS server address, only\\nneed to be changed at the DHCP server, and all network hosts will be reconfigured the next time their DHCP clients\\npoll the DHCP server. As an added advantage, it is also easier to integrate new computers into the network, as there\\nis no need to check for the availability of an IP address. Conflicts in IP address allocation are also reduced.\\n## **DHCP configuration**\\n\\nA DHCP server can provide configuration settings using the following methods:\\n\\n**Manual allocation (MAC address)**',\n",
       " 'This method uses DHCP to identify the unique hardware address of each network card connected to the network,\\nand then supplies a static configuration each time the DHCP client makes a request to the DHCP server using that\\nnetwork device. This ensures that a particular address is assigned automatically to that network card, based on its\\nMAC address.\\n\\n**Dynamic allocation (address pool)**\\n\\nIn this method, the DHCP server assigns an IP address from a pool of addresses (sometimes also called a range or\\nscope) for a period of time (known as a lease) configured on the server, or until the client informs the server that\\nit doesn’t need the address anymore. This way, the clients receive their configuration properties dynamically and\\non a “first come, first served” basis. When a DHCP client is no longer on the network for a specified period, the\\nconfiguration is expired and released back to the address pool for use by other DHCP clients. After the lease period\\nexpires, the client must renegotiate the lease with the server to maintain use of the same address.\\n\\n**Automatic allocation**\\n\\nUsing this method, the DHCP automatically assigns an IP address permanently to a device, selecting it from a pool\\nof available addresses. Usually, DHCP is used to assign a temporary address to a client, but a DHCP server can allow\\nan infinite lease time.\\n\\nThe last two methods can be considered “automatic” because in each case the DHCP server assigns an address with\\nno extra intervention needed. The only difference between them is in how long the IP address is leased; in other words,\\nwhether a client’s address varies over time.\\n## **Available servers**\\n\\nUbuntu makes two DHCP servers available:',\n",
       " ' - isc-dhcp-server :\\nThis server installs dhcpd, the dynamic host configuration protocol daemon. Although Ubuntu still supports\\n\\nisc-dhcp-server [, this software is no longer supported by its vendor.](https://www.isc.org/blogs/isc-dhcp-eol/)\\n\\nFind out how to install and configure isc-dhcp-server .\\n\\n - isc-kea :\\n\\n[Kea was created by ISC to replace](https://www.isc.org/kea/) isc-dhcp-server – It is supported in Ubuntu releases from 23.04 onwards.\\n\\nFind out how to install and configure isc-kea .\\n## **References**\\n\\n[• The isc-dhcp-server Ubuntu Wiki page has more information.](https://help.ubuntu.com/community/isc-dhcp-server)\\n\\n - For more /etc/dhcp/dhcpd.conf [options see the dhcpd.conf man page.](https://manpages.ubuntu.com/manpages/focal/en/man5/dhcpd.conf.5.html)\\n\\n[• ISC dhcp-server](https://www.isc.org/software/dhcp)\\n\\n[• ISC Kea Documentation](https://kb.isc.org/docs/kea-administrator-reference-manual)\\n\\n320\\n\\n\\n-----\\n\\nNetwork Time Protocol (NTP) is a networking protocol for synchronising time over a network. Basically, a client\\nrequests the current time from a server, and uses it to set its own clock.\\n\\nBehind this simple description, there is a lot of complexity. There are three tiers of NTP servers; tier one NTP servers\\nare connected to atomic clocks, while tier two and tier three three servers spread the load of actually handling requests\\nacross the Internet.\\n\\nThe client software is also a lot more complex than you might expect. It must factor in communication delays and\\nadjust the time in a way that does not upset all the other processes that run on the server. Luckily, all that complexity\\nis hidden from you!',\n",
       " 'By default, Ubuntu uses timedatectl / timesyncd to synchronise time, and they are available by default. See our guide\\nIf you would like to know how to configure timedatectl and timesyncd .\\n\\nUsers can also optionally use chrony to serve NTP.\\n## **How time synchronisation works**\\n\\nSince Ubuntu 16.04, timedatectl / timesyncd (which are part of systemd ) replace most of ntpdate / ntp .\\n\\n**About** timesyncd\\n\\ntimesyncd replaces not only ntpdate, but also the client portion of chrony (formerly ntpd ). So, on top of the one-shot\\naction that ntpdate provided on boot and network activation, timesyncd now regularly checks and keeps your local\\ntime in sync. It also stores time updates locally, so that after reboots the time monotonically advances (if applicable).\\n\\n**About** timedatectl\\n\\nIf chrony is installed, timedatectl steps back to let chrony handle timekeeping. This ensures that no two time-syncing\\nservices can conflict with each other.\\n\\nntpdate is now considered deprecated in favor of timedatectl (or chrony ) and is no longer installed by default. timesyncd\\nwill generally keep your time in sync, and chrony will help with more complex cases. But if you had one of a few\\nknown special ntpdate use cases, consider the following:\\n\\n  - If you require a one-shot sync, use: chronyd -q\\n\\n  - If you require a one-shot time check (without setting the time), use: chronyd -Q\\n\\nWhile use of ntpd is no longer recommended, this also still applies to ntpd being installed to retain any previous\\nbehaviour/config that you had through an upgrade. However, it also implies that on an upgrade from a former release,',\n",
       " 'ntp / ntpdate might still be installed and therefore renders the new systemd -based services disabled.\\n## **Further reading**\\n\\n[• ntp.org: home of the Network Time Protocol project](http://www.ntp.org/)\\n\\n[• pool.ntp.org: project of virtual cluster of timeservers](http://www.pool.ntp.org/)\\n\\n[• Freedesktop.org info on timedatectl](https://www.freedesktop.org/software/systemd/man/timedatectl.html)\\n\\n[• Freedesktop.org info on systemd-timesyncd service](https://www.freedesktop.org/software/systemd/man/systemd-timesyncd.service.html)\\n\\n[• Chrony FAQ](https://chrony.tuxfamily.org/faq.html)\\n\\n[• Feeding chrony from GPSD](https://gpsd.gitlab.io/gpsd/gpsd-time-service-howto.html#_feeding_chrony_from_gpsd)\\n\\n[• Also see the Ubuntu Time wiki page for more information.](https://help.ubuntu.com/community/UbuntuTime)\\n\\nThe Data Plane Development Kit (DPDK) is a set of libraries and drivers for fast packet processing, which runs\\nmostly in Linux userland. This set of libraries provides the so-called “Environment Abstraction Layer” (EAL). The\\nEAL hides the details of the environment and provides a standard programming interface. Common use cases are\\naround special solutions, such as network function virtualisation and advanced high-throughput network switching.\\n\\nThe DPDK uses a run-to-completion model for fast data plane performance and accesses devices via polling to eliminate\\nthe latency of interrupt processing, albeit with the tradeoff of higher CPU consumption. It was designed to run on\\nany processor. The first supported CPU was Intel x86 and it is now extended to IBM PPC64 and ARM64.\\n\\nUbuntu provides some additional infrastructure to increase DPDK’s usability.\\n\\n321\\n\\n\\n-----',\n",
       " '## **Prerequisites**\\n\\nThis package is currently compiled for the lowest possible CPU requirements allowed by upstream. Starting with\\n[DPDK 17.08, that means it requires at least SSE4_2 and for anything else activated by -march=corei7 (in GCC) to](https://git.dpdk.org/dpdk/commit/?id=f27769f796a0639368117ce22fb124b6030dbf73)\\nbe supported by the CPU.\\n\\n[The list of upstream DPDK-supported network cards can be found at supported NICs. However, a lot of those are](http://dpdk.org/doc/nics)\\ndisabled by default in the upstream project as they are not yet in a stable state. The subset of network cards that\\nDPDK has enabled in the package (as available in Ubuntu 16.04) is:\\n\\nDPDK has “userspace” drivers for the cards called PMDs.\\nThe packages for these follow the pattern of librte-pmd-<type>-<version> . Therefore the example for an Intel e1000\\nin 18.11 would be librte-pmd-e1000-18.11 .\\n\\nThe more commonly used, tested and fully supported drivers are installed as dependencies of dpdk . But there are\\n[many more “in-universe” that follow the same naming pattern.](https://help.ubuntu.com/community/Repositories/Ubuntu#The_Four_Main_Repositories)\\n## **Unassign the default kernel drivers**\\n\\nCards must be unassigned from their kernel driver and instead be assigned to uio_pci_generic of vfio-pci .\\n\\nuio_pci_generic is older and it’s (usually) easier to get it to work. However, it also has fewer features and less\\nisolation.\\n\\nThe newer VFIO-PCI requires that you activate the following kernel parameters to enable the input-output memory\\nmanagement unit (IOMMU):\\n\\niommu=pt intel_iommu=on\\n\\nAlternatively, on AMD:\\n\\namd_iommu=pt',\n",
       " 'On top of VFIO-PCI, you must also configure and assign the IOMMU groups accordingly. This is mostly\\ndone in firmware and by hardware layout – you can check the group assignment the kernel probed in\\n\\n/sys/kernel/iommu_groups/ .\\n\\n**Note** :\\nVirtIO is special. DPDK can directly work on these devices without vfio_pci / uio_pci_generic . However,\\nto avoid issues that might arise from the kernel and DPDK managing the device, you still need to unassign\\nthe kernel driver.\\n\\nManual configuration and status checks can be done via sysfs, or with the tool dpdk_nic_bind :\\n\\ndpdk_nic_bind.py --help\\n## **Usage**\\n\\ndpdk-devbind.py [options] DEVICE1 DEVICE2 ....\\n\\nwhere DEVICE1, DEVICE2 etc, are specified via PCI \"domain:bus:slot.func\" syntax\\n\\nor \"bus:slot.func\" syntax. For devices bound to Linux kernel drivers, they may\\n\\nalso be referred to by Linux interface name e.g. eth0, eth1, em0, em1, etc.\\n\\nOptions:\\n\\n--help, --usage:\\n\\nDisplay usage information and quit\\n\\n-s, --status:\\n\\nPrint the current status of all known network, crypto, event\\n\\nand mempool devices.\\n\\nFor each device, it displays the PCI domain, bus, slot and function,\\n\\nalong with a text description of the device. Depending upon whether the\\n\\ndevice is being used by a kernel driver, the igb_uio driver, or no\\n\\ndriver, other relevant information will be displayed:\\n\\n  - the Linux interface name e.g. if=eth0\\n\\n  - the driver being used e.g. drv=igb_uio\\n\\n  - any suitable drivers not currently using that device\\n\\ne.g. unused=igb_uio\\n\\n322\\n\\n\\n-----\\n\\nNOTE: if this flag is passed along with a bind/unbind option, the\\n\\nstatus display will always occur after the other operations have taken\\n\\nplace.\\n\\n--status-dev:\\n\\nPrint the status of given device group.',\n",
       " 'Supported device groups are:\\n\\n\"net\", \"crypto\", \"event\", \"mempool\" and \"compress\"\\n\\n-b driver, --bind=driver:\\n\\nSelect the driver to use or \"none\" to unbind the device\\n\\n-u, --unbind:\\n\\nUnbind a device (Equivalent to \"-b none\")\\n\\n--force:\\n\\nBy default, network devices which are used by Linux - as indicated by\\n\\nhaving routes in the routing table - cannot be modified. Using the\\n\\n--force flag overrides this behavior, allowing active links to be\\n\\nforcibly unbound.\\n\\nWARNING: This can lead to loss of network connection and should be used\\n\\nwith caution.\\n\\nExamples:\\n\\n--------\\nTo display current device status:\\n\\ndpdk-devbind.py --status\\n\\nTo display current network device status:\\n\\ndpdk-devbind.py --status-dev net\\n\\nTo bind eth1 from the current driver and move to use igb_uio\\n\\ndpdk-devbind.py --bind=igb_uio eth1\\n\\nTo unbind 0000:01:00.0 from using any driver\\n\\ndpdk-devbind.py -u 0000:01:00.0\\n\\nTo bind 0000:02:00.0 and 0000:02:00.1 to the ixgbe kernel driver\\n\\ndpdk-devbind.py -b ixgbe 02:00.0 02:00.1\\n## **DPDK device configuration**\\n\\nThe package dpdk provides *init* scripts that ease configuration of device assignment and huge pages. It also makes\\nthem persistent across reboots.\\n\\nThe following is an example of the file /etc/dpdk/interfaces configuring two ports of a network card: one with\\n\\nuio_pci_generic and the other with vfio-pci .\\n\\n# <bus> Currently only \"pci\" is supported\\n\\n# <id> Device ID on the specified bus\\n\\n# <driver> Driver to bind against (vfio-pci or uio_pci_generic)\\n\\n#\\n\\n# Be aware that the two DPDK compatible drivers uio_pci_generic and vfio-pci are\\n\\n# part of linux-image-extra-<VERSION> package.\\n\\n# This package is not always installed by default - for example in cloud-images.',\n",
       " \"# So please install it in case you run into missing module issues.\\n\\n#\\n\\n# <bus> <id> <driver>\\n\\npci 0000:04:00.0 uio_pci_generic\\n\\npci 0000:04:00.1 vfio-pci\\n\\nCards are identified by their PCI-ID. If you are need to check, you can use the tool dpdk_nic_bind.py to show the\\ncurrently available devices – and the drivers they are assigned to. For example, running the command dpdk_nic_bind.py\\n\\n--status provides the following details:\\n\\n323\\n\\n\\n-----\\n\\nNetwork devices using DPDK-compatible driver\\n\\n============================================\\n\\n0000:04:00.0 'Ethernet Controller 10-Gigabit X540-AT2' drv=uio_pci_generic unused=ixgbe\\n\\nNetwork devices using kernel driver\\n\\n===================================\\n\\n0000:02:00.0 'NetXtreme BCM5719 Gigabit Ethernet PCIe' if=eth0 drv=tg3 unused=uio_pci_generic *Active*\\n\\n0000:02:00.1 'NetXtreme BCM5719 Gigabit Ethernet PCIe' if=eth1 drv=tg3 unused=uio_pci_generic\\n\\n0000:02:00.2 'NetXtreme BCM5719 Gigabit Ethernet PCIe' if=eth2 drv=tg3 unused=uio_pci_generic\\n\\n0000:02:00.3 'NetXtreme BCM5719 Gigabit Ethernet PCIe' if=eth3 drv=tg3 unused=uio_pci_generic\\n\\n0000:04:00.1 'Ethernet Controller 10-Gigabit X540-AT2' if=eth5 drv=ixgbe unused=uio_pci_generic\\n\\nOther network devices\\n\\n=====================\\n\\n<none>\\n## **DPDK hugepage configuration**\\n\\nDPDK makes heavy use of hugepages to eliminate pressure on the translation lookaside buffer (TLB). Therefore,\\nhugepages need to be configured in your system. The dpdk package has a config file and scripts that try to ease\\nhugepage configuration for DPDK in the form of /etc/dpdk/dpdk.conf .\\n\\nIf you have more consumers of hugepages than just DPDK in your system – or very special requirements for how your\",\n",
       " 'hugepages will be set up – you likely want to allocate/control them yourself. If not, this can be a great simplification\\nto get DPDK configured for your needs.\\n\\nAs an example, we can specify a configuration of 1024 hugepages of 2M each and four 1G pages in /etc/dpdk/dpdk.conf\\nby adding:\\n\\nNR_2M_PAGES=1024\\n\\nNR_1G_PAGES=4\\n\\nThis supports configuring 2M and the larger 1G hugepages (or a mix of both). It will make sure there are proper\\n\\nhugetlbfs mountpoints for DPDK to find both sizes – no matter what size your default hugepage is. The config file\\nitself holds more details on certain corner cases and a few hints if you want to allocate hugepages manually via a kernel\\nparameter.\\n\\nThe size you want depends on your needs: 1G pages are certainly more effective regarding TLB pressure, but there\\nhave been reports of them fragmenting inside the DPDK memory allocations. Also, it can be harder to find enough\\nfree space to set up a certain number of 1G pages later in the life-cycle of a system.\\n## **Compile DPDK applications**\\n\\nCurrently, there are not many consumers of the DPDK library that are stable and released. Open vSwitch DPDK is\\nan exception to that (see below) and more are appearing, but in general it may be that you will want to compile an\\napp against the library.\\n\\nYou will often find guides that tell you to fetch the DPDK sources, build them to your needs and eventually build your\\napplication based on DPDK by setting values RTE_* for the build system. Since Ubuntu provides an already-compiled\\nDPDK for you can can skip all that.',\n",
       " '[DPDK provides a valid pkg-config file to simplify setting the proper variables and options:](https://people.freedesktop.org/~dbn/pkg-config-guide.html)\\n\\nsudo apt-get install dpdk-dev libdpdk-dev\\n\\ngcc testdpdkprog.c $(pkg-config --libs --cflags libdpdk) -o testdpdkprog\\n\\nAn example of a complex (auto-configure) user of pkg-config of DPDK including fallbacks to older non pkg-config style\\n[can be seen in the Open vSwitch build system.](https://github.com/openvswitch/ovs/blob/master/acinclude.m4#L283)\\n\\nDepending on what you are building, it may be a good idea to install all DPDK build dependencies before the make.\\nOn Ubuntu, this can be done automatically with the following command:\\n\\nsudo apt-get install build-dep dpdk\\n## **DPDK in KVM guests**\\n\\nEven if you have no access to DPDK-supported network cards, you can still work with DPDK by using its support\\nfor VirtIO. To do so, you must create guests backed by hugepages (see above). In addition, you will also need to have\\n*at least* Streaming SIMD Extensions 3 (SSE3).\\n\\n324\\n\\n\\n-----\\n\\nThe default CPU model used by QEMU/libvirt is only up to SSE2. So, you will need to define a model that passes\\nthe proper feature flags (or use host-passthrough ). As an example, you can add the following snippet to your virsh\\nXML (or the equivalent virsh interface you use).\\n\\n<cpu mode=\\'host-passthrough\\'>\\n\\nNowadays, VirtIO supports multi-queue, which DPDK in turn can exploit for increased speed. To modify a normal\\nVirtIO definition to have multiple queues, add the following snippet to your interface definition.\\n\\n<driver name=\"vhost\" queues=\"4\"/>',\n",
       " 'This will enhance a normal VirtIO NIC to have multiple queues, which can later be consumed by e.g., DPDK in the\\nguest.\\n## **Use DPDK**\\n\\nSince DPDK itself is only a (massive) library, you most likely will continue to Open vSwitch DPDK as an example to\\nput it to use.\\n## **Resources**\\n\\n[• DPDK documentation](http://dpdk.org/doc)\\n\\n[• Release Notes matching the version packages in Ubuntu 16.04](http://dpdk.org/doc/guides/rel_notes/release_2_2.html)\\n\\n[• Linux DPDK user getting started](http://dpdk.org/doc/guides/linux_gsg/index.html)\\n\\n[• EAL command-line options](http://dpdk.org/doc/guides/testpmd_app_ug/run_app.html)\\n\\n[• DPDK API documentation](http://dpdk.org/doc/api/)\\n\\n[• Open vSwitch DPDK installation](https://github.com/openvswitch/ovs/blob/branch-2.5/INSTALL.DPDK.md)\\n\\n[• Wikipedia definition of DPDK](https://en.wikipedia.org/wiki/Data_Plane_Development_Kit)\\n\\nThe cryptographic library landscape is vast and complex, and there are many crypto libraries available on an Ubuntu\\nsystem. What an application developer decides to use can be governed by many aspects, such as:\\n\\n  - Technical requirements\\n\\n  - Language bindings\\n\\n  - License\\n\\n - Community\\n\\n  - Ease of use\\n\\n  - General availability\\n\\n - Upstream maintenance\\n\\nAmong the most popular and widely used libraries and frameworks, we have:\\n\\n - OpenSSL\\n\\n - GnuTLS\\n\\n - NSS\\n\\n - GnuPG\\n\\n - gcrypt\\n\\nEach one of these has its own implementation details, API, behavior, configuration file, and syntax.\\n\\nThis poses a challenge to system administrators who need to make sure what cryptographic algorithms are being used\\non the systems they deploy. How to ensure no legacy crypto is being used? Or that no keys below a certain size are',\n",
       " 'ever selected or created? And which types of X509 certificates are acceptable for connecting to remote servers?\\n\\nOne has to check all of the crypto implementations installed on the system and their configuration. To make things\\neven more complicated, sometimes an application implements its own crypto, without using anything external.\\n# **How do we know which library an application is using?**\\n\\nUltimately, the only reliable way to determine how an application uses cryptography is via its documentation or\\ninspection of source code. But the code is not always available, and sometimes the documentation lacks this information.\\nWhen the documentation isn’t clear or enough, there are some other practical checks that can be made.\\n\\nFirst, let’s take a look at how an application could use crypto:\\n\\n325\\n\\n\\n-----\\n\\n - **dynamic linking** : This is the most common way, and very easy to spot via package dependencies and helper\\ntools. This is discussed later in this page.\\n\\n - **static linking** : This is harder, as there is no dependency information in the binary package, and this usually\\nrequires inspection of the source package to see Build Dependencies. An example is shown later in this page.\\n\\n - **plugins** : The main binary of an application can not depend directly on a crypto library, but it could load\\ndynamic plugins which do. Usually these would be packaged separately, and then we fall under the dynamic\\nor static linking cases above. Note that via such a plugin mechanism, an application could depend on multiple\\nexternal cryptographic libraries.\\n\\n - **execution of external binary** : The application could just plain call external binaries at runtime for its',\n",
       " 'cryptographic operations, like calling out to openssl or gnupg to encrypt/decrypt data. This will hopefully be\\nexpressed in the dependencies of the package. If it’s not, then it’s a bug that should be reported.\\n\\n - **indirect usage** : The application could be using a third party library or executable which in turn could fall into\\nany of the above categories.\\n# **Identify the crypto libraries used by an application**\\n\\nHere are some tips that can help identifying the crypto libraries used by an application that is installed on an Ubuntu\\nsystem:\\n## **Documentation**\\n\\nRead the application documentation. It might have crypto options directly in its own configuration files, or point at\\nspecific crypto configuration files installed on the system. This may also clarify if the application even uses external\\ncrypto libraries, or if it has its own implementation.\\n## **Package dependencies**\\n\\nThe package dependencies are a good way to check what is needed at runtime by the application.\\n\\nTo find out the package that owns a file, use dpkg -S . For example:\\n\\n$ dpkg -S /usr/bin/lynx\\n\\nlynx: /usr/bin/lynx\\n\\nThen, with the package name in hand, check its dependencies. It’s best to also look for Recommends, as they are\\ninstalled by default. Continuing with the example from before, we have:\\n\\n$ dpkg -s lynx | grep -E \"^(Depends|Recommends)\"\\n\\nDepends: libbsd0 (>= 0.0), libbz2-1.0, libc6 (>= 2.34), libgnutls30 (>= 3.7.0), libidn2-0 (>= 2.0.0), libncursesw6 (>= 6),\\n\\ncommon\\n\\nRecommends: mime-support\\n\\nHere we see that lynx links with libgnutls30, which answers our question: lynx uses the GnuTLS library for its\\ncryptography operations.\\n## **Dynamic linking, plugins**',\n",
       " 'The dynamic libraries that are needed by an application should always be correctly identified in the list of dependencies\\nof the application package. When that is not the case, or if you need to identify what is needed by some plugin that\\nis not part of the package, you can use some system tools to help identify the dependencies.\\n\\nA very helpful tool that is installed in all Ubuntu systems is ldd . It will list all the dynamic libraries that are needed\\nby the given binary, including dependencies of dependencies, i.e., it’s recursive. Going back to the lynx example:\\n\\n$ ldd /usr/bin/lynx\\n\\nlinux-vdso.so.1 (0x00007ffffd2df000)\\n\\nlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007feb69d77000)\\n\\nlibbz2.so.1.0 => /lib/x86_64-linux-gnu/libbz2.so.1.0 (0x00007feb69d64000)\\n\\nlibidn2.so.0 => /lib/x86_64-linux-gnu/libidn2.so.0 (0x00007feb69d43000)\\n\\nlibncursesw.so.6 => /lib/x86_64-linux-gnu/libncursesw.so.6 (0x00007feb69d07000)\\n\\nlibtinfo.so.6 => /lib/x86_64-linux-gnu/libtinfo.so.6 (0x00007feb69cd5000)\\n\\nlibgnutls.so.30 => /lib/x86_64-linux-gnu/libgnutls.so.30 (0x00007feb69aea000)\\n\\nlibbsd.so.0 => /lib/x86_64-linux-gnu/libbsd.so.0 (0x00007feb69ad0000)\\n\\nlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb698a8000)\\n\\nlibunistring.so.2 => /lib/x86_64-linux-gnu/libunistring.so.2 (0x00007feb696fe000)\\n\\nlibp11-kit.so.0 => /lib/x86_64-linux-gnu/libp11-kit.so.0 (0x00007feb695c3000)\\n\\n326\\n\\n\\n-----\\n\\nlibtasn1.so.6 => /lib/x86_64-linux-gnu/libtasn1.so.6 (0x00007feb695ab000)\\n\\nlibnettle.so.8 => /lib/x86_64-linux-gnu/libnettle.so.8 (0x00007feb69565000)\\n\\nlibhogweed.so.6 => /lib/x86_64-linux-gnu/libhogweed.so.6 (0x00007feb6951b000)\\n\\nlibgmp.so.10 => /lib/x86_64-linux-gnu/libgmp.so.10 (0x00007feb69499000)',\n",
       " '/lib64/ld-linux-x86-64.so.2 (0x00007feb69fe6000)\\n\\nlibmd.so.0 => /lib/x86_64-linux-gnu/libmd.so.0 (0x00007feb6948c000)\\n\\nlibffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007feb6947f000)\\n\\nWe again see the GnuTLS library (via libgnutls.so.30 ) in the list, and can reach the same conclusion.\\n\\nAnother way to check for such dependencies, but without the recursion, is via objdump . This may need to be installed\\nvia the binutils package, as it’s not mandatory.\\n\\nThe way to use it is to grep for the NEEDED string:\\n\\n$ objdump -x /usr/bin/lynx|grep NEEDED\\n\\nNEEDED libz.so.1\\n\\nNEEDED libbz2.so.1.0\\n\\nNEEDED libidn2.so.0\\n\\nNEEDED libncursesw.so.6\\n\\nNEEDED libtinfo.so.6\\n\\nNEEDED libgnutls.so.30\\n\\nNEEDED libbsd.so.0\\n\\nNEEDED libc.so.6\\n\\nFinally, if you want to see the dependency *tree*, you can use lddtree from the pax-utils package:\\n\\n$ lddtree /usr/bin/lynx\\n\\nlynx => /usr/bin/lynx (interpreter => /lib64/ld-linux-x86-64.so.2)\\n\\nlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1\\n\\nlibbz2.so.1.0 => /lib/x86_64-linux-gnu/libbz2.so.1.0\\n\\nlibidn2.so.0 => /lib/x86_64-linux-gnu/libidn2.so.0\\n\\nlibunistring.so.2 => /lib/x86_64-linux-gnu/libunistring.so.2\\n\\nlibncursesw.so.6 => /lib/x86_64-linux-gnu/libncursesw.so.6\\n\\nlibtinfo.so.6 => /lib/x86_64-linux-gnu/libtinfo.so.6\\n\\nlibgnutls.so.30 => /lib/x86_64-linux-gnu/libgnutls.so.30\\n\\nlibp11-kit.so.0 => /lib/x86_64-linux-gnu/libp11-kit.so.0\\n\\nlibffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8\\n\\nlibtasn1.so.6 => /lib/x86_64-linux-gnu/libtasn1.so.6\\n\\nlibnettle.so.8 => /lib/x86_64-linux-gnu/libnettle.so.8\\n\\nlibhogweed.so.6 => /lib/x86_64-linux-gnu/libhogweed.so.6\\n\\nlibgmp.so.10 => /lib/x86_64-linux-gnu/libgmp.so.10\\n\\nld-linux-x86-64.so.2 => /lib64/ld-linux-x86-64.so.2',\n",
       " 'libbsd.so.0 => /lib/x86_64-linux-gnu/libbsd.so.0\\n\\nlibmd.so.0 => /lib/x86_64-linux-gnu/libmd.so.0\\n\\nlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6\\n## **Static linking**\\n\\nIdentifying which libraries were used in a static build is a bit more involved. There are two ways, and they are\\ncomplementary most of the time:\\n\\n  - look for the Built-Using header in the binary package\\n\\n  - inspect the Build-Depends header in the source package\\n\\nFor example, let’s try to discover which crypto libraries, if any, the rclone tool uses. First, let’s try the packaging\\ndependencies:\\n\\n$ dpkg -s rclone | grep -E \"^(Depends|Recommends)\"\\n\\nDepends: libc6 (>= 2.34)\\n\\nUh, that’s a short list. But rclone definitely supports encryption, so what is going on? Turns out this is a tool written\\nin the Go language, and that uses static linking of libraries. So let’s try to inspect the package data more carefully,\\nand this time look for the Built-Using header:\\n\\n$ dpkg -s rclone | grep Built-Using\\n\\nBuilt-Using: go-md2man-v2 (= 2.0.1+ds1-1), golang-1.18 (= 1.18-1ubuntu1), golang-bazil-fuse (= 0.0~git20160811.0.371fbbd3), …\\n\\n327\\n\\n\\n-----\\n\\nOk, this time we have a lot of information (truncated above for brevity, since it’s all in one very long line). If we\\nlook at the full output carefully, we can see that rclone was built statically using the golang-go.crypto package, and\\ndocumentation about that package and its crypto implementations is what we should look for.\\n\\nIf the Built-Using header was not there, or didn’t yield any clues, we could try one more step and look for the\\nbuild dependencies. These can be found in the debian/control file of the source package. In the case of rclone',\n",
       " '[for Ubuntu Jammy, that can be seen at https://git.launchpad.net/ubuntu/+source/rclone/tree/debian/control?h=](https://git.launchpad.net/ubuntu/+source/rclone/tree/debian/control?h=ubuntu/jammy-devel#n7)\\n[ubuntu/jammy-devel#n7, and a quick look at the](https://git.launchpad.net/ubuntu/+source/rclone/tree/debian/control?h=ubuntu/jammy-devel#n7) Build-Depends list shows us the golang-golang-x-crypto-dev build\\ndependency, whose source package is golang-go.crypto as expected:\\n\\n$ apt-cache show golang-golang-x-crypto-dev | grep ^Source:\\n\\nSource: golang-go.crypto\\n\\n**NOTE**\\n\\nIf there is no Source: line, then it means the name of the source package is the same as the binary package\\nthat was queried.\\n## **What’s next?**\\n\\nNow that you have uncovered which library your application is using, the following guides will help you to understand\\nthe associated configuration files and what options you have available (including some handy examples!).\\n\\n - OpenSSL guide\\n\\n - GnuTLS guide\\n\\n Troubleshooting TLS/SSL\\n\\nOpenSSL is probably the most well known cryptographic library, used by thousands of projects and applications.\\n\\nThe OpenSSL configuration file is located at /etc/ssl/openssl.cnf and is used both by the library itself and the\\ncommand-line tools included in the package. It is simple in structure, but quite complex in the details, and it won’t\\nbe fully covered here. In particular, we will only cover the settings that control which cryptographic algorithms will\\nbe allowed by default.\\n## **Structure of the config file**\\n\\nThe OpenSSL configuration file is very similar to a standard INI file. It starts with a nameless default section, not',\n",
       " 'inside any *[section]* block, and after that we have the traditional *[section-name]* followed by the *key = value* lines. The\\n[ssl config manpage has all the details.](https://manpages.ubuntu.com/manpages/jammy/en/man5/config.5ssl.html)\\n\\nThis is what it looks like:\\n\\nopenssl_conf = <name-of-conf-section>\\n\\n[name-of-conf-section]\\n\\nssl_conf = <name-of-ssl-section>\\n\\n[name-of-ssl-section]\\n\\nserver = <name of section>\\n\\nclient = <name of section>\\n\\nsystem_default = <name of section>\\n\\nSee how it’s like a chain, where a key ( *openssl_conf* ) points at the name of a section, and that section has a key that\\npoints to another section, and so on.\\n\\nTo adjust the algorithms and ciphers used in a SSL/TLS connection, we are interested in the “SSL Configuration”\\nsection of the library, where we can define the behavior of server, client, and the library defaults.\\n\\nFor example, in an Ubuntu Jammy installation, we have (omitting unrelated entries for brevity):\\n\\nopenssl_conf = openssl_init\\n\\n[openssl_init]\\n\\nssl_conf = ssl_sect\\n\\n[ssl_sect]\\n\\nsystem_default = system_default_sect\\n\\n[system_default_sect]\\n\\nCipherString = DEFAULT:@SECLEVEL=2\\n\\n328\\n\\n\\n-----\\n\\nThis gives us our first information about the default set of ciphers and algorithms used by OpenSSL in an Ubuntu\\ninstallation: DEFAULT:@SECLEVEL=2 [. What that means is detailed inside the SSL_CTX_set_security_level(3) manpage.](https://manpages.ubuntu.com/manpages/jammy/en/man3/SSL_CTX_set_security_level.3ssl.html#default%20callback%20behaviour)\\n\\n**NOTE**\\n\\nIn Ubuntu Jammy, TLS versions below 1.2 are **disabled** in OpenSSL’s SECLEVEL=2 [due to this patch.](https://git.launchpad.net/ubuntu/+source/openssl/tree/debian/patches/tls1.2-min-seclevel2.patch?',\n",
       " 'h=ubuntu/jammy-devel)\\n\\nThat default is also set at package building time, and in the case of Ubuntu, it’s set to [SECLEVEL=2](https://git.launchpad.net/ubuntu/+source/openssl/tree/debian/rules?h=ubuntu/jammy-devel#n15) .\\n\\nThe list of allowed ciphers in a security level can be obtained with the [openssl ciphers](https://www.openssl.org/docs/man3.0/man1/openssl-ciphers.html) command (output truncated\\nfor brevity):\\n\\n$ openssl ciphers -s -v DEFAULT:@SECLEVEL=2\\n\\nTLS_AES_256_GCM_SHA384 TLSv1.3 Kx=any Au=any Enc=AESGCM(256) Mac=AEAD\\n\\nTLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any Au=any Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nTLS_AES_128_GCM_SHA256 TLSv1.3 Kx=any Au=any Enc=AESGCM(128) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESGCM(256) Mac=AEAD\\n\\n(...)\\n\\n**NOTE**\\n\\nThe openssl ciphers command will output even ciphers that are not allowed, unless the -s switch is given.\\nThat option tells the command to list only *supported* ciphers.\\n\\nAll the options that can be set in the system_default_sect [section are detailed in the SSL_CONF_cmd manpage.](https://manpages.ubuntu.com/manpages/jammy/en/man3/SSL_CONF_cmd.3ssl.html#supported%20configuration%20file%20commands)\\n## **Cipher strings, cipher suites, cipher lists**\\n\\nEncrypting data (or signing it) is not a one step process. The whole transformation applied to the source data until\\nit is in its encrypted form has several stages, and each stage typically uses a different cryptographic algorithm. The\\ncombination of these algorithms is called a cipher suite.\\n\\nSimilar to GnuTLS, OpenSSL also uses the concept of cipher strings to group several algorithms and cipher suites\\ntogether.',\n",
       " 'The full list of cipher strings is shown in the [openssl ciphers](https://www.openssl.org/docs/man3.0/man1/openssl-ciphers.html) manpage.\\n\\nOpenSSL distinguishes the ciphers used with TLSv1.3, and those used with TLSv1.2 and older. Specifically for the\\n\\nopenssl ciphers command, we have:\\n\\n - -ciphersuites [: used for the TLSv1.3 ciphersuites. There are so far just five listed in the upstream documentation,](https://www.openssl.org/docs/man3.0/man1/openssl-ciphers.html#TLS-v1.3-cipher-suites)\\nand the defaults are:\\n\\nTLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_128_GCM_SHA256\\n\\n - *cipherlist* : this is a plain argument in the command line of the openssl ciphers command, without a specific\\nparameter, and is expected to be a list of cipher strings used in TLSv1.2 and lower. The default in Ubuntu\\nJammy 22.04 LTS is DEFAULT:@SECLEVEL=2 .\\n\\nThese defaults are built-in in the library, and can be set in /etc/ssl/openssl.cnf via the corresponding configuration\\nkeys CipherString for TLSv1.2 and older, and CipherSuites for TLSv1.3. For example:\\n\\n[system_default_sect]\\n\\nCipherString = DEFAULT:@SECLEVEL=2\\n\\nCipherSuites = TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256\\n\\nIn the end, without other constraints, the library will merge both lists into one set of supported crypto algorithms. If\\nthe crypto negotiation in a connection settles on TLSv1.3, then the list of *CipherSuites* is considered. If it’s TLSv1.2\\nor lower, then *CipherString* is used.\\n\\nopenssl ciphers **examples**\\n\\nThis will list all supported/enabled ciphers, with defaults taken from the library and /etc/ssl/openssl.cnf . Since no',\n",
       " 'other options were given, this will include TLSv1.3 ciphersuites and TLSv1.2 and older cipher strings:\\n\\n$ openssl ciphers -s -v\\n\\nTLS_AES_256_GCM_SHA384 TLSv1.3 Kx=any Au=any Enc=AESGCM(256) Mac=AEAD\\n\\nTLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any Au=any Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nTLS_AES_128_GCM_SHA256 TLSv1.3 Kx=any Au=any Enc=AESGCM(128) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESGCM(256) Mac=AEAD\\n\\nECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=DH Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nECDHE-ECDSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=ECDSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nECDHE-RSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=RSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nDHE-RSA-CHACHA20-POLY1305 TLSv1.2 Kx=DH Au=RSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\n329\\n\\n\\n-----\\n\\nECDHE-ECDSA-AES128-GCM-SHA256 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESGCM(128) Mac=AEAD\\n\\nECDHE-RSA-AES128-GCM-SHA256 TLSv1.2 Kx=ECDH Au=RSA Enc=AESGCM(128) Mac=AEAD\\n\\nDHE-RSA-AES128-GCM-SHA256 TLSv1.2 Kx=DH Au=RSA Enc=AESGCM(128) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AES(256) Mac=SHA384\\n\\nECDHE-RSA-AES256-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AES(256) Mac=SHA384\\n\\nDHE-RSA-AES256-SHA256 TLSv1.2 Kx=DH Au=RSA Enc=AES(256) Mac=SHA256\\n\\nECDHE-ECDSA-AES128-SHA256 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AES(128) Mac=SHA256\\n\\nECDHE-RSA-AES128-SHA256 TLSv1.2 Kx=ECDH Au=RSA Enc=AES(128) Mac=SHA256\\n\\nDHE-RSA-AES128-SHA256 TLSv1.2 Kx=DH Au=RSA Enc=AES(128) Mac=SHA256\\n\\nECDHE-ECDSA-AES256-SHA TLSv1 Kx=ECDH Au=ECDSA Enc=AES(256) Mac=SHA1\\n\\nECDHE-RSA-AES256-SHA TLSv1 Kx=ECDH Au=RSA Enc=AES(256) Mac=SHA1',\n",
       " \"DHE-RSA-AES256-SHA SSLv3 Kx=DH Au=RSA Enc=AES(256) Mac=SHA1\\n\\nECDHE-ECDSA-AES128-SHA TLSv1 Kx=ECDH Au=ECDSA Enc=AES(128) Mac=SHA1\\n\\nECDHE-RSA-AES128-SHA TLSv1 Kx=ECDH Au=RSA Enc=AES(128) Mac=SHA1\\n\\nDHE-RSA-AES128-SHA SSLv3 Kx=DH Au=RSA Enc=AES(128) Mac=SHA1\\n\\nAES256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nAES128-GCM-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(128) Mac=AEAD\\n\\nAES256-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA256\\n\\nAES128-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(128) Mac=SHA256\\n\\nAES256-SHA SSLv3 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA1\\n\\nAES128-SHA SSLv3 Kx=RSA Au=RSA Enc=AES(128) Mac=SHA1\\n\\nLet’s filter this a bit, and just as an example, remove all AES128 ciphers and SHA1 hashes:\\n\\n$ openssl ciphers -s -v 'DEFAULTS:-AES128:-SHA1'\\n\\nTLS_AES_256_GCM_SHA384 TLSv1.3 Kx=any Au=any Enc=AESGCM(256) Mac=AEAD\\n\\nTLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any Au=any Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nTLS_AES_128_GCM_SHA256 TLSv1.3 Kx=any Au=any Enc=AESGCM(128) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESGCM(256) Mac=AEAD\\n\\nECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=DH Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nECDHE-ECDSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=ECDSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nECDHE-RSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=RSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nDHE-RSA-CHACHA20-POLY1305 TLSv1.2 Kx=DH Au=RSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AES(256) Mac=SHA384\\n\\nECDHE-RSA-AES256-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AES(256) Mac=SHA384\\n\\nDHE-RSA-AES256-SHA256 TLSv1.2 Kx=DH Au=RSA Enc=AES(256) Mac=SHA256\",\n",
       " \"AES256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nAES256-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA256\\n\\nSince we didn’t use -ciphersuites, the TLSv1.3 list was unaffected by our filtering, and still contains the *AES128*\\ncipher. But TLSv1.2 and older no longer have *AES128* or *SHA1* . This type of filtering with ‘ + ’, ‘ - ’ and ‘ ! ’ can be\\ndone with the TLSv1.2 and older protocols and is detailed in the [openssl ciphers](https://www.openssl.org/docs/man3.0/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT) manpage.\\n\\nTo filter out TLSv1.3 algorithms, there is no such mechanism, and we must just list explicitly what we want by using\\n\\n-ciphersuites :\\n\\n$ openssl ciphers -s -v -ciphersuites TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256 'DEFAULTS:-AES128:\\nSHA1'\\n\\nTLS_AES_256_GCM_SHA384 TLSv1.3 Kx=any Au=any Enc=AESGCM(256) Mac=AEAD\\n\\nTLS_CHACHA20_POLY1305_SHA256 TLSv1.3 Kx=any Au=any Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AESGCM(256) Mac=AEAD\\n\\nECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=DH Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nECDHE-ECDSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=ECDSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nECDHE-RSA-CHACHA20-POLY1305 TLSv1.2 Kx=ECDH Au=RSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nDHE-RSA-CHACHA20-POLY1305 TLSv1.2 Kx=DH Au=RSA Enc=CHACHA20/POLY1305(256) Mac=AEAD\\n\\nECDHE-ECDSA-AES256-SHA384 TLSv1.2 Kx=ECDH Au=ECDSA Enc=AES(256) Mac=SHA384\\n\\nECDHE-RSA-AES256-SHA384 TLSv1.2 Kx=ECDH Au=RSA Enc=AES(256) Mac=SHA384\\n\\nDHE-RSA-AES256-SHA256 TLSv1.2 Kx=DH Au=RSA Enc=AES(256) Mac=SHA256\",\n",
       " 'AES256-GCM-SHA384 TLSv1.2 Kx=RSA Au=RSA Enc=AESGCM(256) Mac=AEAD\\n\\nAES256-SHA256 TLSv1.2 Kx=RSA Au=RSA Enc=AES(256) Mac=SHA256\\n\\n330\\n\\n\\n-----\\n\\n## **Config file examples**\\n\\nLet’s see some practical examples of how we can use the configuration file to tweak the default cryptographic settings\\nof an application linked with OpenSSL.\\n\\nNote that applications can still override these settings: what is set in the configuration file merely acts as a default\\nthat is used when nothing else in the application command line or its own config says otherwise.\\n\\n**Only use TLSv1.3**\\n\\nTo configure the OpenSSL library to consider TLSv1.3 as the minimum acceptable protocol, we add a MinProtocol\\nparameter to the /etc/ssl/openssl.cnf configuration file like this:\\n\\n[system_default_sect]\\n\\nCipherString = DEFAULT:@SECLEVEL=2\\n\\nMinProtocol = TLSv1.3\\n\\nIf you then try to connect securely to a server that only offers, say TLSv1.2, the connection will fail:\\n\\n$ curl https://j-server.lxd/stats\\n\\ncurl: (35) error:0A00042E:SSL routines::tlsv1 alert protocol version\\n\\n$ wget https://j-server.lxd/stats\\n\\n--2023-01-06 13:41:50-- https://j-server.lxd/stats\\n\\nResolving j-server.lxd (j-server.lxd)... 10.0.100.87\\n\\nConnecting to j-server.lxd (j-server.lxd)|10.0.100.87|:443... connected.\\n\\nOpenSSL: error:0A00042E:SSL routines::tlsv1 alert protocol version\\n\\nUnable to establish SSL connection.\\n\\n**Use only AES256 with TLSv1.3**\\n\\nAs an additional constraint, besides forcing TLSv1.3, let’s only allow AES256. This would do it for OpenSSL applications that do not override this elsewhere:\\n\\n[system_default_sect]\\n\\nCipherString = DEFAULT:@SECLEVEL=2\\n\\nCipherSuites = TLS_AES_256_GCM_SHA384\\n\\nMinProtocol = TLSv1.3',\n",
       " 'Since we are already forcing TLSv1.3, there is no need to tweak the CipherString list, since that applies only to\\nTLSv1.2 and older.\\n\\nThe OpenSSL s_server command is very handy to test this (see the Troubleshooting section for details on how to use\\nit):\\n\\n$ sudo openssl s_server -cert j-server.pem -key j-server.key -port 443 -www\\n\\n**NOTE**\\n\\nBe sure to use another system for this server, or else it will be subject to the same /etc/ssl/openssl.cnf\\nconstraints you are testing on the client, and this can lead to very confusing results.\\n\\nAs expected, a client will end up selecting TLSv1.3 and the TLS_AES_256_GCM_SHA384 cipher suite:\\n\\n$ wget https://j-server.lxd/stats -O /dev/stdout -q | grep Cipher -w\\n\\nNew, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384\\n\\nCipher : TLS_AES_256_GCM_SHA384\\n\\nTo be sure, we can tweak the server to only offer TLS_CHACHA20_POLY1305_SHA256 for example:\\n\\n$ sudo openssl s_server -cert j-server.pem -key j-server.key -port 443 -www -ciphersuites TLS_CHACHA20_POLY1305_SHA256\\n\\nAnd now the client will fail:\\n\\n$ wget https://j-server.lxd/stats -O /dev/stdout\\n\\n--2023-01-06 14:20:55-- https://j-server.lxd/stats\\n\\nResolving j-server.lxd (j-server.lxd)... 10.0.100.87\\n\\nConnecting to j-server.lxd (j-server.lxd)|10.0.100.87|:443... connected.\\n\\nOpenSSL: error:0A000410:SSL routines::sslv3 alert handshake failure\\n\\nUnable to establish SSL connection.\\n\\n331\\n\\n\\n-----\\n\\n**Drop AES128 entirely**\\n\\nIf we want to still allow TLS v1.2, but just drop AES128, then we need to configure the ciphers separatedly for TLS\\nv1.3 and v1.2 or lower:\\n\\n[system_default_sect]\\n\\nCipherString = DEFAULT:@SECLEVEL=2:!AES128\\n\\nCipherSuites = TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256',\n",
       " \"MinProtocol = TLSv1.2\\n\\nTo test, let’s force our test s_server server to only offer TLSv1.2:\\n\\n$ sudo openssl s_server -cert j-server.pem -key j-server.key -port 443 -www -tls1_2\\n\\nAnd our client picks AES256:\\n\\n$ wget https://j-server.lxd/stats -O /dev/stdout -q | grep Cipher -w\\n\\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES256-GCM-SHA384\\n\\nCipher : ECDHE-RSA-AES256-GCM-SHA384\\n\\nBut that could also be just because AES256 is stronger than AES128. Let’s not offer AES256 on the server, and also\\njump ahead and also remove CHACHA20, which would be the next one preferable to AES128:\\n\\n$ sudo openssl s_server -cert j-server.pem -key j-server.key -port 443 -www -tls1_2 -cipher 'DEFAULT:!AES256:!CHACHA20'\\n\\nSurely wget should fail now. Well, turns out it does select AES128:\\n\\n$ wget https://j-server.lxd/stats -O /dev/stdout -q | grep Cipher -w\\n\\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES128-GCM-SHA256\\n\\nCipher : ECDHE-RSA-AES128-GCM-SHA256\\n\\nIt’s unclear why. Maybe it’s a safeguard, or maybe AES128 is always allowed in TLSv1.2 and we produced an invalid\\nconfiguration. This case shows how crypto is complex, and also applications can override any such configuration\\nsetting that comes from the library. As a counter example, OpenSSL’s s_client tool follows the library config, and\\nfails in this case:\\n\\n$ echo | openssl s_client -connect j-server.lxd:443 | grep -w -i cipher\\n\\n4007F4F9D47F0000:error:0A000410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:../ssl/record/rec_layer_s3.c\\n\\nNew, (NONE), Cipher is (NONE)\\n\\nBut we can override that as well with a command-line option and force s_client to allow AES128:\",\n",
       " '$ echo | openssl s_client -connect j-server.lxd:443 --cipher DEFAULT:AES128 2>&1| grep -w -i cipher\\n\\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES128-GCM-SHA256\\n\\nCipher : ECDHE-RSA-AES128-GCM-SHA256\\n# **References**\\n\\n[• OpenSSL home page](https://www.openssl.org)\\n\\n - SECLEVEL description:\\n\\n**–**\\n[https://www.openssl.org/docs/man3.0/man3/SSL_CTX_set_security_level.html](https://www.openssl.org/docs/man3.0/man3/SSL_CTX_set_security_level.html)\\n\\n**–**\\n[https://www.feistyduck.com/library/openssl-cookbook/online/openssl-command-line/understanding-](https://www.feistyduck.com/library/openssl-cookbook/online/openssl-command-line/understanding-security-levels.html)\\n[security-levels.html](https://www.feistyduck.com/library/openssl-cookbook/online/openssl-command-line/understanding-security-levels.html)\\n\\n  - Configuration directives that can be used in the system_default_sect [section: https://manpages.ubuntu.com/](https://manpages.ubuntu.com/manpages/jammy/en/man3/SSL_CONF_cmd.3ssl.html#supported%20configuration%20file%20commands)\\n[manpages/jammy/en/man3/SSL_CONF_cmd.3ssl.html#supported%20configuration%20file%20commands](https://manpages.ubuntu.com/manpages/jammy/en/man3/SSL_CONF_cmd.3ssl.html#supported%20configuration%20file%20commands)\\n\\nWhen initialized, the GnuTLS library tries to read its system-wide configuration file\\n\\n/etc/gnutls/config . If the file doesn’t exist, built-in defaults are used. To make configuration changes, the /etc/gnutls\\ndirectory and the config file in it have to be created manually, as they are not shipped in the Ubuntu packaging.\\n\\nThis config file can be used to disable (or mark as insecure) algorithms and',\n",
       " 'protocols in a system-wide manner, overriding the library defaults. Note that,\\nintentionally, any algorithms or protocols that were disabled or marked as insecure cannot then be re-enabled or\\nmarked as secure.\\n\\nThere are many configuration options available for GnuTLs, and it’s strongly\\nrecommended to carefully read the upstream documentation listed in the\\nReferences section at the end of this page if creating this file or making changes to it.\\n\\n332\\n\\n\\n-----\\n\\n## **Structure of the config file**\\n\\nThe GnuTLS configuration file is structured as an INI-style text file. There\\nare three sections, and each section contains key = values lines. For\\nexample:\\n\\n[global]\\n\\noverride-mode = blocklist\\n\\n[priorities]\\n\\nSYSTEM = NORMAL:-MD5\\n\\n[overrides]\\n\\ntls-disabled-mac = sha1\\n## The global section\\n\\nThe [global] section sets the override mode used in the [overrides] section:\\n\\n - override-mode = blocklist : the algorithms listed in [overrides] are disabled\\n\\n - override-mode = allowlist : the algorithms listed in [overrides] are enabled.\\n\\nNote that in the allowlist mode, all algorithms that should be enabled must be listed in [overrides], as the library\\nstarts with marking all existing algorithms as disabled/insecure. In practice, this means that using allowlist tends\\nto make the list in [overrides] quite large. Additionally, GnuTLS automatically constructs a SYSTEM keyword (that\\ncan be used in [priorities] ) with all the allowed algorithms and ciphers specified in [overrides] .\\n\\nWhen using allowlist, then all options in [overrides] will be of the enabled form. For example:\\n\\n[global]\\n\\noverride-mode = allowlist\\n\\n[overrides]\\n\\nsecure-hash = sha256\\n\\nenabled-curve = secp256r1',\n",
       " 'secure-sig = ecdsa-secp256r1-sha256\\n\\nenabled-version = tls1.3\\n\\ntls-enabled-cipher = aes-128-gcm\\n\\ntls-enabled-mac = aead\\n\\ntls-enabled-group = secp256r1\\n\\nAnd when using blocklist, all [override] options have the opposite meaning (disable):\\n\\n[global]\\n\\noverride-mode = blocklist\\n\\n[overrides]\\n\\ntls-disabled-cipher = aes-128-cbc\\n\\ntls-disabled-cipher = aes-256-cbc\\n\\ntls-disabled-mac = sha1\\n\\ntls-disabled-group = group-ffdhe8192\\n\\nFor other examples and a complete list of the valid keys in the [overrides] [section, please refer to Disabling algorithms](https://www.gnutls.org/manual/html_node/Disabling-algorithms-and-protocols.html)\\n[and protocols.](https://www.gnutls.org/manual/html_node/Disabling-algorithms-and-protocols.html)\\n## **Priority strings**\\n\\nThe [priorities] section is used to construct *priority strings* . These strings are a way to specify the TLS session’s\\nhandshake algorithms and options in a compact, easy-to-use, format. Note that priority strings are not guaranteed to\\nimply the same set of algorithms and protocols between different GnuTLS versions.\\n\\nThe default priority string is selected at package build time by the vendor, and in the case of Ubuntu Jammy it’s\\ndefined in [debian/rules](https://git.launchpad.net/ubuntu/+source/gnutls28/tree/debian/rules?h=ubuntu/jammy-devel#n38) as:\\n\\nNORMAL:-VERS-ALL:+VERS-TLS1.3:+VERS-TLS1.2:+VERS-DTLS1.2:%PROFILE_MEDIUM\\n\\nA priority string can start with a single initial keyword, and then add or remove algorithms or special keywords. The',\n",
       " 'NORMAL [priority string is defined in this table in the upstream documentation reference, which also includes many other](https://gnutls.org/manual/html_node/Priority-Strings.html#tab_003aprio_002dkeywords)\\nuseful keywords that can be used.\\n\\n333\\n\\n\\n-----\\n\\nTo see the resulting list of ciphers and algorithms from a priority string, one can use the gnutls-cli command-line\\ntool. For example, to list all the ciphers and algorithms allowed with the priority string SECURE256 :\\n\\n$ gnutls-cli --list --priority SECURE256\\n\\nCipher suites for SECURE256\\n\\nTLS_AES_256_GCM_SHA384 0x13, 0x02 TLS1.3\\n\\nTLS_CHACHA20_POLY1305_SHA256 0x13, 0x03 TLS1.3\\n\\nTLS_ECDHE_ECDSA_AES_256_GCM_SHA384 0xc0, 0x2c TLS1.2\\n\\nTLS_ECDHE_ECDSA_CHACHA20_POLY1305 0xcc, 0xa9 TLS1.2\\n\\nTLS_ECDHE_ECDSA_AES_256_CCM 0xc0, 0xad TLS1.2\\n\\nTLS_ECDHE_RSA_AES_256_GCM_SHA384 0xc0, 0x30 TLS1.2\\n\\nTLS_ECDHE_RSA_CHACHA20_POLY1305 0xcc, 0xa8 TLS1.2\\n\\nTLS_RSA_AES_256_GCM_SHA384 0x00, 0x9d TLS1.2\\n\\nTLS_RSA_AES_256_CCM 0xc0, 0x9d TLS1.2\\n\\nTLS_DHE_RSA_AES_256_GCM_SHA384 0x00, 0x9f TLS1.2\\n\\nTLS_DHE_RSA_CHACHA20_POLY1305 0xcc, 0xaa TLS1.2\\n\\nTLS_DHE_RSA_AES_256_CCM 0xc0, 0x9f TLS1.2\\n\\nProtocols: VERS-TLS1.3, VERS-TLS1.2, VERS-TLS1.1, VERS-TLS1.0, VERS-DTLS1.2, VERS-DTLS1.0\\n\\nCiphers: AES-256-GCM, CHACHA20-POLY1305, AES-256-CBC, AES-256-CCM\\n\\nMACs: AEAD\\n\\nKey Exchange Algorithms: ECDHE-ECDSA, ECDHE-RSA, RSA, DHE-RSA\\n\\nGroups: GROUP-SECP384R1, GROUP-SECP521R1, GROUP-FFDHE8192\\n\\nPK-signatures: SIGN-RSA-SHA384, SIGN-RSA-PSS-SHA384, SIGN-RSA-PSS-RSAE-SHA384, SIGN-ECDSA-SHA384, SIGN-ECDSA\\nSECP384R1-SHA384, SIGN-EdDSA-Ed448, SIGN-RSA-SHA512, SIGN-RSA-PSS-SHA512, SIGN-RSA-PSS-RSAE-SHA512, SIGN\\nECDSA-SHA512, SIGN-ECDSA-SECP521R1-SHA512',\n",
       " 'You can manipulate the resulting set by manipulating the priority string. For example, to remove CHACHA20-POLY1305\\nfrom the SECURE256 set:\\n\\n$ gnutls-cli --list --priority SECURE256:-CHACHA20-POLY1305\\n\\nCipher suites for SECURE256:-CHACHA20-POLY1305\\n\\nTLS_AES_256_GCM_SHA384 0x13, 0x02 TLS1.3\\n\\nTLS_ECDHE_ECDSA_AES_256_GCM_SHA384 0xc0, 0x2c TLS1.2\\n\\nTLS_ECDHE_ECDSA_AES_256_CCM 0xc0, 0xad TLS1.2\\n\\nTLS_ECDHE_RSA_AES_256_GCM_SHA384 0xc0, 0x30 TLS1.2\\n\\nTLS_RSA_AES_256_GCM_SHA384 0x00, 0x9d TLS1.2\\n\\nTLS_RSA_AES_256_CCM 0xc0, 0x9d TLS1.2\\n\\nTLS_DHE_RSA_AES_256_GCM_SHA384 0x00, 0x9f TLS1.2\\n\\nTLS_DHE_RSA_AES_256_CCM 0xc0, 0x9f TLS1.2\\n\\nProtocols: VERS-TLS1.3, VERS-TLS1.2, VERS-TLS1.1, VERS-TLS1.0, VERS-DTLS1.2, VERS-DTLS1.0\\n\\nCiphers: AES-256-GCM, AES-256-CBC, AES-256-CCM\\n\\nMACs: AEAD\\n\\nKey Exchange Algorithms: ECDHE-ECDSA, ECDHE-RSA, RSA, DHE-RSA\\n\\nGroups: GROUP-SECP384R1, GROUP-SECP521R1, GROUP-FFDHE8192\\n\\nPK-signatures: SIGN-RSA-SHA384, SIGN-RSA-PSS-SHA384, SIGN-RSA-PSS-RSAE-SHA384, SIGN-ECDSA-SHA384, SIGN-ECDSA\\nSECP384R1-SHA384, SIGN-EdDSA-Ed448, SIGN-RSA-SHA512, SIGN-RSA-PSS-SHA512, SIGN-RSA-PSS-RSAE-SHA512, SIGN\\nECDSA-SHA512, SIGN-ECDSA-SECP521R1-SHA512\\n\\nAnd you can give this a new name by adding the following to the [priorities] section:\\n\\n[priorities]\\n\\nMYSET = SECURE256:-CHACHA20-POLY1305\\n\\nWhich allows the MYSET priority string to be used like this:\\n\\n$ gnutls-cli --list --priority @MYSET\\n## **Verification profile (overrides)**\\n\\nWhen verifying a certificate, or TLS session parameters, GnuTLS uses a set of profiles associated with the session\\nto determine whether the parameters seen in the session are acceptable. These profiles are normally set using the',\n",
       " \"%PROFILE priority string, but it is also possible to set a low bar that applications cannot override. This is done with\\nthe min-verification-profile setting in the [overrides] section.\\n\\nFor example:\\n\\n334\\n\\n\\n-----\\n\\n[overrides]\\n\\n# do not allow applications use the LOW or VERY-WEAK profiles.\\n\\nmin-verification-profile = legacy\\n\\n[The list of values that can be used, and their meaning, is show in the Key sizes and security parameters table in the](https://gnutls.org/manual/html_node/Selecting-cryptographic-key-sizes.html#tab_003akey_002dsizes)\\nupstream documentation.\\n## **Practical examples**\\n\\nLet’s see some practical examples of how we can use the configuration file to tweak the default cryptographic settings\\nof an application linked with GnuTLS.\\n\\nContrary to OpenSSL, for example, GnuTLS does not allow a cipher that was once removed, to be allowed again. So\\nif you have a setting in the GnuTLS config file that prohibits *CHACHA20*, an application using GnuTLS will not be\\nable to allow it.\\n\\n**Only use TLSv1.3**\\n\\nOne way to do it is to set a new default priority string that removes all TLS versions and then adds back just TLS\\n1.3:\\n\\n[global]\\n\\noverride-mode = blocklist\\n\\n[overrides]\\n\\ndefault-priority-string = NORMAL:-VERS-TLS-ALL:+VERS-TLS1.3\\n\\nWith our test server providing everything but TLSv1.3:\\n\\n$ sudo openssl s_server -cert j-server.pem -key j-server.key -port 443 -no_tls1_3 -www\\n\\nConnections will fail:\\n\\n$ gnutls-cli j-server.lxd\\n\\nProcessed 125 CA certificate(s).\\n\\nResolving 'j-server.lxd:443'...\\n\\nConnecting to '10.0.100.87:443'...\\n\\n*** Fatal error: A TLS fatal alert has been received.\\n\\n*** Received alert [70]: Error in protocol version\",\n",
       " 'An application linked with GnuTLS will also fail:\\n\\n$ lftp -c \"cat https://j-server.lxd/status\"\\n\\ncat: /status: Fatal error: gnutls_handshake: A TLS fatal alert has been received.\\n\\nBut an application can override these settings, because it’s just the priority string that is being manipulated in the\\nGnuTLS config:\\n\\n$ lftp -c \"set ssl:priority NORMAL:+VERS-TLS-ALL; cat https://j-server.lxd/status\" | grep ^New\\n\\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES256-GCM-SHA384\\n\\nAnother way to limit the TLS versions is via specific protocol version configuration keys:\\n\\n[global]\\n\\noverride-mode = blocklist\\n\\n[overrides]\\n\\ndisabled-version = tls1.1\\n\\ndisabled-version = tls1.2\\n\\ndisabled-version = tls1.0\\n\\nNote that setting the same key multiple times just appends the new value to the previous value(s).\\n\\nIn this scenario, the application cannot override the config anymore:\\n\\n$ lftp -c \"set ssl:priority NORMAL:+VERS-TLS-ALL; cat https://j-server.lxd/status\" | grep ^New\\n\\ncat: /status: Fatal error: gnutls_handshake: A TLS fatal alert has been received.\\n\\n335\\n\\n\\n-----\\n\\n**Use only AES256 with TLSv1.3**\\n\\nTLSv1.3 has a small list of ciphers, but it includes AES128. Let’s remove it:\\n\\n[global]\\n\\noverride-mode = blocklist\\n\\n[overrides]\\n\\ndisabled-version = tls1.1\\n\\ndisabled-version = tls1.2\\n\\ndisabled-version = tls1.0\\n\\ntls-disabled-cipher = AES-128-GCM\\n\\nIf we now connect to a server that was brought up with this config:\\n\\n$ sudo openssl s_server -cert j-server.pem -key j-server.key -port 443 -ciphersuites TLS_AES_128_GCM_SHA256 \\nwww\\n\\nOur GnuTLS client will fail:\\n\\n$ gnutls-cli j-server.lxd\\n\\nProcessed 126 CA certificate(s).\\n\\nResolving \\'j-server.lxd:443\\'...\\n\\nConnecting to \\'10.0.100.87:443\\'...',\n",
       " '*** Fatal error: A TLS fatal alert has been received.\\n\\n*** Received alert [40]: Handshake failed\\n\\nAnd given GnuTLS’s behavior regarding re-enabling a cipher that was once removed, we cannot allow AES128 from\\nthe command line either:\\n\\n$ gnutls-cli --priority=\"NORMAL:+AES-128-GCM\" j-server.lxd\\n\\nProcessed 126 CA certificate(s).\\n\\nResolving \\'j-server.lxd:443\\'...\\n\\nConnecting to \\'10.0.100.87:443\\'...\\n\\n*** Fatal error: A TLS fatal alert has been received.\\n\\n*** Received alert [40]: Handshake failed\\n## **References**\\n\\n[• System-wide configuration](https://www.gnutls.org/manual/html_node/System_002dwide-configuration-of-the-library.html)\\n\\n[• Priority strings](https://gnutls.org/manual/html_node/Priority-Strings.html)\\n\\n - [min-verification-profile](https://gnutls.org/manual/html_node/Selecting-cryptographic-key-sizes.html#tab_003akey_002dsizes) values\\n\\n[• Disabling algorithms and protocols](https://www.gnutls.org/manual/html_node/Disabling-algorithms-and-protocols.html)\\n\\n  - Invoking the [gnutls-cli](https://gnutls.org/manual/html_node/gnutls_002dcli-Invocation.html) command line tool\\n\\nNetwork Security Services, or NSS, is a set of libraries that was originally developed by Netscape and later inherited\\nby Mozilla. In Ubuntu, it’s used mainly in Mozilla products such as Firefox and Thunderbird, but there are modules\\nand language bindings available for other packages to use.\\n\\nGiven its origins in the Netscape browser, this library used to be bundled together with the applications that required\\nit. Up to this day, for example, the Debian package of Mozilla Thunderbird has its own copy of libnss3, ignoring the\\nsystem-wide one shipped by the libnss3 Debian package.\\n## **Config file**',\n",
       " '[NSS doesn’t have a system-wide policy configuration file in Ubuntu (see #2016303 for details).](https://bugs.launchpad.net/ubuntu/+source/nss/+bug/2016303) That leaves the\\nremaining location for the configuration file to be in the NSS “database” directory. Depending on the application, it\\ncan be in the following places by default:\\n\\n - ~/.pki/nssdb/pkcs11.txt\\nThis is where the system-provided libnss3 library will look by default.\\n\\n - ~/snap/firefox/common/.mozilla/firefox/<random>.default/pkcs11.txt\\nThis is where the Firefox snap will look.\\n\\n - ~/.thunderbird/<random>.default-release/pkcs11.txt\\nMozilla Thunderbird ships with its own copy of libnss3, and is configured to look into this directory to find it.\\n\\n336\\n\\n\\n-----\\n\\n - ~/.netscape/pkcs11.txt\\nThis is the default used by the NSS tools shipped in the libnss3-tools Debian package.\\n\\nThe directory where pkcs11.txt is looked up is also the NSS database directory. NSS will store the certificates and\\nprivate keys it imports or generates here, and the directory will typically contain these SQLITE3 database files:\\n\\n - cert9.db : certificates database\\n\\n - key4.db : private key database\\n\\nWith the pkcs11.txt file we can load PKCS#11 modules, including the one built into NSS itself. Other examples of\\nmodules that can be loaded from there are modules for smart cards or other hardware-based cryptographic devices.\\nOf interest to us here, though, is the policy module.\\n## **Configuring the NSS policy module**\\n\\nThe policy module is defined like this in pkcs11.txt :\\n\\nlibrary=\\n\\nname=Policy\\n\\nNSS=flags=policyOnly,moduleDB\\n\\nconfig=\"disallow=<list> allow=<list> flags=<flags>\"',\n",
       " 'It’s via the config= line that we can list which cryptographic algorithms we want to allow and disallow. The terms in\\nthe list are separated with a colon (” : ”) and consist of the following:\\n\\n - **The special keyword “ALL”**, meaning all possible values and algorithms. It’s mostly used with disallow, so\\nthat a clean slate can be constructed with a following allow list. For example, disallow=ALL allow=<list of\\n\\nallowed> would only allow the algorithms explicitly listed in the allow list.\\n\\n - **Algorithm name** : Standard names like sha256, hmac-sha256, chacha20-poly1305, aes128-gcm and others.\\n\\n - **Version specifiers** : A minimum or maximum version for a protocol. These are the available ones:\\n\\n**–** tls-version-min, tls-version-max : Minimum and maximum version for the TLS protocol. For example,\\n\\ntls-version-min=tls1.2 .\\n**–** dtls-version-min, dtls-version-max : As above, but for DTLS (TLS over UDP)\\n\\n - **Key sizes** : Minimum size for a key:\\n\\n**–** DH-MIN : Diffie-Helman minimum key size. For example, DH-MIN=2048 specifies a minimum of 2048 bits.\\n**–** DSA-MIN : Digital Signature Algorithm minimum key size. For example, DSA-MIN=2048 specifies a minimum\\nof 2048 bits.\\n**–** RSA-MIN : RSA minimum key size. For example, RSA-MIN=2048 specifies a minimum of 2048 bits.\\n\\n - **Signature qualifier** : Selects the specified algorithm with a specific type of signature. For example, sha256/cert\\nsignature . Here are some of the qualifiers that are available:\\n\\n**–**\\n/cert-signature : Used in certificate signatures, certificate revocation lists (CRLs) and Online Certificate\\nStatus Protocol (OCSP).\\n\\n**–**\\n/signature : Used in any signature.\\n\\n**–**',\n",
       " '/all : Combines SSL, SSL key exchange, and signatures.\\n\\n**–**\\n/ssl-key-exchange : Used in the SSL key exchange.\\n\\n**–**\\n/ssl : Used in the SSL record protocol.\\n\\nThe disallow rules are always parsed first, and then the allow ones, independent of the order in which they appear.\\n\\nThere are extra flags that can be added to the config line as well, in a comma-separated list if more than one is\\nspecified:\\n\\n - policy-lock : Turn off the ability for applications to change policy with API calls.\\n\\n - ssl-lock : Turn off the ability to change the SSL defaults.\\n## **Practical examples**\\n\\nLet’s see some practical examples of how we can use the configuration file to tweak the default cryptographic settings\\nof an application linked with the system NSS libraries.\\n\\nFor these examples, we will be using the configuration file located in ~/.pki/nssdb/pkcs11.txt . As noted before,\\ndepending on the application this file can be in another directory.\\n\\n337\\n\\n\\n-----\\n\\nThe examples will use the tstclnt test application that is part of the libnss3-tools Debian package. For the server\\npart, we will be using the OpenSSL test server on the same system. Since it uses the OpenSSL library, it won’t be\\naffected by the changes we make to the NSS configuration.\\n\\n**Bootstrapping the NSS database**\\n\\nInstall the libnss3-tools package which has the necessary tools we will need:\\n\\nsudo apt install libnss3-tools\\n\\nIf you don’t have a ~/.pki/nssdb directory yet, it will have to be created first. For that, we will use the certutil\\ncommand, also part of the libnss3-tools package. This will bootstrap the NSS database in that directory, and also\\ncreate the initial pkcs11.txt file we will tweak in the subsequent examples:',\n",
       " 'mkdir -p ~/.pki/nssdb\\n\\ncertutil -d ~/.pki/nssdb -N\\n\\nIf you already have a populated ~/.pki/nssdb directory, there is no need to run the above commands.\\n\\nWhen running the certutil command as shown, you will be asked to choose a password. That password will protect\\nthe NSS database, and will be requested whenever certain changes are made to it.\\n\\nIn the following examples we will make changes to the pkcs11.txt file inside the NSS database directory. The bootstrap\\nprocess above will have created this file for us already. The changes that we will make should be *added* to the file, and\\nnot replace it. For example, these are the contents of ~/.pki/nssdb/pkcs11.txt right after the boostrap process:\\n\\nlibrary=\\n\\nname=NSS Internal PKCS #11 Module\\n\\nparameters=configdir=\\'/home/ubuntu/.pki/nssdb\\' certPrefix=\\'\\' keyPrefix=\\'\\' secmod=\\'secmod.db\\' flags= updatedir=\\'\\' update\\n\\nNSS=Flags=internal,critical trustOrder=75 cipherOrder=100 slotParams=(1={slotFlags=[ECC,RSA,DSA,DH,RC2,RC4,DES,RANDOM,\\n\\nWhen an example asks to configure the policy module, its block should be appended to the existing configuration\\nblock in the file. For example:\\n\\nlibrary=\\n\\nname=NSS Internal PKCS #11 Module\\n\\nparameters=configdir=\\'/home/ubuntu/.pki/nssdb\\' certPrefix=\\'\\' keyPrefix=\\'\\' secmod=\\'secmod.db\\' flags= updatedir=\\'\\' update\\n\\nNSS=Flags=internal,critical trustOrder=75 cipherOrder=100 slotParams=(1={slotFlags=[ECC,RSA,DSA,DH,RC2,RC4,DES,RANDOM,\\n\\nlibrary=\\n\\nname=Policy\\n\\nNSS=flags=policyOnly,moduleDB\\n\\nconfig=\"allow=tls-version-min=tls1.3\"\\n\\n**Test setup**\\n\\nFor these examples, we will be using a simple OpenSSL server on the same system as the NSS client we are testing.',\n",
       " 'For that we will have to generate a certificate and key for the OpenSSL server to use, and then import that into the\\nNSS database so it can be trusted.\\n\\nFirst, generate a keypair for OpenSSL:\\n\\nopenssl req -new -x509 -days 30 -nodes -subj \"/CN=localhost\" -out localhost.pem -keyout localhost.key\\n\\nTo avoid telling tstclnt to ignore certification validation errors, which might mask the crypto policy changes we are\\ntrying to demonstrate, it’s best to import this certificate into the NSS database and mark it as trusted:\\n\\ncertutil -d ~/.pki/nssdb -A -a -i localhost.pem -t TCP -n localhost\\n\\nThis command will ask you for the NSS database password that you supplied when bootstrapping it. The command\\nline options that were used have the following meanings:\\n\\n - -d ~/.pki/nssdb : The path to the NSS database.\\n\\n - -A : Import a certificate.\\n\\n - -a : The certificate is in ASCII mode (PEM).\\n\\n - -i localhost.pem : The file to read (the actual certificate).\\n\\n - -t TCP : Trust flags (see the -t trustargs [argument in the certutil manpage for a full list).](https://manpages.ubuntu.com/manpages/jammy/en/man1/certutil.1.html)\\n\\n**–** T : Trusted CA for client authentication.\\n\\n**–** C : Trusted CA.\\n\\n**–** P : Trusted peer.\\n\\n - -n localhost : A nickname for this certificate, like a label. It can be used later on to select this certificate.\\n\\n338\\n\\n\\n-----\\n\\nWe are now ready to begin our tests. *Unless otherwise noted*, this is how it’s expected that the server will be run:\\n\\nopenssl s_server -accept 4443 -cert localhost.pem -key localhost.key -www\\n\\n**The** tstclnt **tool**\\n\\nThe libnss3-tools package also contains the tstclnt tool, which is what we will use in the following examples to test',\n",
       " 'our NSS configuration changes.\\n\\nThis is the typical command we will use:\\n\\ntstclnt -d ~/.pki/nssdb -h localhost -p 4443\\n\\nWhere the options have the following meanings:\\n\\n - -d ~/.pki/nssdb : Use the NSS database located in the ~/.pki/nssdb directory.\\n\\n - -h localhost : The server to connect to.\\n\\n - -p 4443 : The TCP port to connect to.\\n\\nTo make things a bit easier to see, since this tool prints a lot of information about the connection, we will wrap it like\\nthis:\\n\\necho \"GET / HTTP/1.0\" | tstclnt -d ~/.pki/nssdb -h localhost -p 4443 2>&1 | grep ^New\\n\\nNew, TLSv1.3, Cipher is TLS_AES_128_GCM_SHA256\\n\\n^C\\n\\nThe above tells us that the connection was completed and that it is using TLSv1.3, with a TLS_AES_128_GCM_SHA256\\ncipher suite.\\n\\nIt will not exit on its own, so it’s necessary to press Ctrl+C ( ^C ) to get back to the shell prompt.\\n\\n**Only use TLSv1.3**\\n\\nHere is how we can restrict the TLS protocol version to 1.3 at a minimum:\\n\\nlibrary=\\n\\nname=Policy\\n\\nNSS=flags=policyOnly,moduleDB\\n\\nconfig=\"allow=tls-version-min=tls1.3\"\\n\\nIf we then start the OpenSSL server without TLSv1.3 support, like this (note the extra no_tls1_3 at the end):\\n\\nopenssl s_server -accept 4443 -cert localhost.pem -key localhost.key -www -no_tls1_3\\n\\nThe tstclnt tool will fail to connect:\\n\\necho \"GET / HTTP/1.0\" | tstclnt -d ~/.pki/nssdb -h localhost -p 4443 2>&1 | grep ^New\\n\\necho $?\\n\\n1\\n\\nTo see the actual error, we can remove the grep at the end:\\n\\necho \"GET / HTTP/1.0\" | tstclnt -d ~/.pki/nssdb -h localhost -p 4443 2>&1\\n\\ntstclnt: write to SSL socket failed: SSL_ERROR_PROTOCOL_VERSION_ALERT: Peer reports incompatible or unsupported protocol v\\n\\nIf we allow the server to offer TLSv1.3:',\n",
       " 'openssl s_server -accept 4443 -cert localhost.pem -key localhost.key -www\\n\\nThen the connection completes:\\n\\necho \"GET / HTTP/1.0\" | tstclnt -d ~/.pki/nssdb -h localhost -p 4443 2>&1 | grep ^New\\n\\nNew, TLSv1.3, Cipher is TLS_AES_128_GCM_SHA256\\n\\n^C\\n\\n**Use only AES256 with TLSv1.3**\\n\\nIn the previous example, the connection ended up using TLSv1.3 as expected, but AES128. To enforce AES256, we\\ncan disallow the 128-bit version:\\n\\n339\\n\\n\\n-----\\n\\nlibrary=\\n\\nname=Policy\\n\\nNSS=flags=policyOnly,moduleDB\\n\\nconfig=\"disallow=aes128-gcm allow=tls-version-min=tls1.3\"\\n\\nThis time the client selects something else:\\n\\necho \"GET / HTTP/1.0\" | tstclnt -d ~/.pki/nssdb -h localhost -p 4443 2>&1 | grep ^New\\n\\nNew, TLSv1.3, Cipher is TLS_CHACHA20_POLY1305_SHA256\\n\\nWe can remove that one from the list as well:\\n\\nconfig=\"disallow=aes128-gcm:chacha20-poly1305 allow=tls-version-min=tls1.3\"\\n\\nAnd now we get AES256:\\n\\necho \"GET / HTTP/1.0\" | tstclnt -d ~/.pki/nssdb -h localhost -p 4443 2>&1 | grep ^New\\n\\nNew, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384\\n# **References**\\n\\nUnfortunately most of the upstream Mozilla documentation is either outdated or deprecated, and the best reference\\navailable about the policy module at the moment is in the source code and tests.\\n\\n[• In the source code](https://git.launchpad.net/ubuntu/+source/nss/tree/nss/lib/pk11wrap/pk11pars.c#n144)\\n\\n [In the tests (policy)](https://git.launchpad.net/ubuntu/+source/nss/tree/nss/tests/policy)\\n\\n [In the tests (SSL policy)](https://git.launchpad.net/ubuntu/+source/nss/tree/nss/tests/ssl/sslpolicy.txt)\\n\\nThe Java cryptographic settings are large and complex, with many layers and policies. Here we will focus on one',\n",
       " 'aspect of it, which is how to apply some basic filters to the set of cryptographic algorithms available to applications.\\nThe references section at the end contains links to more information.\\n\\nThere are many versions of Java available in Ubuntu. It’s best to install the “default” one, which is represented\\nby the default-jre (for the Runtime Environment) or default-jdk (for the Development Kit). And their non-GUI\\ncounterparts default-jre-headless and default-jdk-headless, respectively.\\n\\nTo install the default Java Runtime on Ubuntu Server, run the following command:\\n\\nsudo apt install default-jre-headless\\n## **Config file**\\n\\nThe Java installation in Ubuntu ships a system-wide configuration tree under /etc/java-<VERSION>-openjdk . In\\nUbuntu Jammy 22.04 LTS, the default Java version is 11, so this directory will be /etc/java-11/openjdk . In\\nthat directory, the file that defines Java security settings, including cryptographic algorithms, is /etc/java-11\\nopenjdk/security/java.security .\\n\\nThis is a very large file, with many options and comments. Its structure is simple, with configuration keys and their\\nvalues. For crypto algorithms, we will be looking into the following settings:\\n\\n - jdk.certpah.disabledAlgorithms : Restrictions on algorithms and key lengths used in certificate path processing.\\n\\n - jdk.tls.disabledAlgorithms : Restrictions on algorithms and key lengths used in SSL/TLS connections.\\n\\nThe list of restrictions has its own format which allows for constructs that disable whole families of algorithms, key\\nsizes, usage, and more.',\n",
       " 'The java.security [configuration file has comments explaining this syntax with some examples.](https://git.launchpad.net/ubuntu/+source/openjdk-lts/tree/src/java.base/share/conf/security/java.security?h=applied/ubuntu/jammy-devel#n520)\\n\\nChanges to these security settings can be made directly in the /etc/java-11-openjdk/security/java.security file, or\\nin an alternate file that can be specified to a Java application by setting the java.security.properties value. For\\nexample, if your java application is called myapp.java, you can invoke it as shown below to specify an additional\\nsecurity properties file:\\n\\njava -Djava.security.properties=file://$HOME/java.security\\n\\n=\\nWhen using just one equals sign (” ”) as above, the settings from the specified file are appended to the existing ones.\\nIf, however, we use two equals signs:\\n\\njava -Djava.security.properties==file://$HOME/java.security\\n\\n340\\n\\n\\n-----\\n\\nThen the settings from $HOME/java.security completely override the ones from the main file at /etc/java-11\\nopenjdk/security/java.security .\\n\\nTo disable the ability to specify an additional properties file in the command line, set the key security.overridePropertiesFile\\nto false in /etc/java-11-openjdk/security/java.security .\\n## **Practical examples**\\n\\nLet’s see some practical examples of how we can use the configuration file to tweak the default cryptographic settings\\nof a Java application.\\n\\nThe examples will use the Java keytool utility for the client part, and a simple OpenSSL test server on localhost for\\nthe server part. Since OpenSSL has its own separate configuration, it won’t be affected by the changes we make to\\nthe Java security settings.\\n\\n**Test setup**',\n",
       " 'To use the test OpenSSL server, we will have to generate a certificate and key for it to use, and then import that into\\nthe Java Certificate Authority (CA) database so it can be trusted.\\n\\nFirst, generate a keypair for OpenSSL:\\n\\nopenssl req -new -x509 -days 30 -nodes -subj \"/CN=localhost\" -out localhost.pem -keyout localhost.key\\n\\nNow let’s import this new certificate into the system-wide CA database. Execute the following commands:\\n\\nsudo cp localhost.pem /usr/local/share/ca-certificates/localhost-test.crt\\n\\nsudo update-ca-certificates\\n\\nFor our testing purposes, this is how we will launch our OpenSSL test server:\\n\\nopenssl s_server -accept 4443 -cert localhost.pem -key localhost.key | grep ^CIPHER\\n\\nThis will show the cipher that was selected for each connection, as it occurs.\\n\\nThe client part of our setup will be using the keytool utility that comes with Java, but any Java application that is\\ncapable of using SSL/TLS should suffice. We will be running the client as below:\\n\\nkeytool -J-Djava.security.properties=file://$HOME/java.security -printcert -sslserver localhost:4443 > /dev/null;echo $?\\n\\nThese are the parameters:\\n\\n - -J-Djava.security.properties=...\\nThis is used to point at the configuration file snippet that has our changes. It is NOT NEEDED if you are\\nmodifying /etc/java-11-openjdk/security/java.security instead.\\n\\n - -printcert -sslserver localhost:4443\\nConnect to a server on localhost ( -sslserver is a parameter to -printcert, so we need the latter even though\\nwe are not interested in the certificate).\\n\\nThe rest is just to ignore all non-error output, and show us the exit status ( 0 for success, anything else for an error).\\n\\n**Note** :',\n",
       " 'keytool is not really intended as a tool to test SSL/TLS connections, but being part of the Java packaging\\nmakes it convenient and it’s enough for our purposes.\\n\\nLet’s see some examples now.\\n\\n**Only use TLSv1.3**\\n\\nCreate $HOME/java.security with the following content:\\n\\njdk.tls.disabledAlgorithms=TLSv1, TLSv1.1, TLSv1.2, SSLv3, SSLv2\\n\\nNotice that TLSv1.3 is absent.\\n\\nWhen you then run the keytool utility:\\n\\n$ keytool -J-Djava.security.properties=file://$HOME/java.security -printcert -sslserver localhost:4443 > /dev/null;echo\\n\\n0\\n\\nThe server should log:\\n\\n341\\n\\n\\n-----\\n\\n$ openssl s_server -accept 4443 -key localhost.key -cert localhost.pem | grep ^CIPHER\\n\\nCIPHER is TLS_AES_256_GCM_SHA384\\n\\nThat is a TLSv1.3 cipher. To really test that TLSv1.3 is the only protocol available, we can force some failures:\\n\\nForce the client to try to use TLSv1.2:\\n\\n$ keytool \\\\\\n\\n-J-Djava.security.properties=file://$HOME/java.security \\\\\\n\\n-J-Djdk.tls.client.protocols=TLSv1.2 \\\\\\n\\n-printcert -sslserver localhost:4443\\n\\nkeytool error: java.lang.Exception: No certificate from the SSL server\\n\\nRestart the server with the no_tls1_3 option, disabling TLSv1.3, and run the client again as originally (without the\\nextra TLSv1.2 option we added above):\\n\\n**Server** :\\n\\n$ openssl s_server -accept 4443 -key localhost.key -cert localhost.pem -no_tls1_3\\n\\nUsing default temp DH parameters\\n\\nACCEPT\\n\\nERROR\\n\\n40676E75B37F0000:error:0A000102:SSL routines:tls_early_post_process_client_hello:unsupported protocol:../ssl/statem/st\\n\\nshutting down SSL\\n\\nCONNECTION CLOSED\\n\\n**Client** :\\n\\n$ keytool -J-Djava.security.properties=file://$HOME/java.security -printcert -sslserver localhost:4443',\n",
       " 'keytool error: java.lang.Exception: No certificate from the SSL server\\n\\nTo get a little bit more verbosity in the keytool output, you can add the -v option. Then, inside the traceback that\\nwe get back, we can see an error message about an SSL protocol version:\\n\\n$ keytool -J-Djava.security.properties=file://$HOME/java.security -printcert -sslserver localhost:4443 -v\\n\\nkeytool error: java.lang.Exception: No certificate from the SSL server\\n\\njava.lang.Exception: No certificate from the SSL server\\n\\nat java.base/sun.security.tools.keytool.Main.doPrintCert(Main.java:2981)\\n\\nat java.base/sun.security.tools.keytool.Main.doCommands(Main.java:1292)\\n\\nat java.base/sun.security.tools.keytool.Main.run(Main.java:421)\\n\\nat java.base/sun.security.tools.keytool.Main.main(Main.java:414)\\n\\nCaused by: javax.net.ssl.SSLHandshakeException: Received fatal alert: protocol_version\\n\\n...\\n\\n**Prevent a specific cipher**\\n\\n[The Java Security Standard Algorithm Names page lists the names of all the cryptographic algorithms recognised by](https://docs.oracle.com/en/java/javase/11/docs/specs/security/standard-names.html)\\nJava. If you want to prevent a specific algorithm from being used, you can list it in the java.security file.\\n\\nIn the previous example where we allowed only TLSv1.3 we saw that the negotiated algorithm was TLS_AES_256_GCM_SHA384 .\\nBut what happens if we block it?\\n\\nAdd TLS_AES_256_GCM_SHA384 to jdk.tls.disabledAlgorithms in $HOME/java.security like this:\\n\\njdk.tls.disabledAlgorithms=TLSv1, TLSv1.1, TLSv1.2, SSLv3, SSLv2, TLS_AES_256_GCM_SHA384\\n\\nIf we run our client now:\\n\\n$ keytool -J-Djava.security.properties=file://$HOME/java.security -printcert -sslserver localhost:4443 > /dev/null; echo\\n\\n0',\n",
       " 'The server will show the new selected cipher:\\n\\n$ openssl s_server -accept 4443 -key localhost.key -cert localhost.pem | grep ^CIPHER\\n\\nCIPHER is TLS_AES_128_GCM_SHA256\\n\\n342\\n\\n\\n-----\\n\\n## **Blocking cipher “elements”**\\n\\nWith TLSv1.3 ciphers, we must list the exact cipher name. With TLSv1.2 ciphers, however, there is a bit more\\nflexibility and we can list just an “element”.\\n\\nFor example, let’s check out a case where we only allow TLSv1.2 for simplicity by once again modifying\\n\\n$HOME/java.security :\\n\\njdk.tls.disabledAlgorithms=TLSv1, TLSv1.1, TLSv1.3, SSLv2, SSLv3\\n\\nWhen we run the client:\\n\\n$ keytool -J-Djava.security.properties=file://$HOME/java.security -printcert -sslserver localhost:4443 - /dev/null; ech\\n\\n0\\n\\nThe server reports:\\n\\n$ openssl s_server -accept 4443 -key localhost.key -cert localhost.pem | grep ^CIPHER\\n\\nCIPHER is ECDHE-RSA-AES256-GCM-SHA384\\n\\nWe can block just the AES256 component by using:\\n\\njdk.tls.disabledAlgorithms=TLSv1, TLSv1.1, TLSv1.3, SSLv2, SSLv3, AES_256_GCM\\n\\nAnd now the server reports:\\n\\n$ openssl s_server -accept 4443 -key localhost.key -cert localhost.pem | grep ^CIPHER\\n\\nCIPHER is ECDHE-RSA-CHACHA20-POLY1305\\n## **References**\\n\\n[• Additional information on Java’s Cryptographic Algorithms settings](https://www.java.com/en/configure_crypto.html)\\n\\n[• Java Security Standard Algorithm Names](https://docs.oracle.com/en/java/javase/12/docs/specs/security/standard-names.html)\\n\\n[• Keytool upstream documentation](https://docs.oracle.com/en/java/javase/11/tools/keytool.html)',\n",
       " ' - java.security [file with comments – links to the section which explains the crypto algorithm restrictions)](https://git.launchpad.net/ubuntu/+source/openjdk-lts/tree/src/java.base/share/conf/security/java.security?h=applied/ubuntu/jammy-devel#n520)\\n\\nDomain Name System Security Extensions (DNSSEC), which provides a set of security features to DNS, is a broad\\ntopic. In this article, we will briefly show DNSSEC validation happening on a bind9 DNS server, and then introduce\\nthe topic of how we can disable certain cryptographic algorithms from being used in this validation.\\n## **DNSSEC validation**\\n\\nOut of the box, the BIND 9 DNS server is configured to try to use DNSSEC whenever it’s available, doing all the\\nvalidation checks automatically. This is done via the dnssec-validation setting in /etc/bind/named.conf.options :\\n\\noptions {\\n\\n(...)\\n\\ndnssec-validation auto;\\n\\n(...)\\n\\n};\\n\\nThis can be quickly checked with the help of dig . Right after you installed bind9, you can probe ask it about the\\n\\nisc.org domain:\\n\\n$ dig @127.0.0.1 isc.org +dnssec +multiline\\n\\n; <<>> DiG 9.18.12-0ubuntu0.22.04.1-Ubuntu <<>> @127.0.0.1 isc.org +dnssec +multiline\\n\\n; (1 server found)\\n\\n;; global options: +cmd\\n\\n;; Got answer:\\n\\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 57669\\n\\n;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\\n\\n;; OPT PSEUDOSECTION:\\n\\n; EDNS: version: 0, flags: do; udp: 1232\\n\\n; COOKIE: 71aa6b4e4ca6bb4b01000000643fee81edf0840b48d28d44 (good)\\n\\n;; QUESTION SECTION:\\n\\n;isc.org. IN A\\n\\n343\\n\\n\\n-----\\n\\n;; ANSWER SECTION:\\n\\nisc.org. 300 IN A 149.20.1.66\\n\\nisc.org. 300 IN RRSIG A 13 2 300 (\\n\\n20230510161711 20230410161439 27566 isc.org.',\n",
       " \"EUA5QPEjtVC0scPsvf1c/EIBKHRpS8ektiWiOqk6nb3t\\n\\nJhJAt9uCr3e0KNAcc3WDU+wJzEvqDyJrlZoELqT/pQ== )\\n\\n;; Query time: 740 msec\\n\\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\\n\\n;; WHEN: Wed Apr 19 13:37:05 UTC 2023\\n\\n;; MSG SIZE rcvd: 183\\n\\nWe can see that a RRSIG DNSSEC record was returned, but the key information in this output is the ad flag near the\\ntop. That stands for “authenticated data”, and means that the DNSSEC records in the response were validated.\\n\\nTo see an example where this verification fails, we can use the www.dnssec-failed.org domain, which is specially crafted\\nfor this:\\n\\n$ dig @127.0.0.1 www.dnssec-failed.org +dnssec +multiline\\n\\n; <<>> DiG 9.18.12-0ubuntu0.22.04.1-Ubuntu <<>> @127.0.0.1 www.dnssec-failed.org +dnssec +multiline\\n\\n; (1 server found)\\n\\n;; global options: +cmd\\n\\n;; Got answer:\\n\\n;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 56056\\n\\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1\\n\\n;; OPT PSEUDOSECTION:\\n\\n; EDNS: version: 0, flags: do; udp: 1232\\n\\n; COOKIE: 541f6c66a216acdb01000000643fef9ebb21307fee2ea0e3 (good)\\n\\n;; QUESTION SECTION:\\n\\n;www.dnssec-failed.org. IN A\\n\\n;; Query time: 1156 msec\\n\\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\\n\\n;; WHEN: Wed Apr 19 13:41:50 UTC 2023\\n\\n;; MSG SIZE rcvd: 78\\n\\nHere we see that:\\n\\n  - There is no IN A IP address shown in the reply\\n\\n  - The status is SERVFAIL\\n\\n  - There is no ad flag\\n\\nIn the bind9 logs, we will see DNSSEC validation errors:\\n\\n$ journalctl -u named.service -n 10\\n\\nApr 19 13:41:50 j named[3018]: validating dnssec-failed.org/DNSKEY: no valid signature found (DS)\\n\\nApr 19 13:41:50 j named[3018]: no valid RRSIG resolving 'dnssec-failed.org/DNSKEY/IN': 68.87.76.228#53\",\n",
       " 'Apr 19 13:41:50 j named[3018]: broken trust chain resolving \\'www.dnssec-failed.org/A/IN\\': 68.87.85.132#53\\n\\n(...)\\n\\nWe can run dig with the +cd command line parameter which disables this verification, but notice that still we don’t\\nget the ad flag in the reply:\\n\\n$ dig @127.0.0.1 www.dnssec-failed.org +dnssec +multiline +cd\\n\\n; <<>> DiG 9.18.12-0ubuntu0.22.04.1-Ubuntu <<>> @127.0.0.1 www.dnssec-failed.org +dnssec +multiline +cd\\n\\n; (1 server found)\\n\\n;; global options: +cmd\\n\\n;; Got answer:\\n\\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 42703\\n\\n;; flags: qr rd ra cd; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1\\n\\n;; OPT PSEUDOSECTION:\\n\\n344\\n\\n\\n-----\\n\\n; EDNS: version: 0, flags: do; udp: 1232\\n\\n; COOKIE: 3d6a4f4ff0014bdc01000000643ff01c3229ed7d798c5f8d (good)\\n\\n;; QUESTION SECTION:\\n\\n;www.dnssec-failed.org. IN A\\n\\n;; ANSWER SECTION:\\n\\nwww.dnssec-failed.org. 7031 IN A 68.87.109.242\\n\\nwww.dnssec-failed.org. 7031 IN A 69.252.193.191\\n\\nwww.dnssec-failed.org. 7074 IN RRSIG A 5 3 7200 (\\n\\n20230505145108 20230418144608 44973 dnssec-failed.org.\\n\\nR6/u+5Gv3rH93gO8uNvz3sb9ErQNuvFKu6W5rtUleXF/\\n\\nvkqJXbNe8grMuiV6Y+CNEP6jRBu0jOBPncb5cXbfcmfo\\n\\nCoVOjpsLySxt4D1EUl4yByWm2ZAdXRrk6A8SaldIdDv8\\n\\n9t+FguTdQrZv9Si+afKrLyC7L/mltXMllq3stDI= )\\n\\n;; Query time: 0 msec\\n\\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\\n\\n;; WHEN: Wed Apr 19 13:43:56 UTC 2023\\n\\n;; MSG SIZE rcvd: 287\\n## **Restricting DNSSEC algorithms**\\n\\nIt’s possible to limit the cryptographic algorithms used by BIND to validate DNSSEC records. This is done via two\\nconfiguration settings, located inside the options { } block of /etc/named/named.conf.options :\\n\\n - disable-algorithms \"<domain>\" { a; b; ... };',\n",
       " 'Disables the listed algorithms for the specified domain and all subdomains of it.\\n\\n - disable-ds-digests \"<domain>\" { a; b; ... };\\nDisables the listed digital signature digests for the specified domain and all subdomains of it.\\n\\nFor example, the following disables RSAMD5, DSA and GOST for all zones:\\n\\ndisable-algorithms \".\" {\\n\\nRSAMD5;\\n\\nDSA;\\n\\n};\\n\\ndisable-ds-digest \".\" {\\n\\nGOST;\\n\\n};\\n\\n[The list of algorithm names can be obtained at DNSSEC Algorithm Numbers, in the](https://www.iana.org/assignments/dns-sec-alg-numbers/dns-sec-alg-numbers.xhtml) **Mnemonic** column of the\\n**Available Formats** table. The algorithm number is also standardised, and is part of the DNSSEC records.\\n\\nFor example, if we go back to the dig result from before where we inspected the isc.org domain, the *RRSIG* record\\nhad this (showing just the first line for brevity):\\n\\nisc.org. 300 IN RRSIG A 13 2 300 (\\n\\nIn that record, the number 13 is the algorithm number, and in this case it means the algorithm ECDSAP256SHA256 was\\nused.\\n\\nJust to see how BIND would react to an algorithm being disabled, let’s temporarily add ECDSAP256SHA256 to the list\\nof disabled algorithms:\\n\\ndisable-algorithms \".\" {\\n\\nRSAMD5;\\n\\nDSA;\\n\\nECDSAP256SHA256;\\n\\n};\\n\\nAnd restart BIND:\\n\\nsudo systemctl restart bind9.service\\n\\nNow the ad flag is gone, meaning that this answer wasn’t validated:\\n\\n$ dig @127.0.0.1 isc.org +dnssec +multiline\\n\\n; <<>> DiG 9.18.1-1ubuntu1-Ubuntu <<>> @127.0.0.1 isc.org +dnssec +multiline\\n\\n345\\n\\n\\n-----\\n\\n; (1 server found)\\n\\n;; global options: +cmd\\n\\n;; Got answer:\\n\\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43893\\n\\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\\n\\n;; OPT PSEUDOSECTION:',\n",
       " '; EDNS: version: 0, flags: do; udp: 1232\\n\\n; COOKIE: 6527ce585598025d01000000643ff8fa02418ce38af13fa7 (good)\\n\\n;; QUESTION SECTION:\\n\\n;isc.org. IN A\\n\\n;; ANSWER SECTION:\\n\\nisc.org. 300 IN A 149.20.1.66\\n\\nisc.org. 300 IN RRSIG A 13 2 300 (\\n\\n20230510161711 20230410161439 27566 isc.org.\\n\\nEUA5QPEjtVC0scPsvf1c/EIBKHRpS8ektiWiOqk6nb3t\\n\\nJhJAt9uCr3e0KNAcc3WDU+wJzEvqDyJrlZoELqT/pQ== )\\n\\n;; Query time: 292 msec\\n\\n;; SERVER: 127.0.0.1#53(127.0.0.1) (UDP)\\n\\n;; WHEN: Wed Apr 19 14:21:46 UTC 2023\\n\\n;; MSG SIZE rcvd: 183\\n\\nThe logs only say there was no valid signature found:\\n\\nApr 19 14:23:01 j-bind9 named[2786]: validating isc.org/A: no valid signature found\\n\\nNote this is different from rejecting the response: it just means that this response is being treated as if it didn’t have\\nany DNSSEC components in it, or in other words, it’s treated as “insecure”.\\n\\nIn general, as always with cryptography, be careful with which algorithms you decide to disable and remove from\\nDNSSEC validation, as such errors can be hard to diagnose. To help with troubleshooting, the Internet Systems\\nConsortium (ISC) has published a very extensive DNSSEC guide, which contains a detailed troubleshooting section\\n(see below).\\n\\n**Note** :\\n\\nRemember now to remove the disabling of ECDSAP256SHA256 from /etc/bind/named.conf.options and restart\\nBIND 9. This change was just a quick test!\\n# **References**\\n\\n[• ISC’s DNSSEC Guide](https://bind9.readthedocs.io/en/v9.18.14/dnssec-guide.html)\\n\\n[• DNSSEC troubleshooting section of the ISC DNSSEC guide](https://bind9.readthedocs.io/en/v9.18.14/dnssec-guide.html#basic-dnssec-troubleshooting)',\n",
       " '[• Standard algorithms used for DNSSEC](https://www.iana.org/assignments/dns-sec-alg-numbers/dns-sec-alg-numbers.xhtml)\\n\\nEstablishing an SSH connection to a remote service involves multiple stages. Each one of these stages will use some\\nform of encryption, and there are configuration settings that control which cryptographic algorithms can be used at\\neach step.\\n\\nThe default selection of algorithms for each stage should be good enough for the majority of deployment scenarios.\\nSometimes, however, a compliance rule, or a set of legacy servers, or something else, requires a change in this selection.\\nPerhaps a legacy system or piece of hardware that is still in production is not compatible with the current encryption\\nschemes and requires legacy algorithms to be enabled again. Or a compliance rule that isn’t up-to-date with the\\ncurrent crypto standards doesn’t allow a more advanced cipher.\\n\\n**WARNING** :\\n\\nBe careful when restricting cryptographic algorithms in SSH, specially on the server side. You can inadvertently lock yourself out of a remote system!\\n## **Algorithm configuration general rules**\\n\\nMost of the configuration options that take a list of cryptographic algorithms follow a defined set of rules. The first\\nalgorithm in the list that the *client* offers to the server, which matches an offer from the server, is what will be selected.\\nThe rules are as follows:\\n\\n346\\n\\n\\n-----\\n\\n  - The lists are algorithm names separated by commas. For example, Ciphers aes128-gcm@openssh.com,aes256\\ngcm@openssh.com will replace the current set of ciphers with the two named algorithms.',\n",
       " '  - Instead of specifying the full list, which will replace the existing default one, some manipulations are allowed. If\\nthe list starts with:\\n\\n**–** +\\nThe specified algorithm(s) will be appended to the end of the default set. For example, MACs +hmac-sha2\\n512,hmac-sha2-256 will append *both* Message Authentication Code (MAC) algorithms to the end of the\\n\\ncurrent set.\\n\\n**–**    The specified algorithm(s) will be removed from the default set. For example, KexAlgorithms -diffie\\nhellman-group1-sha1,diffie-hellman-group14-sha1 will remove *both* key exchange algorithms from the cur\\nrent set.\\n\\n**–** ^\\nThe specified ciphers will be placed at the beginning of the default set. For example, PubkeyAcceptedAlgo\\nrithms ^ssh-ed25519,ecdsa-sha2-nistp256 will move *both* signature algorithms to the start of the set.\\n\\n**–** Wildcards (    - ) are also allowed, but be careful to not inadvertently include or exclude something that wasn’t\\nintended.\\n\\nWith rare exceptions, the list of algorithms can be queried by running ssh -Q <config>, where <config> is the\\nconfiguration setting name. For example, ssh -Q ciphers will show the available list of ciphers.\\n\\n**Note** :\\nThe output of the ssh -Q <name> command will not take into consideration the configuration changes that\\nmay have been made. It cannot therefore be used to test the crypto configuration changes.\\n## **Configuration settings**\\n\\nIt’s not the goal of this documentation to repeat the excellent upstream documentation (see references). Instead, we\\nwill show the configuration options, and some examples of how to use them.\\n\\nHere are the configuration settings that control the cryptographic algorithms selection. Unless otherwise noted, they',\n",
       " 'apply to both the server and the client.\\n\\n - Ciphers\\nList of symmetric ciphers. Examples include aes256-ctr and chacha20-poly1305@openssh.com .\\n\\n - MACs\\n\\nList of Message Authentication Code algorithms, used for data integrity protection. The -etm versions calculate the MAC after encryption and are considered safer. Examples include hmac-sha2-256 and hmac-sha2-512\\netm@openssh.com .\\n\\n - GSSAPIKexAlgorithms\\n[This option is not available in OpenSSH upstream, and is provided via a patch that Ubuntu and many other](https://git.launchpad.net/ubuntu/+source/openssh/tree/debian/patches/gssapi.patch?h=applied/ubuntu/jammy-devel)\\nLinux Distributions carry. It lists the key exchange (kex) algorithms that are offered for Generic Security\\nServices Application Program Interface (GSSAPI) key exchange, and only applies to connections using GSSAPI.\\nExamples include gss-gex-sha1- and gss-group14-sha256- .\\n\\n - KexAlgorithms\\nList of available key exchange (kex) algorithms. Examples include curve25519-sha256 and sntrup761x25519\\nsha512@openssh.com .\\n\\n - HostKeyAlgorithms\\nThis is a *server-only* configuration option. It lists the available host key signature algorithms that the server\\noffers. Examples include ssh-ed25519-cert-v01@openssh.com and ecdsa-sha2-nistp521-cert-v01@openssh.com .\\n\\n - PubkeyAcceptedAlgorithms\\nList of signature algorithms that will be accepted for public key authentication. Examples include ssh-ed25519\\ncert-v01@openssh.com and rsa-sha2-512-cert-v01@openssh.com .\\n\\n - CASignatureAlgorithms\\nList of algorithms that certificate authorities (CAs) are allowed to use to sign certificates. Certificates signed',\n",
       " 'using any other algorithm will not be accepted for public key or host-based authentication. Examples include\\n\\nssh-ed25519 and ecdsa-sha2-nistp384 .\\n\\n347\\n\\n\\n-----\\n\\nTo check what effect a configuration change has on the server, it’s helpful to use the -T parameter and grep the output\\nfor the configuration key you want to inspect. For example, to check the current value of the Ciphers configuration\\nsetting after having set Ciphers ^3des-cbc in sshd_config :\\n\\n$ sudo sshd -T | grep ciphers\\n\\nciphers 3des-cbc,chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256\\ngcm@openssh.com\\n\\nThe output will include changes made to the configuration key. There is no need to restart the service.\\n## **OpenSSH examples**\\n\\nHere are some examples of how the cryptographic algorithms can be selected in OpenSSH.\\n\\n**Which cipher was used?**\\n\\nOne way to examine which algorithm was selected is to add the -v parameter to the ssh client.\\n\\nFor example, assuming password-less public key authentication is being used (so no password prompt), we can use\\nthis command to initiate the connection and exit right away:\\n\\n$ ssh -v <server> exit 2>&1 | grep \"cipher:\"\\n\\ndebug1: kex: server->client cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none\\n\\ndebug1: kex: client->server cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none\\n\\nIn the above case, the chacha20 cipher was automatically selected. We can influence this decision and only offer one\\nalgorithm:\\n\\n$ ssh -v -c aes128-ctr <server> exit 2>&1 | grep \"cipher:\"\\n\\ndebug1: kex: server->client cipher: aes128-ctr MAC: umac-64-etm@openssh.com compression: none',\n",
       " 'debug1: kex: client->server cipher: aes128-ctr MAC: umac-64-etm@openssh.com compression: none\\n\\nFor the other stages in the ssh connection, like key exchange, or public key authentication, other expressions for the\\n\\ngrep command have to be used. In general, it will all be visible in the full -v output.\\n\\n**Remove AES 128 from server**\\n\\nLet’s configure an OpenSSH server to only offer the AES 256 bit variant of symmetric ciphers for an ssh connection.\\n\\nFirst, let’s see what the default is:\\n\\n$ sudo sshd -T | grep ciphers\\n\\nciphers chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256\\ngcm@openssh.com\\n\\nNow let’s make our change. On the server, we can edit /etc/ssh/sshd_config and add this line:\\n\\nCiphers -aes128*\\n\\nAnd then check what is left:\\n\\n$ sudo sshd -T | grep ciphers\\n\\nciphers chacha20-poly1305@openssh.com,aes192-ctr,aes256-ctr,aes256-gcm@openssh.com\\n\\nTo activate the change, ssh has to be restarted:\\n\\n$ sudo systemctl restart ssh.service\\n\\nAfter we restart the service, clients will no longer be able to use AES 128 to connect to it:\\n\\n$ ssh -c aes128-ctr <server>\\n\\nUnable to negotiate with 10.0.102.49 port 22: no matching cipher found. Their offer: chacha20\\npoly1305@openssh.com,aes192-ctr,aes256-ctr,aes256-gcm@openssh.com\\n\\n348\\n\\n\\n-----\\n\\n**Prioritize AES 256 on the client**\\n\\nIf we just want to prioritise a particular cipher, we can use the “ ^ ” character to move it to the front of the list, without\\ndisabling any other cipher:\\n\\n$ ssh -c ^aes256-ctr -v <server> exit 2>&1 | grep \"cipher:\"\\n\\ndebug1: kex: server->client cipher: aes256-ctr MAC: umac-64-etm@openssh.com compression: none',\n",
       " 'debug1: kex: client->server cipher: aes256-ctr MAC: umac-64-etm@openssh.com compression: none\\n\\nIn this way, if the server we are connecting to does not support AES 256, the negotiation will pick up the next one\\nfrom the list. If we do that on the server via Ciphers -aes256*, this is what the same client, with the same command\\nline, now reports:\\n\\n$ ssh -c ^aes256-ctr -v <server> exit 2>&1 | grep \"cipher:\"\\n\\ndebug1: kex: server->client cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none\\n\\ndebug1: kex: client->server cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none\\n# **References**\\n\\n[• OpenSSH upstream documentation index](https://www.openssh.com/manual.html)\\n\\n - Ubuntu [sshd_config](https://manpages.ubuntu.com/manpages/jammy/man5/sshd_config.5.html) man page\\n\\n - Ubuntu [ssh_config](https://manpages.ubuntu.com/manpages/jammy/man5/ssh_config.5.html) man page\\n\\nDebugging TLS/SSL connections and protocols can be daunting due to their complexity. Here are some troubleshooting\\ntips.\\n## **Separate client and server**\\n\\nWhenever testing TLS/SSL connections over the network, it’s best to really separate the client and the server. Remember that the crypto library configuration file is read by the library, not just by a server or a client. It’s read by\\nboth. Therefore having separate systems acting as clients and servers, with their own configuration files, makes things\\nsimpler to analyse.\\n## **Tools**\\n\\nHere are some tools to help troubleshooting a TLS/SSL configuration.\\n\\n**OpenSSL server and client apps**\\n\\nThe OpenSSL server and client tools are very handy to quickly bring up a server with a selection of ciphers and',\n",
       " 'protocols and test it with a client. Being part of OpenSSL, these tools will also initialize the library defaults directly\\nfrom the OpenSSL config file, so they are very useful to test your configuration changes.\\n\\nTo bring up an OpenSSL server, a certificate with a private key is needed. There are many ways to generate a pair,\\nand here is a quick one:\\n\\n$ openssl req -new -x509 -nodes -days 30 -out myserver.pem -keyout myserver.key\\n\\nAnswer the questions as you prefer, but the one that needs special attention is the *commonName* ( *CN* ) one, which\\nshould match the hostname of this server. Then bring up the OpenSSL server with this command:\\n\\n$ openssl s_server -cert myserver.pem -key myserver.key\\n\\nThat will bring up a TLS/SSL server on port 4433. Extra options that can be useful:\\n\\n - -port N : Set a port number. Remember that ports below 1024 require root privileges, so use sudo if that’s the\\n\\ncase.\\n\\n - -www : Will send back a summary of the connection information, like ciphers used, protocols, etc.\\n\\n - -tls1_2, -tls1_3, -no_tls1_3, -no_tls1_2 : Enable only the mentioned protocol version, or, with the no_ prefix\\nvariant, disable it.\\n\\n - -cipher <string> : Use the specified cipher string for TLS1.2 and lower.\\n\\n - -ciphersuite <string> : Use the specified string for TLS1.3 ciphers.\\n\\nThe client connection tool can be used like this when connecting to server :\\n\\n$ echo | openssl s_client -connect server:port 2>&1 | grep ^New\\n\\nThat will generally show the TLS version used, and the selected cipher:\\n\\n349\\n\\n\\n-----\\n\\n$ echo | openssl s_client -connect j-server.lxd:443 2>&1 | grep ^New\\n\\nNew, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384',\n",
       " 'The ciphers and protocols can also be selected with the same command line options as the server:\\n\\n$ echo | openssl s_client -connect j-server.lxd:443 -no_tls1_3 2>&1 | grep ^New\\n\\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES256-GCM-SHA384\\n\\n$ echo | openssl s_client -connect j-server.lxd:443 -no_tls1_3 2>&1 -cipher DEFAULT:-AES256 | grep ^New\\n\\nNew, TLSv1.2, Cipher is ECDHE-RSA-CHACHA20-POLY1305\\n\\n**The** sslscan **tool**\\n\\nThe sslscan tool comes from a package with the same name, and it will scan a server and list the supported algorithms\\nand protocols. It’s super useful for determining if your configuration has really disabled or enabled a particular cipher\\nor TLS version.\\n\\nTo use the tool, point it at the server you want to scan:\\n\\n$ sslscan j-server.lxd\\n\\n[And you will get a report of the ciphers and algorithms supported by that server. Consult its manpage for more](https://manpages.ubuntu.com/manpages/jammy/man1/sslscan.1.html)\\ndetails.\\n# **References**\\n\\n[• OpenSSL s_server](https://manpages.ubuntu.com/manpages/kinetic/en/man1/openssl-s_server.1ssl.html)\\n\\n[• OpenSSL s_client](https://manpages.ubuntu.com/manpages/kinetic/en/man1/openssl-s_client.1ssl.html)\\n\\n[• sslscan](https://manpages.ubuntu.com/manpages/jammy/man1/sslscan.1.html)\\n\\n[• https://badssl.com: excellent website that can be used to test a client against a multitude of certificates. algo-](https://badssl.com/)\\nrithms, key sizes, protocol versions, and more.\\n\\nLet’s take a look at some of the major tools and technologies available in the Ubuntu virtualization stack, in order of\\nincreasing abstraction.\\n\\n**KVM**\\n\\n**Abstraction layer** : Hardware virtualization',\n",
       " '[Kernel-Based Virtual Machine (KVM) is a Linux kernel module that enables hardware-assisted virtualization. It is](https://www.linux-kvm.org/page/Main_Page)\\nthe default virtualization technology supported by Ubuntu.\\n\\nFor Intel and AMD hardware, KVM requires virtualization extensions in order to run. KVM is also available for IBM\\nZ and LinuxONE, IBM POWER, and ARM64.\\n## **QEMU**\\n\\n**Abstraction layer** : Emulation\\n\\n[Quick Emulator (QEMU) is a versatile and powerful open source machine emulator. It emulates complete virtual](https://www.qemu.org/)\\nmachines, which allows users to run machines with different operating systems than the underlying host system –\\nwithout needing to purchase dedicated hardware.\\n\\nQEMU primarily functions as the user-space backend for KVM. When used in collaboration with KVM kernel components, it harnesses the hardware virtualization capability that KVM provides in order to efficiently virtualize guests.\\n\\n[It also has a command line interface and a monitor for interacting with running guests. However, these are typically](https://qemu-project.gitlab.io/qemu/system/invocation.html)\\nonly used for development purposes.\\n\\nTo find out how to get started with QEMU quickly, check out this guide on how to set up QEMU.\\n## **libvirt**\\n\\n**Abstraction layer** : API and toolkit\\n\\n[libvirt provides an abstraction layer away from specific versions and hypervisors, giving users a command-line toolkit](https://libvirt.org/)\\nand API for managing virtualizations.\\n\\nBy providing an abstraction away from the underlying technologies (such as QEMU/KVM), libvirt makes it possible to',\n",
       " 'manage all kinds of virtual resources – across different platforms and hypervisors – using one single, common interface.\\nThis can greatly simplify administration and automation tasks.\\n\\n350\\n\\n\\n-----\\n\\nFor details of how to get libvirt set up and the basics of how to use it, see this guide on how to use libvirt.\\n## **Multipass and UVtool**\\n\\n**Abstraction layer** : User-friendly, CLI-based VM management\\n\\n[Multipass and UVtool provide an abstraction layer away from libvirt, using command-line interfaces to simplify VM](https://multipass.run/install)\\nmanagement. Both Multipass and UVtool are widely used in development and testing; they are lightweight and\\nstraightforward to use, and can greatly simplify the process of creating and managing VMs.\\n\\nUVtool is essentially a wrapper around libvirt, providing an additional abstraction layer to simplify its use. Multipass\\nis not based on libvirt, but can be integrated with it. This means that both tools can be used as part of a virtualization\\n“stack” based around QEMU and libvirt.\\n\\nIf you want to get started with either of these tools, you can see our guides on how to use Multipass or how to use\\nUVtool.\\n## **virt-manager**\\n\\n**Abstraction layer** : GUI-based VM management\\n\\n[Virt-manager, the Virtual Machine Manager, provides another high-level way to manage VMs. Like UVtool, virt-](https://virt-manager.org/)\\nmanager uses libvirt on the backend. However, unlike UVtool, its abstraction is presented in the form of a graphical\\nuser interface (GUI).\\n\\nAlthough in many ways this makes virt-manager easier to use than Multipass and UVtool, it also introduces more\\ncomplex tooling that supports more advanced users.',\n",
       " \"To get started with virt-manager, this how-to guide showcases all the basic functionality and tooling.\\n\\nMicroVMs are a special case of virtual machines (VMs), which were designed to be used in a container-like way to\\nprovide better isolation than containers, but which are optimised for initialisation speed and minimal resource use.\\n\\nBecause they are so lightweight, they are particularly useful in dynamic workload situations where demands change\\nrapidly and new resources need to be quickly provisioned or de-provisioned to meet those demands.\\n\\nThey are also useful in situations where resources are limited (e.g. in IoT devices), or where the cost of using resources\\nis a factor, thanks to their small footprint and overall efficiency.\\n## **QEMU microVMs**\\n\\nQEMU provides additional components that were added to support this special use case:\\n\\n1. The microvm [machine type](https://www.qemu.org/docs/master/system/i386/microvm.html)\\n2. Alternative simple firmware (FW) that can boot linux called [qboot](https://github.com/bonzini/qboot)\\n3. QEMU build with reduced features matching these use cases called qemu-system-x86-microvm (we will call this\\n”minimised qemu ”)\\n## **Basic command**\\n\\nAs an example, if you happen to already have a stripped-down workload that has all it would execute contained in an\\ninitrd, you might run it like this:\\n\\nsudo qemu-system-x86_64 \\\\\\n\\n-M ubuntu-q35 \\\\\\n\\n-cpu host \\\\\\n\\n-m 1024 \\\\\\n\\n-enable-kvm \\\\\\n\\n-serial mon:stdio \\\\\\n\\n-nographic \\\\\\n\\n-display curses \\\\\\n\\n-append 'console=ttyS0,115200,8n1' \\\\\\n\\n-kernel vmlinuz-5.4.0-21 \\\\\\n\\n-initrd /boot/initrd.img-5.4.0-21-workload\\n\\n**The** microvm **case**\",\n",
       " \"To run the same basic command with microvm you would run it with with type microvm, so we change -M to -M microvm .\\n\\nOur command then becomes:\\n\\n351\\n\\n\\n-----\\n\\nsudo qemu-system-x86_64 \\\\\\n\\n-M microvm ubuntu-q35 \\\\\\n\\n-cpu host \\\\\\n\\n-m 1024 \\\\\\n\\n-enable-kvm \\\\\\n\\n-serial mon:stdio \\\\\\n\\n-nographic \\\\\\n\\n-display curses \\\\\\n\\n-append 'console=ttyS0,115200,8n1' \\\\\\n\\n-kernel vmlinuz-5.4.0-21 \\\\\\n\\n-initrd /boot/initrd.img-5.4.0-21-workload\\n\\n**The** qboot **case**\\n\\nTo run the basic command with qboot instead, we would use the qboot bios by adding -bios /usr/share/qemu/bios\\nmicrovm.bin .\\n\\nsudo qemu-system-x86_64 \\\\\\n\\n-M ubuntu-q35 \\\\\\n\\n-cpu host \\\\\\n\\n-m 1024 \\\\\\n\\n-enable-kvm \\\\\\n\\n-serial mon:stdio \\\\\\n\\n-nographic \\\\\\n\\n-display curses \\\\\\n\\n-append 'console=ttyS0,115200,8n1' \\\\\\n\\n-kernel vmlinuz-5.4.0-21 \\\\\\n\\n-initrd /boot/initrd.img-5.4.0-21-workload \\\\\\n\\n-bios /usr/share/qemu/bios-microvm.bin\\n\\n**The minimised** qemu **case**\\n\\nTo run the the basic command instead using the minimised qemu, you would first need to install the feature-minimised\\n\\nqemu-system package, with:\\n\\nsudo apt install qemu-system-x86-microvm\\n\\nThen, our basic command will look like this:\\n\\nsudo qemu-system-x86_64 \\\\\\n\\n-M microvm \\\\\\n\\n-bios /usr/share/qemu/bios-microvm.bin \\\\\\n\\n-cpu host \\\\\\n\\n-m 1024 \\\\\\n\\n-enable-kvm \\\\\\n\\n-serial mon:stdio \\\\\\n\\n-nographic \\\\\\n\\n-display curses \\\\\\n\\n-append 'console=ttyS0,115200,8n1' \\\\\\n\\n-kernel vmlinuz-5.4.0-21 \\\\\\n\\n-initrd /boot/initrd.img-5.4.0-21-workload\\n\\nThis will cut down the qemu, bios and virtual-hw initialisation time a lot. You will now – more than you already were\\nbefore – spend the majority of time inside the guest, which implies that further tuning probably has to go into that\\nkernel and user-space initialisation time.\",\n",
       " '## **Further considerations**\\n\\nFor now, microvm, the qboot BIOS, and other components of this are rather new upstream and not as verified as many\\nother parts of the virtualisation stack.\\n\\nTherefore, none of the above options are the default. Being the default would mean many upgraders would regress\\nupon finding a QEMU that doesn’t have most of the features they are accustomed to using.\\n\\nBecause of that the qemu-system-x86-microvm package (the minimised qemu option) is intentionally a strong opt-in that\\nconflicts with the normal qemu-system-x86 package.\\n\\n352\\n\\n\\n-----\\n\\nUpgrading the machine type of a virtual machine (VM) can be thought of in the same way as buying (virtual) hardware\\nof the same spec but with a newer release date. Whereas to upgrade a physical machine you might buy an improved\\nCPU, more RAM, or increased storage, with a virtual machine you can change the configuration to achieve the same\\nresults.\\n## **Why should you do this for a VM?**\\n\\nThere are several reasons why you might want to update the machine type of an existing VM. For example, to:\\n\\n  - Improve performance with additional computing power\\n\\n - Add a virtual GPU\\n\\n  - Scale up the allocated resources to cope with increased workloads\\n\\n  - Obtain the latest security fixes and features\\n\\n  - Continue using a guest created on a now-unsupported release\\n\\n  - Prepare for future expansion by upgrading in advance\\n## **How does this work?**\\n\\nIt is generally recommended to update machine types when upgrading QEMU/KVM to a new major version. However,\\nthis can likely never be an automated task as the change is “guest visible”; the guest devices might change in appearance,',\n",
       " \"new features will be announced to the guest, and so on.\\n\\nLinux is usually very good at tolerating such changes – but, it depends so heavily on the setup and workload of the\\nguest that this has to be evaluated by the owner/admin of the system.\\n\\nOther operating systems were known to often be severely impacted by changing the hardware. Consider a machine\\ntype change as similar to replacing all devices and firmware of a physical machine to the latest revision. **All** of the\\nconsiderations that apply to firmware upgrades apply to evaluating a machine type upgrade as well.\\n## **Backing up guest definitions**\\n\\nAs usual, with major configuration changes it is wise to back up your guest definition and disk state to be able to do\\na rollback – just in case something goes wrong.\\n## **Upgrade the machine type**\\n\\nThere is no integrated single command to update the machine type via virsh or similar tools. It is a normal part of\\nyour machine definition, and therefore updated the same way as most others.\\n\\n**Shut down the VM**\\n\\nFirst shutdown your machine and wait until it has reached that state:\\n\\nvirsh shutdown <your_machine>\\n\\nYou can check the status of the machine with the following command:\\n\\nvirsh list --inactive\\n\\n**Edit the guest definition**\\n\\nOnce the machine is listed as “shut off”, you can then edit the machine definition and find the type in the type tag\\ngiven at the machine attribute.\\n\\nvirsh edit <your_machine>\\n\\n<type arch='x86_64' machine='pc-i440fx-bionic'>hvm</type>\\n\\nChange this to the value you want. If you need to check what machine types are available via the kvm -M ? command\",\n",
       " 'first, then note that while upstream types are provided for convenience, only Ubuntu types are supported. There you\\ncan also see what the current default would be, as in this example:\\n\\n$ kvm -M ?\\n\\npc-i440fx-xenial Ubuntu 16.04 PC (i440FX + PIIX, 1996) (default)\\n\\n...\\n\\npc-i440fx-bionic Ubuntu 18.04 PC (i440FX + PIIX, 1996) (default)\\n\\n...\\n\\nWe strongly recommend that you change to newer types (if possible), not only to take advantage of newer features,\\nbut also to benefit from bug fixes that only apply to the newer device virtualisation.\\n\\n353\\n\\n\\n-----\\n\\n**Restart the guest**\\n\\nAfter this you can start your guest again. You can check the current machine type from guest and host depending on\\nyour needs.\\n\\nvirsh start <your_machine>\\n\\n# check from host, via dumping the active xml definition\\n\\nvirsh dumpxml <your_machine> | xmllint --xpath \"string(//domain/os/type/@machine)\" \\n# or from the guest via dmidecode (if supported)\\n\\nsudo dmidecode | grep Product -A 1\\n\\nProduct Name: Standard PC (i440FX + PIIX, 1996)\\n\\nVersion: pc-i440fx-bionic\\n\\nIf you keep non-live definitions around – such as .xml files – remember to update those as well.\\n## **Further reading**\\n\\n[• This process is also documented along with some more constraints and considerations at the Ubuntu Wiki](https://wiki.ubuntu.com/QemuKVMMigration#Upgrade_machine_type)\\n\\nLet’s take a look at some of the most commonly used tools and technologies available in the Ubuntu container space.\\n## **LXC**\\n\\n**Container type** : System containers\\n\\n[Linux Containers, or LXC (pronounced “lex-see”), is a program that creates and administers containers on your](https://linuxcontainers.org/)\\nlocal system.',\n",
       " 'It is the foundation of several other system container technologies and provides both an API (to allow\\nhigher-level managers like LXD to administer containers), and an interface through which the user can interact with\\nkernel containment features (often called the “userspace interface”). LXC interacts directly with the kernel to isolate\\nprocesses, resources, etc, and provides the necessary tools - and a container runtime - for creating and managing\\nsystem containers.\\n\\nTo get started with LXC containers, check out our how-to guide.\\n## **LXD**\\n\\n**Container type** : System containers\\n\\n[The Linux Containers Daemon, or LXD (pronounced “lex-dee”) is the lightervisor, or lightweight container hypervisor.](https://ubuntu.com/lxd)\\nIt is a system container management tool built on top of LXC. Since it is an abstraction layer away from LXC it offers\\na more user-friendly interface, including both a REST API and a command-line interface. The LXD API deals with\\n“remotes”, which serve images and containers. In fact, it comes with a built-in image store, so that containers can be\\ncreated more quickly.\\n\\nTo get started with LXD from an Ubuntu Server administrator’s point of view, check out our how to get started with\\n[LXD guide. For a more general beginner’s introduction to LXD, we recommend this tutorial from the LXD team.](https://documentation.ubuntu.com/lxd/en/latest/tutorial/)\\n\\n[In addition to creating and managing containers, LXD can also be used to create virtual machines.](https://documentation.ubuntu.com/lxd/en/latest/howto/instances_create/#launch-a-virtual-machine)\\n## **Docker**\\n\\n**Container type** : Application containers',\n",
       " 'Docker is one of the most popular containerization platforms, which allows developers to package applications together with their dependencies - into lightweight containers. This provides a consistently reproducible environment\\nfor deploying applications, which makes it easy to build, ship, and run them even in different environments. Docker\\nincludes a command-line interface and a daemon to create and manage containers.\\n\\nAlthough Docker is widely used by developers, it can also be used by system administrators to manage resources and\\napplications. For instance, by encapsulating applications (and their libraries and dependencies) in a single package,\\nand providing version control, deployment of software and updates can be simplified. It also helps to optimise resource\\nuse - particularly through its alignment with microservices architecture.\\n\\nTo get started with Docker from a system administrator’s point of view, check out our Docker guide for sysadmins.\\n\\nOpenStack is the most popular open source cloud computing platform that enables the management of distributed\\ncompute, network and storage resources in the data centre.\\n\\nWhile the reference virtualisation stack (consisting of QEMU/KVM and libvirt) enables hardware virtualisation and\\nthe management of virtual machines (VMs) on a single host, in most cases the computing, network and storage\\nresources are distributed across multiple hosts in the data centre.\\n\\n354\\n\\n\\n-----\\n\\nThis creates an obvious challenge with centralised management of those resources, scheduling VMs, etc. OpenStack\\nsolves this problem by aggregating distributed pools of resources, allocating them to VMs on-demand and enabling',\n",
       " 'automated VM provisioning through a self-service portal.\\n\\nOpenStack consists of the following primary components:\\n\\n - **Keystone** :\\nServes as an identity service, providing authentication and authorisation functions for the users and enabling\\nmulti-tenancy.\\n\\n - **Glance** :\\n\\nThis is an image service, responsible for uploading, managing and retrieving cloud images for VMs running on\\nOpenStack.\\n\\n - **Nova** :\\n\\nThis is the primary compute engine of OpenStack, responsible for VM scheduling, creation and termination.\\n\\n - **Neutron** :\\n\\nProvides network connectivity between VMs, enabling multi-VM deployments.\\n\\n - **Cinder** :\\n\\nThis is a storage component that is responsible for provisioning, management and termination of persistent block\\ndevices.\\n\\n - **Swift** :\\n\\nThis is another storage component that provides a highly available and scalable object storage service.\\n\\nThere are also many other OpenStack components and supporting services available in the OpenStack ecosystem,\\nenabling more advanced functions, such as load balancing, secrets management, etc.\\n## **OpenStack installation**\\n\\n[The most straightforward way to get started with OpenStack on Ubuntu is to use MicroStack since the entire instal-](https://microstack.run/docs/single-node)\\nlation process requires only 2 commands and takes around 20 minutes.\\n\\nApart from MicroStack, multiple different installation methods for OpenStack on Ubuntu are available. These include:\\n\\n[• OpenStack Charms](https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/)\\n\\n[• OpenStack Ansible](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/)',\n",
       " '[• Manual Installation](https://docs.openstack.org/install-guide/)\\n\\n[• DevStack](https://docs.openstack.org/devstack/latest/)\\n\\nThe primary function of a **web server** is to store, process and deliver **web pages** to clients. The clients communicate\\nwith the server by sending HTTP requests.\\n\\nClients, mostly via **web browsers**, request specific resources and the server responds with the content of that resource\\n(or an error message). The response is usually a web page in the form of HTML documents – which may include\\nimages, style sheets, scripts, and text.\\n## **URLs**\\n\\nUsers enter a Uniform Resource Locator (URL) to point to a web server by means of its Fully Qualified Domain Name\\n[(FQDN) and a path to the required resource. For example, to view the home page of the Ubuntu Web site a user will](https://www.ubuntu.com)\\nenter only the FQDN:\\n\\nwww.ubuntu.com\\n\\n[To view the community sub-page, a user will enter the FQDN followed by a path:](https://www.ubuntu.com/community)\\n\\nwww.ubuntu.com/community\\n## **Transfer protocols**\\n\\nThe most common protocol used to transfer web pages is the Hyper Text Transfer Protocol (HTTP). Protocols such\\nas Hyper Text Transfer Protocol over Secure Sockets Layer (HTTPS), and File Transfer Protocol (FTP), a protocol\\nfor uploading and downloading files, are also supported.\\n\\n355\\n\\n\\n-----\\n\\n**HTTP status codes**\\n\\nWhen accessing a web server, every HTTP request received is responded to with content and a HTTP status code.\\nHTTP status codes are three-digit codes, which are grouped into five different classes. The class of a status code can\\nbe quickly identified by its first digit:\\n\\n - **1xx** : *Informational*  - Request received, continuing process',\n",
       " ' - **2xx** : *Success*  - The action was successfully received, understood, and accepted\\n\\n - **3xx** : *Redirection*  - Further action must be taken in order to complete the request\\n\\n - **4xx** : *Client Error*  - The request contains bad syntax or cannot be fulfilled\\n\\n - **5xx** : *Server Error*  - The server failed to fulfill an apparently valid request\\n\\n[For more information about status codes, check the RFC 2616.](https://www.w3.org/Protocols/rfc2616/rfc2616-sec6.html#sec6.1.1)\\n## **Implementation**\\n\\nWeb servers are heavily used in the deployment of websites, and there are two different implementations:\\n\\n - **Static web server** : The content of the server’s response will be the hosted files “as-is”.\\n\\n - **Dynamic web server** : Consists of a web server plus additional software (usually an *application server* and a\\n*database* ).\\n\\nFor example, to produce the web pages you see in your web browser, the application server might fill an HTML\\ntemplate with contents from a database. We can therefore say that the content of the server’s response is\\ngenerated dynamically.\\n\\n[Squid is a proxy cache server which provides proxy and cache services for Hyper Text Transport Protocol (HTTP),](http://www.squid-cache.org/)\\nFile Transfer Protocol (FTP), and other popular network protocols.\\n\\nIt acts as an intermediary between web servers and clients. When a client sends a request for content, Squid fetches the\\ncontent from the web server and creates a local copy. Then, if a request is made again, it shows the local, cached copy\\ninstead of making another request to the web server. In this way, performance is improved and network bandwidth is\\noptimised.',\n",
       " 'It can also filter web traffic, helping to improve security.\\n## **Features**\\n\\nThe Squid proxy cache server scales from the branch office to enterprise level networks. It provides extensive, granular\\naccess controls, and monitoring of critical parameters via the Simple Network Management Protocol (SNMP).\\n\\nWhen selecting a computer system for use as a dedicated Squid caching proxy server, it is helpful to configure it with\\na large amount of physical memory as Squid maintains an in-memory cache for increased performance.\\n## **Caching**\\n\\nSquid can implement caching and proxying of Secure Sockets Layer (SSL) requests and caching of Domain Name\\nServer (DNS) lookups, and perform transparent caching. Squid also supports a wide variety of caching protocols,\\nsuch as Internet Cache Protocol (ICP), the Hyper Text Caching Protocol (HTCP), the Cache Array Routing Protocol\\n(CARP), and the Web Cache Coordination Protocol (WCCP).\\n\\nIf you would like to know how to install and configure your own Squid server, refer to our installation guide.\\n## **Further reading**\\n\\n[• The Squid website](http://www.squid-cache.org/)\\n\\n[• Ubuntu Wiki page on Squid.](https://help.ubuntu.com/community/Squid)\\n\\nVirtualization is a technology that allows you to create safe, isolated environments on your server. For developers,\\nthis often means creating self-contained sandboxes for development and testing that cannot negatively affect the host\\nmachine. For systems administrators, it allows resources to be scaled to meet changing demand, giving greater control\\nand flexibility in managing infrastructure.\\n\\nThe virtualization stack is made using layers of **abstraction** .',\n",
       " 'Each layer hides some of the complexity of the layer (or\\nlayers) beneath, presenting an increasingly high-level view of the technology. This makes the underlying technology\\nprogressively easier to understand and work with.\\n\\n356\\n\\n\\n-----\\n\\n## **Virtual machines**\\n\\nVirtual machines (VMs) are essentially computers-within-computers. Every VM includes its own operating system\\nand simulated resources, making it completely independent of the host machine (and any other VM). Although more\\nresource-intensive (and slower to boot) than a container, a VM provides strong isolation and reduces the need for\\nadditional hardware when running different operating system environments. To find out more, see this overview of\\nthe different VM tools and technologies available in the Ubuntu space.\\n## **Containers**\\n\\nContainers, on the other hand, are a more lightweight virtualization technology. They share the operating system\\nof the host machine, so they are much quicker to provision when demand for resources is high. They are often used\\nfor packaging and running applications, since they contain everything the application needs (including any required\\ndependencies and libraries). This ensures consistency across different environments. Containers come in two main\\nflavours: **system** containers, and **application** containers.\\n\\n**System containers**\\n\\nSystem containers simulate a full machine in a similar way to a VM. However, since containers run on the host kernel\\nthey don’t need to install an operating system, making them quick to start and stop. They are often used for separating\\n\\nuser spaces.\\n\\n**Application containers**',\n",
       " 'Application containers package all of the components needed for a specific application or process to run, including\\nany required dependencies and libraries. This means the application can run consistently, even across different environments, without running into problems of missing dependencies. Application containers are particularly useful for\\nrunning microservices.\\n\\nFor more details about container tools available in the Ubuntu space, take a look at this overview.\\n\\nNetworks consist of two or more devices, such as computer systems, printers, and related equipment, which are\\nconnected by either physical cabling or wireless links for the purpose of sharing and distributing information among\\nthe connected devices.\\n\\nIn this overview, we’ll take a look at some of the key principles involved in networks, and at some of the most popular\\ntools available to help you manage your networks.\\n## **Networking key concepts**\\n\\nIf you’re new to networking, our explanatory “Networking key concepts” section provides an overview of some important\\nconcepts. It includes detailed discussion of the popular network protocols: TCP/IP; IP routing; TCP and UDP; and\\nICMP.\\n## **Configuring networks**\\n\\nUbuntu ships with a number of graphical utilities to configure your network devices. Our explanatory guide on\\n“configuring networks” is geared toward server administrators focuses on managing your network on the command\\nline.\\n## **Network tools and services**\\n\\n**DHCP**\\n\\nThe Dynamic Host Configuration Protocol (DHCP) enables host computers to be automatically assigned settings from\\na server. To learn more about DHCP and how configuration works, we have an explanatory guide.',\n",
       " 'There are two DHCP servers available on Ubuntu. We have instructions on how to install its replacement, isc-kea\\n(available from 23.04 onwards).\\n\\n**Time synchronisation**\\n\\nSynchronising time over a network is handled by the Network Time Protocol (NTP). It is a networking protocol that\\nsyncronises time between all computers on a network to within a few milliseconds of Coordinated Universal Time\\n(UTC). This explanation guide will tell you more about time synchronisation.\\n\\nIn Ubuntu, time synchronisation is primarily handled by timedatectl and timesyncd, which are installed by default as\\npart of systemd . To find out how to configure this service, read our how-to guide.\\n\\n357\\n\\n\\n-----\\n\\nIf you want to set up a server to *provide* NTP information, then we have a guide on how to serve NTP using chrony .\\n\\n**The DPDK library**\\n\\n[The Data Plane Development Kit (DPDK) provides a set of libraries that accelerate packet processing workloads. If](https://www.dpdk.org/)\\nyou would like to find out more about DPDK and its use in Ubuntu, refer to our explanation page.\\n\\nOne popular piece of software that makes use of DPDK is Open vSwitch (OVS), which can be run inside a VM and\\nprovides access to all virtual machines in the server hypervisor layer. Check out our guide to find out how to use\\nDPDK with Open vSwitch.\\n## **Other networking functionality**\\n\\n - **Samba**\\n\\nIf you need to network together both Ubuntu and Microsoft machines, you will want to make use of Samba. To\\nget started, check out our introduction to Samba.\\n\\nComputer networks are often comprised of diverse systems. While operating a network made up entirely of Ubuntu',\n",
       " 'desktop and server computers would definitely be fun, some network environments require both Ubuntu and Microsoft\\nWindows systems working together in harmony.\\n\\n[This is where Samba comes in! Samba provides various tools for configuring your Ubuntu Server to share network](https://www.samba.org)\\nresources with Windows clients. In this overview, we’ll look at some of the key principles, how to install and configure\\nthe tools available, and some common Samba use cases.\\n## **Samba functionality**\\n\\nThere are several services common to Windows environments that your Ubuntu system needs to integrate with in\\norder to set up a successful network. These services share data and configuration details of the computers and users\\non the network between them, and can each be classified under one of three main categories of functionality.\\n\\n**File and printer sharing services**\\n\\nThese services use the Server Message Block (SMB) protocol to facilitate the sharing of files, folders, volumes, and the\\nsharing of printers throughout the network.\\n\\n - **File server**\\nSamba can be configured as a file server to share files with Windows clients - our guide will walk you through\\nthat process.\\n\\n - **Print server**\\nSamba can also be configured as a print server to share printer access with Windows clients, as detailed in this\\nguide.\\n\\n**Directory services**\\n\\nThese services share vital information about the computers and users of the network. They use technologies like the\\nLightweight Directory Access Protocol (LDAP) and Microsoft Active Directory.\\n\\n - **Microsoft Active Directory**\\nAn Active Directory domain is a collection of users, groups, or hardware components within a Microsoft Active',\n",
       " 'Directory network. This guide will show you how to set up your server as a member of an Active Directory\\ndomain.\\n\\n - NT4 Domain Controller *(deprecated)*\\nThis guide will show you how to configure your Samba server to appear as a Windows NT4-style domain\\ncontroller.\\n\\n - OpenLDAP backend *(deprecated)*\\nThis guide will show you how to integrate Samba with LDAP in Windows NT4 mode.\\n\\n**Authentication and access**\\n\\nThese services establish the identity of a computer or network user, and determine the level of access that should be\\ngranted to the computer or user. The services use such principles and technologies as file permissions, group policies,\\nand the Kerberos authentication service.\\n\\n - **Share access controls**\\n\\nThis article provides more details on controlling access to shared directories.\\n\\n358\\n\\n\\n-----\\n\\n - **AppArmor profile for Samba**\\nThis guide will briefly cover how to set up a profile for Samba using the Ubuntu security module, AppArmor.\\n\\nWeb servers are used to serve web pages requested by client computers. Clients typically request and view web pages\\nusing web browser applications such as Firefox, Opera, Chromium, or Internet Explorer.\\n\\nIf you’re new to web servers, see this page for more information on the key concepts.\\n## **Squid proxy server**\\n\\nSquid is a popular, open-source, proxy caching server that can help optimise network efficiency and improve response\\ntimes by saving local copies of frequently accessed content. Read more how to install a Squid server.\\n## **LAMP**\\n\\nLAMP installations (Linux + Apache + MySQL + PHP/Perl/Python) are a popular setup for Ubuntu servers. Linux',\n",
       " 'provides the operating system, while the rest of the stack is composed of a web server, a database server, and a\\nscripting language.\\n\\nOne advantage of LAMP is the substantial flexibility it provides for combining different web server, database, and\\nscripting languages. Popular substitutes for MySQL include PostgreSQL and SQLite. Python, Perl, and Ruby are\\nalso frequently used instead of PHP. Apache can be replaced by Nginx, Cherokee and Lighttpd.\\n\\nIn this documentation, we can show you how to get started with LAMP quickly, but also how to separately install\\nand configure some of the different tooling options in the classic LAMP stack.\\n\\n**Web server**\\n\\nApache is the most commonly used web server on Linux systems, and the current version is Apache2. It is robust,\\nreliable, and highly configurable. This set of guides will show you:\\n\\n  - How to install and configure Apache2\\n\\n  - How to configure Apache2 for your needs\\n\\n  - How to extend Apache2’s functionality with modules\\n\\nNginx is a popular alternative web server also widely used on Linux, with a focus on static file serving performance,\\nease of configuration, and use as both a web server and reverse proxy server.\\n\\n  - How to install Nginx\\n\\n  - How to configure Nginx\\n\\n  - How to use Nginx modules\\n\\n**Database server**\\n\\nThe database server, when included in the LAMP stack, allows data for web applications to be stored and managed.\\nMySQL is one of the most popular open source Relational Database Management Systems (RDBMS) available, and\\nyou can find out in this guide how to install MySQL – or PostgreSQL, as another popular alternative.\\n\\n**Scripting languages**',\n",
       " 'Server-side scripting languages allow for the creation of dynamic web content, processing of web forms, and interacting\\nwith databases (amongst other crucial tasks). PHP is most often used, and we can show you how to install PHP, or\\nif you prefer, we can show you how to install Ruby on Rails.\\n\\nWhichever scripting language you choose, you will need to have installed and configured your web and database servers\\nbeforehand.\\n\\n**LAMP applications**\\n\\nOnce your LAMP stack is up-and-running, you’ll need some applications to use with it. Some popular LAMP\\napplications include wikis, management software such as phpMyAdmin, and Content Management Systems (CMSs)\\nlike WordPress. These guides will show you how to install and configure phpMyAdmin and WordPress as part of your\\nLAMP stack.\\n\\nThe process of getting an email from one person to another over a network or the Internet involves many systems\\nworking together. Each of these systems must be correctly configured for the process to work.\\n\\nThe sender uses a *Mail User Agent* (MUA), or email client, to send the message through one or more *Mail Transfer*\\n*Agents* (MTA), the last of which will hand it off to a *Mail Delivery Agent* (MDA) for delivery to the recipient’s mailbox,\\nfrom which it will be retrieved by the recipient’s email client, usually via a POP3 or IMAP server.\\n\\n359\\n\\n\\n-----\\n\\n## **Mail User Agent**\\n\\n - **Thunderbird**\\n\\n[The default MUA used by Ubuntu is Thunderbird. It comes pre-installed on all Ubuntu machines from Ubuntu](https://www.thunderbird.net/)\\n16.04 LTS (Xenial) onwards.',\n",
       " '[If you need to install Thunderbird manually, this short guide will walk you through the steps.](https://snapcraft.io/install/thunderbird/ubuntu)\\n## **Mail Transfer Agent**\\n\\n - **Postfix**\\n[On Ubuntu, Postfix is the default supported MTA. It aims to be fast and secure, with flexibility in administration.](https://www.postfix.org/)\\n[It is compatible with the sendmail MTA.](https://www.authsmtp.com/sendmail/index.html)\\n\\nThis guide explains how to install and configure Postfix, including how to configure SMTP for secure communi\\ncations.\\n\\n - **Exim4**\\n\\n[Exim4 was developed at the University of Cambridge for use on Unix systems connected to the Internet. Exim](https://www.exim.org/)\\ncan be installed in place of sendmail, although its configuration is quite different.\\n\\nThis guide explains how to install and configure Exim4 on Ubuntu.\\n## **Mail Delivery Agent**\\n\\n - **Dovecot**\\n\\n[Dovecot is an MDA written with security primarily in mind. It supports the mbox and Maildir mailbox formats.](https://www.dovecot.org/)\\n\\nThis guide explains how to set up Dovecot as an IMAP or POP3 server.\\n\\nThere are many ways to back up an Ubuntu installation. The most important thing about backups is to develop a\\nbackup plan that consists of:\\n\\n  - What should be backed up\\n\\n  - How often to back it up\\n\\n  - Where backups should be stored\\n\\n  - How to restore your backups\\n\\nIt is good practice to take backup media off-site in case of a disaster. For backup plans involving physical tape or\\nremovable hard drives, the tapes or drives can be manually taken off-site. However, in other cases this may not be\\npractical and the archives will need to be copied over a WAN link to a server in another location.',\n",
       " '## **Backup options**\\n\\nOn Ubuntu, two of the primary ways of backing up your system are through **backup utilities** and **shell scripts** .\\nWherever possible, it’s better to build redundancy into your backup systems by combining backup methods so that\\nyou are not reliant on a single system.\\n\\n**Backup utilities**\\n\\n[The most straightforward way to create backups is through using a dedicated tool like Bacula or rsnapshot. These](http://www.bacula.org/)\\ntools offer specialised features such as automation, compression, data recovery, encryption and incremental/differential\\nbackups – but with an easy-to-use interface or CLI to help simplify the backup management process.\\n\\n - **Bacula**\\nThis tool uses incremental backups, which only store changes made since the last backup. This can significantly\\ndecrease the storage space and backup time required. It can also manage backups of multiple systems over a\\nnetwork. With more advanced features and support for additional customisation, it is often used by users with\\nmore complex needs (e.g. in enterprise environments).\\n\\n**–** Find out how to install and configure Bacula.\\n\\n - **rsnapshot** uses rsync to take periodic “snapshots” of your files, which makes it easier to access previous versions.\\nIt’s often used for local backups on a single system and is ideal for individual users or small-scale organisations\\nwho want a simpler and more efficient solution.\\n\\n**–** Find out how to install and configure rsnapshot.\\n\\n360\\n\\n\\n-----\\n\\n**Shell scripts**\\n\\nUsing shell scripts to manage your backups can be easy or complicated, depending on the complexity of your setup.',\n",
       " 'However, the advantage of shell scripts over using backup utilities, is that they offer full flexibility and customisation.\\nThrough backup shell scripts, you can fully tailor the backup process to your specific requirements without using\\nthird-party tools.\\n\\nRefer to this guide for instructions on how to use shell scripts for backups – or you can take a look at our reference\\nexamples:\\n\\n  - A basic backup shell script\\n\\n  - An example of archive rotation with shell scripts\\n\\nOur *reference* section is used for quickly checking what software and commands are available, and how to interact\\nwith various tools.\\n\\n**Reference**\\n\\nCloud images\\n\\nIntroduction\\n\\nAmazon EC2\\nGoogle Compute Engine (GCE)\\nMicrosoft Azure\\n\\nMultipath\\n\\nIntroduction\\nConfiguration\\nSetup\\nUsage and debug\\nSecurity\\n\\nIntroduction\\n\\nUsers\\n\\nSmart cards\\n\\nSSH\\n\\nAppArmor\\nFirewall\\nCertificates\\nCA trust store\\n\\nConsole\\n\\nHigh Availability\\n\\nIntroduction\\n\\nPacemaker - resource agents\\nPacemaker - fence agents\\nDistributed Replicated Block Device (DRBD)\\nUbuntu HA - Migrate from crmsh to pcs\\nDatabases\\n\\nIntroduction\\n\\nMySQL\\nPostgreSQL\\nMonitoring\\n\\nLogging, Monitoring and Alerting (LMA)\\nNagios and Munin\\nTools - Logwatch\\nOther Services\\n\\nCUPS\\n\\nDebuginfod\\nDebuginfod FAQ\\nDomain Name Service (DNS)\\nFTP\\n\\niSCSI\\n\\nNFS\\n\\nOpenSSH\\nOpenVPN\\ngitolite\\nVPN clients\\n\\nTools\\n\\nbyobu\\n\\n361\\n\\n\\n-----\\n\\n## **Backups**\\n\\n\\n**Reference**\\n\\netckeeper\\n\\nmunin\\n\\nnagios\\npam_motd\\nPuppet\\n\\nExample scripts\\n\\nBasic backup shell script\\nArchive rotation shell script\\n\\n\\nCanonical produces a variety of cloud-specific images, which are available directly via the clouds themselves, as well\\n[as on https://cloud-images.ubuntu.com.](https://cloud-images.ubuntu.com)\\n## **Public clouds**',\n",
       " '**Compute offerings**\\n\\nUsers can find Ubuntu images for virtual machines and bare-metal offerings published directly to the following clouds:\\n\\n Amazon Elastic Compute Cloud (EC2)\\n\\n Google Cloud Engine (GCE)\\n\\n - IBM Cloud\\n\\n  - Microsoft Azure\\n\\n - Oracle Cloud\\n\\n**Container offerings**\\n\\nUbuntu images are also produced for a number of container offerings:\\n\\n  - Amazon Elastic Kubernetes Service (EKS)\\n\\n - Google Kubernetes Engine (GKE)\\n## **Private clouds**\\n\\n[On cloud-images.ubuntu.com, users can find standard and minimal images for the following:](https://cloud-images.ubuntu.com)\\n\\n - Hyper-V\\n\\n - KVM\\n\\n - OpenStack\\n\\n - Vagrant\\n\\n - VMware\\n## **Release support**\\n\\n[Cloud images are published and supported throughout the lifecycle of an Ubuntu release. During this time images](https://ubuntu.com/about/release-cycle)\\ncan receive all published security updates and bug fixes.\\n\\nFor users wanting to upgrade from one release to the next, the recommended path is to launch a new image with the\\ndesired release and then migrate any workload or data to the new image.\\n\\nSome cloud image customisation must be applied during image creation, which would be missing if an in-place upgrade\\nwere performed. For that reason, in-place upgrades of cloud images are not recommended.\\n\\nAmazon Web Service’s Elastic Compute Cloud (EC2) provides a platform for deploying and running applications.\\n## **Images**\\n\\nOn EC2, cloud images are referred to as Amazon Machine Images (AMIs). Canonical produces a wide variety of\\nimages to support numerous features found on EC2:\\n\\n  - Generally, all images use Elastic Block Storage (EBS) and HVM virtualisation types. Older releases may also',\n",
       " 'support PV and instance-store, but users benefit from the newer storage and virtualisation technologies.\\n\\n362\\n\\n\\n-----\\n\\n  - Standard server images, as well as minimal images for amd64, and arm64 images for the standard server set.\\n\\n  - Daily (untested) and release images are published.\\n\\n**Find images with SSM**\\n\\nThe AWS Systems Manager (SSM) agent is used by Canonical to store the latest AMI release versions for EC2. This\\nprovides users with a programmatic method of querying for the latest AMI ID.\\n\\nCanonical stores SSM parameters under /aws/service/canonical/ . To find the latest AMI ID, users can use the AWS\\nCLI:\\n\\naws ssm get-parameters --names \\\\\\n\\n/aws/service/canonical/ubuntu/server/20.04/stable/current/amd64/hvm/ebs-gp2/ami-id\\n\\nThe path follows this format:\\n\\nubuntu/$PRODUCT/$RELEASE/stable/current/$ARCH/$VIRT_TYPE/$VOL_TYPE/ami-id\\n\\n - PRODUCT: server or server-minimal\\n\\n - RELEASE: focal, 20.04, bionic, 18.04, xenial, or 16.04\\n\\n - ARCH: amd64 or arm64\\n\\n - VIRT_TYPE: pv or hvm\\n\\n - VOL_TYPE: ebs-gp2, ebs-io1, ebs-standard, or instance-store\\n\\nThe given serial for an image (e.g., 20210222) is also uploaded in place of current:\\n\\nubuntu/$PRODUCT/$RELEASE/stable/$SERIAL/$ARCH/$VIRT_TYPE/$VOL_TYPE/ami-id\\n\\n[For more details on SSM check out this Discourse thread.](https://discourse.ubuntu.com/t/finding-ubuntu-images-with-the-aws-ssm-parameter-store/15507)\\n\\n**Ownership verification**\\n\\nUsers can verify that an AMI was published by Canonical by ensuring the OwnerId field of an image is 099720109477\\n(in the default partition). For the GovCloud partition, the OwnerId field is 513442679011 . For the China partition, the\\n\\nOwnerId field is 837727238323 .',\n",
       " 'This ID is stored in SSM and is discoverable by running:\\n\\naws ssm get-parameters --names /aws/service/canonical/meta/publisher-id\\n\\nWith the value returned by that command, users can then run the describe-images command against an AMI ID and\\nverify the OwnerId field matches the above ID:\\n\\naws ec2 describe-images --image-ids $AMI_ID\\n\\nNote that listings on the AWS Marketplace will always show the OwnerId as Amazon (e.g. 679593333241 ). In these\\ncases, users can verify the Amazon ID and look for aws-marketplace/ubuntu in the ImageLocation field.\\n\\n**Image locator**\\n\\n[Canonical also produces an Ubuntu Cloud Image Finder where users can filter down based on a variety of criteria](https://cloud-images.ubuntu.com/locator/)\\nsuch as region or release, etc.\\n## **AWS EKS**\\n\\nAmazon’s Elastic Kubernetes Service (EKS) is a managed Kubernetes service provided by AWS that lets users run\\nKubernetes applications in the cloud or on-premises.\\n\\n[Canonical provides minimized Ubuntu images customised for use with EKS. These are fully tested release images that](https://cloud-images.ubuntu.com/docs/aws/eks/)\\ncover all Kubernetes versions supported by the EKS service.\\n\\nThe latest EKS AMI ID can be found in the SSM parameter store:\\n\\naws ssm get-parameters --names /aws/service/canonical/ubuntu/eks/20.04/1.21/stable/current/amd64/hvm/ebs\\ngp2/ami-id\\n\\nThe path follows this format:\\n\\nubuntu/eks/$RELEASE/$K8S_VERSION/stable/current/$ARCH/$VIRT_TYPE/$VOL_TYPE/ami-id\\n\\nFor newer Kubernetes versions (>= 1.20), there are also *arm64* images available (in addition to the *amd64* images).\\n\\n363\\n\\n\\n-----\\n\\n## **AWS Marketplace**',\n",
       " 'AWS Marketplace is a digital catalogue with thousands of software listings from independent software vendors that\\nmake it easy to find, test, buy, and deploy software that runs on AWS.\\n\\n[Canonical maintains image listings for recent Ubuntu releases on AWS Marketplace, including images in minimal and](https://aws.amazon.com/marketplace/seller-profile?id=565feec9-3d43-413e-9760-c651546613f2)\\narm64 flavors.\\n\\nThose images can also be found in the SSM parameter store:\\n\\naws ssm get-parameter --name /aws/service/marketplace/<identifier>/latest\\n\\nCustomers can also use the AWS Marketplace to launch and subscribe to official Ubuntu Pro images that allow users\\nto pay for additional support. These Ubuntu Pro images can also be found in the SSM parameter store.\\n\\nNot all releases are in GovCloud at this time (a list of available products is shown below). To query GovCloud, be\\nsure to use a GovCloud region and credentials:\\n\\naws --region us-gov-west-1 --profile govcloud ssm get-parameter --name /aws/service/marketplace/<identifier>/latest\\n\\nPlease use this chart to find the product identifier and availability:\\n\\nName Architecture identifier GovCloud\\n\\nUbuntu 20.04 LTS amd64 prod-x7h6cigkuiul6\\n\\nUbuntu 20.04 LTS arm64 prod-gprrntd234sfc\\n\\nMinimal Ubuntu 20.04 LTS amd64 prod-df2jln3gjtwps\\n\\nMinimal Ubuntu 20.04 LTS arm64 prod-emtsb6upxf6us\\n\\nUbuntu 22.04 LTS amd64 prod-lfutkwiaknxsk\\n\\nUbuntu 22.04 LTS arm64 prod-amd2rg3s3i7tc\\n\\n364\\n\\n\\n-----\\n\\nName Architecture identifier GovCloud\\n\\nMinimal Ubuntu 22.04 LTS amd64 prod-smq32swynllqw\\n\\nMinimal Ubuntu 22.04 LTS arm64 prod-zvdejcufto6ps\\n\\nUbuntu 23.04 amd64 prod-5k5cl7gpbkr26\\n\\nUbuntu 23.04 arm64 prod-ad4brywknigv4',\n",
       " 'Minimal Ubuntu 23.04 amd64 prod-4q2ov7d2gozxk\\n\\nMinimal Ubuntu 23.04 arm64 prod-wq7aqb3abhaay\\n\\nUbuntu 23.10 amd64 prod-tx7uupluohrfk\\n\\nUbuntu 23.10 arm64 prod-2ajtlwthyvpc6\\n\\nMinimal Ubuntu 23.10 amd64 prod-segy4h4e5x5zq\\n\\n365\\n\\n\\n-----\\n\\nName Architecture identifier GovCloud\\n\\nMinimal Ubuntu 23.10 arm64 prod-xxfuncfabpasi\\n\\nUbuntu Pro FIPS 16.04 LTS amd64 prod-hykkbajyverq4\\n\\nUbuntu Pro FIPS 18.04 LTS amd64 prod-7izp2xqnddwdc\\n\\nUbuntu Pro FIPS 20.04 LTS amd64 prod-k6fgbnayirmrc\\n\\nUbuntu Pro 14.04 LTS amd64 prod-7u42cjnp5pcuo\\n\\nUbuntu Pro 16.04 LTS amd64 prod-f6ogoaqs7lwre\\n\\nUbuntu Pro 18.04 LTS amd64 prod-jlgu4232gpnwa\\n\\nUbuntu Pro 20.04 LTS amd64 prod-3sk4unyn4iwqu\\n\\nUbuntu Pro 22.04 LTS amd64 prod-uwytposjsg3du\\n\\n366\\n\\n\\n-----\\n\\nName Architecture identifier GovCloud\\n\\nUbuntu 20.04 LTS for EKS 1.23 amd64 prod-d6nvbzhhqtsnc\\n\\nUbuntu 20.04 LTS for EKS 1.23 arm64 prod-w4kdatfulcfk2\\n\\nUbuntu 20.04 LTS for EKS 1.24 amd64 prod-q56ww7nfnmv72\\n\\nUbuntu 20.04 LTS for EKS 1.24 arm64 prod-pikds4f276lq6\\n\\nUbuntu 20.04 LTS for EKS 1.25 amd64 prod-h232pdamfnj5w\\n\\nUbuntu 20.04 LTS for EKS 1.25 arm64 prod-exeixbb2fbrog\\n\\nUbuntu 20.04 LTS for EKS 1.26 amd64 prod-paebp5ekgg6uu\\n\\nUbuntu 20.04 LTS for EKS 1.26 arm64 prod-lrqczh7vqe7ve\\n\\nUbuntu 20.04 LTS for EKS 1.27 amd64 prod-ldmift6l2jtbk\\n\\n367\\n\\n\\n-----\\n\\nName Architecture identifier GovCloud\\n\\nUbuntu 20.04 LTS for EKS 1.27 arm64 prod-rmbj4hjbxq3s4\\n\\nUbuntu 20.04 LTS for EKS 1.28 amd64 prod-liwkeak7e3q7e\\n\\nUbuntu 20.04 LTS for EKS 1.28 arm64 prod-lnp3hmtlqxkes\\n\\nGoogle Cloud Platform lets you build and host applications and websites, store data, and analyse data on Google’s\\nscalable infrastructure.\\n## **Images**',\n",
       " 'On GCE, Canonical produces standard server and minimal images for all supported releases.\\n\\n**Finding images**\\n\\nUsers can find the latest Ubuntu images on the GCE UI by selecting “Ubuntu” as the Operating System under the\\n‘Boot Disk’ settings.\\n\\nFor a programmatic method, users can use the gcloud command to find the latest release images:\\n\\ngcloud compute images list --filter ubuntu-os-cloud\\n\\nDaily, untested images are found under the ubuntu-os-cloud-devel project:\\n\\ngcloud compute images --project ubuntu-os-cloud-devel list --filter ubuntu-os-cloud-devel\\n\\n**Image locator**\\n\\n[Canonical also produces an Ubuntu Cloud Image Finder where users can filter down based on a variety of criteria,](https://cloud-images.ubuntu.com/locator/)\\nsuch as region or release, etc.\\n## **Latest images**\\n\\n**Free Long Term Support offers**\\n\\n[To learn more about LTS versions of Ubuntu read this article.](https://ubuntu.com/blog/what-is-an-ubuntu-lts-release)\\n\\n**Ubuntu 22.04 LTS - Jammy Jellyfish**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-server-jammy22_04-lts-ARM)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\nArchitecture: **Arm64**\\n\\nHyper-V Generation: **Gen2**\\n\\n\\nCanonical:0001-com-ubuntu-server-jammy:22_04-lts:latest\\n\\nCanonical:0001-com-ubuntu-server-jammy:22_04-lts-gen2:latest\\n\\nCanonical:0001-com-ubuntu-server-jammy:22_04-lts-arm64:latest\\n\\n368\\n\\n\\n-----\\n\\n**Ubuntu 20.04 LTS - Focal Fossa**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-server-focal20_04-lts-ARM)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**',\n",
       " 'Hyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\nArchitecture: **Arm64**\\n\\nHyper-V Generation: **Gen2**\\n\\n**Interim releases**\\n\\n\\nCanonical:0001-com-ubuntu-server-focal:20_04-lts:latest\\n\\nCanonical:0001-com-ubuntu-server-focal:20_04-lts-gen2:latest\\n\\nCanonical:0001-com-ubuntu-server-focal:20_04-lts-arm64:latest\\n\\n\\n[To learn more about the difference between LTS releases and interim releases, see this page.](https://ubuntu.com/about/release-cycle#ubuntu)\\n\\n**Ubuntu 23.10 - Mantic Minotaur**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-server-mantic23_10-gen2)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\nArchitecture: **Arm64**\\n\\nHyper-V Generation: **Gen2**\\n\\n**Ubuntu 23.04 - Lunar Lobster**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-server-lunar23_04-gen2)\\n\\n\\nCanonical:0001-com-ubuntu-server-mantic:23_10:latest\\n\\nCanonical:0001-com-ubuntu-server-mantic:23_10-gen2:latest\\n\\nCanonical:0001-com-ubuntu-server-mantic:23_10-arm64:latest\\n\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\nArchitecture: **Arm64**\\n\\nHyper-V Generation: **Gen2**\\n\\n**Ubuntu Pro Offers**\\n\\n\\nCanonical:0001-com-ubuntu-server-lunar:23_04:latest\\n\\nCanonical:0001-com-ubuntu-server-lunar:23_04-gen2:latest\\n\\nCanonical:0001-com-ubuntu-server-lunar:23_04-arm64:latest\\n\\n\\n[To learn more about Ubuntu Pro on Azure: Ubuntu Pro for Azure](https://ubuntu.com/azure/pro)\\n\\n**Ubuntu Pro 22.04 LTS - Jammy Jellyfish**',\n",
       " '[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-pro-jammypro-22_04-lts)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n\\nCanonical:0001-com-ubuntu-pro-jammy:pro-22_04-lts:latest\\n\\nCanonical:0001-com-ubuntu-pro-jammy:pro-22_04-lts-gen2:latest\\n\\n\\n**Ubuntu Pro 20.04 LTS - Focal Fossa**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-pro-focalpro-20_04-lts)\\n\\n\\n369\\n\\n\\n-----\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n\\nCanonical:0001-com-ubuntu-pro-focal:pro-20_04-lts:latest\\n\\nCanonical:0001-com-ubuntu-pro-focal:pro-20_04-lts-gen2:latest\\n\\n\\n**Ubuntu Pro 18.04 LTS - Bionic Beaver**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-pro-bionicpro-18_04-lts)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n\\nCanonical:0001-com-ubuntu-pro-bionic:pro-18_04-lts:latest\\n\\nCanonical:0001-com-ubuntu-pro-bionic:pro-18_04-lts-gen2:latest\\n\\n\\n**Confidential Compute capable offers**\\n\\n[To learn more about Confidential Compute: Azure confidential computing](https://azure.microsoft.com/en-us/solutions/confidential-compute/#overview)\\n\\n**Ubuntu CVM 22.04 LTS - Jammy Jellyfish**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-confidential-vm-jammy22_04-lts-cvm)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**',\n",
       " 'Canonical:0001-com-ubuntu-confidential-vm-jammy:22_04-lts-cvm:latest\\n\\n\\n**Ubuntu CVM 20.04 LTS - Focal Fossa**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-confidential-vm-focal20_04-lts-cvm)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n**FIPS compliant offers**\\n\\n\\nCanonical:0001-com-ubuntu-confidential-vm-focal:20_04-lts-cvm:latest\\n\\n\\n[To learn more about FIPS: FIPS for Ubuntu](https://ubuntu.com/security/certifications/docs/fips)\\n\\n**Ubuntu Pro FIPS 20.04 LTS - Focal Fossa**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-pro-focal-fipspro-fips-20_04)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n\\nCanonical:0001-com-ubuntu-pro-focal-fips:pro-fips-20_04:latest\\n\\nCanonical:0001-com-ubuntu-pro-focal-fips:pro-fips-20_04-gen2:latest\\n\\n\\n**Ubuntu Pro FIPS 18.04 LTS - Bionic Beaver**\\n\\n[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-pro-bionic-fipspro-fips-18_04)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\n\\nCanonical:0001-com-ubuntu-pro-bionic-fips:pro-fips-18_04:latest\\n\\n370\\n\\n\\n-----\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n**CIS Hardened offer**\\n\\n\\nCanonical:0001-com-ubuntu-pro-bionic-fips:pro-fips-18_04-gen2:latest\\n\\n\\n[To learn more about CIS compliance: CIS compliance with Ubuntu LTS](https://ubuntu.com/security/certifications/docs/cis)\\n\\n**Ubuntu Minimal Pro CIS 20.04 LTS - Focal Fossa**',\n",
       " '[Quick Start: portal.azure.com](https://portal.azure.com/#create/canonical.0001-com-ubuntu-pro-minimal-cis-focalpro-cis-minimal-20_04)\\n\\n**Kind** **URN**\\n\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen1**\\n\\nArchitecture: **AMD64**\\n\\nHyper-V Generation: **Gen2**\\n\\n\\nCanonical:0001-com-ubuntu-pro-minimal-cis-focal:pro-cis-minimal-20_04:latest\\n\\nCanonical:0001-com-ubuntu-pro-minimal-cis-focal:pro-cis-minimal-20_04-gen2:latest\\n\\n## **List all images published by Canonical**\\n\\nCanonical publishes Ubuntu images under the Canonical publisher ID. One can find all our images using this az\\ncommand:\\n\\naz vm image list -p Canonical --all -o table\\n\\nVia the Portal, make sure to look for Canonical rather than Ubuntu to find the official Ubuntu images. Also, always\\nmake sure the offer is published by Canonical. **Non-Pro LTS offers are always FREE** .\\n\\nDevice mapper multipathing (which we will refer to as *multipath* ) allows you to configure multiple input/output (I/O)\\npaths between server nodes and storage arrays into a single device. These I/O paths are physical storage area network\\n(SAN) connections that can include separate cables, switches, and controllers.\\n\\nMultipathing aggregates the I/O paths, creating a new device that consists of those aggregated paths. This chapter\\nprovides an **introduction and a high-level overview of multipath** .\\n## **Benefits of multipath**\\n\\nMultipath can be used to provide:\\n\\n - **Redundancy**\\nMultipath provides failover in an active/passive configuration. In an active/passive configuration, only half the\\npaths are used at any time for I/O. If any element of an I/O path (the cable, switch, or controller) fails, multipath\\nswitches to an alternate path.',\n",
       " ' - **Improved performance**\\nMultipath can be configured in active/active mode, where I/O is spread over the paths in a round-robin fashion.\\nIn some configurations, multipath can detect loading on the I/O paths and dynamically re-balance the load.\\n## **Storage array overview**\\n\\nIt is a very good idea to consult your storage vendor’s installation guide for the recommended multipath configuration\\nvariables for your storage model. The default configuration will probably work but will likely need adjustments based\\non your storage setup.\\n## **Multipath components**\\n\\nComponent Description\\n\\ndm_multipath kernel module Reroutes I/O and supports *failover* for paths and path groups.\\n\\nmultipath command Lists and configures *multipath* devices. Normally started up with /etc/rc.sysinit, it can also be\\n\\nmultipathd daemon Monitors paths; as paths fail and come back, it may initiate path group switches. Provides for in\\n\\nkpartx command Creates device mapper devices for the partitions on a device. It is necessary to use this comman\\n\\n371\\n\\n\\n-----\\n\\n## **Multipath setup overview**\\n\\nSetting up multipath is often a simple procedure, since it includes compiled-in default settings that are suitable for\\ncommon multipath configurations. The basic procedure for configuring your system with multipath is as follows:\\n\\n1. Install the multipath-tools and multipath-tools-boot packages.\\n\\n2. Create an empty config file called /etc/multipath.conf .\\n\\n3. Edit the multipath.conf file to modify default values and save the updated file.\\n\\n4. Start the multipath daemon.\\n\\n5. Update initial RAM disk.\\n\\nFor detailed setup instructions for multipath configuration see DM-Multipath configuration and DM-Multipath setup.',\n",
       " '## **Multipath devices**\\n\\nWithout multipath, each path from a server node to a storage controller is treated by the system as a separate device,\\neven when the I/O path connects the same server node to the same storage controller. Multipath provides a way of\\norganizing the I/O paths logically, by creating a single device on top of the underlying paths.\\n\\n**Multipath device identifiers**\\n\\nEach multipath device has a World Wide Identifier (WWID), which is guaranteed to be globally unique and unchanging.\\nBy default, the name of a multipath device is set to its WWID. Alternatively, you can set the *user_friendly_names*\\noption in multipath.conf, which causes multipath to use a *node-unique* alias of the form *mpathn* as the name.\\n\\nFor example, a node with two host bus adapters (HBAs) attached to a storage controller with two ports via a single\\nunzoned FC switch sees four devices: /dev/sda, /dev/sdb, /dev/sdc, and /dev/sdd . Multipath creates a single device\\nwith a unique WWID that reroutes I/O to those four underlying devices according to the multipath configuration.\\n\\nWhen the *user_friendly_names* configuration option is set to ‘yes’, the name of the multipath device is set to *mpathn* .\\nWhen new devices are brought under the control of multipath, the new devices may be seen in two different places\\nunder the /dev directory: /dev/mapper/mpathn and /dev/dm-n .\\n\\n  - The devices in /dev/mapper are created early in the boot process. **Use these devices to access the multi-**\\n**pathed devices.**\\n\\n  - Any devices of the form /dev/dm-n are for **internal use only** and should never be used directly.',\n",
       " 'You can also set the name of a multipath device to a name of your choosing by using the *alias* option in the *multipaths*\\nsection of the multipath configuration file.\\n\\n**See also** :\\nFor information on the multipath configuration defaults, including the *user_friendly_names* and *alias*\\nconfiguration options, see DM-Multipath configuration.\\n\\n**Consistent multipath device names in a cluster**\\n\\nWhen the *user_friendly_names* configuration option is set to ‘yes’, the name of the multipath device is unique to a\\nnode, but it is not guaranteed to be the same on all nodes using the multipath device. Similarly, if you set the *alias*\\noption for a device in the *multipaths* section of /etc/multipath.conf, the name is not automatically consistent across\\nall nodes in the cluster.\\n\\nThis should not cause any difficulties if you use LVM to create logical devices from the multipath device, but if\\nyou require that your multipath device names be consistent in every node it is recommended that you leave the\\n*user_friendly_names* option set to ‘no’ and that you *do not* configure aliases for the devices.\\n\\nIf you configure an alias for a device that you would like to be consistent across the nodes in the cluster, you should\\nensure that the /etc/multipath.conf file is the same for each node in the cluster by following the same procedure:\\n\\n1. Configure the aliases for the multipath devices in the in the multipath.conf file on one machine.\\n\\n2. Disable all of your multipath devices on your other machines by running the following commands as root:\\n\\nsystemctl stop multipath-tools.service\\n\\nmultipath -F\\n\\n3.',\n",
       " 'Copy the /etc/multipath.conf file from the first machine to all other machines in the cluster.\\n\\n4. Re-enable the multipathd daemon on all the other machines in the cluster by running the following command as\\n\\nroot:\\n\\n372\\n\\n\\n-----\\n\\nsystemctl start multipath-tools.service\\n\\nWhen you add a new device you will need to repeat this process.\\n\\n**Multipath device attributes**\\n\\nIn addition to the *user_friendly_names* and *alias* options, a multipath device has numerous attributes. You can\\nmodify these attributes for a specific multipath device by creating an entry for that device in the *multipaths* section\\nof /etc/multipath.conf .\\n\\nFor information on the *multipaths* section of the multipath configuration file, see DM-Multipath configuration.\\n\\n**Multipath devices in logical volumes**\\n\\nAfter creating multipath devices, you can use the multipath device names just as you would use a physical device\\nname when creating an LVM physical volume.\\n\\nFor example, if /dev/mapper/mpatha is the name of a multipath device, the following command (run as root) will mark\\n\\n/dev/mapper/mpatha as a physical volume:\\n\\npvcreate /dev/mapper/mpatha\\n\\nYou can use the resulting LVM physical device when you create an LVM volume group just as you would use any other\\nLVM physical device.\\n\\n**Note** :\\nIf you try to create an LVM physical volume on a whole device on which you have configured partitions,\\nthe pvcreate command will fail.\\n\\nWhen you create an LVM logical volume that uses active/passive multipath arrays as the underlying physical devices,\\nyou should include filters in the lvm.conf file to exclude the disks that underlie the multipath devices. This is because',\n",
       " 'if the array automatically changes the active path to the passive path when it receives I/O, multipath will failover\\nand fallback whenever LVM scans the passive path if these devices are not filtered.\\n\\nFor active/passive arrays that require a command to make the passive path active, LVM prints a warning message\\nwhen this occurs. To filter all SCSI devices in the LVM configuration file ( lvm.conf ), include the following filter in the\\ndevices section of the file:\\n\\nfilter = [ \"r/block/\", \"r/disk/\", \"r/sd.*/\", \"a/.*/\" ]\\n\\nAfter updating /etc/lvm.conf, it’s necessary to update the *initrd* so that this file will be copied there, where the filter\\nmatters the most – during boot.\\n\\nPerform:\\n\\nupdate-initramfs -u -k all\\n\\n**Note** :\\nEvery time either /etc/lvm.conf or /etc/multipath.conf is updated, the initrd should be rebuilt to reflect\\nthese changes. This is imperative when *denylists* and filters are necessary to maintain a stable storage\\nconfiguration.\\n\\nBefore moving on with this session it is recommended that you read Device Mapper Multipathing - Introduction. For\\nconsistency, we will refer here to device mapper multipathing as *multipath* .\\n\\nMultipath is usually able to work out-of-the-box with most common storages. This doesn’t mean the default configuration variables should be used in production: they don’t treat important parameters your storage might need.\\n\\nIt’s a good idea to consult your storage manufacturer’s install guide for the Linux Multipath configuration options.\\nStorage vendors very commonly provide the most adequate options for Linux, including minimal versions required for\\nkernel and multipath-tools.',\n",
       " 'Default configuration values for DM-Multipath can be overridden by editing the /etc/multipath.conf file and restarting\\nthe multipathd service.\\n\\nThis chapter provides information on parsing and modifying the multipath.conf file and it is split into the following\\nconfiguration file sections:\\n\\n  - Configuration file overview\\n\\n  - Configuration file defaults\\n\\n  - Configuration file blacklist & exceptions\\n\\n  - Configuration file multipath section\\n\\n373\\n\\n\\n-----\\n\\n  - Configuration file devices section\\n## **Configuration file overview**\\n\\nThe configuration file contains entries of the form:\\n\\n<section> {\\n\\n<attribute> <value>\\n\\n...\\n\\n<subsection> {\\n\\n<attribute> <value>\\n\\n...\\n\\n}\\n\\n}\\n\\nThe following keywords are recognised:\\n\\n - defaults : Defines default values for attributes used whenever no values are given in the appropriate device or\\nmultipath sections.\\n\\n - blacklist : Defines which devices should be excluded from the multipath topology discovery.\\n\\n - blacklist_exceptions : Defines which devices should be included in the multipath topology discovery, despite\\nbeing listed in the blacklist section.\\n\\n - multipaths : Defines the multipath topologies. They are indexed by a World Wide Identifier (WWID). Attributes\\nset in this section take precedence **over all others** .\\n\\n - devices : Defines the device-specific settings. Devices are identified by vendor, product, and revision.\\n\\n - overrides : This section defines values for attributes that should override the device-specific settings for all\\ndevices.\\n## **Configuration file defaults**\\n\\nCurrently, the multipath configuration file ONLY includes a minor *defaults* section that sets the *user_friendly_names*\\nparameter to ‘yes’:\\n\\ndefaults {',\n",
       " 'user_friendly_names yes\\n\\n}\\n\\nThis overwrites the default value of the *user_friendly_names* parameter.\\n\\nAll the multipath attributes that can be set in the *defaults* section of the multipath.conf [file can be found here in the](https://manpages.ubuntu.com/manpages/focal/en/man5/multipath.conf.5.html#defaults%20section)\\n[man pages with an explanation of what they mean. The attributes are:](https://manpages.ubuntu.com/manpages/focal/en/man5/multipath.conf.5.html#defaults%20section)\\n\\n - verbosity\\n\\n - polling_interval\\n\\n - max_polling_interval\\n\\n - reassign_maps\\n\\n - multipath_dir\\n\\n - path_selector\\n\\n - path_grouping_policy\\n\\n - uid_attrs\\n\\n - uid_attribute\\n\\n - getuid_callout\\n\\n - prio\\n\\n - prio_args\\n\\n - features\\n\\n - path_checker\\n\\n - alias_prefix\\n\\n - failback\\n\\n - rr_min_io\\n\\n - rr_min_io_rq\\n\\n - max_fds\\n\\n - rr_weight\\n\\n - no_path_retry\\n\\n - queue_without_daemon\\n\\n374\\n\\n\\n-----\\n\\n - checker_timeout\\n\\n - flush_on_last_del\\n\\n - user_friendly_names\\n\\n - fast_io_fail_tmo\\n\\n - dev_loss_tmo\\n\\n - bindings_file\\n\\n - wwids_file\\n\\n - prkeys_file\\n\\n - log_checker_err\\n\\n - reservation_key\\n\\n - all_tg_pt\\n\\n - retain_attached_hw_handler\\n\\n - detect_prio\\n\\n - detect_checker\\n\\n - force_sync\\n\\n - strict_timing\\n\\n - deferred_remove\\n\\n - partition_delimiter\\n\\n - config_dir\\n\\n - san_path_err_threshold\\n\\n - san_path_err_forget_rate\\n\\n - san_path_err_recovery_time\\n\\n - marginal_path_double_failed_time\\n\\n - marginal_path_err_sample_time\\n\\n - marginal_path_err_rate_threshold\\n\\n - marginal_path_err_recheck_gap_time\\n\\n - delay_watch_checks\\n\\n - delay_wait_checks\\n\\n - marginal_pathgroups\\n\\n - find_multipaths\\n\\n - find_multipaths_timeout\\n\\n - uxsock_timeout\\n\\n - retrigger_tries\\n\\n - retrigger_delay\\n\\n - missing_uev_wait_timeout\\n\\n - skip_kpartx',\n",
       " ' - disable_changed_wwids\\n\\n - remove_retries\\n\\n - max_sectors_kb\\n\\n - ghost_delay\\n\\n - enable_foreign\\n\\n**Note** :\\nPreviously, the multipath-tools project provided a complete configuration file with all the most commonly\\nused options for each of the most-used storage devices. Currently, you can see all those default options by\\nexecuting [sudo multipath -t](https://paste.ubuntu.com/p/gfGkHwGyXw/) . This will dump a used configuration file including all the embedded default\\noptions.\\n## **Configuration file blacklist and exceptions**\\n\\nThe blacklist section is used to exclude specific devices from the multipath topology. It is most commonly used to\\nexclude local disks, non-multipathed, or non-disk devices.\\n\\n1. Blacklist by devnode\\n\\nThe default blacklist consists of the regular expressions \"^(ram|zram|raw|loop|fd|md|dm-|sr|scd|st|dcssblk)[0\\n9]\" and \"^(td|hd|vd)[a-z]\" . This causes virtual devices, non-disk devices, and some other device types to be\\nexcluded from multipath handling by default.\\n\\nblacklist {\\n\\ndevnode \"^(ram|zram|raw|loop|fd|md|dm-|sr|scd|st|dcssblk)[0-9]\"\\n\\ndevnode \"^(td|hd|vd)[a-z]\"\\n\\ndevnode \"^cciss!c[0-9]d[0-9]*\"\\n\\n}\\n\\n2. Blacklist by wwid\\n\\n375\\n\\n\\n-----\\n\\nRegular expression for the World Wide Identifier of a device to be excluded/included\\n\\n3. Blacklist by device\\n\\nSubsection for the device description. This subsection recognizes the *vendor* and *product* keywords. Both are\\nregular expressions.\\n\\ndevice {\\n\\nvendor \"LENOVO\"\\n\\nproduct \"Universal Xport\"\\n\\n}\\n\\n4. Blacklist by property\\n\\nRegular expression for an udev property. All devices that have matching udev properties will be excluded/included.',\n",
       " 'The handling of the property keyword is special, because devices must have at least one\\nwhitelisted udev property; otherwise they’re treated as blacklisted, and the message “blacklisted, udev property\\nmissing” is displayed in the logs.\\n\\n5. Blacklist by protocol\\n\\nThe protocol strings that multipath recognizes are scsi:fcp, scsi:spi, scsi:ssa, scsi:sbp, scsi:srp, scsi:iscsi,\\n\\nscsi:sas, scsi:adt, scsi:ata, scsi:unspec, ccw, cciss, nvme, and undef . The protocol that a path is using can\\nbe viewed by running:\\n\\nmultipathd show paths format \"%d %P\"\\n\\n6. Blacklist exceptions\\n\\nThe *blacklist_exceptions* section is used to revert the actions of the blacklist section. This allows one to selectively\\ninclude (“whitelist”) devices which would normally be excluded via the blacklist section.\\n\\nblacklist_exceptions {\\n\\nproperty \"(SCSI_IDENT_|ID_WWN)\"\\n\\n}\\n\\n**Note** :\\nA common usage is to blacklist “everything” using a catch-all regular expression, and create specific *black-*\\n*list_exceptions* entries for those devices that should be handled by multipath-tools .\\n## **Configuration file multipath section**\\n\\nThe *multipaths* section allows setting attributes of *multipath maps* . The attributes set via the multipaths section (see\\nlist below) take precedence over all other configuration settings, including those from the overrides section.\\n\\nThe only recognised attribute for the multipaths section is the multipath subsection. If there is more than one\\nmultipath subsection matching a given WWID, the contents of these sections are merged, and settings from later\\nentries take precedence.\\n\\nThe multipath subsection recognises the following attributes:\\n\\n - wwid : (Mandatory) World Wide Identifier.',\n",
       " 'Detected multipath maps are matched against this attribute. Note\\nthat, unlike the wwid attribute in the blacklist section, this is not a regular expression or a substring; WWIDs\\nmust match exactly inside the multipaths section.\\n\\n - alias : Symbolic name for the multipath map. This takes precedence over an entry for the same WWID in the\\n\\nbindings_file .\\n\\nThe following attributes are optional; if not set, the default values are taken from the overrides, devices, or *[defaults](https://manpages.ubuntu.com/manpages/focal/en/man5/multipath.conf.5.html#defaults%20section)*\\n\\n*[section](https://manpages.ubuntu.com/manpages/focal/en/man5/multipath.conf.5.html#defaults%20section)* :\\n\\n - path_grouping_policy\\n\\n - path_selector\\n\\n - prio\\n\\n - prio_args\\n\\n - failback\\n\\n - rr_weight\\n\\n - no_path_retry\\n\\n - rr_min_io\\n\\n - rr_min_io_rq\\n\\n - flush_on_last_del\\n\\n - features\\n\\n376\\n\\n\\n-----\\n\\n - reservation_key\\n\\n - user_friendly_names\\n\\n - deferred_remove\\n\\n - san_path_err_threshold\\n\\n - san_path_err_forget_rate\\n\\n - san_path_err_recovery_time\\n\\n - marginal_path_err_sample_time\\n\\n - marginal_path_err_rate_threshold\\n\\n - marginal_path_err_recheck_gap_time\\n\\n - marginal_path_double_failed_time\\n\\n - delay_watch_checks\\n\\n - delay_wait_checks\\n\\n - skip_kpartx\\n\\n - max_sectors_kb\\n\\n - ghost_delay\\n\\nExample:\\n\\nmultipaths {\\n\\nmultipath {\\n\\nwwid 3600508b4000156d700012000000b0000\\n\\nalias yellow\\n\\npath_grouping_policy multibus\\n\\npath_selector \"round-robin 0\"\\n\\nfailback manual\\n\\nrr_weight priorities\\n\\nno_path_retry 5\\n\\n}\\n\\nmultipath {\\n\\nwwid 1DEC_____321816758474\\n\\nalias red\\n\\n}\\n\\n}\\n## **Configuration file devices section**\\n\\nmultipath-tools has a built-in device table with reasonable defaults for more than 100 known multipath-capable',\n",
       " 'storage devices. The devices section can be used to override these settings. If there are multiple matches for a given\\ndevice, the attributes of all matching entries are applied to it. If an attribute is specified in several matching device\\nsubsections, later entries take precedence.\\n\\nThe only recognised attribute for the devices section is the device subsection. Devices detected in the system are\\nmatched against the device entries using the vendor, product, and revision fields.\\n\\nThe vendor, product, and revision fields that multipath or multipathd detect for devices in a system depend on the\\ndevice type. For SCSI devices, they correspond to the respective fields of the SCSI INQUIRY page. In general, the\\ncommand multipathd show paths format \"%d %s\" command can be used to see the detected properties for all devices\\nin the system.\\n\\nThe device subsection recognizes the following attributes:\\n\\n1. vendor\\n(Mandatory) Regular expression to match the vendor name.\\n\\n2. product\\n(Mandatory) Regular expression to match the product name.\\n\\n3. revision\\n\\nRegular expression to match the product revision.\\n\\n4. product_blacklist\\nProducts with the given vendor matching this string are blacklisted.\\n\\n5. alias_prefix\\nThe *user_friendly_names* prefix to use for this device type, instead of the default *mpath* .\\n\\n6. hardware_handler\\nThe hardware handler to use for this device type. The following hardware handler are implemented (all of these\\nare hardware-dependent):\\n\\n377\\n\\n\\n-----\\n\\n  - **1 emc**   - Hardware handler for DGC class arrays as CLARiiON CX/AX and EMC VNX and Unity families.\\n\\n  - **1 rdac**   - Hardware handler for LSI / Engenio / NetApp RDAC class as NetApp SANtricity E/EF Series,',\n",
       " 'and OEM arrays from IBM DELL SGI STK and SUN.\\n\\n  - **1 hp_sw**   - Hardware handler for HP/COMPAQ/DEC HSG80 and MSA/HSV arrays with Active/Standby\\nmode exclusively.\\n\\n  - **1 alua**   - Hardware handler for SCSI-3 ALUA-compatible arrays.\\n\\n  - **1 ana**   - Hardware handler for NVMe ANA-compatible arrays.\\n\\nThe following attributes are optional – if not set the default values are taken from the defaults section:\\n\\n - path_grouping_policy\\n\\n - uid_attribute\\n\\n - getuid_callout\\n\\n - path_selector\\n\\n - path_checker\\n\\n - prio\\n\\n - prio_args\\n\\n - features\\n\\n - failback\\n\\n - rr_weight\\n\\n - no_path_retry\\n\\n - rr_min_io\\n\\n - rr_min_io_rq\\n\\n - fast_io_fail_tmo\\n\\n - dev_loss_tmo\\n\\n - flush_on_last_del\\n\\n - user_friendly_names\\n\\n - retain_attached_hw_handler\\n\\n - detect_prio\\n\\n - detect_checker\\n\\n - deferred_remove\\n\\n - san_path_err_threshold\\n\\n - san_path_err_forget_rate\\n\\n - san_path_err_recovery_time\\n\\n - marginal_path_err_sample_time\\n\\n - marginal_path_err_rate_threshold\\n\\n - marginal_path_err_recheck_gap_time\\n\\n - marginal_path_double_failed_time\\n\\n - delay_watch_checks\\n\\n - delay_wait_checks\\n\\n - skip_kpartx\\n\\n - max_sectors_kb\\n\\n - ghost_delay\\n\\n - all_tg_pt\\n\\nExample:\\n\\ndevices {\\n\\ndevice {\\n\\nvendor \"3PARdata\"\\n\\nproduct \"VV\"\\n\\npath_grouping_policy \"group_by_prio\"\\n\\nhardware_handler \"1 alua\"\\n\\nprio \"alua\"\\n\\nfailback \"immediate\"\\n\\nno_path_retry 18\\n\\nfast_io_fail_tmo 10\\n\\ndev_loss_tmo \"infinity\"\\n\\n}\\n\\ndevice {\\n\\nvendor \"DEC\"\\n\\nproduct \"HSG80\"\\n\\npath_grouping_policy \"group_by_prio\"\\n\\npath_checker \"hp_sw\"\\n\\nhardware_handler \"1 hp_sw\"\\n\\n378\\n\\n\\n-----\\n\\nprio \"hp_sw\"\\n\\nno_path_retry \"queue\"\\n\\n}\\n\\n}\\n\\nBefore moving on with this session it is recommended that you read:\\n\\n1. Device mapper multipathing - introduction\\n2.',\n",
       " 'Device mapper multipathing - configuration\\n\\nFor consistency with those sections, we will refer here to device mapper multipathing as *multipath* .\\n\\nThis section provides step-by-step example procedures for setting up and configuring multipath. It includes the\\nfollowing procedures:\\n\\n  - Basic setup\\n\\n**–** Main *defaults* and *devices* attributes.\\n**–** Shows how to *ignore disks* using blacklists\\n**–** Shows how to *rename disks* using WWIDs\\n\\n  - Configuring active/active paths\\n## **Basic setup**\\n\\nBefore setting up multipath on your system, ensure that your system has been updated and includes the multipath\\ntools package. If boot from the storage area network (SAN) is desired, then the multipath-tools-boot package is also\\nrequired.\\n\\nA very simple /etc/multipath.conf file exists, as explained in device mapper multipathing - configuration. All attributes not declared in multipath.conf are taken from the multipath-tools internal database and its internal blacklist.\\n\\nThe internal attributes database can be acquired by running the following on the command line:\\n\\n[sudo multipath -t](https://paste.ubuntu.com/p/gfGkHwGyXw/)\\n\\nMultipath is usually able to work out-of-the-box with most common storages. This *does not* mean the default configuration variables should be used in production: they don’t treat important parameters your storage might need.\\n\\nWith the *internal attributes*, described above, and the given example below, you will likely be able to create your\\n\\n/etc/multipath.conf file by squashing the code blocks below. Make sure to read the *defaults* section attribute comments\\nand make any changes based on your environment needs.\\n\\n  - Example of a *defaults* section:',\n",
       " 'defaults {\\n\\n#\\n\\n# name : polling_interval\\n\\n# scope : multipathd\\n\\n# desc : interval between two path checks in seconds. For\\n\\n# properly functioning paths, the interval between checks\\n\\n# will gradually increase to (4 * polling_interval).\\n\\n# values : n > 0\\n\\n# default : 5\\n\\n#\\n\\npolling_interval 10\\n\\n#\\n\\n# name : path_selector\\n\\n# scope : multipath & multipathd\\n\\n# desc : the default path selector algorithm to use\\n\\n# these algorithms are offered by the kernel multipath target\\n\\n# values : \"round-robin 0\" = Loop through every path in the path group,\\n\\n# sending the same amount of IO to each.\\n\\n# \"queue-length 0\" = Send the next bunch of IO down the path\\n\\n# with the least amount of outstanding IO.\\n\\n# \"service-time 0\" = Choose the path for the next bunch of IO\\n\\n# based on the amount of outstanding IO to\\n\\n# the path and its relative throughput.\\n\\n# default : \"service-time 0\"\\n\\n#\\n\\n379\\n\\n\\n-----\\n\\npath_selector \"round-robin 0\"\\n\\n#\\n\\n# name : path_grouping_policy\\n\\n# scope : multipath & multipathd\\n\\n# desc : the default path grouping policy to apply to unspecified\\n\\n# multipaths\\n\\n# values : failover = 1 path per priority group\\n\\n# multibus = all valid paths in 1 priority group\\n\\n# group_by_serial = 1 priority group per detected serial\\n\\n# number\\n\\n# group_by_prio = 1 priority group per path priority\\n\\n# value\\n\\n# group_by_node_name = 1 priority group per target node name\\n\\n# default : failover\\n\\n#\\n\\npath_grouping_policy multibus\\n\\n#\\n\\n# name : uid_attribute\\n\\n# scope : multipath & multipathd\\n\\n# desc : the default udev attribute from which the path\\n\\n# identifier should be generated.\\n\\n# default : ID_SERIAL\\n\\n#\\n\\nuid_attribute \"ID_SERIAL\"\\n\\n#\\n\\n# name : getuid_callout\\n\\n# scope : multipath & multipathd',\n",
       " '# desc : the default program and args to callout to obtain a unique\\n\\n# path identifier. This parameter is deprecated.\\n\\n# This parameter is deprecated, superseded by uid_attribute\\n\\n# default : /lib/udev/scsi_id --whitelisted --device=/dev/%n\\n\\n#\\n\\ngetuid_callout \"/lib/udev/scsi_id --whitelisted --device=/dev/%n\"\\n\\n#\\n\\n# name : prio\\n\\n# scope : multipath & multipathd\\n\\n# desc : the default function to call to obtain a path\\n\\n# priority value. The ALUA bits in SPC-3 provide an\\n\\n# exploitable prio value for example.\\n\\n# default : const\\n\\n#\\n\\n# prio \"alua\"\\n\\n#\\n\\n# name : prio_args\\n\\n# scope : multipath & multipathd\\n\\n# desc : The arguments string passed to the prio function\\n\\n# Most prio functions do not need arguments. The\\n\\n# datacore prioritizer need one.\\n\\n# default : (null)\\n\\n#\\n\\n# prio_args \"timeout=1000 preferredsds=foo\"\\n\\n#\\n\\n# name : features\\n\\n# scope : multipath & multipathd\\n\\n# desc : The default extra features of multipath devices.\\n\\n# Syntax is \"num[ feature_0 feature_1 ...]\", where `num\\' is the\\n\\n# number of features in the following (possibly empty) list of\\n\\n380\\n\\n\\n-----\\n\\n# features.\\n\\n# values : queue_if_no_path = Queue IO if no path is active; consider\\n\\n# using the `no_path_retry\\' keyword instead.\\n\\n# no_partitions = Disable automatic partitions generation via\\n\\n# kpartx.\\n\\n# default : \"0\"\\n\\n#\\n\\nfeatures \"0\"\\n\\n#features \"1 queue_if_no_path\"\\n\\n#features \"1 no_partitions\"\\n\\n#features \"2 queue_if_no_path no_partitions\"\\n\\n#\\n\\n# name : path_checker, checker\\n\\n# scope : multipath & multipathd\\n\\n# desc : the default method used to determine the paths\\' state\\n\\n# values : readsector0|tur|emc_clariion|hp_sw|directio|rdac|cciss_tur\\n\\n# default : directio\\n\\n#\\n\\npath_checker directio\\n\\n#\\n\\n# name : rr_min_io',\n",
       " '# scope : multipath & multipathd\\n\\n# desc : the number of IO to route to a path before switching\\n\\n# to the next in the same path group for the bio-based\\n\\n# multipath implementation. This parameter is used for\\n\\n# kernels version up to 2.6.31; newer kernel version\\n\\n# use the parameter rr_min_io_rq\\n\\n# default : 1000\\n\\n#\\n\\nrr_min_io 100\\n\\n#\\n\\n# name : rr_min_io_rq\\n\\n# scope : multipath & multipathd\\n\\n# desc : the number of IO to route to a path before switching\\n\\n# to the next in the same path group for the request-based\\n\\n# multipath implementation. This parameter is used for\\n\\n# kernels versions later than 2.6.31.\\n\\n# default : 1\\n\\n#\\n\\nrr_min_io_rq 1\\n\\n#\\n\\n# name : flush_on_last_del\\n\\n# scope : multipathd\\n\\n# desc : If set to \"yes\", multipathd will disable queueing when the\\n\\n# last path to a device has been deleted.\\n\\n# values : yes|no\\n\\n# default : no\\n\\n#\\n\\nflush_on_last_del yes\\n\\n#\\n\\n# name : max_fds\\n\\n# scope : multipathd\\n\\n# desc : Sets the maximum number of open file descriptors for the\\n\\n# multipathd process.\\n\\n# values : max|n > 0\\n\\n# default : None\\n\\n#\\n\\nmax_fds 8192\\n\\n381\\n\\n\\n-----\\n\\n#\\n\\n# name : rr_weight\\n\\n# scope : multipath & multipathd\\n\\n# desc : if set to priorities the multipath configurator will assign\\n\\n# path weights as \"path prio * rr_min_io\"\\n\\n# values : priorities|uniform\\n\\n# default : uniform\\n\\n#\\n\\nrr_weight priorities\\n\\n#\\n\\n# name : failback\\n\\n# scope : multipathd\\n\\n# desc : tell the daemon to manage path group failback, or not to.\\n\\n# 0 means immediate failback, values >0 means deffered\\n\\n# failback expressed in seconds.\\n\\n# values : manual|immediate|n > 0\\n\\n# default : manual\\n\\n#\\n\\nfailback immediate\\n\\n#\\n\\n# name : no_path_retry\\n\\n# scope : multipath & multipathd',\n",
       " '# desc : tell the number of retries until disable queueing, or\\n\\n# \"fail\" means immediate failure (no queueing),\\n\\n# \"queue\" means never stop queueing\\n\\n# values : queue|fail|n (>0)\\n\\n# default : (null)\\n\\n#\\n\\nno_path_retry fail\\n\\n#\\n\\n# name : queue_without_daemon\\n\\n# scope : multipathd\\n\\n# desc : If set to \"no\", multipathd will disable queueing for all\\n\\n# devices when it is shut down.\\n\\n# values : yes|no\\n\\n# default : yes\\n\\nqueue_without_daemon no\\n\\n#\\n\\n# name : user_friendly_names\\n\\n# scope : multipath & multipathd\\n\\n# desc : If set to \"yes\", using the bindings file\\n\\n# /etc/multipath/bindings to assign a persistent and\\n\\n# unique alias to the multipath, in the form of mpath<n>.\\n\\n# If set to \"no\" use the WWID as the alias. In either case\\n\\n# this be will be overriden by any specific aliases in this\\n\\n# file.\\n\\n# values : yes|no\\n\\n# default : no\\n\\nuser_friendly_names yes\\n\\n#\\n\\n# name : mode\\n\\n# scope : multipath & multipathd\\n\\n# desc : The mode to use for the multipath device nodes, in octal.\\n\\n# values : 0000 - 0777\\n\\n# default : determined by the process\\n\\nmode 0644\\n\\n382\\n\\n\\n-----\\n\\n#\\n\\n# name : uid\\n\\n# scope : multipath & multipathd\\n\\n# desc : The user id to use for the multipath device nodes. You\\n\\n# may use either the numeric or symbolic uid\\n\\n# values : <user_id>\\n\\n# default : determined by the process\\n\\nuid 0\\n\\n#\\n\\n# name : gid\\n\\n# scope : multipath & multipathd\\n\\n# desc : The group id to user for the multipath device nodes. You\\n\\n# may use either the numeric or symbolic gid\\n\\n# values : <group_id>\\n\\n# default : determined by the process\\n\\ngid disk\\n\\n#\\n\\n# name : checker_timeout\\n\\n# scope : multipath & multipathd\\n\\n# desc : The timeout to use for path checkers and prioritizers',\n",
       " '# that issue scsi commands with an explicit timeout, in\\n\\n# seconds.\\n\\n# values : n > 0\\n\\n# default : taken from /sys/block/sd<x>/device/timeout\\n\\nchecker_timeout 60\\n\\n#\\n\\n# name : fast_io_fail_tmo\\n\\n# scope : multipath & multipathd\\n\\n# desc : The number of seconds the scsi layer will wait after a\\n\\n# problem has been detected on a FC remote port before failing\\n\\n# IO to devices on that remote port.\\n\\n# values : off | n >= 0 (smaller than dev_loss_tmo)\\n\\n# default : determined by the OS\\n\\nfast_io_fail_tmo 5\\n\\n#\\n\\n# name : dev_loss_tmo\\n\\n# scope : multipath & multipathd\\n\\n# desc : The number of seconds the scsi layer will wait after a\\n\\n# problem has been detected on a FC remote port before\\n\\n# removing it from the system.\\n\\n# values : infinity | n > 0\\n\\n# default : determined by the OS\\n\\ndev_loss_tmo 120\\n\\n#\\n\\n# name : bindings_file\\n\\n# scope : multipath\\n\\n# desc : The location of the bindings file that is used with\\n\\n# the user_friendly_names option.\\n\\n# values : <full_pathname>\\n\\n# default : \"/var/lib/multipath/bindings\"\\n\\n# bindings_file \"/etc/multipath/bindings\"\\n\\n#\\n\\n# name : wwids_file\\n\\n# scope : multipath\\n\\n# desc : The location of the wwids file multipath uses to\\n\\n# keep track of the created multipath devices.\\n\\n# values : <full_pathname>\\n\\n383\\n\\n\\n-----\\n\\n# default : \"/var/lib/multipath/wwids\"\\n\\n# wwids_file \"/etc/multipath/wwids\"\\n\\n#\\n\\n# name : reservation_key\\n\\n# scope : multipath\\n\\n# desc : Service action reservation key used by mpathpersist.\\n\\n# values : <key>\\n\\n# default : (null)\\n\\n# reservation_key \"mpathkey\"\\n\\n#\\n\\n# name : force_sync\\n\\n# scope : multipathd\\n\\n# desc : If set to yes, multipath will run all of the checkers in\\n\\n# sync mode, even if the checker has an async mode.\\n\\n# values : yes|no',\n",
       " '# default : no\\n\\nforce_sync yes\\n\\n#\\n\\n# name : config_dir\\n\\n# scope : multipath & multipathd\\n\\n# desc : If not set to an empty string, multipath will search\\n\\n# this directory alphabetically for files ending in \".conf\"\\n\\n# and it will read configuration information from these\\n\\n# files, just as if it was in /etc/multipath.conf\\n\\n# values : \"\" or a fully qualified pathname\\n\\n# default : \"/etc/multipath/conf.d\"\\n\\n#\\n\\n# name : delay_watch_checks\\n\\n# scope : multipathd\\n\\n# desc : If set to a value greater than 0, multipathd will watch\\n\\n# paths that have recently become valid for this many\\n\\n# checks. If they fail again while they are being watched,\\n\\n# when they next become valid, they will not be used until\\n\\n# they have stayed up for delay_wait_checks checks.\\n\\n# values : no|<n> > 0\\n\\n# default : no\\n\\ndelay_watch_checks 12\\n\\n#\\n\\n# name : delay_wait_checks\\n\\n# scope : multipathd\\n\\n# desc : If set to a value greater than 0, when a device that has\\n\\n# recently come back online fails again within\\n\\n# delay_watch_checks checks, the next time it comes back\\n\\n# online, it will marked and delayed, and not used until\\n\\n# it has passed delay_wait_checks checks.\\n\\n# values : no|<n> > 0\\n\\n# default : no\\n\\ndelay_wait_checks 12\\n\\n}\\n\\n- Example of a *multipaths* section.\\n\\n**Note** :\\n\\nYou can obtain the WWIDs for your LUNs by executing: multipath -ll\\nafter the service multipath-tools.service has been restarted.\\n\\nmultipaths {\\n\\nmultipath {\\n\\nwwid 360000000000000000e00000000030001\\n\\n384\\n\\n\\n-----\\n\\nalias yellow\\n\\n}\\n\\nmultipath {\\n\\nwwid 360000000000000000e00000000020001\\n\\nalias blue\\n\\n}\\n\\nmultipath {\\n\\nwwid 360000000000000000e00000000010001\\n\\nalias red\\n\\n}\\n\\nmultipath {\\n\\nwwid 360000000000000000e00000000040001\\n\\nalias green\\n\\n}\\n\\nmultipath {',\n",
       " 'wwid 360000000000000000e00000000050001\\n\\nalias purple\\n\\n}\\n\\n}\\n\\n- Small example of a *devices* section:\\n\\n# devices {\\n\\n# device {\\n\\n# vendor \"IBM\"\\n\\n# product \"2107900\"\\n\\n# path_grouping_policy group_by_serial\\n\\n# }\\n\\n# }\\n\\n#\\n\\n- Example of a *blacklist* section:\\n\\n# name : blacklist\\n\\n# scope : multipath & multipathd\\n\\n# desc : list of device names to discard as not multipath candidates\\n\\n#\\n\\n# Devices can be identified by their device node name \"devnode\",\\n\\n# their WWID \"wwid\", or their vender and product strings \"device\"\\n\\n# default : fd, hd, md, dm, sr, scd, st, ram, raw, loop, dcssblk\\n\\n#\\n\\n# blacklist {\\n\\n# wwid 26353900f02796769\\n\\n# devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]\\\\*\"\\n\\n# devnode \"^hd[a-z]\"\\n\\n# devnode \"^dcssblk[0-9]\\\\*\"\\n\\n# device {\\n\\n# vendor DEC.\\\\*\\n\\n# product MSA[15]00\\n\\n# }\\n\\n# }\\n\\n- Example of a *blacklist exception* section:\\n\\n# name : blacklist_exceptions\\n\\n# scope : multipath & multipathd\\n\\n# desc : list of device names to be treated as multipath candidates\\n\\n# even if they are on the blacklist.\\n\\n#\\n\\n# Note: blacklist exceptions are only valid in the same class.\\n\\n# It is not possible to blacklist devices using the devnode keyword\\n\\n# and to exclude some devices of them using the wwid keyword.\\n\\n# default : \\n#\\n\\n# blacklist_exceptions {\\n\\n# devnode \"^dasd[c-d]+[0-9]\\\\*\"\\n\\n385\\n\\n\\n-----\\n\\n# wwid \"IBM.75000000092461.4d00.34\"\\n\\n# wwid \"IBM.75000000092461.4d00.35\"\\n\\n# wwid \"IBM.75000000092461.4d00.36\"\\n\\n# }\\n\\nBefore moving on with this session it is recommended that you read:\\n\\n[1. Device mapper multipathing - introduction](https://discourse.ubuntu.com/t/device-mapper-multipathing-introduction/)\\n[2.',\n",
       " \"Device mapper multipathing - configuration](https://discourse.ubuntu.com/t/device-mapper-multipathing-configuration)\\n[3. Device mapper multipathing - setup](https://discourse.ubuntu.com/t/device-mapper-multipathing-setup/)\\n\\nFor consistency with those sections, we will refer here to device mapper multipathing as *multipath* .\\n\\nThis section provides step-by-step example procedures for configuring multipath, and includes the following procedures:\\n\\n - *Resizing* online multipath devices\\n\\n  - Moving root file system from a *single path* device to a *multipath* device\\n\\n - The multipathd daemon\\n\\n  - Issues with queue_if_no_path\\n\\n - Multipath *command output*\\n\\n - Multipath *queries* with multipath command\\n\\n - Determining *device mapper entries* with dmsetup command\\n\\n  - Troubleshooting with the *multipathd* interactive console\\n## **Resizing online multipath devices**\\n\\nFirst find all the paths to the logical unit number (LUN) about to be resized by running the following command:\\n\\n$ sudo multipath -ll\\n\\nmpathb (360014056eee8ec6e1164fcb959086482) dm-0 LIO-ORG,lun01\\n\\nsize=1.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:1 sde 8:64 active ready running\\n\\n`-+- policy='service-time 0' prio=50 status=enabled\\n\\n`- 8:0:0:1 sdf 8:80 active ready running\\n\\nmpatha (36001405e3c2841430ee4bf3871b1998b) dm-1 LIO-ORG,lun02\\n\\nsize=1.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:2 sdc 8:32 active ready running\\n\\n`-+- policy='service-time 0' prio=50 status=enabled\\n\\n`- 8:0:0:2 sdd 8:48 active ready running\",\n",
       " \"Now let’s reconfigure mpathb (with wwid = 360014056eee8ec6e1164fcb959086482 ) to have 2GB instead of just 1Gb and\\ncheck if it has changed:\\n\\n$ echo 1 | sudo tee /sys/block/sde/device/rescan\\n\\n1\\n\\n$ echo 1 | sudo tee /sys/block/sdf/device/rescan\\n\\n1\\n\\n$ sudo multipath -ll\\n\\nmpathb (360014056eee8ec6e1164fcb959086482) dm-0 LIO-ORG,lun01\\n\\nsize=1.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:1 sde 8:64 active ready running\\n\\n`-+- policy='service-time 0' prio=50 status=enabled\\n\\n`- 8:0:0:1 sdf 8:80 active ready running\\n\\nmpatha (36001405e3c2841430ee4bf3871b1998b) dm-1 LIO-ORG,lun02\\n\\nsize=1.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:2 sdc 8:32 active ready running\\n\\n`-+- policy='service-time 0' prio=50 status=enabled\\n\\n386\\n\\n\\n-----\\n\\n`- 8:0:0:2 sdd 8:48 active ready running\\n\\nNot yet! We still need to re-scan the multipath map:\\n\\n$ sudo multipathd resize map mpathb\\n\\nok\\n\\nAnd then we are good:\\n\\n$ sudo multipath -ll\\n\\nmpathb (360014056eee8ec6e1164fcb959086482) dm-0 LIO-ORG,lun01\\n\\nsize=2.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:1 sde 8:64 active ready running\\n\\n.`-+- policy='service-time 0' prio=50 status=enabled\\n\\n`- 8:0:0:1 sdf 8:80 active ready running\\n\\nmpatha (36001405e3c2841430ee4bf3871b1998b) dm-1 LIO-ORG,lun02\\n\\nsize=1.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:2 sdc 8:32 active ready running\\n\\n`-+- policy='service-time 0' prio=50 status=enabled\\n\\n`- 8:0:0:2 sdd 8:48 active ready running\",\n",
       " \"Make sure to run resize2fs /dev/mapper/mpathb to resize the filesystem.\\n## **Moving root file system from a single path device to a multipath device**\\n\\nThis is dramatically simplified by the use of UUIDs to identify devices as an intrinsic label. Simply install multipath\\ntools-boot and reboot. This will rebuild the initial RAM disk and afford multipath the opportunity to build its paths\\nbefore the root filesystem is mounted by UUID.\\n\\n**Note** :\\n\\nWhenever multipath.conf is updated, so should the initrd by executing:\\n\\nupdate-initramfs -u -k all\\nThe reason for this is multipath.conf is copied to the RAM disk and is integral to determining the available\\ndevices to map via its *denylist* and *devices* sections.\\n## The multipathd daemon\\n\\nIf you have trouble implementing a multipath configuration, you should ensure the multipath daemon is running as\\n[described in device mapper multipathing - setup. The](https://discourse.ubuntu.com/t/device-mapper-multipathing-setup/) multipathd daemon must be running in order to use multipath\\ndevices.\\n## **Multipath command output**\\n\\nWhen you create, modify, or list a multipath device, you get a printout of the current device setup. The format is as\\nfollows for each multipath device:\\n\\naction_if_any: alias (wwid_if_different_from_alias) dm_device_name_if_known vendor,product\\n\\nsize=size features='features' hwhandler='hardware_handler' wp=write_permission_if_known\\n\\nFor each path group:\\n\\n-+- policy='scheduling_policy' prio=prio_if_known\\n\\nstatus=path_group_status_if_known\\n\\nFor each path:\\n\\n`- host:channel:id:lun devnode major:minor dm_status_if_known path_status\\n\\nonline_status\\n\\nFor example, the output of a multipath command might appear as follows:\",\n",
       " \"mpathb (360014056eee8ec6e1164fcb959086482) dm-0 LIO-ORG,lun01\\n\\nsize=2.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=50 status=active\\n\\n| `- 7:0:0:1 sde 8:64 active ready running\\n\\n`-+- policy='service-time 0' prio=50 status=enabled\\n\\n387\\n\\n\\n-----\\n\\n`- 8:0:0:1 sdf 8:80 active ready running\\n\\nIf the path is up and ready for I/O, the status of the path is *ready* or *ghost* . If the path is down, the status is *faulty*\\nor *shaky* . The path status is updated periodically by the multipathd daemon based on the *polling interval* defined in\\nthe /etc/multipath.conf file.\\n\\nThe dm_status is similar to the path status, but from the kernel’s point of view. The dm_status has two states: *failed*,\\nwhich is analogous to *faulty*, and *active*, which covers all other path states. Occasionally, the *path* state and the *dm*\\nstate of a device will temporary not agree.\\n\\nThe possible values for online_status are *running* and *offline* . A status of *offline* means that the SCSI device has\\nbeen disabled.\\n## Multipath queries with multipath command\\n\\nYou can use the -l and -ll options of the multipath command to display the current multipath configuration.\\n\\n - -l\\n\\nDisplays multipath topology gathered from information in sysfs and the device mapper.\\n\\n - -ll\\n\\nDisplays the information the -l displays in addition to all other available components of the system.\\n\\nWhen displaying the multipath configuration, there are three verbosity levels you can specify with the -v option of the\\n\\nmultipath command. Specifying -v0 yields *no output* . Specifying -v1 outputs the *created or updated multipath names*\\n*only*, which you can then feed to other tools such as kpartx .\",\n",
       " \"Specifying -v2 prints *all detected paths, multipaths, and*\\n*device maps* .\\n\\n**Note** :\\nThe default *verbosity* level of multipath is 2 and can be globally modified by defining the verbosity attribute\\nin the *defaults* section of multipath.conf .\\n\\nThe following example shows the output of a sudo multipath -l command:\\n\\nmpathb (360014056eee8ec6e1164fcb959086482) dm-0 LIO-ORG,lun01\\n\\nsize=2.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=0 status=active\\n\\n| `- 7:0:0:1 sde 8:64 active undef running\\n\\n`-+- policy='service-time 0' prio=0 status=enabled\\n\\n`- 8:0:0:1 sdf 8:80 active undef running\\n\\nmpatha (36001405e3c2841430ee4bf3871b1998b) dm-1 LIO-ORG,lun02\\n\\nsize=1.0G features='0' hwhandler='1 alua' wp=rw\\n\\n|-+- policy='service-time 0' prio=0 status=active\\n\\n| `- 7:0:0:2 sdc 8:32 active undef running\\n\\n`-+- policy='service-time 0' prio=0 status=enabled\\n\\n`- 8:0:0:2 sdd 8:48 active undef running\\n## Determining device mapper entries with dmsetup command\\n\\nYou can use the dmsetup command to find out which device mapper entries match the *multipathed* devices. The\\nfollowing command displays all the device mapper devices and their major and minor numbers. The minor numbers\\ndetermine the name of the *dm* device. For example, a minor number of 1 corresponds to the multipathd device\\n\\n/dev/dm-1 .\\n\\n$ sudo dmsetup ls\\n\\nmpathb (253:0)\\n\\nmpatha (253:1)\\n\\n$ ls -lahd /dev/dm*\\n\\nbrw-rw---- 1 root disk 253, 0 Apr 27 14:49 /dev/dm-0\\n\\nbrw-rw---- 1 root disk 253, 1 Apr 27 14:47 /dev/dm-1\\n## Troubleshooting with the multipathd interactive console\\n\\nThe multipathd -k command is an interactive interface to the multipathd daemon. Entering this command brings up\",\n",
       " \"an interactive multipath console. After entering this command, you can enter help to get a list of available commands,\\nyou can enter an interactive command, or you can enter Ctrl+D to quit.\\n\\nThe multipathd interactive console can be used to troubleshoot problems with your system. For example, the following\\ncommand sequence displays the multipath configuration, including the defaults, before exiting the console.\\n\\n388\\n\\n\\n-----\\n\\n$ sudo multipathd -k\\n\\n - show config\\n\\n - CTRL-D\\n\\nThe following command sequence ensures that multipath has picked up any changes to the multipath.conf :\\n\\n$ sudo multipathd -k\\n\\n- reconfigure\\n\\n- CTRL-D\\n\\nUse the following command sequence to ensure that the path checker is working properly:\\n\\n$ sudo multipathd -k\\n\\n- show paths\\n\\n- CTRL-D\\n\\nCommands can also be streamed into multipathd using STDIN like so:\\n\\n$ echo 'show config' | sudo multipathd -k\\n\\nSecurity should always be considered when installing, deploying, and using any type of computer system. Although\\na fresh installation of Ubuntu is relatively safe for immediate use on the Internet, it is important to have a balanced\\nunderstanding of your system’s security posture based on how it will be used after deployment.\\n\\nThis chapter provides an overview of security-related topics as they pertain to Ubuntu Server Edition, and outlines\\nsimple measures you may use to protect your server and network from any number of potential security threats.\\n\\nIf instead of security related administration and concepts for Ubuntu Server please be aware that there is more.\\nDepending on what you were looking for please consider those references:\",\n",
       " '[• Further information about security at Ubuntu, have a look at Ubuntu Security](https://ubuntu.com/security)\\n\\n  - Information about known vulnerabilities:\\n\\n**–** [per CVE check out the CVE overview](https://ubuntu.com/security/cves)\\n\\n**–**\\n[per Package have a look at the Ubuntu Security Notices](https://ubuntu.com/security/notices)\\n\\n[• Reporting a security issue, have a look at the disclosure policy](https://ubuntu.com/security/disclosure-policy)\\n\\nUser management is a critical part of maintaining a secure system. Ineffective user and privilege management often\\nlead many systems into being compromised. Therefore, it is important that you understand how you can protect your\\nserver through simple and effective user account management techniques.\\n## **Where is root?**\\n\\nUbuntu developers made a conscientious decision to disable the administrative root account by default in all Ubuntu\\ninstallations. This does not mean that the root account has been deleted or that it may not be accessed. It merely\\nhas been given a password hash which matches no possible value, therefore may not log in directly by itself.\\n\\nInstead, users are encouraged to make use of a tool by the name of ‘sudo’ to carry out system administrative duties.\\nSudo allows an authorized user to temporarily elevate their privileges using their own password instead of having to\\nknow the password belonging to the root account. This simple yet effective methodology provides accountability for all\\nuser actions, and gives the administrator granular control over which actions a user can perform with said privileges.\\n\\n  - If for some reason you wish to enable the root account, simply give it a password:\\n\\nsudo passwd',\n",
       " 'Sudo will prompt you for your password, and then ask you to supply a new password for root as shown below:\\n\\n[sudo] password for username: (enter your own password)\\n\\nEnter new UNIX password: (enter a new password for root)\\n\\nRetype new UNIX password: (repeat new password for root)\\n\\npasswd: password updated successfully\\n\\n  - To disable the root account password, use the following passwd syntax:\\n\\nsudo passwd -l root\\n\\n  - You should read more on Sudo by reading the man page:\\n\\nman sudo\\n\\nBy default, the initial user created by the Ubuntu installer is a member of the group sudo which is added to the file\\n\\n/etc/sudoers as an authorized sudo user. If you wish to give any other account full root access through sudo, simply\\nadd them to the sudo group.\\n\\n389\\n\\n\\n-----\\n\\n## **Adding and Deleting Users**\\n\\nThe process for managing local users and groups is straightforward and differs very little from most other GNU/Linux\\noperating systems. Ubuntu and other Debian based distributions encourage the use of the ‘adduser’ package for\\naccount management.\\n\\n  - To add a user account, use the following syntax, and follow the prompts to give the account a password and\\nidentifiable characteristics, such as a full name, phone number, etc.\\n\\nsudo adduser username\\n\\n  - To delete a user account and its primary group, use the following syntax:\\n\\nsudo deluser username\\n\\nDeleting an account does not remove their respective home folder. It is up to you whether or not you wish to\\ndelete the folder manually or keep it according to your desired retention policies.\\n\\nRemember, any user added later on with the same UID/GID as the previous owner will now have access to this',\n",
       " 'folder if you have not taken the necessary precautions.\\n\\nYou may want to change these UID/GID values to something more appropriate, such as the root account, and\\nperhaps even relocate the folder to avoid future conflicts:\\n\\nsudo chown -R root:root /home/username/\\n\\nsudo mkdir /home/archived_users/\\n\\nsudo mv /home/username /home/archived_users/\\n\\n  - To temporarily lock or unlock a user password, use the following syntax, respectively:\\n\\nsudo passwd -l username\\n\\nsudo passwd -u username\\n\\n  - To add or delete a personalized group, use the following syntax, respectively:\\n\\nsudo addgroup groupname\\n\\nsudo delgroup groupname\\n\\n  - To add a user to a group, use the following syntax:\\n\\nsudo adduser username groupname\\n## **User Profile Security**\\n\\nWhen a new user is created, the adduser utility creates a brand new home directory named /home/username . The\\ndefault profile is modeled after the contents found in the directory of /etc/skel, which includes all profile basics.\\n\\nIf your server will be home to multiple users, you should pay close attention to the user home directory permissions to\\nensure confidentiality. By default, user home directories in Ubuntu are created with world read/execute permissions.\\nThis means that all users can browse and access the contents of other users home directories. This may not be suitable\\nfor your environment.\\n\\n  - To verify your current user home directory permissions, use the following syntax:\\n\\nls -ld /home/username\\n\\nThe following output shows that the directory /home/username has world-readable permissions:\\n\\ndrwxr-xr-x 2 username username 4096 2007-10-02 20:03 username\\n\\n  - You can remove the world readable-permissions using the following syntax:',\n",
       " 'sudo chmod 0750 /home/username\\n\\n**Note**\\n\\nSome people tend to use the recursive option (-R) indiscriminately which modifies all child folders and\\nfiles, but this is not necessary, and may yield other undesirable results. The parent directory alone is\\nsufficient for preventing unauthorized access to anything below the parent.\\n\\nA much more efficient approach to the matter would be to modify the adduser global default permissions when\\ncreating user home folders. Simply edit the file /etc/adduser.conf and modify the DIR_MODE variable to something\\nappropriate, so that all new home directories will receive the correct permissions.\\n\\nDIR_MODE=0750\\n\\n390\\n\\n\\n-----\\n\\n  - After correcting the directory permissions using any of the previously mentioned techniques, verify the results\\nusing the following syntax:\\n\\nls -ld /home/username\\n\\nThe results below show that world-readable permissions have been removed:\\n\\ndrwxr-x--- 2 username username 4096 2007-10-02 20:03 username\\n## **Password Policy**\\n\\nA strong password policy is one of the most important aspects of your security posture. Many successful security\\nbreaches involve simple brute force and dictionary attacks against weak passwords. If you intend to offer any form of\\nremote access involving your local password system, make sure you adequately address minimum password complexity\\nrequirements, maximum password lifetimes, and frequent audits of your authentication systems.\\n\\n**Minimum Password Length**\\n\\nBy default, Ubuntu requires a minimum password length of 6 characters, as well as some basic entropy checks. These\\nvalues are controlled in the file /etc/pam.d/common-password, which is outlined below.',\n",
       " 'password [success=1 default=ignore] pam_unix.so obscure sha512\\n\\nIf you would like to adjust the minimum length to 8 characters, change the appropriate variable to min=8. The\\nmodification is outlined below.\\n\\npassword [success=1 default=ignore] pam_unix.so obscure sha512 minlen=8\\n\\n**Note**\\n\\nBasic password entropy checks and minimum length rules do not apply to the administrator using sudo\\nlevel commands to setup a new user.\\n\\n**Password Expiration**\\n\\nWhen creating user accounts, you should make it a policy to have a minimum and maximum password age forcing\\nusers to change their passwords when they expire.\\n\\n  - To easily view the current status of a user account, use the following syntax:\\n\\nsudo chage -l username\\n\\nThe output below shows interesting facts about the user account, namely that there are no policies applied:\\n\\nLast password change : Jan 20, 2015\\n\\nPassword expires : never\\n\\nPassword inactive : never\\n\\nAccount expires : never\\n\\nMinimum number of days between password change : 0\\n\\nMaximum number of days between password change : 99999\\n\\nNumber of days of warning before password expires : 7\\n\\n  - To set any of these values, simply use the following syntax, and follow the interactive prompts:\\n\\nsudo chage username\\n\\nThe following is also an example of how you can manually change the explicit expiration date (-E) to 01/31/2015,\\nminimum password age (-m) of 5 days, maximum password age (-M) of 90 days, inactivity period (-I) of 30 days\\nafter password expiration, and a warning time period (-W) of 14 days before password expiration:\\n\\nsudo chage -E 01/31/2015 -m 5 -M 90 -I 30 -W 14 username\\n\\n  - To verify changes, use the same syntax as mentioned previously:\\n\\nsudo chage -l username',\n",
       " 'The output below shows the new policies that have been established for the account:\\n\\nLast password change : Jan 20, 2015\\n\\nPassword expires : Apr 19, 2015\\n\\nPassword inactive : May 19, 2015\\n\\nAccount expires : Jan 31, 2015\\n\\nMinimum number of days between password change : 5\\n\\nMaximum number of days between password change : 90\\n\\n391\\n\\n\\n-----\\n\\nNumber of days of warning before password expires : 14\\n## **Other Security Considerations**\\n\\nMany applications use alternate authentication mechanisms that can be easily overlooked by even experienced system\\nadministrators. Therefore, it is important to understand and control how users authenticate and gain access to services\\nand applications on your server.\\n\\n**SSH Access by Disabled Users**\\n\\nSimply disabling/locking a user password will not prevent a user from logging into your server remotely if they have\\npreviously set up SSH public key authentication. They will still be able to gain shell access to the server, without\\nthe need for any password. Remember to check the users home directory for files that will allow for this type of\\nauthenticated SSH access, e.g. /home/username/.ssh/authorized_keys .\\n\\nRemove or rename the directory .ssh/ in the user’s home folder to prevent further SSH authentication capabilities.\\n\\nBe sure to check for any established SSH connections by the disabled user, as it is possible they may have existing\\ninbound or outbound connections. Kill any that are found.\\n\\nwho | grep username (to get the pts/# terminal)\\n\\nsudo pkill -f pts/#\\n\\nRestrict SSH access to only user accounts that should have it. For example, you may create a group called “sshlogin”',\n",
       " 'and add the group name as the value associated with the AllowGroups variable located in the file /etc/ssh/sshd_config .\\n\\nAllowGroups sshlogin\\n\\nThen add your permitted SSH users to the group “sshlogin”, and restart the SSH service.\\n\\nsudo adduser username sshlogin\\n\\nsudo systemctl restart sshd.service\\n\\n**External User Database Authentication**\\n\\nMost enterprise networks require centralized authentication and access controls for all system resources. If you have\\nconfigured your server to authenticate users against external databases, be sure to disable the user accounts both\\nexternally and locally. This way you ensure that local fallback authentication is not possible.\\n\\nAmong some of the popular uses for smart cards is the ability to control access to computer systems. To operate the\\nowner must have the smart card and they must know the PIN to unlock the card. This provides a higher degree of\\nsecurity than single-factor authentication such as just using a password.\\n\\nThe following sections describe how to enable smart card authentication on Ubuntu. They apply to Ubuntu 18.04\\nonward, but see the relevant sections for newer versions.\\n\\n**Disclaimer**\\n\\nThis guide is meant for Ubuntu **Server** [, if you want to configure a desktop installation refer to the desktop guide.](https://ubuntu.com/tutorials/how-to-use-smart-card-authentication-in-ubuntu-desktop#1-overview)\\n## **Requirements**\\n\\n**Software**\\n\\nThe following packages must be installed to obtain a smart card configuration on Ubuntu.\\n\\n**Ubuntu 18.04**\\n\\n - **pcscd** : contains the drivers needed to communicate with the CCID smart card readers.\\n\\n - **opensc-pkcs11** : contains the smart card drivers, such as PIV or CAC.',\n",
       " ' - **libpam-pkcs11** : contains the PAM module to allow X.509 certificate logins via smart cards.\\n\\nTo install:\\n\\n$ sudo apt install opensc-pkcs11 libpam-pkcs11 pcscd\\n\\n392\\n\\n\\n-----\\n\\n**Ubuntu 20.04 and newer**\\n\\n - **pcscd** : contains the drivers needed to communicate with the CCID smart card readers.\\n\\n - **opensc-pkcs11** : contains the smart card drivers, such as PIV or CAC.\\n\\n - **sssd** : the authentication daemon that manages smart card access and certificate verification\\n\\nTo install:\\n\\n$ sudo apt install opensc-pkcs11 pcscd sssd libpam-sss\\n\\n**Hardware**\\n\\nAny PIV or CAC smart card with the corresponding reader should be sufficient. USB smart cards like Yubikey embed\\nthe reader, and work like regular PIV cards.\\n\\nEach smart card is expected to contain an X.509 certificate and the corresponding private key to be used for authen\\ntication.\\n\\n**Smart card PKCS#11 modules**\\n\\nWhile opensc-pkcs11 supports a wide number of smart cards, some of them may require specific PKCS#11 modules,\\nand you must refer to your vendor to install the proper one.\\n\\n[Starting from Ubuntu 20.04 all the modules supported by p11-kit can be used.](https://p11-glue.github.io/p11-glue/p11-kit.html)\\n\\n[In case that custom PKCS#11 modules are used, you need to ensure that p11-kit is properly configured.](https://p11-glue.github.io/p11-glue/p11-kit/manual/config.html)\\n\\nIn any case, p11-kit can be used to see all the configured modules that can be used for authentication:\\n\\n$ p11-kit list-modules\\n\\np11-kit-trust: p11-kit-trust.so\\n\\nlibrary-description: PKCS#11 Kit Trust Module\\n\\nlibrary-manufacturer: PKCS#11 Kit\\n\\nlibrary-version: 0.23\\n\\ntoken: System Trust\\n\\nmanufacturer: PKCS#11 Kit\\n\\nmodel: p11-kit-trust',\n",
       " 'serial-number: 1\\n\\nhardware-version: 0.23\\n\\nflags:\\n\\nwrite-protected\\n\\ntoken-initialized\\n\\nopensc-pkcs11: opensc-pkcs11.so\\n\\nlibrary-description: OpenSC smartcard framework\\n\\nlibrary-manufacturer: OpenSC Project\\n\\nlibrary-version: 0.20\\n\\ntoken: MARCO TREVISAN (PIN CNS0)\\n\\nmanufacturer: IC: STMicroelectronics; mask:...\\n\\nmodel: PKCS#15 emulated\\n\\nserial-number: 6090010669298009\\n\\nflags:\\n\\nlogin-required\\n\\nuser-pin-initialized\\n\\ntoken-initialized\\n\\nuser-pin-locked\\n\\n**X.509 Smart Card certificates**\\n\\nThe authentication is based on X.509 certificate validation and a smart card can provide one or more certificates that\\ncan be used for this purpose.\\n\\nIn order to proceed to the steps below it may be required to export or reference the certificate ID that it must be used\\nand associated to each user; such operation can be performed in the following ways:\\n\\n**Using p11tool**\\n\\nThis is a more generic implementation that just uses the PKCS#11 protocol so it should work with all the modules\\nout there\\n\\n393\\n\\n\\n-----\\n\\n$ sudo apt install gnutls-bin\\n\\n$ p11tool --list-tokens\\n\\n# Alternatively urls can be listed via\\n\\n$ p11tool --list-token-urls\\n\\nFor example:\\n\\nToken 1:\\n\\nURL: pkcs11:model=PKCS%2315%20emulated;manufacturer=IC%3A%20Infineon%3B%20mask%3A%20IDEMIA%20%28O...;serial=6090033\\n\\nLabel: MARCO TREVISAN (PIN CNS1)\\n\\nType: Hardware token\\n\\nFlags: Requires login\\n\\nManufacturer: IC: Infineon; mask: IDEMIA (O...\\n\\nModel: PKCS#15 emulated\\n\\nSerial: 6090033068507002\\n\\nModule: opensc-pkcs11.so\\n\\nThe command above will show all the available smart cards in the system and its associated PKCS#11 URI. Copy\\nthe URI’s token of selected card in the following command.',\n",
       " \"This command will print all certificates that can be used for authentication and their associated token URI.\\n\\n$ p11tool --list-all-certs 'pkcs11:token=[TOKEN-ID]'\\n\\nSo in the above example:\\n\\n$ p11tool --list-all-certs 'pkcs11:token=MARCO%20TREVISAN%20%28PIN%20CNS1%29'\\n\\nObject 0:\\n\\nURL: pkcs11:model=PKCS%2315%20emulated;manufacturer=IC%3A%20Infineon%3B%20mask%3A%20IDEMIA%20%28O...;serial=6090033\\n\\nType: X.509 Certificate (RSA-2048)\\n\\nExpires: ven 17 dic 2027, 00:00:00\\n\\nLabel: CNS1\\n\\nID: 02\\n\\nNow, once the URI of the certificate that will be used for authentication is known, let’s extract the Common Name\\nfrom the certificate. In the example we are assuming that our certificate URI is pkcs11:id=%02;type=cert .\\n\\nIt can be exported as text PEM format using:\\n\\n$ p11tool --export 'pkcs11:id=%02;type=cert'\\n\\n**Using opensc**\\n\\n$ sudo apt install opensc\\n\\nCertificates can be via:\\n\\n$ pkcs15-tool --list-certificates\\n\\nAnd exported using\\n\\n$ pkcs15-tool --read-certificate [CERTIFICATE_ID]\\n\\nSo, for example:\\n\\n$ pkcs15-tool --list-certificates\\n\\nUsing reader with a card: Alcor Micro AU9560 00 00\\n\\nX.509 Certificate [CNS1]\\n\\nObject Flags : [0x00]\\n\\nAuthority : no\\n\\nPath : 3f00140090012002\\n\\nID : 02\\n\\nEncoded serial : 02 10 0357B1EC0EB725BA67BD2D838DDF93D5\\n\\n$ pkcs15-tool --read-certificate 2\\n\\nUsing reader with a card: Alcor Micro AU9560 00 00\\n\\n-----BEGIN CERTIFICATE----\\nMIIHXDCCBUSgAwIBAgIQA1ex7A6.....\\n\\n394\\n\\n\\n-----\\n\\n**Troubleshooting**\\n\\nThe card certificate verification can be simulated using openssl:\\n\\n$ sudo apt install openssl\\n\\n# Save the certificate, using one of the method stated above\\n\\n$ pkcs15-tool --read-certificate 2 > card-cert.pem\\n\\n$ p11tool --export 'pkcs11:id=%02;type=cert' > card-cert.pem\",\n",
       " \"# See the certificate contents with\\n\\n$ openssl x509 -text -noout -in card-cert.pem\\n\\n# Verify it is valid for the given CA, where 'Ca-Auth-CERT.pem'\\n\\n# contains all the certificates chain\\n\\n$ openssl verify -verbose -CAfile CA-Auth-CERT.pem card-cert.pem\\n\\n# If only the parent CA Certificate is available, can use -partial_chain:\\n\\n$ openssl verify -verbose -partial_chain -CAfile intermediate_CA_cert.pem\\n## **PAM configuration**\\n\\nTo enable smart card authentication we should rely on a module that allows PAM supported systems to use X.509\\ncertificates to authenticate logins. The module relies on a PKCS#11 library, such as opensc-pkcs11 to access the\\nsmart card for the credentials it will need.\\n\\nWhen a PAM smart card module is enabled, the login process is as follows:\\n\\n1. Enter login\\n2. Enter PIN\\n3. Validate the X.509 certificate\\n4. Map the certificate to a user\\n5. Verify the login and match\\n\\nTo enable that process we have to configure the PAM module, add the relevant certificate authorities, add the PAM\\nmodule to PAM configuration and set the mapping of certificate names to logins.\\n\\n**Setup guide for Ubuntu 18.04**\\n\\nEven though this configuration may be supported also by versions newer than Ubuntu 18.04, using pam-pkcs11 directly\\nis considered deprecated, so for later versions we suggest following the Ubuntu 20.04 guide instead.\\n\\n**Configure the pam_pkcs11 module**\\n\\n$ cd /etc/pam_pkcs11\\n\\n$ sudo cp /usr/share/doc/libpam-pkcs11/examples/pam_pkcs11.conf.example pkcs11.conf\\n\\nCheck the module, cert_policy, and use_pkcs11_module options defined within the pkcs11_module opensc {} entry\\nin the pam_pkcs11.conf file.\",\n",
       " 'The module option should contain the absolute path of the open-pkcs11.so on the system.\\nThe cert_policy option should include oscp as one of its certificate verification policies.\\n\\nIn particular it should contain the following lines in Ubuntu 20.04.\\n\\nuse_pkcs11_module = opensc;\\n\\nmodule = /usr/lib/x86_64-linux-gnu/opensc-pkcs11.so;\\n\\ncert_policy = ca,signature,oscp_on;\\n\\nLeave debug = true until everything is setup and is operating as desired.\\n\\n**Map certificate names to login**\\n\\nThis PAM module allows certificates to be used for login, though our Linux system needs to know the username. The\\n\\npam_pkcs11 module provides a variety of cert mappers to do this. Each cert mapper uses specific information from the\\ncertificate to map to a user on the system. The different cert mappers may even be stacked. In other words, if the\\nfirst defined mapper fails to map to a user on the system, the next one will be tried, and so on until a user is found.\\n\\nFor the purposes of this guide, we will use the pwent mapper. This mapper uses the getpwent() system call to examine\\nthe pw_name and pw_gecos fields of every user for a match to the CN name. If either matches, the pw_name is returned\\n\\n395\\n\\n\\n-----\\n\\nas the login name. Next, it matches this result to the PAM login name to determine if a match was found or not. Set\\npwent as the mapper in the pam_pkcs11.conf file by modifying the existing entry:\\n\\nuse_mappers = pwent;\\n\\n**Set the Certificate Authority and CRLs**\\n\\nTo validate the smart card certificates the pam_pkcs11 module needs to know the acceptable Certificate Authorities for\\nsigning user certificates and any available CRLs. You can add these in the following paths.',\n",
       " ' - **Certificate Authorities** : /etc/pam_pkcs11/cacerts\\n\\n - **CRLs** : /etc/pam_pkcs11/crls\\n\\nAssuming the Certificate Authority is in ca.crt, the following example sets it up.\\n\\n$ sudo mkdir -p /etc/pam_pkcs11/cacerts\\n\\n$ sudo cp ca.crt /etc/pam_pkcs11/cacerts\\n\\n$ cd /etc/pam_pkcs11/cacerts\\n\\n$ sudo pkcs11_make_hash_link\\n\\nSimilarly for the CRLs.\\n\\n**Add pam_pkcs11 to PAM**\\n\\nThe next step includes the pam_pkcs11 module into the PAM stack. There are various ways to do this depending on\\nyour local policy. The following example enables smart card support for general authentication.\\n\\nEdit /etc/pam.d/common-auth to include the pam_pkcs11 module as follows.\\n\\n# require pkcs11 smart card login\\n\\nauth [success=2 default=ignore] pam_pkcs11.so\\n\\nThe above configuration will require the system to perform a smart card authentication only. If a user fails to\\nauthenticate with a smart card, then the login will fail. All the PAM services in the /etc/pam.d directory that include\\ncommon-auth will require the smart card authentication.\\n\\n**Warning:** A global configuration such as this requires a smart card for su and sudo authentication as well!\\n\\n**Configure the pwent mapper**\\n\\nNow that pam_pkcs11 and PAM have been configured for certificate logins, there is one more action. The pwent mapper\\nrequires the CN in the certificate to be in the /etc/passwd gecos field of the user. The CN must be extracted from the\\ncertificate on the smart card and added in passwd .\\n\\nThis can be done as explained in the X.509 Smart Card certificates section.\\n\\nNow, once the URI of the certificate that will be used for authentication is known, let’s extract the Common Name\\nfrom the certificate.',\n",
       " 'In the example we are assuming that our certificate URI is pkcs11:id=%02;type=cert .\\n\\n$ p11tool --export \\'pkcs11:id=%02;type=cert\\' | openssl x509 -noout -subject\\n\\nsubject=CN = PIVKey BA366DFE3722C7449EC906B9274C8BAC\\n\\nThe CN is PIVKey BA366DFE3722C7449EC906B9274C8BAC .\\nEdit the /etc/passwd file and add this CN to the gecos field of the user the certificate belongs to.\\n\\n$ sudo usermod -c \"PIVKey BA366DFE3722C7449EC906B9274C8BAC\" foo\\n\\nThe OS is now ready to do a smart card login for the user foo.\\n\\n**Setup guide for Ubuntu 20.04 and newer**\\n\\nThis configuration uses SSSD as authenticatoin mechanism, and the example shown here is showing a possible usage\\nfor local users, but more complex setups using external remote identity managers such as FreeIPA, LDAP, Kerberos\\nor others can be used.\\n\\n[Refer to SSSD documentation to learn more about this.](https://sssd.io/docs/introduction.html)\\n\\n396\\n\\n\\n-----\\n\\n**Enable SSSD PAM service**\\n\\nPam service must be enabled in SSSD configuration, it can be done by ensuring that /etc/sssd/sssd.conf contains:\\n\\n[sssd]\\n\\nservices = pam\\n\\n[pam]\\n\\npam_cert_auth = True\\n\\nFurther [pam] configuration options can be changed accroding to [man sssd.conf](https://manpages.ubuntu.com/manpages/jammy/en/man5/sssd.conf.5.html#services%20sections) .\\n\\n**Configure SSSD Certificate Authorities database**\\n\\nThe card certificate must be allowed by a Certificate Authority, these should be part of /etc/sssd/pki/sssd_auth_ca_db.pem\\n(or any other location configured in [pam] config section of sssd.conf as pam_cert_db_path ).\\n\\nAs per SSSD using openssl, we need to add the whole certificates chain to the SSSD CA certificates path (if not',\n",
       " 'changed via sssd.certificate_verification ), so adding the certificates to the pam_cert_db_path is enough:\\n\\nsudo cat Ca-Auth-CERT*.pem >> /etc/sssd/pki/sssd_auth_ca_db.pem\\n\\nCertification Revocation List can be also defined in sssd.conf, providing a CRL file path in PEM format\\n\\n[sssd]\\n\\ncrl_file = /etc/sssd/pki/sssd_auth_crl.pem\\n\\nsoft_crl = /etc/sssd/pki/sssd_auth_soft_crl.pem\\n\\nIn case that a full certificate authority chain is not available, openssl won’t verify the card certificate, and so sssd\\nshould be instructed about.\\n\\nThis is not suggested, but it can be done changing /etc/sssd/sssd.conf so that it contains:\\n\\n[sssd]\\n\\ncertificate_verification = partial_chain\\n\\n**Troubleshooting**\\n\\nCard certificate verification can be simulated using SSSD tools directly, by using the command SSSD’s p11_child :\\n\\n# In ubuntu 20.04\\n\\n$ sudo /usr/libexec/sssd/p11_child --pre -d 10 --debug-fd=2 --nssdb=/etc/sssd/pki/sssd_auth_ca_db.pem\\n\\n# In ubuntu 22.04 and later versions\\n\\n$ sudo /usr/libexec/sssd/p11_child --pre -d 10 --debug-fd=2 --ca_db=/etc/sssd/pki/sssd_auth_ca_db.pem\\n\\nIf certificate verification succeeds, the tool should output the card certificate name, its ID and the certificate itself in\\nbase64 format (other than debug data):\\n\\n(Mon Sep 11 16:33:32:129558 2023) [p11_child[1965]] [do_card] (0x4000): Found certificate has key id [02].\\n\\nMARCO TREVISAN (PIN CNS1)\\n\\n/usr/lib/x86_64-linux-gnu/pkcs11/opensc-pkcs11.so\\n\\n02\\n\\nCNS1\\n\\nMIIHXDCCBUSgAwIBAgIQA1ex7....\\n\\nFor checking if the smartcard works, without doing any verification check (and so for debugging purposes the option)\\n\\n--verify=no_ocsp can also be used, while --verify=partial_chain can be used to do partial CA verification.',\n",
       " '**Map certificates to user names**\\n\\nThe sss PAM module allows certificates to be used for login, though our Linux system needs to know the username\\nassociated to a certificate. SSSD provides a variety of cert mappers to do this. Each cert mapper uses specific\\ninformation from the certificate to map to a user on the system. The different cert mappers may even be stacked. In\\nother words, if the first defined mapper fails to map to a user on the system, the next one will be tried, and so on\\nuntil a user is found.\\n\\nFor the purposes of this guide, we will use a simple local user mapping as reference.\\n\\n[Mapping for more complex configurations can be done following the official SSSD documentation depending on](https://sssd.io/design-pages/matching_and_mapping_certificates.html)\\n[providers. For up-to-date information on certificate mapping, please also consult the sss-certmap manpage.](https://sssd.io/design-pages/certmaps_for_LDAP_AD_file.html)\\n\\n397\\n\\n\\n-----\\n\\n**Local users mapping**\\n\\nWhen using only local users, sssd can be easily configured to define an implicit_domain that maps all the local users.\\n\\nCertificate mapping for local users can be easily done using the certificate Subject check, in our example:\\n\\nopenssl x509 -noout -subject -in card-cert.pem | sed \"s/, /,/g;s/ = /=/g\"\\n\\nsubject=C=IT,O=Actalis S.p.A.,OU=REGIONE TOSCANA,SN=TREVISAN,GN=MARCO,CN=TRVMRC[...data-removed...]/6090033068507002.U\\n\\nSo we can use for the user foo :\\n\\n[sssd]\\n\\nenable_files_domain = True\\n\\nservices = pam\\n\\n[certmap/implicit_files/foo]\\n\\nmatchrule = <SUBJECT>.*CN=TRVMRC[A-Z0-9]+/6090033068507002\\\\.UyMnHxfF3gkAeBYHhxa6V1Edazs=.*\\n\\n[pam]\\n\\npam_cert_auth = True\\n\\n**Troubleshooting**',\n",
       " 'User mapping can be tested working in versions newer than Ubuntu 20.04 with:\\n\\n$ sudo dbus-send --system --print-reply \\\\\\n\\n--dest=org.freedesktop.sssd.infopipe \\\\\\n\\n/org/freedesktop/sssd/infopipe/Users \\\\\\n\\norg.freedesktop.sssd.infopipe.Users.ListByCertificate \\\\\\n\\nstring:\"$(cat card-cert.pem)\" uint32:10\\n\\nThat should return the object path containing the expected user ID:\\n\\nmethod return time=1605127192.698667 sender=:1.1628 -> destination=:1.1629 serial=6 reply_serial=2\\n\\narray [\\n\\nobject path \"/org/freedesktop/sssd/infopipe/Users/implicit_5ffiles/1000\"\\n\\n]\\n\\n**Basic SSSD configuration**\\n\\nThe SSSD configuration for accessing to the system is out of the scope of this document, however for smart card login\\nit should contain at least such values:\\n\\n[sssd]\\n\\n# Comma separated list of domains\\n\\n;domains = your-domain1, your-domain2\\n\\n# comma-separated list of SSSD services\\n\\n# pam might be implicitly loaded already, so the line is optional\\n\\nservices = pam\\n\\n# You can enable debug of the SSSD daemon\\n\\n# Logs will be in /var/log/sssd/sssd.log\\n\\n;debug_level = 10\\n\\n# A mapping between the SC certificate and users\\n\\n;[certmap/your-domain1/<username>]\\n\\n;matchrule = <SUBJECT>.*CN=<REGEX MATCHING YOUR CN>.*\\n\\n[pam]\\n\\npam_cert_auth = True\\n\\n# The Certificate DB to be used:\\n\\n# - Needs to be an openSSL CA certificates\\n\\n;pam_cert_db_path = /etc/ssl/certs/ca-certificates.crt\\n\\n# You can enable debug infos for the PAM module\\n\\n# Logs will be in /var/log/sssd/sssd_pam.log\\n\\n398\\n\\n\\n-----\\n\\n# p11 child logs are in /var/log/sssd/p11_child.log\\n\\n# standard auth logs are in /var/log/auth.log\\n\\n;pam_verbosity = 10\\n\\n;debug_level = 10',\n",
       " 'In general what’s in the configuration file will affect the way SSSD will call the p11_child tool (that is the one in\\ncharge for the actual authentication).\\nCheck man sssd.conf for details.\\n\\nRemember that this file should be owned by root and have permission set to 600, otherwise won’t be loaded and SSSD\\nwill not complain gracefully.\\nOn errors you can test running SSSD temporary with sudo sssd -d9 -i .\\n\\nEvery time the configuration is changed sssd should be restarted ( systemctl restart sssd ).\\n\\n**Add** pam_sss **to PAM**\\n\\nThe next step includes the pam_sss module into the PAM stack. There are various ways to do this depending on your\\nlocal policy. The following example enables smart card support for general authentication.\\n\\nEdit /etc/pam.d/common-auth to include the pam_sss module as follows:\\n\\n**For Ubuntu later than 23.10**\\n\\n$ sudo pam-auth-update\\n\\nThen you can interactively enable SSSD profiles for smart-card only or optional smart card access.\\n\\nYou can also set this non-interactively by using:\\n\\n# To use smart-card only authentication\\n\\n$ sudo pam-auth-update --disable sss-smart-card-optional --enable sss-smart-card-required\\n\\n# To use smart-card authentication with fallback\\n\\n$ sudo pam-auth-update --disable sss-smart-card-required --enable sss-smart-card-optional\\n\\n**For Ubuntu 23.10 and lower**\\n\\n# require SSSD smart card login\\n\\nauth [success=done default=die] pam_sss.so allow_missing_name require_cert_auth\\n\\nor only try to use it:\\n\\n# try SSSD smart card login\\n\\nauth [success=ok default=ignore] pam_sss.so allow_missing_name try_cert_auth',\n",
       " 'See [man pam.conf](https://manpages.ubuntu.com/manpages/jammy/en/man5/pam.conf.5.html), [man pam_sss](https://manpages.ubuntu.com/manpages/jammy/en/man8/pam_sss.8.html) for further details.\\n\\n**Warning:** A global configuration such as this requires a smart card for su and sudo authentication as well!\\nIf you want to reduce the scope of this module, move it to the appropriate pam configuration file in /etc/pam.d and\\nensure that’s referenced by pam_p11_allowed_services in sssd.conf .\\n\\nThe OS is now ready to do a smart card login for the user foo.\\n\\n**Troubleshooting**\\n\\npamtester is your friend!\\n\\nTo get better debug logging, also increase the SSSD verbosity by changing /etc/sssd/sssd.conf so that it has:\\n\\n[pam]\\n\\npam_verbosity = 10\\n\\ndebug_level = 10\\n\\nYou can use it to check your configuration without having to login/logout for real, by just using:\\n\\n# Install it!\\n\\n$ sudo apt install pamtester\\n\\n399\\n\\n\\n-----\\n\\n# Run the authentication service as standalone\\n\\n$ pamtester -v login $USER authenticate\\n\\n# Run the authentication service to get user from cert\\n\\n$ pamtester -v login \"\" authenticate\\n\\n# You can check what happened in the logs, reading:\\n\\nsudo less /var/log/auth.log\\n\\nsudo less /var/log/sssd/sssd_pam.log\\n\\nsudo less /var/log/sssd/p11_child.log\\n## **SSH authentication**\\n\\nSee this page on SSH authentication with smart cards.\\n\\nOne of the authentication methods supported by the SSH protocol is public key authentication. A public key is copied\\nto the SSH server where it is stored and marked as authorized. The owner of the corresponding private key in the\\nsmart card can then SSH login to the server.',\n",
       " 'We will use opensc-pkcs11 on the client to access the smart card drivers, and we will copy the public key from the\\nsmart card to the SSH server to make the authentication work.\\n\\nThe following instructions apply to Ubuntu 18.04 later.\\n## **Server configuration**\\n\\nThe SSH server and client must be configured to permit smart card authentication.\\n\\n**Configure the SSH server**\\n\\nThe SSH server needs to allow public key authentication set in its configuration file and it needs the user’s public key.\\n\\nEnsure the server has the PubkeyAuthentication option set to ‘yes’ in its /etc/ssh/sshd_config file. In a default\\n\\n/etc/ssh/sshd_config in Ubuntu, the\\nPubkeyAuthentication option is commented out. However, the default is ‘yes’. To ensure the setting, edit the\\n\\nsshd_config file and set accordingly.\\n\\nPubkeyAuthentication yes\\n\\n**Restart the SSH server**\\n\\nsudo systemctl restart sshd\\n\\n**Set the public key on the server**\\n\\nExtract the user’s public key from the smart card on the SSH client. Use sshkeygen to read the public key from the\\nsmart card and into a format consumable\\n\\nfor SSH.\\n\\nssh-keygen -D /usr/lib/x86_64-linux-gnu/opensc-pkcs11.so > smartcard.pub\\n\\nCopy this key to the SSH server.\\n\\nssh-copy-id -f -i smartcard.pub ubuntu@server-2\\n\\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: “smartcard.pub”\\n\\nubuntu@server-2’s password:\\n\\nNumber of key(s) added: 1\\n\\nNow try logging into the machine, with: “ssh ‘ubuntu@server-2’”\\n\\nand check to make sure that only the key(s) you wanted were added.\\n## **Client configuration**\\n\\nThe SSH client needs to identify its PKCS#11 provider. To do that set the PKCS11Provider option in the',\n",
       " '~/.ssh/config file of each user desiring to use SSH smart card login.\\n\\nPKCS11Provider /usr/lib/x86_64-linux-gnu/opensc-pkcs11.so\\n\\n400\\n\\n\\n-----\\n\\nUse this method to enforce SSH smart card login on a per user basis.\\n\\nAfter this step you can SSH into the server using the smart card for authentication.\\n\\nAppArmor is a Linux Security Module implementation of name-based mandatory access controls. AppArmor confines\\nindividual programs to a set of listed files and posix 1003.1e draft capabilities.\\n\\nAppArmor is installed and loaded by default. It uses *profiles* of an application to determine what files and permissions\\nthe application requires. Some packages will install their own profiles, and additional profiles can be found in the\\napparmor-profiles package.\\n\\nTo install the apparmor-profiles package from a terminal prompt:\\n\\nsudo apt install apparmor-profiles\\n\\nAppArmor profiles have two modes of execution:\\n\\n  - Complaining/Learning: profile violations are permitted and logged. Useful for testing and developing new\\nprofiles.\\n\\n  - Enforced/Confined: enforces profile policy as well as logging the violation.\\n## **Using AppArmor**\\n\\nThe optional apparmor-utils package contains command line utilities that you can use to change the AppArmor\\nexecution mode, find the status of a profile, create new profiles, etc.\\n\\n  - apparmor_status is used to view the current status of AppArmor profiles.\\n\\nsudo apparmor_status\\n\\n  - aa-complain places a profile into *complain* mode.\\n\\nsudo aa-complain /path/to/bin\\n\\n  - aa-enforce places a profile into *enforce* mode.\\n\\nsudo aa-enforce /path/to/bin\\n\\n - The /etc/apparmor.d directory is where the AppArmor profiles are located. It can be used to manipulate the',\n",
       " '*mode* of all profiles.\\n\\nEnter the following to place all profiles into complain mode:\\n\\nsudo aa-complain /etc/apparmor.d/*\\n\\nTo place all profiles in enforce mode:\\n\\nsudo aa-enforce /etc/apparmor.d/*\\n\\n  - apparmor_parser is used to load a profile into the kernel. It can also be used to reload a currently loaded profile\\nusing the *-r* option after modifying it to have the changes take effect.\\nTo reload a profile:\\n\\nsudo apparmor_parser -r /etc/apparmor.d/profile.name\\n\\n - systemctl can be used to *reload* all profiles:\\n\\nsudo systemctl reload apparmor.service\\n\\n - The /etc/apparmor.d/disable directory can be used along with the apparmor_parser -R option to *disable* a\\nprofile.\\n\\nsudo ln -s /etc/apparmor.d/profile.name /etc/apparmor.d/disable/\\n\\nsudo apparmor_parser -R /etc/apparmor.d/profile.name\\n\\nTo *re-enable* a disabled profile remove the symbolic link to the profile in /etc/apparmor.d/disable/ . Then load\\nthe profile using the *-a* option.\\n\\nsudo rm /etc/apparmor.d/disable/profile.name\\n\\ncat /etc/apparmor.d/profile.name | sudo apparmor_parser -a\\n\\n  - AppArmor can be disabled, and the kernel module unloaded by entering the following:\\n\\nsudo systemctl stop apparmor.service\\n\\nsudo systemctl disable apparmor.service\\n\\n  - To re-enable AppArmor enter:\\n\\n401\\n\\n\\n-----\\n\\nsudo systemctl enable apparmor.service\\n\\nsudo systemctl start apparmor.service\\n\\n**Note**\\n\\nReplace *profile.name* with the name of the profile you want to manipulate. Also, replace /path/to/bin/\\nwith the actual executable file path. For example for the ping command use /bin/ping\\n## **Profiles**\\n\\nAppArmor profiles are simple text files located in /etc/apparmor.d/ . The files are named after the full path to the',\n",
       " 'executable they profile replacing the “/” with “.”. For example /etc/apparmor.d/bin.ping is the AppArmor profile for\\nthe /bin/ping command.\\n\\nThere are two main type of rules used in profiles:\\n\\n - *Path entries:* detail which files an application can access in the file system.\\n\\n - *Capability entries:* determine what privileges a confined process is allowed to use.\\n\\nAs an example, take a look at /etc/apparmor.d/bin.ping :\\n\\n#include <tunables/global>\\n\\n/bin/ping flags=(complain) {\\n\\n#include <abstractions/base>\\n\\n#include <abstractions/consoles>\\n\\n#include <abstractions/nameservice>\\n\\ncapability net_raw,\\n\\ncapability setuid,\\n\\nnetwork inet raw,\\n\\n/bin/ping mixr,\\n\\n/etc/modules.conf r,\\n\\n}\\n\\n - *#include <tunables/global>:* include statements from other files. This allows statements pertaining to multiple\\napplications to be placed in a common file.\\n\\n - */bin/ping flags=(complain):* path to the profiled program, also setting the mode to *complain* .\\n\\n - *capability net_raw,:* allows the application access to the CAP_NET_RAW Posix.1e capability.\\n\\n - */bin/ping mixr,:* allows the application read and execute access to the file.\\n\\n**Note**\\n\\nAfter editing a profile file the profile must be reloaded. See above at Using AppArmor for details.\\n\\n**Creating a Profile**\\n\\n - *Design a test plan:* Try to think about how the application should be exercised. The test plan should be divided\\ninto small test cases. Each test case should have a small description and list the steps to follow.\\n\\nSome standard test cases are:\\n\\n**–**\\nStarting the program.\\n\\n**–**\\nStopping the program.\\n\\n**–**\\nReloading the program.\\n\\n**–**\\nTesting all the commands supported by the init script.',\n",
       " ' - *Generate the new profile:* Use aa-genprof to generate a new profile. From a terminal:\\n\\nsudo aa-genprof executable\\n\\nFor example:\\n\\nsudo aa-genprof slapd\\n\\n  - To get your new profile included in the apparmor-profiles package, file a bug in *Launchpad* [against the AppArmor](https://bugs.launchpad.net/ubuntu/+source/apparmor/+filebug)\\n\\npackage:\\n\\n402\\n\\n\\n-----\\n\\n**–**\\nInclude your test plan and test cases.\\n\\n**–**\\nAttach your new profile to the bug.\\n\\n**Updating Profiles**\\n\\nWhen the program is misbehaving, audit messages are sent to the log files. The program aa-logprof can be used to\\nscan log files for AppArmor audit messages, review them and update the profiles. From a terminal:\\n\\nsudo aa-logprof\\n\\n**Further pre-existing Profiles**\\n\\nThe packages apport-profiles and apparmor-profiles-extra ship some experimental profiles for AppArmor security\\npolicies.\\nDo not expect these profiles to work out-of-the-box, but they can give you a head start when trynig to create a new\\nprofile by starting off a base that exists.\\n\\nThese profiles are not considered mature enough to be shipped in enforce mode by default. Therefore they are shipped\\nin complain mode so that users can test them, choose which are desired, and help improve them upstream if needed.\\n\\nSome even more experimental profiles carried by the package are placed in /usr/share/doc/apparmor-profiles/extras/\\n## **Checking and debugging denies**\\n\\nYou will see in ‘dmesg’ and any log that collects kernel messages if you have hit a deny.\\nRight away it is worth to know that this will cover any access that was denied because it was not allowed, but\\n\\nexplicit denies will put no message in your logs at all.\\n\\nExamples might look like:',\n",
       " '[1521056.552037] audit: type=1400 audit(1571868402.378:24425): apparmor=\"DENIED\" operation=\"open\" profile=\"/usr/sbin/cu\\n\\nbrowsed\" name=\"/var/lib/libvirt/dnsmasq/\" pid=1128 comm=\"cups-browsed\" requested_mask=\"r\" denied_mask=\"r\" fsuid=0 ouid=0\\n\\n[1482106.651527] audit: type=1400 audit(1571829452.330:24323): apparmor=\"DENIED\" operation=\"sendmsg\" profile=\"snap.lxd.\\n\\nThat follows a generic structure starting with a timestamp, an audit tag and the category apparmor=\"DENIED\" .\\nFrom the following fields you can derive what was going on and why it was failing.\\n\\nIn the examples above that would be\\n\\nFirst example:\\n\\n  - operation: open (program tried to open a file)\\n\\n  - profile: /usr/sbin/cups-browsed (you’ll find /etc/apparmor.d/usr.bin.cups-browsed )\\n\\n - name: /var/lib/libvirt/dnsmasq (what it wanted to access)\\n\\n  - pid/comm: the program that did trigger the access\\n\\n  - requested_mask/denied_mask/fsuid/ouid: parameters of that open call\\n\\nSecond example:\\n\\n  - operation: sendmsg (program tried send via network)\\n\\n  - profile: snap.lxd.lxc (snaps are special, you’ll find /var/lib/snapd/apparmor/profiles/snap.lxd.lxc )\\n\\n  - pid/comm: the program that did trigger the access\\n\\n  - laddr/lport/faddr/fport/family/sock_type/protocol: parameters of that sendmsg call\\n\\nThat way you know in which profile and at what action you have to start if you consider either debugging or adapting\\nthe profiles.\\n## **Profile customization**\\n\\nProfiles are meant to provide security and thereby can’t be all too open. But quite often a very special setup would\\nwork with a profile if it wold *just allow this one extra access* . To handle that there are three ways.\\n\\n  - modify the profile itself\\n\\n**–**',\n",
       " 'always works, but has the drawback that profiles are in /etc and considered conffiles. So after modification\\non a related package update you might get a conffile prompt. Worst case depending on configuration\\nautomatic updates might even override it and your custom rule is gone.\\n\\n  - use tunables\\n\\n**–**\\nthose provide variables that can be used in templates, for example if you want a custom dir considered as it\\nwould be a home directory you could modify /etc/apparmor.d/tunables/home which defines the base path\\nrules use for home directories\\n\\n403\\n\\n\\n-----\\n\\n**–**\\nby design those variables will only influence profiles that use them\\n\\n  - modify a local override\\n\\n**–** to mitigate the drawbacks of above approaches *local includes* got introduced adding the ability to write\\narbitrary rules that will be used, and not get issues on upgrades that modify the packaged rule.\\n\\n**–**\\nThe files can be found in /etc/apparmor.d/local/ and exist for the packages that are known to sometimes\\nneed slight tweaks for special setups\\n## **References**\\n\\n[• See the AppArmor Administration Guide for advanced configuration options.](http://www.novell.com/documentation/apparmor/apparmor201_sp10_admin/index.html?page=/documentation/apparmor/apparmor201_sp10_admin/data/book_apparmor_admin.html)\\n\\n[• For details using AppArmor with other Ubuntu releases see the AppArmor Community Wiki page.](https://help.ubuntu.com/community/AppArmor)\\n\\n[• The OpenSUSE AppArmor page is another introduction to AppArmor.](http://en.opensuse.org/SDB:AppArmor_geeks)\\n\\n[• (https://wiki.debian.org/AppArmor) is another introduction and basic howto for AppArmor.](https://wiki.debian.org/AppArmor)',\n",
       " '  - A great place to get involved with the Ubuntu Server community and to ask for AppArmor assistance is the\\n*#ubuntu-server* [IRC channel on Libera. The](https://libera.chat) *#ubuntu-security* IRC channel may also be of use.\\n## **Introduction**\\n\\nThe Linux kernel includes the *Netfilter* subsystem, which is used to manipulate or decide the fate of network traffic\\nheaded into or through your server. All modern Linux firewall solutions use this system for packet filtering.\\n\\nThe kernel’s packet filtering system would be of little use to administrators without a userspace interface to manage it.\\nThis is the purpose of iptables: When a packet reaches your server, it will be handed off to the Netfilter subsystem for\\nacceptance, manipulation, or rejection based on the rules supplied to it from userspace via iptables. Thus, iptables is\\nall you need to manage your firewall, if you’re familiar with it, but many frontends are available to simplify the task.\\n## **ufw - Uncomplicated Firewall**\\n\\nThe default firewall configuration tool for Ubuntu is ufw. Developed to ease iptables firewall configuration, ufw\\nprovides a user-friendly way to create an IPv4 or IPv6 host-based firewall.\\n\\nufw by default is initially disabled. From the ufw man page:\\n\\n“ufw is not intended to provide complete firewall functionality via its command interface, but instead provides an easy\\nway to add or remove simple rules. It is currently mainly used for host-based firewalls.”\\n\\nThe following are some examples of how to use ufw:\\n\\n  - First, ufw needs to be enabled. From a terminal prompt enter:\\n\\nsudo ufw enable\\n\\n  - To open a port (SSH in this example):\\n\\nsudo ufw allow 22',\n",
       " '  - Rules can also be added using a *numbered* format:\\n\\nsudo ufw insert 1 allow 80\\n\\n  - Similarly, to close an opened port:\\n\\nsudo ufw deny 22\\n\\n  - To remove a rule, use delete followed by the rule:\\n\\nsudo ufw delete deny 22\\n\\n  - It is also possible to allow access from specific hosts or networks to a port. The following example allows SSH\\naccess from host 192.168.0.2 to any IP address on this host:\\n\\nsudo ufw allow proto tcp from 192.168.0.2 to any port 22\\n\\nReplace 192.168.0.2 with 192.168.0.0/24 to allow SSH access from the entire subnet.\\n\\n  - Adding the *–dry-run* option to a *ufw* command will output the resulting rules, but not apply them. For example,\\nthe following is what would be applied if opening the HTTP port:\\n\\nsudo ufw --dry-run allow http\\n\\n*filter\\n\\n:ufw-user-input - [0:0]\\n\\n:ufw-user-output - [0:0]\\n\\n404\\n\\n\\n-----\\n\\n:ufw-user-forward - [0:0]\\n\\n:ufw-user-limit - [0:0]\\n\\n:ufw-user-limit-accept - [0:0]\\n\\n### RULES ###\\n\\n### tuple ### allow tcp 80 0.0.0.0/0 any 0.0.0.0/0\\n\\n-A ufw-user-input -p tcp --dport 80 -j ACCEPT\\n\\n### END RULES ###\\n\\n-A ufw-user-input -j RETURN\\n\\n-A ufw-user-output -j RETURN\\n\\n-A ufw-user-forward -j RETURN\\n\\n-A ufw-user-limit -m limit --limit 3/minute -j LOG --log-prefix \"[UFW LIMIT]: \"\\n\\n-A ufw-user-limit -j REJECT\\n\\n-A ufw-user-limit-accept -j ACCEPT\\n\\nCOMMIT\\n\\nRules updated\\n\\n  - ufw can be disabled by:\\n\\nsudo ufw disable\\n\\n  - To see the firewall status, enter:\\n\\nsudo ufw status\\n\\n  - And for more verbose status information use:\\n\\nsudo ufw status verbose\\n\\n  - To view the *numbered* format:\\n\\nsudo ufw status numbered\\n\\n**Note**\\n\\nIf the port you want to open or close is defined in /etc/services, you can use the port name instead of the\\nnumber.',\n",
       " 'In the above examples, replace *22* with *ssh* .\\n\\nThis is a quick introduction to using ufw. Please refer to the ufw man page for more information.\\n\\n**ufw Application Integration**\\n\\nApplications that open ports can include an ufw profile, which details the ports needed for the application to function\\nproperly. The profiles are kept in /etc/ufw/applications.d, and can be edited if the default ports have been changed.\\n\\n  - To view which applications have installed a profile, enter the following in a terminal:\\n\\nsudo ufw app list\\n\\n  - Similar to allowing traffic to a port, using an application profile is accomplished by entering:\\n\\nsudo ufw allow Samba\\n\\n  - An extended syntax is available as well:\\n\\nufw allow from 192.168.0.0/24 to any app Samba\\n\\nReplace *Samba* and *192.168.0.0/24* with the application profile you are using and the IP range for your network.\\n\\n**Note**\\n\\nThere is no need to specify the *protocol* for the application, because that information is detailed in the\\nprofile. Also, note that the *app* name replaces the *port* number.\\n\\n  - To view details about which ports, protocols, etc., are defined for an application, enter:\\n\\nsudo ufw app info Samba\\n\\nNot all applications that require opening a network port come with ufw profiles, but if you have profiled an application\\nand want the file to be included with the package, please file a bug against the package in Launchpad.\\n\\nubuntu-bug nameofpackage\\n\\n405\\n\\n\\n-----\\n\\n## **IP Masquerading**\\n\\nThe purpose of IP Masquerading is to allow machines with private, non-routable IP addresses on your network to\\naccess the Internet through the machine doing the masquerading. Traffic from your private network destined for the',\n",
       " 'Internet must be manipulated for replies to be routable back to the machine that made the request. To do this, the\\nkernel must modify the *source* IP address of each packet so that replies will be routed back to it, rather than to the\\nprivate IP address that made the request, which is impossible over the Internet. Linux uses *Connection Tracking*\\n(conntrack) to keep track of which connections belong to which machines and reroute each return packet accordingly.\\nTraffic leaving your private network is thus “masqueraded” as having originated from your Ubuntu gateway machine.\\nThis process is referred to in Microsoft documentation as Internet Connection Sharing.\\n\\n**ufw Masquerading**\\n\\nIP Masquerading can be achieved using custom ufw rules. This is possible because the current back-end for ufw is\\niptables-restore with the rules files located in /etc/ufw/*.rules . These files are a great place to add legacy iptables\\nrules used without ufw, and rules that are more network gateway or bridge related.\\n\\nThe rules are split into two different files, rules that should be executed before ufw command line rules, and rules that\\nare executed after ufw command line rules.\\n\\n  - First, packet forwarding needs to be enabled in ufw. Two configuration files will need to be adjusted, in\\n\\n/etc/default/ufw change the *DEFAULT_FORWARD_POLICY* to “ACCEPT”:\\n\\nDEFAULT_FORWARD_POLICY=\"ACCEPT\"\\n\\nThen edit /etc/ufw/sysctl.conf and uncomment:\\n\\nnet/ipv4/ip_forward=1\\n\\nSimilarly, for IPv6 forwarding uncomment:\\n\\nnet/ipv6/conf/default/forwarding=1\\n\\n  - Now add rules to the /etc/ufw/before.rules file. The default rules only configure the *filter* table, and to enable',\n",
       " \"masquerading the *nat* table will need to be configured. Add the following to the top of the file just after the\\nheader comments:\\n\\n# nat Table rules\\n\\n*nat\\n\\n:POSTROUTING ACCEPT [0:0]\\n\\n# Forward traffic from eth1 through eth0.\\n\\n-A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE\\n\\n# don't delete the 'COMMIT' line or these nat table rules won't be processed\\n\\nCOMMIT\\n\\nThe comments are not strictly necessary, but it is considered good practice to document your configuration. Also,\\nwhen modifying any of the *rules* files in /etc/ufw, make sure these lines are the last line for each table modified:\\n\\n# don't delete the 'COMMIT' line or these rules won't be processed\\n\\nCOMMIT\\n\\nFor each *Table* a corresponding *COMMIT* statement is required. In these examples only the *nat* and *filter* tables\\nare shown, but you can also add rules for the *raw* and *mangle* tables.\\n\\n**Note**\\n\\nIn the above example replace *eth0*, *eth1*, and *192.168.0.0/24* with the appropriate interfaces and IP\\nrange for your network.\\n\\n  - Finally, disable and re-enable ufw to apply the changes:\\n\\nsudo ufw disable && sudo ufw enable\\n\\nIP Masquerading should now be enabled. You can also add any additional FORWARD rules to the /etc/ufw/before.rules .\\nIt is recommended that these additional rules be added to the *ufw-before-forward* chain.\\n\\n**iptables Masquerading**\\n\\niptables can also be used to enable Masquerading.\\n\\n406\\n\\n\\n-----\\n\\n  - Similar to ufw, the first step is to enable IPv4 packet forwarding by editing /etc/sysctl.conf and uncomment\\nthe following line:\\n\\nnet.ipv4.ip_forward=1\\n\\nIf you wish to enable IPv6 forwarding also uncomment:\\n\\nnet.ipv6.conf.default.forwarding=1\",\n",
       " '  - Next, execute the sysctl command to enable the new settings in the configuration file:\\n\\nsudo sysctl -p\\n\\n  - IP Masquerading can now be accomplished with a single iptables rule, which may differ slightly based on your\\nnetwork configuration:\\n\\nsudo iptables -t nat -A POSTROUTING -s 192.168.0.0/16 -o ppp0 -j MASQUERADE\\n\\nThe above command assumes that your private address space is 192.168.0.0/16 and that your Internet-facing\\ndevice is ppp0. The syntax is broken down as follows:\\n\\n**–**\\n-t nat – the rule is to go into the nat table\\n\\n**–**\\n-A POSTROUTING – the rule is to be appended (-A) to the POSTROUTING chain\\n\\n**–**\\n-s 192.168.0.0/16 – the rule applies to traffic originating from the specified address space\\n\\n**–**\\n-o ppp0 – the rule applies to traffic scheduled to be routed through the specified network device\\n\\n**–**\\n-j MASQUERADE – traffic matching this rule is to “jump” (-j) to the MASQUERADE target to be\\nmanipulated as described above\\n\\n  - Also, each chain in the filter table (the default table, and where most or all packet filtering occurs) has a default\\n*policy* of ACCEPT, but if you are creating a firewall in addition to a gateway device, you may have set the policies\\nto DROP or REJECT, in which case your masqueraded traffic needs to be allowed through the FORWARD chain\\nfor the above rule to work:\\n\\nsudo iptables -A FORWARD -s 192.168.0.0/16 -o ppp0 -j ACCEPT\\n\\nsudo iptables -A FORWARD -d 192.168.0.0/16 -m state \\\\\\n\\n--state ESTABLISHED,RELATED -i ppp0 -j ACCEPT\\n\\nThe above commands will allow all connections from your local network to the Internet and all traffic related to\\nthose connections to return to the machine that initiated them.',\n",
       " '  - If you want masquerading to be enabled on reboot, which you probably do, edit /etc/rc.local and add any\\ncommands used above. For example add the first command with no filtering:\\n\\niptables -t nat -A POSTROUTING -s 192.168.0.0/16 -o ppp0 -j MASQUERADE\\n## **Logs**\\n\\nFirewall logs are essential for recognizing attacks, troubleshooting your firewall rules, and noticing unusual activity\\non your network. You must include logging rules in your firewall for them to be generated, though, and logging rules\\nmust come before any applicable terminating rule (a rule with a target that decides the fate of the packet, such as\\nACCEPT, DROP, or REJECT).\\n\\nIf you are using ufw, you can turn on logging by entering the following in a terminal:\\n\\nsudo ufw logging on\\n\\nTo turn logging off in ufw, simply replace *on* with *off* in the above command.\\n\\nIf using iptables instead of ufw, enter:\\n\\nsudo iptables -A INPUT -m state --state NEW -p tcp --dport 80 \\\\\\n\\n-j LOG --log-prefix \"NEW_HTTP_CONN: \"\\n\\nA request on port 80 from the local machine, then, would generate a log in dmesg that looks like this (single line split\\ninto 3 to fit this document):\\n\\n[4304885.870000] NEW_HTTP_CONN: IN=lo OUT= MAC=00:00:00:00:00:00:00:00:00:00:00:00:08:00\\n\\nSRC=127.0.0.1 DST=127.0.0.1 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=58288 DF PROTO=TCP\\n\\nSPT=53981 DPT=80 WINDOW=32767 RES=0x00 SYN URGP=0\\n\\n407\\n\\n\\n-----\\n\\nThe above log will also appear in /var/log/messages, /var/log/syslog, and /var/log/kern.log . This behavior can\\nbe modified by editing /etc/syslog.conf appropriately or by installing and configuring ulogd and using the ULOG\\ntarget instead of LOG.',\n",
       " 'The ulogd daemon is a userspace server that listens for logging instructions from the kernel\\nspecifically for firewalls, and can log to any file you like, or even to a PostgreSQL or MySQL database. Making sense\\nof your firewall logs can be simplified by using a log analyzing tool such as logwatch, fwanalog, fwlogwatch, or lire.\\n## **Other Tools**\\n\\nThere are many tools available to help you construct a complete firewall without intimate knowledge of iptables. A\\ncommand-line tool with plain-text configuration files:\\n\\n[• Shorewall is a very powerful solution to help you configure an advanced firewall for any network.](http://www.shorewall.net/)\\n## **References**\\n\\n[• The Ubuntu Firewall wiki page contains information on the development of ufw.](https://wiki.ubuntu.com/UncomplicatedFirewall)\\n\\n  - Also, the ufw manual page contains some very useful information: man ufw .\\n\\n[• See the packet-filtering-HOWTO for more information on using iptables.](http://www.netfilter.org/documentation/HOWTO/packet-filtering-HOWTO.html)\\n\\n[• The nat-HOWTO contains further details on masquerading.](http://www.netfilter.org/documentation/HOWTO/NAT-HOWTO.html)\\n\\n[• The IPTables HowTo in the Ubuntu wiki is a great resource.](https://help.ubuntu.com/community/IptablesHowTo)\\n\\nOne of the most common forms of cryptography today is *public-key* cryptography. Public-key cryptography utilizes a\\n*public key* and a *private key* . The system works by encrypting information using the public key. The information can\\nthen only be decrypted using the private key.\\n\\nA common use for public-key cryptography is encrypting application traffic using a Secure Socket Layer (SSL) or',\n",
       " 'Transport Layer Security (TLS) connection. One example: configuring Apache to provide *HTTPS*, the HTTP protocol\\nover SSL/TLS. This allows a way to encrypt traffic using a protocol that does not itself provide encryption.\\n\\nA *certificate* is a method used to distribute a *public key* and other information about a server and the organization\\nwho is responsible for it. Certificates can be digitally signed by a *Certification Authority*, or CA. A CA is a trusted\\nthird party that has confirmed that the information contained in the certificate is accurate.\\n## **Types of Certificates**\\n\\nTo set up a secure server using public-key cryptography, in most cases, you send your certificate request (including\\nyour public key), proof of your company’s identity, and payment to a CA. The CA verifies the certificate request\\nand your identity, and then sends back a certificate for your secure server. Alternatively, you can create your own\\n*self-signed* certificate.\\n\\n**Note**\\n\\nNote that self-signed certificates should not be used in most production environments.\\n\\nContinuing the HTTPS example, a CA-signed certificate provides two important capabilities that a self-signed certificate does not:\\n\\n  - Browsers (usually) automatically recognize the CA signature and allow a secure connection to be made without\\nprompting the user.\\n\\n  - When a CA issues a signed certificate, it is guaranteeing the identity of the organization that is providing the\\nweb pages to the browser.\\n\\nMost of the software supporting SSL/TLS have a list of CAs whose certificates they automatically accept. If a browser',\n",
       " 'encounters a certificate whose authorizing CA is not in the list, the browser asks the user to either accept or decline\\nthe connection. Also, other applications may generate an error message when using a self-signed certificate.\\n\\nThe process of getting a certificate from a CA is fairly easy. A quick overview is as follows:\\n\\n1. Create a private and public encryption key pair.\\n\\n2. Create a certificate signing request based on the public key. The certificate request contains information about\\nyour server and the company hosting it.\\n\\n3. Send the certificate request, along with documents proving your identity, to a CA. We cannot tell you which\\ncertificate authority to choose. Your decision may be based on your past experiences, or on the experiences of\\nyour friends or colleagues, or purely on monetary factors.\\n\\n408\\n\\n\\n-----\\n\\nOnce you have decided upon a CA, you need to follow the instructions they provide on how to obtain a certificate\\nfrom them.\\n\\n4. When the CA is satisfied that you are indeed who you claim to be, they send you a digital certificate.\\n\\n5. Install this certificate on your secure server, and configure the appropriate applications to use the certificate.\\n## **Generating a Certificate Signing Request (CSR)**\\n\\nWhether you are getting a certificate from a CA or generating your own self-signed certificate, the first step is to\\ngenerate a key.\\n\\nIf the certificate will be used by service daemons, such as Apache, Postfix, Dovecot, etc., a key without a passphrase\\nis often appropriate. Not having a passphrase allows the services to start without manual intervention, usually the\\npreferred way to start a daemon.',\n",
       " 'This section will cover generating a key with a passphrase, and one without. The non-passphrase key will then be\\nused to generate a certificate that can be used with various service daemons.\\n\\n**Warning**\\n\\nRunning your secure service without a passphrase is convenient because you will not need to enter the\\npassphrase every time you start your secure service. But it is insecure and a compromise of the key means\\na compromise of the server as well.\\n\\nTo generate the *keys* for the Certificate Signing Request (CSR) run the following command from a terminal prompt:\\n\\nopenssl genrsa -des3 -out server.key 2048\\n\\nGenerating RSA private key, 2048 bit long modulus\\n\\n..........................++++++\\n\\n.......++++++\\n\\ne is 65537 (0x10001)\\n\\nEnter pass phrase for server.key:\\n\\nYou can now enter your passphrase. For best security, it should at least contain eight characters. The minimum length\\nwhen specifying -des3 is four characters. As a best practice it should include numbers and/or punctuation and not be\\na word in a dictionary. Also remember that your passphrase is case-sensitive.\\n\\nRe-type the passphrase to verify. Once you have re-typed it correctly, the server key is generated and stored in the\\n\\nserver.key file.\\n\\nNow create the insecure key, the one without a passphrase, and shuffle the key names:\\n\\nopenssl rsa -in server.key -out server.key.insecure\\n\\nmv server.key server.key.secure\\n\\nmv server.key.insecure server.key\\n\\nThe insecure key is now named server.key, and you can use this file to generate the CSR without passphrase.\\n\\nTo create the CSR, run the following command at a terminal prompt:\\n\\nopenssl req -new -key server.key -out server.csr\\n\\nIt will prompt you enter the passphrase.',\n",
       " 'If you enter the correct passphrase, it will prompt you to enter Company\\nName, Site Name, Email Id, etc. Once you enter all these details, your CSR will be created and it will be stored in\\nthe server.csr file.\\n\\nYou can now submit this CSR file to a CA for processing. The CA will use this CSR file and issue the certificate. On\\nthe other hand, you can create self-signed certificate using this CSR.\\n## **Creating a Self-Signed Certificate**\\n\\nTo create the self-signed certificate, run the following command at a terminal prompt:\\n\\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\\n\\nThe above command will prompt you to enter the passphrase. Once you enter the correct passphrase, your certificate\\nwill be created and it will be stored in the server.crt file.\\n\\n**Warning**\\n\\nIf your secure server is to be used in a production environment, you probably need a CA-signed certificate.\\nIt is not recommended to use self-signed certificate.\\n\\n409\\n\\n\\n-----\\n\\n## **Installing the Certificate**\\n\\nYou can install the key file server.key and certificate file server.crt, or the certificate file issued by your CA, by\\nrunning following commands at a terminal prompt:\\n\\nsudo cp server.crt /etc/ssl/certs\\n\\nsudo cp server.key /etc/ssl/private\\n\\nNow simply configure any applications, with the ability to use public-key cryptography, to use the *certificate* and *key*\\nfiles. For example, Apache can provide HTTPS, Dovecot can provide IMAPS and POP3S, etc.\\n## **Certification Authority**\\n\\nIf the services on your network require more than a few self-signed certificates it may be worth the additional effort\\nto setup your own internal Certification Authority (CA).',\n",
       " 'Using certificates signed by your own CA, allows the various\\nservices using the certificates to easily trust other services using certificates issued from the same CA.\\n\\nFirst, create the directories to hold the CA certificate and related files:\\n\\nsudo mkdir /etc/ssl/CA\\n\\nsudo mkdir /etc/ssl/newcerts\\n\\nThe CA needs a few additional files to operate, one to keep track of the last serial number used by the CA, each\\ncertificate must have a unique serial number, and another file to record which certificates have been issued:\\n\\nsudo sh -c \"echo \\'01\\' > /etc/ssl/CA/serial\"\\n\\nsudo touch /etc/ssl/CA/index.txt\\n\\nThe third file is a CA configuration file. Though not strictly necessary, it is very convenient when issuing multiple\\ncertificates. Edit /etc/ssl/openssl.cnf, and in the *[ CA_default ]* change:\\n\\ndir = /etc/ssl # Where everything is kept\\n\\ndatabase = $dir/CA/index.txt # database index file.\\n\\ncertificate = $dir/certs/cacert.pem # The CA certificate\\n\\nserial = $dir/CA/serial # The current serial number\\n\\nprivate_key = $dir/private/cakey.pem# The private key\\n\\nNext, create the self-signed root certificate:\\n\\nopenssl req -new -x509 -extensions v3_ca -keyout cakey.pem -out cacert.pem -days 3650\\n\\nYou will then be asked to enter the details about the certificate.\\n\\nNow install the root certificate and key:\\n\\nsudo mv cakey.pem /etc/ssl/private/\\n\\nsudo mv cacert.pem /etc/ssl/certs/\\n\\nYou are now ready to start signing certificates. The first item needed is a Certificate Signing Request (CSR), see\\nGenerating a Certificate Signing Request (CSR) for details. Once you have a CSR, enter the following to generate a\\ncertificate signed by the CA:\\n\\nsudo openssl ca -in server.csr -config /etc/ssl/openssl.cnf',\n",
       " 'After entering the password for the CA key, you will be prompted to sign the certificate, and again to commit the new\\ncertificate. You should then see a somewhat large amount of output related to the certificate creation.\\n\\nThere should now be a new file, /etc/ssl/newcerts/01.pem, containing the same output. Copy and paste everything\\nbeginning with the line: *-----BEGIN CERTIFICATE-----* and continuing through the line: *----END CERTIFICATE-*\\n\\n*----*\\nlines to a file named after the hostname of the server where the certificate will be installed. For example\\n\\nmail.example.com.crt, is a nice descriptive name.\\n\\nSubsequent certificates will be named 02.pem, 03.pem, etc.\\n\\n**Note**\\n\\nReplace *mail.example.com.crt* with your own descriptive name.\\n\\nFinally, copy the new certificate to the host that needs it, and configure the appropriate applications to use it. The\\ndefault location to install certificates is /etc/ssl/certs . This enables multiple services to use the same certificate\\nwithout overly complicated file permissions.\\n\\nFor applications that can be configured to use a CA certificate, you should also copy the /etc/ssl/certs/cacert.pem\\nfile to the /etc/ssl/certs/ directory on each server.\\n\\n410\\n\\n\\n-----\\n\\n## **References**\\n\\n[• The Wikipedia HTTPS page has more information regarding HTTPS.](http://en.wikipedia.org/wiki/HTTPS)\\n\\n  - For more information on *OpenSSL* [see the OpenSSL Home Page.](https://www.openssl.org/)\\n\\n[• Also, O’Reilly’s Network Security with OpenSSL is a good in-depth reference.](http://oreilly.com/catalog/9780596002701/)\\n\\nEnterprise environments sometimes have a local Certificate Authority (CA) that issues certificates for use within the\\norganization.',\n",
       " 'For an Ubuntu server to be functional and trust the hosts in this environment this CA must be installed\\nin Ubuntu’s trust store.\\n## **How to recognize the form (PEM or DER)?**\\n\\nTo install a certificate in the trust store it must be in PEM form. A PEM-formatted certificate is human-readable in\\nbase64 format, and starts with the lines ----BEGIN CERTIFICATE---- . If you see these lines, you’re ready to install. If\\nnot, it is most likely a DER certificate and needs to be converted.\\n## **Installing a certificate in PEM form**\\n\\nAssuming a PEM-formatted root CA certificate is in local-ca.crt, follow the steps below to install it.\\n\\n**Note:** It is important to have the .crt extension on the file, otherwise it will not be processed.\\n\\n$ sudo apt-get install -y ca-certificates\\n\\n$ sudo cp local-ca.crt /usr/local/share/ca-certificates\\n\\n$ sudo update-ca-certificates\\n\\nAfter this point you can use Ubuntu’s tools like curl and wget to connect to local sites.\\n## **Converting from DER-form to PEM-form**\\n\\nConvert a DER-formatted certificate called local-ca.der to PEM form like this:\\n\\n$ sudo openssl x509 -inform der -outform pem -in local-ca.der -out local-ca.crt\\n## **The CA trust store location**\\n\\nThe CA trust store as generated by update-ca-certificates is available at the following locations:\\n\\n  - As a single file (PEM bundle) in /etc/ssl/certs/ca-certificates.crt\\n\\n  - As an OpenSSL compatible certificate directory in /etc/ssl/certs\\n\\nAs with any other security barrier you put in place to protect your server, it is pretty tough to defend against untold\\ndamage caused by someone with physical access to your environment, for example, theft of hard drives, power or',\n",
       " 'service disruption, and so on. Therefore, console security should be addressed merely as one component of your overall\\nphysical security strategy. A locked “screen door” may deter a casual criminal, or at the very least slow down a\\ndetermined one, so it is still advisable to perform basic precautions with regard to console security.\\n\\nThe following instructions will help defend your server against issues that could otherwise yield very serious conse\\nquences.\\n## **Disable Ctrl+Alt+Delete**\\n\\nAnyone that has physical access to the keyboard can simply use the Ctrl+Alt+Delete key combination to reboot the\\nserver without having to log on. While someone could simply unplug the power source, you should still prevent the\\nuse of this key combination on a production server. This forces an attacker to take more drastic measures to reboot\\nthe server, and will prevent accidental reboots at the same time.\\n\\nTo disable the reboot action taken by pressing the Ctrl+Alt+Delete key combination, run the following two commands:\\n\\nsudo systemctl mask ctrl-alt-del.target\\n\\nsudo systemctl daemon-reload\\n\\n[A definition of High Availability Clusters from Wikipedia:](https://en.wikipedia.org/wiki/High-availability_cluster)\\n## **High Availability Clusters**\\n\\n**High-availability clusters** (also known as **HA clusters**, **fail-over clusters** or **Metroclusters Ac-**\\n**tive/Active** [) are groups of computers that support server applications that can be reliably utilized with](https://en.wikipedia.org/wiki/Computer)\\n[a minimum amount of down-time.](https://en.wikipedia.org/wiki/High_availability)\\n\\n411\\n\\n\\n-----',\n",
       " '[They operate by using high availability software to harness redundant computers in groups or clusters that](https://en.wikipedia.org/wiki/High_availability_software)\\nprovide continued service when system components fail.\\n\\nWithout clustering, if a server running a particular application crashes, the application will be unavailable\\nuntil the crashed server is fixed. HA clustering remedies this situation by detecting hardware/software\\nfaults, and immediately restarting the application on another system without requiring administrative\\n[intervention, a process known as failover.](https://en.wikipedia.org/wiki/Failover)\\n\\nAs part of this process, clustering software may configure the node before starting the application on it.\\nFor example, appropriate file systems may need to be imported and mounted, network hardware may have\\nto be configured, and some supporting applications may need to be running as well.\\n\\n[HA clusters are often used for critical databases, file sharing on a network, business applications, and](https://en.wikipedia.org/wiki/Database_management_system)\\n[customer services such as electronic commerce websites.](https://en.wikipedia.org/wiki/Electronic_commerce)\\n## **High Availability Cluster Heartbeat**\\n\\nHA cluster implementations attempt to build redundancy into a cluster to eliminate single points of failure,\\n[including multiple network connections and data storage which is redundantly connected via storage area](https://en.wikipedia.org/wiki/Storage_area_network)\\n[networks.](https://en.wikipedia.org/wiki/Storage_area_network)',\n",
       " '[HA clusters usually use a heartbeat private network connection which is used to monitor the health and](https://en.wikipedia.org/wiki/Heartbeat_(computing))\\nstatus of each node in the cluster. One subtle but serious condition all clustering software must be able\\n[to handle is split-brain, which occurs when all of the private links go down simultaneously, but the cluster](https://en.wikipedia.org/wiki/Split-brain_(computing))\\nnodes are still running.\\n\\nIf that happens, each node in the cluster may mistakenly decide that every other node has gone down and\\nattempt to start services that other nodes are still running. Having duplicate instances of services may\\ncause data corruption on the shared storage.\\n## **High Availability Cluster Quorum**\\n\\n[HA clusters often also use quorum witness storage (local or cloud) to avoid this scenario. A witness device](https://en.wikipedia.org/wiki/Quorum_(distributed_computing))\\ncannot be shared between two halves of a split cluster, so in the event that all cluster members cannot\\ncommunicate with each other (e.g., failed heartbeat), if a member cannot access the witness, it cannot\\nbecome active.\\n\\n412\\n\\n\\n-----\\n\\n## **Example** **Fencing**\\n\\nFencing protects your data from being corrupted, and your application from becoming unavailable, due to unintended\\nconcurrent access by rogue nodes.\\n\\nJust because a node is unresponsive doesn’t mean it has stopped accessing your data. The only way to be 100% sure\\nthat your data is safe, is to use fencing to ensure that the node is truly offline before allowing the data to be accessed\\nfrom another node.\\n\\nFencing also has a role to play in the event that a clustered service cannot be stopped.',\n",
       " 'In this case, the cluster uses\\nfencing to force the whole node offline, thereby making it safe to start the service\\nelsewhere. The most popular example of fencing is cutting a host’s power.\\n\\nKey Benefits:\\n\\n  - Active countermeasure taken by a functioning host to isolate a misbehaving (usually dead) host from shared\\ndata.\\n\\n - **MOST CRITICAL** part of a cluster utilizing SAN or other shared storage technology ( *Ubuntu HA Clusters*\\n*can only be supported if the fencing mechanism is configured* ).\\n\\n  - Required by OCFS2, GFS2, cLVMd (before Ubuntu 20.04), lvmlockd (from 20.04 and beyond).\\n## **Linux High Availability Projects**\\n\\nThere are many upstream high availability related projects that are included in Ubuntu Linux. This section will\\ndescribe the most important ones.\\n\\nThe following packages are present in latest Ubuntu LTS release:\\n\\n**Ubuntu HA Core Packages**\\n\\nPackages in this list are supported just like any other package available in **[main] repository** would be.\\n\\n413\\n\\n\\n-----\\n\\nPackage URL\\n\\nlibqb [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/libqb)\\nkronosnet [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/kronosnet)\\ncorosync [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/corosync)\\npacemaker [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/pacemaker)\\nresource-agents [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/resource-agents)\\nfence-agents [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/fence-agents)\\ncrmsh [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/crmsh)\\npcs* [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/pcs)',\n",
       " 'cluster-glue [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/cluster-glue)\\ndrbd-utils [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/drbd-utils)\\ndlm [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/dlm)\\ngfs2-utils [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/gfs2-utils)\\nkeepalived [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/keepalived)\\n\\n - **libqb**  - Library which provides a set of high performance client-server reusable features. It offers high performance logging, tracing, IPC and poll. Its initial features were spun off the *Corosync* cluster communication suite\\nto make them accessible for other projects.\\n\\n - **Kronosnet**  - Kronosnet, often referred to as knet, is a network abstraction layer designed for High Availability.\\n[Corosync uses Kronosnet to provide multiple networks for its interconnect (replacing the old Totem Redundant](https://discourse.ubuntu.com/t/corosync-and-redundant-rings/11627)\\n[Ring Protocol) and add support for some more features like interconnect network hot-plug.](https://discourse.ubuntu.com/t/corosync-and-redundant-rings/11627)\\n\\n - **Corosync**  - or *Cluster Membership Layer*, provides reliable messaging, membership and quorum information\\nabout the cluster. Currently, Pacemaker supports Corosync as this layer.\\n\\n - **Pacemaker**  - or *Cluster Resource Manager*, provides the brain that processes and reacts to events that occur in\\nthe cluster. Events might be: nodes joining or leaving the cluster, resource events caused by failures, maintenance,\\nor scheduled activities. To achieve the desired availability, Pacemaker may start and stop resources and fence\\nnodes.',\n",
       " ' - **Resource Agents**  - Scripts or operating system components that start, stop or monitor resources, given a set\\nof resource parameters. These provide a uniform interface between pacemaker and the managed services.\\n\\n - **Fence Agents**  - Scripts that execute node fencing actions, given a target and fence device parameters.\\n\\n - **crmsh**  - Advanced command-line interface for High-Availability cluster management in GNU/Linux.\\n\\n - **pcs**  - Pacemaker command line interface and GUI. It permits users to easily view, modify and create pacemaker\\nbased clusters. pcs also provides pcsd, which operates as a GUI and remote server for pcs . Together pcs and pcsd\\nform the recommended configuration tool for use with pacemaker. *NOTE: It was added to the [main] repository*\\n*in Ubuntu Lunar Lobster (23.10)* .\\n\\n - **cluster-glue**  - Reusable cluster components for Linux HA. This package contains node fencing plugins, an error\\nreporting utility, and other reusable cluster components from the Linux HA project.\\n\\n - **DRBD**  - Distributed Replicated Block Device, **DRBD** [is a distributed replicated storage system for the Lin-](https://en.wikipedia.org/wiki/Distributed_Replicated_Block_Device)\\nuxplatform. It is implemented as a kernel driver, several userspace management applications, and some shell\\nscripts. DRBD is traditionally used in high availability (HA) clusters.\\n\\n - **DLM**  - A distributed lock manager (DLM) runs in every machine in a cluster, with an identical copy of a clusterwide lock database. In this way DLM provides software applications which are distributed across a cluster on\\nmultiple machines with a means to synchronize their accesses to shared resources.',\n",
       " ' - **gfs2-utils**  - Global File System 2 - filesystem tools. The Global File System allows a cluster of machines to\\nconcurrently access shared storage hardware like SANs or iSCSI and network block devices.\\n\\n - **Keepalived**  - Keepalived provides simple and robust facilities for loadbalancing and high-availability to Linux\\n[system and Linux based infrastructures. Loadbalancing framework relies on well-known and widely used Linux](http://www.linux-vs.org/)\\n[Virtual Server (IPVS) kernel module providing Layer4 loadbalancing. Keepalived implements a set of checkers](http://www.linux-vs.org/)\\nto dynamically and adaptively maintain and manage loadbalanced server pool according their health. On the\\n[other hand high-availability is achieved by VRRP protocol.](http://datatracker.ietf.org/wg/vrrp/)\\n\\n**Ubuntu HA Community Packages**\\n\\nPackages in this list are supported just like any other package available in **[universe] repository** would be.\\n\\n414\\n\\n\\n-----\\n\\nPackage URL\\n\\npcs* [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/libqb)\\ncsync2 [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/csync2)\\ncorosync-qdevice [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/corosync-qdevice)\\nfence-virt [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/fence-virt)\\nsbd [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/sbd)\\nbooth [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/booth)\\n\\n - **Corosync-Qdevice**  - Its primary use is for even-node clusters, operates at corosync (quorum) layer. CorosyncQdevice is an independent arbiter for solving split-brain situations. (qdevice-net supports multiple algorithms).',\n",
       " ' - **SBD**  - A Fencing Block Device can be particularly useful in environments where traditional fencing mechanisms\\nare not possible. SBD integrates with Pacemaker, a watchdog device and shared storage to arrange for nodes to\\nreliably self-terminate when fencing is required.\\n\\nNote: **pcs** was added to the [main] repository in Ubuntu Lunar Lobster (23.04).\\n\\n**Ubuntu HA Deprecated Packages**\\n\\nPackages in this list are **only supported by the upstream community** . All bugs opened against these agents\\nwill be forwarded to upstream IF makes sense (affected version is close to upstream).\\n\\nPackage URL\\n\\nocfs2-tools [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/ocfs2-tools)\\n\\n**Ubuntu HA Related Packages**\\n\\nPackages in this list aren’t necessarily **HA** related packages, but they have a very important role in High Availability\\nClusters and are supported like any other package provide by the **[main]** repository.\\n\\nPackage URL\\n\\nmultipath-tools [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/multipath-tools)\\nopen-iscsi [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/open-iscsi)\\nsg3-utils [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/sg3-utils)\\ntgt OR targetcli-fb* [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/tgt)\\nlvm2 [Ubuntu | Upstream](https://launchpad.net/ubuntu/+source/lvm2)\\n\\n - **LVM2** in a Shared-Storage Cluster Scenario:\\n\\n**CLVM**  - supported before **Ubuntu 20.04**\\nA distributed lock manager (DLM) is used to broker concurrent LVM metadata accesses. Whenever a cluster node\\nneeds to modify the LVM metadata, it must secure permission from its local clvmd, which is in constant contact',\n",
       " 'with other clvmd daemons in the cluster and can communicate a desire to get a lock on a particular set of objects.\\n\\n**[lvmlockd](http://manpages.ubuntu.com/manpages/focal/man8/lvmlockd.8.html)**   - supported after **Ubuntu 20.04**\\nAs of 2017, a stable LVM component that is designed to replace clvmd by making the locking of LVM objects\\ntransparent to the rest of LVM, without relying on a distributed lock manager.\\n\\nThe lvmlockd benefits over clvm are:\\n\\n**–**\\nlvmlockd supports two cluster locking plugins: DLM and SANLOCK. SANLOCK plugin can supports up\\nto ~2000 nodes that benefits LVM usage in big virtualization / storage cluster, while DLM plugin fits HA\\ncluster.\\n\\n**–**\\nlvmlockd has better design than clvmd. clvmd is command-line level based locking system, which means\\nthe whole LVM software will get hang if any LVM command gets dead-locking issue.\\n**–** lvmlockd can work with lvmetad.\\n\\nNote: **targetcli-fb (Linux LIO)** will likely replace **tgt** in future Ubuntu versions.\\n\\n415\\n\\n\\n-----\\n\\n## **Upstream Documentation**\\n\\nThe server guide does not have the intent to document every existing option for all the HA related softwares described in this page, but to document recommended scenarios for Ubuntu HA Clusters. You will find more complete\\ndocumentation upstream at:\\n\\n  - ClusterLabs\\n\\n**–** [Clusters From Scratch](https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/2.0/html-single/Clusters_from_Scratch/index.html)\\n\\n**–**\\n[Managing Pacemaker Clusters](https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/2.0/html-single/Pacemaker_Administration/index.html)\\n\\n**–**',\n",
       " '[Pacemaker Configuration Explained](https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/2.0/html-single/Pacemaker_Explained/index.html)\\n\\n**–**\\n[Pacemaker Remote - Scaling HA Clusters](https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/2.0/html-single/Pacemaker_Remote/index.html)\\n\\n - Other\\n\\n**–**\\n[Ubuntu Bionic HA in Shared Disk Environments (Azure)](https://discourse.ubuntu.com/t/ubuntu-high-availability-corosync-pacemaker-shared-disk-environments/14874)\\n\\n[A very special thanks, and all the credits, to ClusterLabs Project for all that detailed documentation.](https://clusterlabs.org/)\\n\\nFrom the ClusterLabs definition:\\n\\nResource agents are the abstraction that allows Pacemaker to manage services it knows nothing about.\\nThey contain the logic for what to do when the cluster wishes to start, stop or check the health of a service.\\nThis particular set of agents conform to the Open Cluster Framework (OCF) specification.\\n\\nAt the moment, the resource-agents binary package has been split into two: resource-agents-base and resource\\nagents-extra . The resource-agents-base binary package contains a set of curated agents which the Ubuntu Server\\nteam continuously run tests to make sure everything is working as expected. All the other agents previously in the\\n\\nresource-agents binary package are now moved to the resource-agents-extra .\\n\\nThe resource-agents-base binary package contains the following agents in the latest Ubuntu release:\\n\\n - IPaddr2\\n\\n - iscsi\\n\\n - iSCSILogicalUnit\\n\\n - iSCSITarget\\n\\n - LVM-activate\\n\\n - systemd\\n\\nAll those agents are in main and are fully supported. The remaining agents are in resource-agents-extra and most',\n",
       " 'of them are supported by upstream but they are not curated by the Ubuntu Server team. The set of resource agents\\nthat are not maintained by upstream is listed in /usr/share/doc/resource-agents-extra/DEPRECATED_AGENTS, the use\\nof those agents is discouraged.\\n\\nFor the resource agents provided by resource-agents-base, we will be briefly describing how to use them.\\n\\n**Note** :\\n\\nThere are two well known tools used to manage fence agents, they are crmsh and pcs . Here we will present\\nexamples with both, since crmsh is the recommended and supported tool until Ubuntu 22.10 Kinetic Kudu,\\nand pcs is the recommended and supported tool from Ubuntu 23.04 Lunar Lobster onwards. For more\\ninformation on how to migrate from crmsh to pcs please refer to this migration guide.\\n## **IPaddr2**\\n\\nFrom its manpage:\\n\\nThis Linux-specific resource manages IP alias IP addresses. It can add an IP alias, or remove one. In\\naddition, it can implement Cluster Alias IP functionality if invoked as a clone resource.\\n\\nOne could configure a IPaddr2 resource with the following command:\\n\\n$ crm configure primitive $RESOURCE_NAME ocf:heartbeat:IPaddr2 \\\\\\n\\nip=$IP_ADDRESS \\\\\\n\\ncidr_netmask=$NET_MASK \\\\\\n\\nop monitor interval=30s\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs resource create $RESOURCE_NAME ocf:heartbeat:IPaddr2 \\\\\\n\\nip=$IP_ADDRESS \\\\\\n\\ncidr_netmask=$NET_MASK \\\\\\n\\nop monitor interval=30s\\n\\nThis is one way to set up IPaddr2, for more information please check its manpage.\\n\\n416\\n\\n\\n-----\\n\\n## **iscsi**\\n\\nFrom its manpage:\\n\\nManages a local iSCSI initiator and its connections to iSCSI targets.',\n",
       " 'Once the iSCSI target is ready to accept connections from the initiator(s), with all the appropriate permissions, one\\ncould configure the iscsi resource with the following command:\\n\\n$ crm configure primitive $RESOURCE_NAME ocf:heartbeat:iscsi \\\\\\n\\ntarget=$TARGET \\\\\\n\\nportal=$PORTAL\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs resource create $RESOURCE_NAME ocf:heartbeat:iscsi \\\\\\n\\ntarget=$TARGET \\\\\\n\\nportal=$PORTAL\\n\\nWhere $TARGET is the IQN (iSCSI Qualified Name) of the iSCSI target and $PORTAL its address, which can be, for\\ninstance, formed by the IP address and port number used by the target daemon.\\n\\nThis is one way to set up iscsi, for more information please check its manpage.\\n## **iSCSILogicalUnit**\\n\\nFrom its manpage:\\n\\nManages iSCSI Logical Unit. An iSCSI Logical unit is a subdivision of an SCSI Target, exported via a\\ndaemon that speaks the iSCSI protocol.\\n\\nThis agent is usually used alongside with iSCSITarget to manage the target itself and its Logical Units. The supported\\nimplementation of iSCSI targets is using targetcli-fb, due to that, make sure to use lio-t as the implementation type.\\nConsidering one has an iSCSI target in place, the iSCSILogicalUnit resource could be configured with the following\\ncommand:\\n\\n$ crm configure primitive $RESOURCE_NAME ocf:heartbeat:iSCSILogicalUnit \\\\\\n\\nimplementation=lio-t \\\\\\n\\ntarget_iqn=$IQN_TARGET \\\\\\n\\npath=$DEVICE \\\\\\n\\nlun=$LUN\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs resource create $RESOURCE_NAME ocf:heartbeat:iSCSILogicalUnit \\\\\\n\\nimplementation=lio-t \\\\\\n\\ntarget_iqn=$IQN_TARGET \\\\\\n\\npath=$DEVICE \\\\\\n\\nlun=$LUN',\n",
       " 'Where implementation is set to lio-t as mentioned before, $IQN_TARGET is the IQN (iSCSI Qualified Name) that this\\nLogical Unit belongs to, $DEVICE is the path to the exposed block device, and $LUN is the number representing the\\nLogical Unit which will be exposed to initiators.\\n\\nThis is one way to set up iSCSILogicalUnit, for more information please check its manpage.\\n## **iSCSITarget**\\n\\nFrom its manpage:\\n\\nManages iSCSI targets. An iSCSI target is a collection of SCSI Logical Units (LUs) exported via a daemon\\nthat speaks the iSCSI protocol.\\n\\nThis agent is usually used alongside with iSCSILogicalUnit to manage the target itself and its Logical Units. The\\nsupported implementation of iSCSI targets is using targetcli-fb, due to that, make sure to use lio-t as the implementation type. Considering one has targetcli-fb installed on the system, the iSCSITarget resource could be configured\\nwith the following command:\\n\\n$ crm configure primitive $RESOURCE_NAME ocf:heartbeat:iSCSITarget \\\\\\n\\nimplementation=lio-t \\\\\\n\\niqn=$IQN_TARGET\\n\\nOne can do the same using pcs via the following command:\\n\\n417\\n\\n\\n-----\\n\\n$ pcs resource create $RESOURCE_NAME ocf:heartbeat:iSCSITarget \\\\\\n\\nimplementation=lio-t \\\\\\n\\niqn=$IQN_TARGET\\n\\nWhere implementation is set to lio-t as mentioned before and $IQN_TARGET is the IQN (iSCSI Qualified Name) of the\\ntarget.\\n\\nThis is one way to set up iSCSITarget, for more information please check its manpage.\\n## **LVM-activate**\\n\\nFrom its manpage:\\n\\nThis agent manages LVM activation/deactivation work for a given volume group.\\n\\nConsidering the LVM setup is ready to be activated and deactivated by this resource agent (make sure the sys',\n",
       " 'tem_id_resource is set to uname in /etc/lvm/lvm.conf ), one could configure a LVM-activate resource with the following\\ncommand:\\n\\n$ crm configure primitive $RESOURCE_NAME ocf:heartbeat:LVM-activate \\\\\\n\\nvgname=$VOLUME_GROUP \\\\\\n\\nvg_access_mode=system_id\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs resource create $RESOURCE_NAME ocf:heartbeat:LVM-activate \\\\\\n\\nvgname=$VOLUME_GROUP \\\\\\n\\nvg_access_mode=system_id\\n\\nThis is one way to set up LVM-activate, for more information please check its manpage.\\n## **Systemd**\\n\\nThere is also a way to manage systemd unit files via a resource agent. One need to have the systemd unit file in place\\n(already loaded by systemd) and configure a resource using the following command:\\n\\n$ crm configure primitive $RESOURCE_NAME systemd:$SERVICE_NAME\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs resource create $RESOURCE_NAME systemd:$SERVICE_NAME\\n\\nThe $SERVICE_NAME can be any service managed by a systemd unit file, and it needs to be available for the cluster\\nnodes.\\n## **References**\\n\\n[• ClusterLabs website](http://www.clusterlabs.org/)\\n\\n[• The OCF resource-agent developer’s guide](https://github.com/ClusterLabs/resource-agents/blob/master/doc/dev-guides/ra-dev-guide.asc)\\n\\n[• Users mailing list](http://oss.clusterlabs.org/mailman/listinfo/users)\\n\\nFrom the ClusterLabs definition:\\n\\nA *fence agent* (or *fencing agent* ) is a **stonith** -class resource agent.\\n\\nThe fence agent standard provides commands (such as off and reboot ) that the cluster can use to fence\\nnodes. As with other resource agent classes, this allows a layer of abstraction so that Pacemaker doesn’t',\n",
       " 'need any knowledge about specific fencing technologies — that knowledge is isolated in the agent.\\n\\nAt the moment, the fence-agents binary package has been split into two: fence-agents-base and fence-agents-extra .\\nThe fence-agents-base binary package contains a set of curated agents which the Ubuntu Server team continuously\\nrun tests to make sure everything is working as expected. All the other agents previously in the fence-agents binary\\npackage are now moved to the fence-agents-extra .\\n\\nThe fence-agents-base binary package contains the following agents in the latest Ubuntu release:\\n\\n - fence_ipmilan\\n\\n**–**\\nfence_idrac\\n\\n**–**\\nfence_ilo3\\n\\n**–**\\nfence_ilo4\\n\\n**–**\\nfence_ilo5\\n\\n**–**\\nfence_imm\\n\\n418\\n\\n\\n-----\\n\\n**–**\\nfence_ipmilanplus\\n\\n - fence_mpath\\n\\n - fence_sbd\\n\\n - fence_scsi\\n\\n - fence_virsh\\n\\nAll those agents are in main and are fully supported. The remaining agents in fence-agents-extra are supported by\\nupstream but they are not curated by the Ubuntu Server team.\\n\\nFor the fence agents provided by fence-agents-base, we will be briefly describing how to use them.\\n\\n**Note** :\\n\\nThere are two well known tools used to manage fence agents, they are crmsh and pcs . Here we will present\\nexamples with both, since crmsh is the recommended and supported tool until Ubuntu 22.10 Kinetic Kudu,\\nand pcs is the recommended and supported tool from Ubuntu 23.04 Lunar Lobster onwards. For more\\ninformation on how to migrate from crmsh to pcs please refer to this migration guide.\\n## **fence_ipmilan**\\n\\nThe content of this section is also applicable to the following fence agents: fence_idrac, fence_ilo3, fence_ilo4,\\n\\nfence_ilo5, fence_imm, and fence_ipmilanplus .',\n",
       " 'All of them are symlinks to fence_ipmilan .\\n\\nFrom its manpage:\\n\\nfence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent\\ncalls support software ipmitool . WARNING! This fence agent might report success before the node is\\npowered off. You should use -m/method onoff if your fence device works correctly with that option.\\n\\nIn a system which supports IPMI and with ipmitool installed, one could configure a fence_ipmilan resource with the\\nfollowing command:\\n\\n$ crm configure primitive $RESOURCE_NAME stonith:fence_ipmilan \\\\\\n\\nip=$IP \\\\\\n\\nipport=$PORT \\\\\\n\\nusername=$USER \\\\\\n\\npassword=$PASSWD \\\\\\n\\nlanplus=1 \\\\\\n\\naction=$ACTION\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs stonith create $RESOURCE_NAME fence_ipmilan \\\\\\n\\nip=$IP \\\\\\n\\nipport=$PORT \\\\\\n\\nusername=$USER \\\\\\n\\npassword=$PASSWD \\\\\\n\\nlanplus=1 \\\\\\n\\naction=$ACTION\\n\\nWhere $IP is the IP address or hostname of fencing device, $PORT is the TCP/UDP port to use for connection, $USER\\nis the login name and $PASSWD its password, and $ACTION is the fencing actions which by default is reboot .\\n\\nThis is one way to set up fence_ipmilan, for more information please check its manpage.\\n## **fence_mpath**\\n\\nFrom its manpage:\\n\\nfence_mpath is an I/O fencing agent that uses SCSI-3 persistent reservations to control access multipath\\ndevices. Underlying devices must support SCSI-3 persistent reservations (SPC-3 or greater) as well as the\\n“preempt-and-abort” subcommand. The fence_mpath agent works by having a unique key for each node\\nthat has to be set in /etc/multipath.conf . Once registered, a single node will become the reservation',\n",
       " 'holder by creating a “write exclusive, registrants only” reservation on the device(s). The result is that\\nonly registered nodes may write to the device(s). When a node failure occurs, the fence_mpath agent will\\nremove the key belonging to the failed node from the device(s). The failed node will no longer be able to\\nwrite to the device(s). A manual reboot is required.\\n\\nOne could configure a fence_mpath resource with the following command:\\n\\n419\\n\\n\\n-----\\n\\n$ crm configure primitive $RESOURCE_NAME stonith:fence_mpath \\\\\\n\\npcmk_host_map=\"$NODE1:$NODE1_RES_KEY;$NODE2:$NODE2_RES_KEY;$NODE3:$NODE3_RES_KEY\" \\\\\\n\\npcmk_host_argument=key \\\\\\n\\npcmk_monitor_action=metadata \\\\\\n\\npcmk_reboot_action=off \\\\\\n\\ndevices=$MPATH_DEVICE \\\\\\n\\nmeta provides=unfencing\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs stonith create $RESOURCE_NAME fence_mpath \\\\\\n\\npcmk_host_map=\"$NODE1:$NODE1_RES_KEY;$NODE2:$NODE2_RES_KEY;$NODE3:$NODE3_RES_KEY\" \\\\\\n\\npcmk_host_argument=key \\\\\\n\\npcmk_monitor_action=metadata \\\\\\n\\npcmk_reboot_action=off \\\\\\n\\ndevices=$MPATH_DEVICE \\\\\\n\\nmeta provides=unfencing\\n\\nThe $NODE1_RES_KEY is the reservation key used by this node 1 (same for the others node with access to the multipath\\ndevice), please make sure you have reservation_key <key> in the default section inside /etc/multipath.conf and the\\nmultipathd service was reloaded after it.\\n\\nThis is one way to set up fence_mpath, for more information please check its manpage.\\n## **fence_sbd**\\n\\nFrom its manpage:\\n\\nfence_sbd is I/O Fencing agent which can be used in environments where sbd can be used (shared storage).\\n\\nWith SBD (STONITH Block Device) configured on a system, one could configure the fence_sbd resource with the\\nfollowing command:',\n",
       " '$ crm configure primitive $RESOURCE_NAME stonith:fence_sbd devices=$DEVICE\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs stonith create $RESOURCE_NAME fence_sbd devices=$DEVICE\\n\\nThis is one way to set up fence_sbd, for more information please check its manpage.\\n## **fence_scsi**\\n\\nFrom its manpage:\\n\\nfence_scsi is an I/O fencing agent that uses SCSI-3 persistent reservations to control access to shared\\nstorage devices. These devices must support SCSI-3 persistent reservations (SPC-3 or greater) as well as\\nthe “preempt-and-abort” subcommand. The fence_scsi agent works by having each node in the cluster\\nregister a unique key with the SCSI device(s). Reservation key is generated from “node id” (default) or from\\n“node name hash” (RECOMMENDED) by adjusting “key_value” option. Using hash is recommended to\\nprevent issues when removing nodes from cluster without full cluster restart. Once registered, a single\\nnode will become the reservation holder by creating a “write exclusive, registrants only” reservation on the\\ndevice(s). The result is that only registered nodes may write to the device(s). When a node failure occurs,\\nthe fence_scsi agent will remove the key belonging to the failed node from the device(s). The failed node\\nwill no longer be able to write to the device(s). A manual reboot is required.\\n\\nOne could configure a fence_scsi resource with the following command:\\n\\n$ crm configure primitive $RESOURCE_NAME stonith:fence_scsi \\\\\\n\\npcmk_host_list=\"$NODE1 $NODE2 $NODE3\" \\\\\\n\\ndevices=$SCSI_DEVICE \\\\\\n\\nmeta provides=unfencing\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs stonith create $RESOURCE_NAME fence_scsi \\\\',\n",
       " 'pcmk_host_list=\"$NODE1 $NODE2 $NODE3\" \\\\\\n\\ndevices=$SCSI_DEVICE \\\\\\n\\nmeta provides=unfencing\\n\\n420\\n\\n\\n-----\\n\\nThe pcmk_host_list parameter contains a list of cluster nodes that can access the managed SCSI device.\\n\\nThis is one way to set up fence_scsi, for more information please check its manpage.\\n## **fence_virsh**\\n\\nFrom its manpage:\\n\\nfence_virsh is an I/O Fencing agent which can be used with the virtual machines managed by libvirt. It\\nlogs via ssh to a dom0 and there run virsh command, which does all work. By default, virsh needs root\\naccount to do properly work. So you must allow ssh login in your sshd_config.\\n\\nOne could configure a fence_virsh resource with the following command:\\n\\n$ crm configure primitive $RESOURCE_NAME stonith:fence_virsh \\\\\\n\\nip=$HOST_IP_ADDRESS \\\\\\n\\nlogin=$HOST_USER \\\\\\n\\nidentity_file=$SSH_KEY \\\\\\n\\nplug=$NODE \\\\\\n\\nssh=true \\\\\\n\\nuse_sudo=true\\n\\nOne can do the same using pcs via the following command:\\n\\n$ pcs stonith create $RESOURCE_NAME fence_virsh \\\\\\n\\nip=$HOST_IP_ADDRESS \\\\\\n\\nlogin=$HOST_USER \\\\\\n\\nidentity_file=$SSH_KEY \\\\\\n\\nplug=$NODE \\\\\\n\\nssh=true \\\\\\n\\nuse_sudo=true\\n\\nThis is one way to set up fence_virsh, for more information please check its manpage.\\n\\nIn order to avoid running the resource in the same node that should be fenced, we need to add a location restriction:\\n\\n$ crm configure location fence-$NODE-location $RESOURCE_NAME -inf: $NODE\\n\\nUsing pcs :\\n\\n$ pcs constraint location $RESOURCE_NAME avoids $NODE\\n## **References**\\n\\n[• ClusterLabs website](https://clusterlabs.org/)\\n\\n[• Fence agents API documentation](https://github.com/ClusterLabs/fence-agents/blob/master/doc/FenceAgentAPI.md)\\n\\n[• Users mailing list](https://oss.clusterlabs.org/mailman/listinfo/users)',\n",
       " 'Distributed Replicated Block Device (DRBD) mirrors block devices between multiple hosts. The replication is transparent to other applications on the host systems. Any block device hard disks, partitions, RAID devices, logical\\nvolumes, etc can be mirrored.\\n\\nTo get started using drbd, first install the necessary packages. From a terminal enter:\\n\\nsudo apt install drbd-utils\\n\\n**Note**\\n\\nIf you are using the *virtual kernel* as part of a virtual machine you will need to manually compile the\\ndrbd module. It may be easier to install the linux-modules-extra-$(uname -r) package inside the virtual\\nmachine.\\n\\nThis section covers setting up a drbd to replicate a separate /srv partition, with an ext3 filesystem between two hosts.\\nThe partition size is not particularly relevant, but both partitions need to be the same size.\\n## **Configuration**\\n\\nThe two hosts in this example will be called *drbd01* and *drbd02* . They will need to have name resolution configured\\neither through DNS or the /etc/hosts file. See DNS for details.\\n\\n  - To configure drbd, on the first host edit /etc/drbd.conf :\\n\\n421\\n\\n\\n-----\\n\\nglobal { usage-count no; }\\n\\ncommon { syncer { rate 100M; } }\\n\\nresource r0 {\\n\\nprotocol C;\\n\\nstartup {\\n\\nwfc-timeout 15;\\n\\ndegr-wfc-timeout 60;\\n\\n}\\n\\nnet {\\n\\ncram-hmac-alg sha1;\\n\\nshared-secret \"secret\";\\n\\n}\\n\\non drbd01 {\\n\\ndevice /dev/drbd0;\\n\\ndisk /dev/sdb1;\\n\\naddress 192.168.0.1:7788;\\n\\nmeta-disk internal;\\n\\n}\\n\\non drbd02 {\\n\\ndevice /dev/drbd0;\\n\\ndisk /dev/sdb1;\\n\\naddress 192.168.0.2:7788;\\n\\nmeta-disk internal;\\n\\n}\\n\\n}\\n\\n**Note**\\n\\nThere are many other options in /etc/drbd.conf, but for this example their default values are fine.\\n\\n - Now copy /etc/drbd.conf to the second host:\\n\\nscp /etc/drbd.conf drbd02:~',\n",
       " ' - And, on *drbd02* move the file to /etc :\\n\\nsudo mv drbd.conf /etc/\\n\\n  - Now using the drbdadm utility initialize the meta data storage. On each server execute:\\n\\nsudo drbdadm create-md r0\\n\\n  - Next, on both hosts, start the drbd daemon:\\n\\nsudo systemctl start drbd.service\\n\\n - On the *drbd01*, or whichever host you wish to be the primary, enter the following:\\n\\nsudo drbdadm -- --overwrite-data-of-peer primary all\\n\\n  - After executing the above command, the data will start syncing with the secondary host. To watch the progress,\\non *drbd02* enter the following:\\n\\nwatch -n1 cat /proc/drbd\\n\\nTo stop watching the output press *Ctrl+c* .\\n\\n  - Finally, add a filesystem to /dev/drbd0 and mount it:\\n\\nsudo mkfs.ext3 /dev/drbd0\\n\\nsudo mount /dev/drbd0 /srv\\n## **Testing**\\n\\nTo test that the data is actually syncing between the hosts copy some files on the *drbd01*, the primary, to /srv :\\n\\nsudo cp -r /etc/default /srv\\n\\nNext, unmount /srv :\\n\\nsudo umount /srv\\n\\n*Demote* the *primary* server to the *secondary* role:\\n\\n422\\n\\n\\n-----\\n\\nsudo drbdadm secondary r0\\n\\nNow on the *secondary* server *promote* it to the *primary* role:\\n\\nsudo drbdadm primary r0\\n\\nLastly, mount the partition:\\n\\nsudo mount /dev/drbd0 /srv\\n\\nUsing *ls* you should see /srv/default copied from the former *primary* host *drbd01* .\\n## **References**\\n\\n[• For more information on DRBD see the DRBD web site.](http://www.drbd.org/)\\n\\n[• The drbd.conf man page contains details on the options not covered in this guide.](http://manpages.ubuntu.com/manpages/jammy/en/man5/drbd.conf.5.html)\\n\\n[• Also, see the drbdadm man page.](http://manpages.ubuntu.com/manpages/jammy/en/man8/drbdadm.8.html)',\n",
       " '[• The DRBD Ubuntu Wiki page also has more information.](https://help.ubuntu.com/community/DRBD)\\n\\nFrom Ubuntu 23.04 Lunar Lobster onwards pcs is the recommended and supported tool to set up and manage\\nCorosync/Pacemaker clusters in Ubuntu. This is the last Ubuntu release where crmsh will be supported, but not\\nrecommended, so users will have time to migrate out of it.\\n\\nThe migration from crmsh to pcs is not that complex since both have a similar CLI. Below one can find a direct\\nmapping of some useful commands from crmsh to pcs .\\n\\n**Action** **crmsh**\\n\\nShow configuration (raw XML) *crm configure show xml*\\nShow configuration (human-friendly) *crm configure show*\\nShow cluster status *crm status*\\n\\nPut a node in standby mode *crm node standby NODE*\\nRemove a node from standby mode *crm node online NODE*\\nSet cluster property *crm configure property PROPERTY=VALUE*\\nList Resource Agent classes *crm ra classes*\\nList available Resource Agents by standard *crm ra list ocf*\\nList available Resource Agents by OCF provider *crm ra list ocf pacemaker*\\nList available Resource Agent parameters *crm ra info AGENT*\\nShow available Fence Agent parameters *crm ra info stonith:AGENT*\\nCreate a resource *crm configure primitive NAME AGENT params PARAMETERS*\\nShow configuration of all resources *crm configure show*\\nShow configuration of one resource *crm configure show RESOURCE*\\nShow configuration of fencing resources *crm resource status*\\nStart a resource *crm resource start RESOURCE*\\n\\nStop a resource *crm resource stop RESOURCE*\\nRemove a resource *crm configure delete RESOURCE*\\nModify a resource’s instance parameters *crm resource param RESOURCE set PARAMETER=VALUE*',\n",
       " 'Delete a resource’s instance parameters *crm resource param RESOURCE delete PARAMETER*\\nList current resource defaults *crm configure show type:rsc_defaults*\\nSet resource defaults *crm configure rsc_defaults OPTION=VALUE*\\nList current operation defaults *crm configure show type:op_defaults*\\nSet operation defaults *crm configure op_defaults OPTION=VALUE*\\nClear fail counts for a resource *crm resource cleanup RESOURCE*\\nCreate a colocation constraint *crm configure colocation NAME INFINITY: RESOURCE_1 RESOUR*\\nCreate an ordering constraint *crm configure order NAME mandatory: RESOURCE_1 RESOURCE_*\\nCreate a location constraint *crm configure location NAME RESOURCE 50: NODE*\\nMove a resource to a specific node *crm resource move RESOURCE NODE*\\nMove a resource away from its current node *crm resource ban RESOURCE NODE*\\nRemove any constraints created by moving a resource *crm resource unmove RESOURCE*\\n\\nUbuntu provides two popular database servers. They are:\\n\\n[• MySQL](https://www.mysql.com/)\\n\\n[• PostgreSQL](https://www.postgresql.org/)\\n\\nBoth are popular choices among developers, with similar feature sets and performance capabilities. Historically,\\n\\n423\\n\\n\\n-----\\n\\nPostgres tended to be a preferred choice for its attention to standards conformance, features, and extensibility, whereas\\nMySQL may be more preferred for higher performance requirements. However, over time each has made good strides\\ncatching up with the other. Specialised needs may make one a better option for a certain application, but in general\\nboth are good, strong options.\\n\\nThey are available in the Main repository and equally supported by Ubuntu. This section explains how to install and',\n",
       " 'configure these database servers.\\n\\n[MySQL is a fast, multi-threaded, multi-user, and robust SQL database server. It is intended for mission-critical,](https://www.mysql.com/)\\nheavy-load production systems and mass-deployed software.\\n## **Installation**\\n\\nTo install MySQL, run the following command from a terminal prompt:\\n\\nsudo apt install mysql-server\\n\\nOnce the installation is complete, the MySQL server should be started automatically. You can quickly check its current\\nstatus via systemd:\\n\\nsudo service mysql status\\n\\nWhich should provide an output like the following:\\n\\n- mysql.service - MySQL Community Server\\n\\nLoaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\\n\\nActive: active (running) since Tue 2019-10-08 14:37:38 PDT; 2 weeks 5 days ago\\n\\nMain PID: 2028 (mysqld)\\n\\nTasks: 28 (limit: 4915)\\n\\nCGroup: /system.slice/mysql.service\\n\\n└─2028 /usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid\\n\\nOct 08 14:37:36 db.example.org systemd[1]: Starting MySQL Community Server...\\n\\nOct 08 14:37:38 db.example.org systemd[1]: Started MySQL Community Server.\\n\\nThe network status of the MySQL service can also be checked by running the ss command at the terminal prompt:\\n\\nsudo ss -tap | grep mysql\\n\\nWhen you run this command, you should see something similar to the following:\\n\\nLISTEN 0 151 127.0.0.1:mysql 0.0.0.0:* users:((\"mysqld\",pid=149190,fd=29))\\n\\nLISTEN 0 70 *:33060 *:* users:((\"mysqld\",pid=149190,fd=32))\\n\\nIf the server is not running correctly, you can type the following command to start it:\\n\\nsudo service mysql restart\\n\\nA good starting point for troubleshooting problems is the systemd journal, which can be accessed from the terminal',\n",
       " 'prompt with this command:\\n\\nsudo journalctl -u mysql\\n## **Configuration**\\n\\nYou can edit the files in /etc/mysql/ to configure the basic settings – log file, port number, etc. For example, to\\nconfigure MySQL to listen for connections from network hosts, in the file /etc/mysql/mysql.conf.d/mysqld.cnf, change\\nthe *bind-address* directive to the server’s IP address:\\n\\nbind-address = 192.168.0.5\\n\\n**Note** :\\n\\nReplace 192.168.0.5 with the appropriate address, which can be determined via the ip address show\\ncommand.\\n\\nAfter making a configuration change, the MySQL daemon will need to be restarted with the following command:\\n\\nsudo systemctl restart mysql.service\\n## **Database engines**\\n\\nWhilst the default configuration of MySQL provided by the Ubuntu packages is perfectly functional and performs well\\nthere are things you may wish to consider before you proceed.\\n\\n424\\n\\n\\n-----\\n\\nMySQL is designed to allow data to be stored in different ways. These methods are referred to as either database\\n[or storage engines. There are two main storage engines that you’ll be interested in: InnoDB and MyISAM. Storage](https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html)\\nengines are transparent to the end user. MySQL will handle things differently under the surface, but regardless of\\nwhich storage engine is in use, you will interact with the database in the same way.\\n\\nEach engine has its own advantages and disadvantages.\\n\\nWhile it is possible (and may be advantageous) to mix and match database engines on a table level, doing so reduces\\nthe effectiveness of the performance tuning you can do as you’ll be splitting the resources between two engines instead\\nof dedicating them to one.',\n",
       " '  - MyISAM is the older of the two. It can be faster than InnoDB under certain circumstances and favours a\\nread-only workload. Some web applications have been tuned around MyISAM (though that’s not to imply that\\nthey will be slower under InnoDB). MyISAM also supports the FULLTEXT data type, which allows very fast\\nsearches of large quantities of text data. However MyISAM is only capable of locking an entire table for writing.\\nThis means only one process can update a table at a time. As any application that uses the table scales this may\\nprove to be a hindrance. It also lacks journaling, which makes it harder for data to be recovered after a crash.\\n[The following link provides some points for consideration about using MyISAM on a production database.](http://www.mysqlperformanceblog.com/2006/06/17/using-myisam-in-production/)\\n\\n[• InnoDB is a more modern database engine, designed to be ACID compliant which guarantees database transac-](http://en.wikipedia.org/wiki/ACID)\\ntions are processed reliably. Write locking can occur on a row level basis within a table. That means multiple\\nupdates can occur on a single table simultaneously. Data caching is also handled in memory within the database\\nengine, allowing caching on a more efficient row level basis rather than file block. To meet ACID compliance all\\ntransactions are journaled independently of the main tables. This allows for much more reliable data recovery\\nas data consistency can be checked.\\n\\nAs of MySQL 5.5 InnoDB is the default engine, and is highly recommended over MyISAM unless you have specific\\nneeds for features unique to that engine.\\n## **Advanced configuration**\\n\\n**Creating a tuned configuration**',\n",
       " 'There are a number of parameters that can be adjusted within MySQL’s configuration files. This will allow you to\\nimprove the server’s performance over time.\\n\\nMany parameters can be adjusted with the existing database, however some may affect the data layout and thus need\\nmore care to apply.\\n\\nFirst, if you have existing data, you will first need to carry out a mysqldump and reload:\\n\\nmysqldump --all-databases --routines -u root -p > ~/fulldump.sql\\n\\nThis will then prompt you for the root password before creating a copy of the data. It is advisable to make sure there\\nare no other users or processes using the database whilst this takes place. Depending on how much data you’ve got in\\nyour database, this may take a while. You won’t see anything on the screen during this process.\\n\\nOnce the dump has been completed, shut down MySQL:\\n\\nsudo service mysql stop\\n\\nIt’s also a good idea to backup the original configuration:\\n\\nsudo rsync -avz /etc/mysql /root/mysql-backup\\n\\nNext, make any desired configuration changes. Then, delete and re-initialise the database space and make sure\\nownership is correct before restarting MySQL:\\n\\nsudo rm -rf /var/lib/mysql/*\\n\\nsudo mysqld --initialize\\n\\nsudo chown -R mysql: /var/lib/mysql\\n\\nsudo service mysql start\\n\\nThe final step is re-importation of your data by piping your SQL commands to the database.\\n\\ncat ~/fulldump.sql | mysql\\n\\nFor large data imports, the ‘Pipe Viewer’ utility can be useful to track import progress. Ignore any ETA times\\nproduced by pv ; they’re based on the average time taken to handle each row of the file, but the speed of inserting can\\nvary wildly from row to row with mysqldumps :\\n\\nsudo apt install pv\\n\\npv ~/fulldump.sql | mysql\\n\\n425',\n",
       " '-----\\n\\nOnce this step is complete, you are good to go!\\n\\n**Note** :\\n\\nThis is not necessary for all my.cnf changes. Most of the variables you can change to improve performance\\nare adjustable even whilst the server is running. As with anything, make sure to have a good backup copy\\nof your config files and data before making changes.\\n\\n**MySQL Tuner**\\n\\n[MySQL Tuner is a Perl script that connects to a running MySQL instance and offers configuration suggestions for](https://github.com/major/MySQLTuner-perl)\\noptimising the database for your workload. The longer the server has been running, the better the advice mysqltuner\\ncan provide. In a production environment, consider waiting for at least 24 hours before running the tool. You can\\ninstall mysqltuner from the Ubuntu repositories:\\n\\nsudo apt install mysqltuner\\n\\nThen once it has been installed, simply run: mysqltuner – and wait for its final report.\\n\\nThe top section provides general information about the database server, and the bottom section provides tuning\\nsuggestions to alter in your my.cnf . Most of these can be altered live on the server without restarting; look through\\nthe official MySQL documentation (link in Resources section at the bottom of this page) for the relevant variables\\nto change in production. The following example is part of a report from a production database showing potential\\nbenefits from increasing the query cache:\\n\\n-------- Recommendations ----------------------------------------------------\\nGeneral recommendations:\\n\\nRun OPTIMIZE TABLE to defragment tables for better performance\\n\\nIncrease table_cache gradually to avoid file descriptor limits\\n\\nVariables to adjust:\\n\\nkey_buffer_size (> 1.4G)',\n",
       " 'query_cache_size (> 32M)\\n\\ntable_cache (> 64)\\n\\ninnodb_buffer_pool_size (>= 22G)\\n\\nIt goes without saying that performance optimisation strategies vary from application to application. So for example,\\nwhat works best for WordPress might not be the best for Drupal or Joomla. Performance can be dependent on the\\ntypes of queries, use of indexes, how efficient the database design is and so on.\\n\\nYou may find it useful to spend some time searching for database tuning tips based on what applications you’re using.\\nOnce you’ve reached the point of diminishing returns from database configuration adjustments, look to the application\\nitself for improvements, or invest in more powerful hardware and/or scaling up the database environment.\\n## **Resources**\\n\\n[• Full documentation is available in both online and offline formats from the MySQL Developers portal](http://dev.mysql.com/doc/)\\n\\n[• For general SQL information see the O’Reilly books Getting Started with SQL: A Hands-On Approach for](http://shop.oreilly.com/product/0636920044994.do)\\n[Beginners by Thomas Nield as an entry point and SQL in a Nutshell as a quick reference.](http://shop.oreilly.com/product/0636920044994.do)\\n\\n[• The Apache MySQL PHP Ubuntu Wiki page also has useful information.](https://help.ubuntu.com/community/ApacheMySQLPHP)\\n\\n[PostgreSQL (also known as Postgres) is an object-relational database system that has the features of traditional com-](https://www.postgresql.org/)\\nmercial database systems with enhancements to be found in next-generation database management systems (DBMS).\\n## **Installation**\\n\\nTo install PostgreSQL, run the following command in the command prompt:\\n\\nsudo apt install postgresql',\n",
       " \"The database service is automatically configured with viable defaults, but can be customised based on your specific\\nneeds.\\n## **Configuration**\\n\\nPostgreSQL supports multiple client authentication methods. In Ubuntu, peer is the default authentication method\\nused for local connections, while scram-sha-256 is the default for host connections (this used to be md5 until Ubuntu\\n[21.10). Please refer to the PostgreSQL Administrator’s Guide if you would like to configure alternatives like Kerberos.](http://www.postgresql.org/docs/current/static/admin.html)\\n\\n426\\n\\n\\n-----\\n\\nThe following discussion assumes that you wish to enable TCP/IP connections and use the MD5 method for client authentication. PostgreSQL configuration files are stored in the /etc/postgresql/<version>/main directory. For example,\\nif you install PostgreSQL 14, the configuration files are stored in the /etc/postgresql/14/main directory.\\n\\n**Tip** :\\nTo configure *IDENT* authentication, add entries to the /etc/postgresql/*/main/pg_ident.conf file. There\\nare detailed comments in the file to guide you.\\n\\nBy default only connections from the local system are allowed, to enable all other computers to connect to your\\nPostgreSQL server, edit the file /etc/postgresql/*/main/postgresql.conf . Locate the line: *#listen_addresses =*\\n*‘localhost’* and change it to - :\\n\\nlisten_addresses = '*'\\n\\n**Note** :\\n‘*’ will allow all available IP interfaces (IPv4 and IPv6), to only listen for IPv4 set ‘0.0.0.0’ while ‘::’ allows\\nlistening for all IPv6 addresses.\\n\\n[For details on other parameters, refer to the configuration file or to the PostgreSQL documentation for information](https://www.postgresql.org/docs/)\\non how they can be edited.\",\n",
       " \"Now that we can connect to our PostgreSQL server, the next step is to set a password for the *postgres* user. Run the\\nfollowing command at a terminal prompt to connect to the default PostgreSQL template database:\\n\\nsudo -u postgres psql template1\\n\\nThe above command connects to PostgreSQL database *template1* as user *postgres* . Once you connect to the PostgreSQL\\nserver, you will be at an SQL prompt. You can run the following SQL command at the psql prompt to configure the\\npassword for the user *postgres* .\\n\\nALTER USER postgres with encrypted password 'your_password';\\n\\nAfter configuring the password, edit the file /etc/postgresql/*/main/pg_hba.conf to use *scram-sha-256* authentication\\nwith the *postgres* user, allowed for the template1 database, from any system in the local network (which in the example\\nis 192.168.122.1/24) :\\n\\nhostssl template1 postgres 192.168.122.1/24 scram-sha-256\\n\\n**Note** :\\nThe config statement ‘hostssl’ used here will reject tcp connections that would not use ssl. Postgresql in\\nUbuntu has the ssl feature built in and configured by default, so it works right away. On your postgresql\\nserver this uses the certificate created by ‘ssl-cert’ package which is great, but for production use you\\nshould consider updating that with a proper certificate from a recognized CA.\\n\\nFinally, you should restart the PostgreSQL service to initialise the new configuration. From a terminal prompt enter\\nthe following to restart PostgreSQL:\\n\\nsudo systemctl restart postgresql.service\\n\\n**Warning** :\\n[The above configuration is not complete by any means. Please refer to the PostgreSQL Administrator’s](http://www.postgresql.org/docs/current/static/admin.html)\",\n",
       " \"[Guide to configure more parameters.](http://www.postgresql.org/docs/current/static/admin.html)\\n\\nYou can test server connections from other machines by using the PostgreSQL client as follows, replacing the domain\\nname with your actual server domain name or IP address:\\n\\nsudo apt install postgresql-client\\n\\npsql --host your-servers-dns-or-ip --username postgres --password --dbname template1\\n\\n**Streaming replication**\\n\\nPostgreSQL has a nice feature called Streaming Replication which provides the capability to continuously ship and\\n[apply the Write-Ahead Log (WAL) XLOG records to some number of standby servers in order to keep them current.](http://www.postgresql.org/docs/current/static/wal.html)\\nHere is presented a very basic and simple way to replicate a PostgreSQL server (main) to a standby server.\\n\\nFirst, create a replication user in the main server to be used from the standby server:\\n\\nsudo -u postgres createuser --replication -P -e replicator\\n\\nLet’s configure the main server to turn on the streaming replication. Open the file /etc/postgresql/*/main/postgresql.conf\\nand make sure you have the following lines:\\n\\nlisten_addresses = '*'\\n\\nwal_level = replica\\n\\n427\\n\\n\\n-----\\n\\nAlso edit the file /etc/postgresql/*/main/pg_hba.conf to add an extra line to allow the standby server connection for\\nreplication (that is a special keyword) using the replicator user:\\n\\nhost replication replicator <IP address of the standby> scram-sha-256\\n\\nRestart the service to apply changes:\\n\\nsudo systemctl restart postgresql\\n\\nNow, in the standby server, let’s stop the PostgreSQL service:\\n\\nsudo systemctl stop postgresql\\n\\nEdit the /etc/postgresql/*/main/postgresql.conf to set up hot standby:\\n\\nhot_standby = on\",\n",
       " 'Back up the current state of the main server (those commands are still issued on the standby system):\\n\\nsudo su - postgres\\n\\n# backup the current content of the standby server (update the version of your postgres accordingly)\\n\\ncp -R /var/lib/postgresql/14/main /var/lib/postgresql/14/main_bak\\n\\n# remove all the files in the data directory\\n\\nrm -rf /var/lib/postgresql/14/main/*\\n\\npg_basebackup -h <IP address of the main server> -D /var/lib/postgresql/14/main -U replicator -P -v -R\\n\\nAfter the above this will have done a full single pass copying the content of the main database onto the local system\\nbeing the standby. In the pg_basebackup command the flags represent the following:\\n\\n - -h : The hostname or IP address of the main server\\n\\n - -D : The data directory\\n\\n - -U : The user to be used in the operation\\n\\n - -P : Turns on progress reporting\\n\\n - -v : Enables verbose mode\\n\\n - -R : Creates a standby.signal file and appends connection settings to postgresql.auto.conf\\n\\nFinally, let’s start the PostgreSQL service on standby server:\\n\\nsudo systemctl start postgresql\\n\\nTo make sure it is working, go to the main server and run the following command:\\n\\nsudo -u postgres psql -c \"select * from pg_stat_replication;\"\\n\\nAs mentioned, this is a very simple introduction, there are way more great details in the upstream documentation\\n[about the configuration of replication as well as further High Availability, Load Balancing, and Replication.](https://www.postgresql.org/docs/current/static/runtime-config-replication.html)\\n\\nTo test the replication you can now create a test database in the main server and check if it is replicated in the standby\\n\\nserver:',\n",
       " 'sudo -u postgres createdb test # on the main server\\n\\nsudo -u postgres psql -c \"\\\\l\" # on the standby server\\n\\nYou need to be able to see the test database, that was created on the main server, in the standby server.\\n## **Backups**\\n\\n[PostgreSQL databases should be backed up regularly. Refer to the PostgreSQL Administrator’s Guide for different](http://www.postgresql.org/docs/current/static/backup.html)\\napproaches.\\n## **Resources**\\n\\n[• As mentioned above, the PostgreSQL Administrator’s Guide is an excellent resource. The guide is also available](http://www.postgresql.org/docs/current/static/admin.html)\\nin the postgresql-doc package. Execute the following in a terminal to install the package:\\n\\nsudo apt install postgresql-doc\\n\\nThis package provides further man pages on postgresql ‘dblink’ and ‘server programming interface’ as well as the html\\nguide that you’d find upstream. To view the guide enter xdg-open /usr/share/doc/postgresql-doc-*/html/index.html\\nor point your browser at it.\\n\\n[• For general SQL information see the O’Reilly books Getting Started with SQL: A Hands-On Approach for](http://shop.oreilly.com/product/0636920044994.do)\\n[Beginners by Thomas Nield as an entry point and SQL in a Nutshell as a quick reference.](http://shop.oreilly.com/product/0636920044994.do)\\n\\n[• Also, see the PostgreSQL Ubuntu Wiki page for more information.](https://help.ubuntu.com/community/PostgreSQL)\\n\\n428\\n\\n\\n-----\\n\\n# **LMA →COS**\\n\\nThe LMA stack is being succeeded by COS. While the current LMA will continue to work for the time\\n[being, most users are recommended to instead consider COS. More information available at Charmhub |](https://charmhub.io/topics/canonical-observability-stack/)',\n",
       " '[Canonical Observability Stack](https://charmhub.io/topics/canonical-observability-stack/)\\n\\nLogging, Monitoring, and Alerting (LMA) is a collection of tools used to guarantee the availability of your running\\ninfrastructure. Your LMA stack will help point out issues in load, networking, and other resources before it becomes\\na failure point.\\n## **Architectural Overview**\\n\\nCanonical’s LMA stack involves several discrete software services acting in concert, including:\\n\\n - Prometheus\\n\\n - Prometheus Alertmanager\\n\\n - Grafana\\n\\n  - Telegraf\\n\\nTelegraf collects metrics from the operating system, running software, and other inputs. Its plugin system permits\\nexport of data in any arbitrary format; for this system we collect the data in a central data manager called Prometheus.\\n\\nPrometheus works as a hub, polling data from different Telegraf nodes and sending it to various outputs, including\\npersistent storage. For this LMA stack, visualization is handled via Grafana and email/pager alerts are generated via\\nthe Prometheus Alertmanager plugin.\\n## **Getting Started**\\n\\nLet’s set up a basic demonstration with two nodes, the first acting as a placeholder load with Telegraf installed the *“Workload”*, and the second acting as our data visualization system - the *“Monitor”* . This will help familiarize\\nourselves with the various components and how they interoperate.\\n\\nThe Workload node will be running Telegraf to collect metrics from whatever load we’re monitoring. For demo\\npurposes we’ll just read the cpu/mem data from the node. In a real environment, we’d have multiple hosts each with\\ntheir own Telegraf instance collecting hardware, network, and software status particular to that node.',\n",
       " 'Our Monitor node will double as both data store and web UI, receiving data from the Workload, storing it to disk,\\nand displaying it for analysis.\\n\\nFor clarity, we’ll refer to these two hosts as named ‘workload’ and ‘monitor’. If you select other hostnames,\\nsimply substitute the correct ones as we go.\\n\\nAs reference, here are the ports we’ll be binding for each service:\\n\\nPrometheus monitor:9090\\n\\nAlertmanager monitor:9093\\nGrafana monitor:3000\\n\\nTelegraf workload:9273\\n\\n**Workload Node**\\n\\nFirst, let’s set up the Workload. We’ll be using LXD as our container technology, but there’s very little that depends\\non this particularly; any VM, container, or bare metal host should work for purposes of this example, so long as it’s\\nrunning Ubuntu 20.10. With LXD installed in our host we can use its lxc command line tool to create our containers:\\n\\n$ lxc launch ubuntu:20.10 workload\\n\\nCreating workload\\n\\nStarting workload\\n\\n$ lxc exec workload -- bash\\n\\nworkload:~#\\n\\nOn the Workload, install Telegraf:\\n\\nworkload:~# apt update\\n\\nworkload:~# apt install telegraf\\n\\nTelegraf processes input data to transform, filter, and decorate it, and then performs selected aggregation functions on\\nit such as tallies, averages, etc. The results are published for collection by external services; in our case Prometheus\\n\\n429\\n\\n\\n-----\\n\\nwill be collecting the cpu:mem data to the Monitor node.\\n\\nOpen /etc/telegraf/telegraf.conf and scroll down to the “INPUT PLUGINS” section. What we’ll need is the\\nfollowing configuration settings, which you should find already enabled by default:\\n\\n[[inputs.cpu]]\\n\\npercpu = true\\n\\ntotalcpu = true\\n\\ncollect_cpu_time = false\\n\\nreport_active = false',\n",
       " 'Looking at the config file you’ll notice it’s almost entirely commented out. There are three different types of sections in\\nthe file: [[inputs]], such as we set above; [[outputs]], that we’ll set up next; and the [[agent]] setting with several\\nperformance tuning parameters such as the collection interval, which we’re setting to 10 seconds. The agent defaults\\nare fine for our demo and for basic usage.\\n\\nFinally, we need to define where Telegraf will provide output. Open /etc/telegraf/telegraf.conf and scroll down to\\nthe “OUTPUT PLUGINS” section and add the following output configuration:\\n\\n[[outputs.prometheus_client]]\\n\\nlisten = \"workload:9273\"\\n\\nmetric_version = 2\\n\\n#[[outputs.influxdb]]\\n\\nWe won’t be using Influxdb, so you can comment that section out if it’s enabled.\\n\\nNow restart the Telegraf service:\\n\\nworkload:~# systemctl restart telegraf\\n\\nworkload:~# systemctl status telegraf\\n\\n- telegraf.service - The plugin-driven server agent for reporting metrics into InfluxDB\\n\\nLoaded: loaded (/lib/systemd/system/telegraf.service; enabled; vendor preset: enabled)\\n\\nActive: active (running) since Sat 2020-10-31 02:17:57 UTC; 6s ago\\n\\nDocs: https://github.com/influxdata/telegraf\\n\\nMain PID: 2562 (telegraf)\\n\\nTasks: 17 (limit: 77021)\\n\\nMemory: 42.2M\\n\\nCGroup: /system.slice/telegraf.service\\n\\n└─2562 /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d\\n\\n...I! Loaded inputs: swap system cpu disk diskio kernel mem processes\\n\\n...I! Loaded outputs: http prometheus_client\\n\\n...I! [agent] Config: Interval:10s, Quiet:false, Hostname:\"workload\", Flush Interval:10s\\n\\n...I! [outputs.prometheus_client] Listening on http://127.0.0.1:9273/metrics',\n",
       " 'Verify that it is collecting metrics by connecting to Telegraf’s web interface:\\n\\nworkload:~# wget -O- http://workload:9273/metrics\\n\\n# HELP cpu_usage_guest Telegraf collected metric\\n\\n# TYPE cpu_usage_guest gauge\\n\\ncpu_usage_guest{cpu=\"cpu-total\",host=\"workload\"} 0\\n\\ncpu_usage_guest{cpu=\"cpu0\",host=\"workload\"} 0\\n\\ncpu_usage_guest{cpu=\"cpu1\",host=\"workload\"} 0\\n\\ncpu_usage_guest{cpu=\"cpu10\",host=\"workload\"} 0\\n\\n...\\n\\ncpu_usage_idle{cpu=\"cpu-total\",host=\"workload\"} 92.74914376428686\\n\\ncpu_usage_idle{cpu=\"cpu0\",host=\"workload\"} 86.72897196325539\\n\\ncpu_usage_idle{cpu=\"cpu1\",host=\"workload\"} 90.11857707405758\\n\\ncpu_usage_idle{cpu=\"cpu10\",host=\"workload\"} 95.95141700494543\\n\\n**Monitor Node**\\n\\nNow let’s create the Monitor. As before, we’ll be using LXD as the container technology but if you use some other\\napproach it should only require some adaptation:\\n\\n$ lxc launch ubuntu:20.10 monitor\\n\\nCreating monitor\\n\\n430\\n\\n\\n-----\\n\\nStarting monitor\\n\\n$ lxc exec monitor -- bash\\n\\nmonitor:~#\\n\\nMake a note of the newly created container’s IP address, which we’ll need later on;\\n\\nmonitor:~# ip addr | grep \\'inet .* global\\'\\n\\ninet 10.69.244.104/24 brd 10.69.244.255 scope global dynamic eth0\\n\\nVerify the Workload’s Telegraf instance can be reached from the Monitor:\\n\\nmonitor:~# wget -O- http://workload:9273/metrics\\n\\nWe’ll be setting up a few components to run on this node using their Snap packages. LXD images should normally\\nhave snap pre-installed but if not install it manually:\\n\\nmonitor:~# apt install snapd\\n\\n**A. Prometheus**\\n\\nPrometheus will be our data manager. It collects data from external sources - Telegraf in our case - and distributes it',\n",
       " 'to various destinations such as email/pager alerts, web UI’s, API clients, remote storage services, etc. We’ll get into\\nthose shortly.\\n\\nLet’s install Prometheus itself and the Prometheus Alertmanager plugin for alerts, along with a bunch of required\\ndependencies:\\n\\nmonitor:~# snap install prometheus\\n\\nmonitor:~# snap install prometheus-alertmanager\\n\\nThe snap will automatically configure and start the service. To verify this, run:\\n\\nmonitor:~# snap services\\n\\nService Startup Current Notes\\n\\nlxd.activate enabled inactive \\nlxd.daemon enabled inactive socket-activated\\n\\nprometheus.prometheus enabled active \\nprometheus-alertmanager.alertmanager enabled active \\nVerify that Prometheus is listening on the port as we expect:\\n\\nvisualizer:~# ss -tulpn | grep prometheus\\n\\ntcp LISTEN 0 128 *:9090 *:* users:((\"prometheus\",pid=618,fd=8))\\n\\njournalctl can be also used to review the state of snap services, particularly if more detail is desired. For example, to\\nsee where Prometheus is loading its config from:\\n\\nmonitor:~# journalctl | grep \"prometheus.*config\"\\n\\n...\\n\\n...msg=\"Completed loading of configuration file\" filename=/var/snap/prometheus/32/prometheus.yml\\n\\nAlthough the filename points to a specific snap revision ( 32, in this case), we can use the generic file\\n\\n/var/snap/prometheus/current/prometheus.yml here in order to make things more generic.\\n\\nEdit this config file to register the targets we’ll be reading data from. This will go under the scrape_configs section\\nof the file:\\n\\nscrape_configs:\\n\\n# The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\\n\\n - job_name: \\'prometheus\\'\\n\\n# metrics_path defaults to \\'/metrics\\'\\n\\n# scheme defaults to \\'http\\'.',\n",
       " 'static_configs:\\n\\n  - targets: [\\'localhost:9090\\']\\n\\n - job_name: \\'telegraf\\'\\n\\nstatic_configs:\\n\\n  - targets: [\\'workload:9273\\']\\n\\nThen restart Prometheus:\\n\\n431\\n\\n\\n-----\\n\\nmonitor:~# snap restart prometheus\\n\\nWhile we’ll be using Grafana for visualization, Prometheus also has a web interface for viewing and interacting with\\nthe collected data. At this stage we can load it just to verify our setup is working properly. In a web browser, navigate\\nto the Monitor’s IP address, and port 9090. You should see Prometheus’ interface, as in the following image:\\n\\nIn the entry box, enter cpu_usage_system, select the Graph tab and click Execute. This should show a graph of our\\ncollected cpu data so far. Prometheus also has a secondary web UI using React.js.\\n\\n**B. Alerts**\\n\\nLet’s tackle the Alert Manager next.\\n\\nEdit /var/snap/prometheus/current/prometheus.yml again, adding the following to the alerting and rules_files sec\\ntions:\\n\\n## /var/snap/prometheus/current/prometheus.yml\\n\\n#...\\n\\n# Alertmanager configuration\\n\\nalerting:\\n\\nalertmanagers:\\n\\n - static_configs:\\n\\n  - targets:\\n\\n   - 127.0.0.1:9093\\n\\nrule_files:\\n\\n432\\n\\n\\n-----\\n\\n - \\'alerts.yml\\'\\n\\nNow create /var/snap/prometheus/current/alerts.yml with the following contents:\\n\\n## /var/snap/prometheus/current/alerts.yml\\n\\ngroups:\\n\\n- name: demo-alerts\\n\\nrules:\\n\\n - alert: HighLoad\\n\\nexpr: node_load1 > 2.0\\n\\nfor: 60m\\n\\nlabels:\\n\\nseverity: normal\\n\\nannotations:\\n\\ndescription: \\'{{ $labels.instance }} of job {{ $labels.job }} is under high load.\\'\\n\\nsummary: Instance {{ $labels.instance }} under high load.\\n\\nvalue: \\'{{ $value }}\\'\\n\\n - alert: InstanceDown\\n\\nexpr: up == 0\\n\\nfor: 5m\\n\\nlabels:\\n\\nseverity: major\\n\\nannotations:\\n\\nsummary: \"Instance {{ $labels.instance }} down\"',\n",
       " 'description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\\n\\nThis adds two alerts: one for high processor load, and one to report if the node has been unreachable for over 5 minutes.\\nWe’re considering high cpu to be a load of 2 or higher for an hour; obviously this would need set to something more\\nsensible for the style of workloads your production system experiences.\\n\\nWith the alerts themselves now defined, we next need to instruct Prometheus’s alert manager how to handle them.\\nThere is a sample configuration installed to /var/snap/prometheus-alertmanager/current/alertmanager.yml, however\\nit’s full of example data. Instead, replace it entirely with this content:\\n\\n## /var/snap/prometheus-alertmanager/current/alertmanager.yml\\n\\nglobal:\\n\\nresolve_timeout: 5m\\n\\nroute:\\n\\ngroup_by: [\\'alertname\\']\\n\\ngroup_wait: 10s\\n\\ngroup_interval: 10s\\n\\nrepeat_interval: 1h\\n\\ninhibit_rules:\\n\\n - source_match:\\n\\nseverity: \\'critical\\'\\n\\ntarget_match:\\n\\nseverity: \\'warning\\'\\n\\nequal: [\\'alertname\\', \\'dev\\', \\'instance\\']\\n\\nRestart the alertmanager after making the configuration change:\\n\\nworkload:~# snap restart prometheus-alertmanager\\n\\n**C. Grafana**\\n\\nGrafana provides our main dashboard, from which we can generate graphs and other visuals to study the collected\\nmetrics. Grafana can read its data directly from log files, but we’ll focus on using Prometheus as its principle data\\nsource. Grafana is available as a snap. You can install it like this:\\n\\nmonitor:~# snap install grafana\\n\\ngrafana 6.7.4 from Alvaro Uría (aluria) installed\\n\\nIt uses port 3000 :\\n\\n# ss -tulpn | grep grafana\\n\\n433\\n\\n\\n-----\\n\\ntcp LISTEN 0 128 *:3000 *:* users:((\"grafana-server\",pid=1449,fd=10))',\n",
       " 'We next need to know where it expects its configuration:\\n\\nmonitor:~# journalctl | grep \"grafana.*conf\"\\n\\n... msg=\"Config loaded from\" logger=settings file=/snap/grafana/36/conf/defaults.ini\\n\\n... msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_PATHS_PROVISIONING=/var/snap/grafana/commo\\n\\n... error=\"open /var/snap/grafana/common/conf/provisioning/datasources: no such file or directory\"\\n\\n...\\n\\nSo we see it’s getting its defaults from /snap/grafana/36/conf/, but /snap/ is a read-only directoy and therefore we\\ncannot edit the file. Instead, we should put our customizations inside /var/snap/grafana/36/conf/grafana.ini . You\\ncan also use the generic path /var/snap/grafana/current/conf/grafana.ini .\\n\\nFor a production installation, the defaults.ini has numerous parameters we’d want to customize for our site, however\\nfor the demo we’ll accept all the defaults. We do need to configure our data sources, but can do this via the web\\ninterface:\\n\\n$ firefox http://10.69.244.104:3000\\n\\nLogin with ‘admin’ and ‘admin’ as the username and password. This should bring you to the main Grafana page,\\nwhere you can find links to tutorials and documentation. Go ahead and delete any example data sources and/or\\ndashboards.\\n\\nSelect the button to add a new data source and select “Prometheus”. On the “Data Sources / Prometheus” edit page,\\nset the name to Prometheus, URL to http://localhost:9090, and Access to “Server (default)” to make Grafana pull\\ndata from the Prometheus service we set up. The remaining settings can be left to defaults. Click “Save & Test”.\\n\\n434\\n\\n\\n-----\\n\\nReturning to the Grafana home page, next set up a “New Dashboard”. A dashboard can hold one or more panels,',\n",
       " 'each of which can be connected to one or more data queries. Let’s add a panel for CPU data. For the query, enter\\n“cpu_usage_system” in the Metrics field.\\n\\nOn the left you can see four buttons to configure four elements of the panel: It’s data source, its visualization, general\\nsettings, and alerts. The general settings page allows us to set a title for the panel, for instance. Make any other\\ncustomizations you desire, and then save the dashboard using the save icon at the top of the page.\\n\\nUsing the same procedure, add additional panels for processor load and memory usage. Panels can be used to present\\nother types of data as well, such as numerical indicators, logs, newsfeeds, or markdown-formatted documentation. For\\nexample, you can add a panel to display the system uptime, such as in the following image:\\n\\n435\\n\\n\\n-----\\n\\nTry also adding a panel with the “Text” visualization option for entering descriptive text about our demo. Save, and\\nthen view the final dashboard:\\n\\n[Logs are an invaluable source of information about problems that may arise in your server. Logwatch keeps an eye on](https://sourceforge.net/projects/logwatch/)\\nyour logs for you, flags items that may be of interest, and reports them via email.\\n## Install logwatch\\n\\nInstall logwatch using the following command:\\n\\nsudo apt install logwatch\\n\\nYou will also need to manually create a temporary directory in order for it to work:\\n\\nsudo mkdir /var/cache/logwatch\\n\\n436\\n\\n\\n-----\\n\\n## Configure logwatch\\n\\nLogwatch’s default configuration is kept in /usr/share/logwatch/default.conf/logwatch.conf . However, configuration',\n",
       " 'changes made directly to that file can be overwritten during updates, so instead the file should be copied into /etc\\nand modified there:\\n\\nsudo cp /usr/share/logwatch/default.conf/logwatch.conf /etc/logwatch/conf/\\n\\nWith your favorite editor, open /etc/logwatch/conf/logwatch.conf . The uncommented lines indicate the default\\nconfiguration values. First, lets customise some of the basics:\\n\\nOutput = mail\\n\\nMailTo = me@mydomain.org\\n\\nMailFrom = logwatch@host1.mydomain.org\\n\\nDetail = Low\\n\\nService = All\\n\\nThis assumes you’ve already set up mail services on host1 that will allow mail to be delivered to your me@mydomain.org\\naddress. These emails will be addressed from logwatch@host1.mydomain.org .\\n\\nThe **Detail** level defines how much information is included in the reports. Possible values are: Low, Medium, and High .\\n\\nLogwatch will then monitor logs for all services on the system, unless specified otherwise with the **Service** parameter.\\nIf there are undesired services included in the reports, they can be disabled by removing them with additional **Service**\\nfields. E.g.:\\n\\nService = \"-http\"\\n\\nService = \"-eximstats\"\\n\\nNext, run logwatch manually to verify your configuration changes are valid:\\n\\nsudo logwatch --detail Low --range today\\n\\nThe report produced should look something like this:\\n\\n################### Logwatch 7.4.3 (12/07/16) ####################\\nProcessing Initiated: Fri Apr 24 16:58:14 2020\\nDate Range Processed: today\\n( 2020-Apr-24 )\\nPeriod is day.\\nDetail Level of Output: 0\\nType of Output/Format: stdout / text\\nLogfiles for Host: host1.mydomain.org\\n################################################################\\n\\n--------------------- pam_unix Begin -----------------------',\n",
       " 'sudo:\\n\\nSessions Opened:\\nbryce -> root: 1 Time(s)\\n\\n---------------------- pam_unix End ------------------------\\n--------------------- rsnapshot Begin -----------------------\\nERRORS:\\n/usr/bin/rsnapshot hourly: completed, but with some errors: 5 Time(s)\\n/usr/bin/rsync returned 127 while processing root@host2:/etc/: 5 Time(s)\\n/usr/bin/rsync returned 127 while processing root@host2:/home/: 5 Time(s)\\n/usr/bin/rsync returned 127 while processing root@host2:/proc/uptime: 5 Time(s)\\n/usr/bin/rsync returned 127 while processing root@host3:/etc/: 5 Time(s)\\n/usr/bin/rsync returned 127 while processing root@host3:/home/: 5 Time(s)\\n/usr/bin/rsync returned 127 while processing root@host3:/proc/uptime: 5 Time(s)\\n\\n---------------------- rsnapshot End ------------------------\\n--------------------- SSHD Begin -----------------------\\nUsers logging in through sshd:\\nbryce:\\n192.168.1.123 ( host4.mydomain.org ): 1 time\\n\\n---------------------- SSHD End ------------------------\\n437\\n\\n\\n-----\\n\\n--------------------- Sudo (secure-log) Begin -----------------------\\nbryce => root\\n\\n------------\\n/bin/bash - 1 Time(s).\\n\\n---------------------- Sudo (secure-log) End ------------------------\\n--------------------- Disk Space Begin -----------------------\\nFilesystem Size Used Avail Use% Mounted on\\n/dev/sdc1 220G 19G 190G 9% /\\n/dev/loop1 157M 157M 0 100% /snap/gnome-3-28-1804/110\\n/dev/loop11 1.0M 1.0M 0 100% /snap/gnome-logs/81\\n/dev/md5 9.1T 7.3T 1.8T 81% /srv/Products\\n/dev/md6 9.1T 5.6T 3.5T 62% /srv/Archives\\n/dev/loop14 3.8M 3.8M 0 100% /snap/gnome-system-monitor/127\\n/dev/loop17 15M 15M 0 100% /snap/gnome-characters/399\\n/dev/loop18 161M 161M 0 100% /snap/gnome-3-28-1804/116',\n",
       " '/dev/loop6 55M 55M 0 100% /snap/core18/1668\\n/dev/md1 1.8T 1.3T 548G 71% /srv/Staff\\n/dev/md0 3.6T 3.5T 84G 98% /srv/Backup\\n/dev/loop2 1.0M 1.0M 0 100% /snap/gnome-logs/93\\n/dev/loop5 15M 15M 0 100% /snap/gnome-characters/495\\n/dev/loop8 3.8M 3.8M 0 100% /snap/gnome-system-monitor/135\\n/dev/md7 3.6T 495G 3.0T 15% /srv/Customers\\n/dev/loop9 55M 55M 0 100% /snap/core18/1705\\n/dev/loop10 94M 94M 0 100% /snap/core/8935\\n/dev/loop0 55M 55M 0 100% /snap/gtk-common-themes/1502\\n/dev/loop4 63M 63M 0 100% /snap/gtk-common-themes/1506\\n/dev/loop3 94M 94M 0 100% /snap/core/9066\\n\\n/srv/Backup (/dev/md0) => 98% Used. Warning. Disk Filling up.\\n\\n---------------------- Disk Space End ------------------------\\n###################### Logwatch End #########################\\n## **Further reading**\\n\\n[• The Ubuntu manpage for logwatch contains many more detailed options.](https://manpages.ubuntu.com/manpages/jammy/en/man8/logwatch.8.html)\\n\\nThe monitoring of essential servers and services is an important part of system administration. This guide will show\\n[you how to set up Munin for performance monitoring.](https://munin-monitoring.org/)\\n\\nIn this example, we will use two servers with hostnames: server01 and server02 .\\n\\nServer01 will be set up with the munin package to gather information from the network. Using the munin-node package,\\n\\nserver02 will be configured to send information to server01 .\\n## **Prerequisites**\\n\\nBefore installing Munin on server01, Apache2 will need to be installed. The default configuration is fine for running\\n\\na munin server.\\n## Install munin and munin-node\\n\\nFirst, on server01 install the munin package. In a terminal enter the following command:\\n\\nsudo apt install munin',\n",
       " 'Now on server02, install the munin-node package:\\n\\nsudo apt install munin-node\\n## Configure munin\\n\\nOn server01 edit the /etc/munin/munin.conf file, adding the IP address for server02 :\\n\\n438\\n\\n\\n-----\\n\\n## First our \"normal\" host.\\n\\n[server02]\\n\\naddress 172.18.100.101\\n\\n**Note** :\\n\\nReplace server02 and 172.18.100.101 with the actual hostname and IP address for your server.\\n## Configure munin-node\\n\\nNext, configure munin-node on server02 . Edit /etc/munin/munin-node.conf to allow access by server01 :\\n\\nallow ^172\\\\.18\\\\.100\\\\.100$\\n\\n**Note** :\\n\\nReplace ^172\\\\.18\\\\.100\\\\.100$ with the IP address for your munin server.\\n\\nNow restart munin-node on server02 for the changes to take effect:\\n\\nsudo systemctl restart munin-node.service\\n## **Test the setup**\\n\\nIn a browser, go to http://server01/munin, and you should see links to nice graphs displaying information from the\\nstandard **munin-plugins** for disk, network, processes, and system. However, it should be noted that since this is a\\nnew installation, it may take some time for the graphs to display anything useful.\\n## **Additional Plugins**\\n\\nThe munin-plugins-extra package contains performance checks and additional services such as DNS, DHCP, and\\nSamba, etc. To install the package, from a terminal enter:\\n\\nsudo apt install munin-plugins-extra\\n\\nBe sure to install the package on both the server and node machines.\\n## **References**\\n\\n[• See the Munin website for more details.](https://munin-monitoring.org/)\\n\\n[• Specifically the Munin Documentation page includes information on additional plugins, writing plugins, etc.](https://munin.readthedocs.io/en/latest/)\\n\\n**Note** :\\n\\nNagios Core 3 has been deprecated and is now replaced by Nagios Core 4.',\n",
       " 'The nagios3 package was last\\nsupported in Bionic, so subsequent releases should use nagios4 instead.\\n\\nThe monitoring of essential servers and services is an important part of system administration. This guide walks\\nthrough how to install and configure Nagios Core 3 for availability monitoring.\\n\\nThe example in this guide uses two servers with hostnames: server01 and server02 .\\n\\nServer01 will be configured with Nagios to monitor services on itself and on server02, while server02 will be configured\\nto send data to server01 .\\n## Install nagios3 on server01\\n\\nFirst, on server01, install the nagios3 package by entering the following command into your terminal:\\n\\nsudo apt install nagios3 nagios-nrpe-plugin\\n\\nYou will be asked to enter a password for the **nagiosadmin** user. The user’s credentials are stored in\\n\\n/etc/nagios3/htpasswd.users . To change the nagiosadmin password, or add more users to the Nagios CGI\\nscripts, use the htpasswd that is part of the apache2-utils package.\\n\\nFor example, to change the password for the nagiosadmin user, enter:\\n\\nsudo htpasswd /etc/nagios3/htpasswd.users nagiosadmin\\n\\nTo add a user:\\n\\nsudo htpasswd /etc/nagios3/htpasswd.users steve\\n\\n439\\n\\n\\n-----\\n\\n## Install nagios-nrpe-server on server02\\n\\nNext, on server02 install the nagios-nrpe-server package. From a terminal on server02 enter:\\n\\nsudo apt install nagios-nrpe-server\\n\\n**Note** :\\n\\nNRPE allows you to execute local checks on remote hosts. There are other ways of accomplishing this\\nthrough other Nagios plugins, as well as other checks.\\n## **Configuration overview**\\n\\nThere are a couple of directories containing Nagios configuration and check files.',\n",
       " ' - /etc/nagios3 : Contains configuration files for the operation of the Nagios daemon, CGI files, hosts, etc.\\n\\n - /etc/nagios-plugins : Contains configuration files for the service checks.\\n\\n - /etc/nagios : On the remote host, contains the nagios-nrpe-server configuration files.\\n\\n - /usr/lib/nagios/plugins/ : Where the check binaries are stored. To see the options of a check use the -h option.\\nFor example: /usr/lib/nagios/plugins/check_dhcp -h\\n\\nThere are multiple checks Nagios can be configured to execute for any given host. For this example, Nagios will be\\nconfigured to check disk space, DNS, and a MySQL hostgroup. The DNS check will be on server02, and the MySQL\\nhostgroup will include both server01 and server02 .\\n\\n**Note** :\\n\\nSee these guides for details on setting up Apache, MySQL.\\n\\nAdditionally, there are some terms that once explained will hopefully make understanding Nagios configuration easier:\\n\\n - *Host* : A server, workstation, network device, etc. that is being monitored.\\n\\n - *Host Group* : A group of similar hosts. For example, you could group all web servers, file server, etc.\\n\\n - *Service* : The service being monitored on the host, such as HTTP, DNS, NFS, etc.\\n\\n - *Service Group* : Allows you to group multiple services together. This is useful for grouping multiple HTTP for\\nexample.\\n\\n - *Contact* : Person to be notified when an event takes place. Nagios can be configured to send emails, SMS\\nmessages, etc.\\n\\nBy default, Nagios is configured to check HTTP, disk space, SSH, current users, processes, and load on the **localhost** .\\nNagios will also ping check the **gateway** .\\n\\nLarge Nagios installations can be quite complex to configure.',\n",
       " 'It is usually best to start small, with one or two hosts,\\nto get things configured the way you want before expanding.\\n## **Configure Nagios**\\n\\n**Create host config file for server02**\\n\\nFirst, create a **host** configuration file for server02 . Unless otherwise specified, run all these commands on server01 .\\nIn a terminal enter:\\n\\nsudo cp /etc/nagios3/conf.d/localhost_nagios2.cfg \\\\\\n\\n/etc/nagios3/conf.d/server02.cfg\\n\\n- **Note**:\\n\\n- In all command examples, replace \"`server01`\", \"`server02`\", `172.18.100.100`, and `172.18.100.101` with the host names\\n\\n### Edit the host config file\\n\\nNext, edit `/etc/nagios3/conf.d/server02.cfg`:\\n\\n```text\\n\\ndefine host{\\n\\nuse generic-host ; Name of host template to use\\n\\nhost_name server02\\n\\nalias Server 02\\n\\naddress 172.18.100.101\\n\\n440\\n\\n\\n-----\\n\\n}\\n\\n# check DNS service.\\n\\ndefine service {\\n\\nuse generic-service\\n\\nhost_name server02\\n\\nservice_description DNS\\n\\ncheck_command check_dns!172.18.100.101\\n\\n}\\n\\nRestart the Nagios daemon to enable the new configuration:\\n\\nsudo systemctl restart nagio3.service\\n\\n**Add service definition**\\n\\nNow add a service definition for the MySQL check by adding the following to /etc/nagios3/conf.d/services_nagios2.cfg :\\n\\n# check MySQL servers.\\n\\ndefine service {\\n\\nhostgroup_name mysql-servers\\n\\nservice_description MySQL\\n\\ncheck_command check_mysql_cmdlinecred!nagios!secret!$HOSTADDRESS\\n\\nuse generic-service\\n\\nnotification_interval 0 ; set > 0 if you want to be renotified\\n\\n}\\n\\nA **mysql-servers** hostgroup now needs to be defined. Edit /etc/nagios3/conf.d/hostgroups_nagios2.cfg and add the\\nfollowing:\\n\\n# MySQL hostgroup.\\n\\ndefine hostgroup {\\n\\nhostgroup_name mysql-servers\\n\\nalias MySQL servers\\n\\nmembers localhost, server02\\n\\n}',\n",
       " 'The Nagios check needs to authenticate to MySQL. To add a nagios user to MySQL enter:\\n\\nmysql -u root -p -e \"create user nagios identified by \\'secret\\';\"\\n\\n**Note** :\\n\\nThe nagios user will need to be added to all hosts in the **mysql-servers** hostgroup.\\n\\nRestart nagios to start checking the MySQL servers.\\n\\nsudo systemctl restart nagios3.service\\n\\n**Configure NRPE**\\n\\nLastly configure NRPE to check the disk space on *server02* .\\n\\nOn server01 add the service check to /etc/nagios3/conf.d/server02.cfg :\\n\\n# NRPE disk check.\\n\\ndefine service {\\n\\nuse generic-service\\n\\nhost_name server02\\n\\nservice_description nrpe-disk\\n\\ncheck_command check_nrpe_1arg!check_all_disks!172.18.100.101\\n\\n}\\n\\nNow on server02 edit /etc/nagios/nrpe.cfg changing:\\n\\nallowed_hosts=172.18.100.100\\n\\nAnd below, in the command definition area, add:\\n\\ncommand[check_all_disks]=/usr/lib/nagios/plugins/check_disk -w 20% -c 10% -e\\n\\nFinally, restart nagios-nrpe-server :\\n\\nsudo systemctl restart nagios-nrpe-server.service\\n\\n441\\n\\n\\n-----\\n\\nAlso, on server01 restart Nagios:\\n\\nsudo systemctl restart nagios3.service\\n\\nYou should now be able to see the host and service checks in the Nagios CGI files. To access them, point a browser\\nto http://server01/nagios3 . You will then be prompted for the **nagiosadmin** username and password.\\n## **Further reading**\\n\\nThis section has just scratched the surface of Nagios’ features. The nagios-plugins-extra and nagios-snmp-plugins\\ncontain many more service checks.\\n\\n[• For more information about Nagios, see the Nagios website.](https://www.nagios.org/)',\n",
       " '[• The Nagios Core Documentation and Nagios Core 3 Documentation may also be useful.](https://library.nagios.com/library/products/nagios-core/documentation/)\\n\\n[• They also provide a list of books related to Nagios and network monitoring.](https://www.nagios.org/propaganda/books/)\\n\\n[• The Nagios Ubuntu Wiki page also has more details.](https://help.ubuntu.com/community/Nagios3)\\n\\nThe following shell script uses tar to create an archive file on a remotely mounted NFS file system. The archive\\nfilename is determined using additional command line utilities.\\n\\nEither copy the code into a file, or for instructions of how to use the script, refer to this guide.\\n\\n#!/bin/bash\\n\\n####################################\\n\\n#\\n\\n# Backup to NFS mount script.\\n\\n#\\n\\n####################################\\n\\n# What to backup.\\n\\nbackup_files=\"/home /var/spool/mail /etc /root /boot /opt\"\\n\\n# Where to backup to.\\n\\ndest=\"/mnt/backup\"\\n\\n# Create archive filename.\\n\\nday=$(date +%A)\\n\\nhostname=$(hostname -s)\\n\\narchive_file=\"$hostname-$day.tgz\"\\n\\n# Print start status message.\\n\\necho \"Backing up $backup_files to $dest/$archive_file\"\\n\\ndate\\n\\necho\\n\\n# Backup the files using tar.\\n\\ntar czf $dest/$archive_file $backup_files\\n\\n# Print end status message.\\n\\necho\\n\\necho \"Backup finished\"\\n\\ndate\\n\\n# Long listing of files in $dest to check file sizes.\\n\\nls -lh $dest\\n\\n - $backup_files : A variable listing which directories you would like to backup. The list should be customized to\\nfit your needs.\\n\\n - $day : A variable holding the day of the week (Monday, Tuesday, Wednesday, etc). This is used to create an\\narchive file for each day of the week, giving a backup history of seven days. There are other ways to accomplish',\n",
       " 'this including using the date utility.\\n\\n - $hostname : A variable containing the *short* hostname of the system. Using the hostname in the archive filename\\ngives you the option of placing daily archive files from multiple systems in the same directory.\\n\\n - $archive_file : The full archive filename.\\n\\n442\\n\\n\\n-----\\n\\n - $dest : Destination of the archive file. The directory needs to be created and in this case *mounted* before executing\\nthe backup script. See NFS for details of using NFS.\\n\\n - status messages : Optional messages printed to the console using the echo utility.\\n\\n - tar czf $dest/$archive_file $backup_files : The tar command used to create the archive file.\\n\\n**–** c : Creates an archive.\\n\\n**–** z : Filter the archive through the gzip utility, compressing the archive.\\n\\n**–** f : Output to an archive file. Otherwise the tar output will be sent to STDOUT.\\n\\n - ls -lh $dest : Optional statement prints a -l long listing in -h human-readable format of the destination directory.\\nThis is useful for a quick file size check of the archive file. This check should not replace testing the archive file.\\n\\nThis is a simple example of a backup shell script; however there are many options that can be included in such a script.\\n[For more information on shell scripting see the Advanced Bash-Scripting Guide.](http://tldp.org/LDP/abs/html/)\\n## **Further reading**\\n\\n[• The CronHowto Wiki Page contains details on advanced](https://help.ubuntu.com/community/CronHowto) cron options.\\n\\n[• See the GNU tar Manual for more](http://www.gnu.org/software/tar/manual/index.html) tar options.',\n",
       " '[• The Wikipedia Backup Rotation Scheme article contains information on other backup rotation schemes.](http://en.wikipedia.org/wiki/Backup_rotation_scheme)\\n\\n  - The shell script uses tar to create the archive, but there many other command line utilities that can be used.\\nFor example:\\n\\n**–**\\n[cpio](http://www.gnu.org/software/cpio/) : Used to copy files to and from archives.\\n\\n**–** [dd](http://www.gnu.org/software/coreutils/) : Part of the coreutils package. A low level utility that can copy data from one format to another.\\n\\n**–** [rsnapshot](http://www.rsnapshot.org/) : A filesystem snapshot utility used to create copies of an entire file system. Also check the Install\\nrsnapshot guide for more information.\\n\\n**–** [rsync](http://manpages.ubuntu.com/manpages/focal/man1/rsync.1.html) : A flexible utility used to create incremental copies of files.\\n\\nThe simple backup shell script only allows for seven different archives. For a server whose data doesn’t change often,\\nthis may be enough. If the server has a large amount of data, a more complex rotation scheme should be used.\\n## **Rotating NFS archives**\\n\\nHere, the shell script is slightly modified to implement a grandparent-parent-child rotation scheme (monthly-weeklydaily):\\n\\n  - The rotation will do a *daily* backup from Sunday to Friday.\\n\\n  - On Saturday, a *weekly* backup is done – giving four weekly backups per month.\\n\\n - The *monthly* backup is done on the first day of the month, rotating two monthly backups based on whether the\\nmonth is odd or even.\\n\\nHere is the new script:\\n\\n#!/bin/bash\\n\\n####################################\\n\\n#\\n\\n# Backup to NFS mount script with\\n\\n# grandparent-parent-child rotation.\\n\\n#',\n",
       " '####################################\\n\\n# What to backup.\\n\\nbackup_files=\"/home /var/spool/mail /etc /root /boot /opt\"\\n\\n# Where to backup to.\\n\\ndest=\"/mnt/backup\"\\n\\n# Setup variables for the archive filename.\\n\\nday=$(date +%A)\\n\\nhostname=$(hostname -s)\\n\\n443\\n\\n\\n-----\\n\\n# Find which week of the month 1-4 it is.\\n\\nday_num=$(date +%-d)\\n\\nif (( $day_num <= 7 )); then\\n\\nweek_file=\"$hostname-week1.tgz\"\\n\\nelif (( $day_num > 7 && $day_num <= 14 )); then\\n\\nweek_file=\"$hostname-week2.tgz\"\\n\\nelif (( $day_num > 14 && $day_num <= 21 )); then\\n\\nweek_file=\"$hostname-week3.tgz\"\\n\\nelif (( $day_num > 21 && $day_num < 32 )); then\\n\\nweek_file=\"$hostname-week4.tgz\"\\n\\nfi\\n\\n# Find if the Month is odd or even.\\n\\nmonth_num=$(date +%m)\\n\\nmonth=$(expr $month_num % 2)\\n\\nif [ $month -eq 0 ]; then\\n\\nmonth_file=\"$hostname-month2.tgz\"\\n\\nelse\\n\\nmonth_file=\"$hostname-month1.tgz\"\\n\\nfi\\n\\n# Create archive filename.\\n\\nif [ $day_num == 1 ]; then\\n\\narchive_file=$month_file\\n\\nelif [ $day != \"Saturday\" ]; then\\n\\narchive_file=\"$hostname-$day.tgz\"\\n\\nelse\\n\\narchive_file=$week_file\\n\\nfi\\n\\n# Print start status message.\\n\\necho \"Backing up $backup_files to $dest/$archive_file\"\\n\\ndate\\n\\necho\\n\\n# Backup the files using tar.\\n\\ntar czf $dest/$archive_file $backup_files\\n\\n# Print end status message.\\n\\necho\\n\\necho \"Backup finished\"\\n\\ndate\\n\\n# Long listing of files in $dest to check file sizes.\\n\\nls -lh $dest/\\n\\nThe script can be executed using the same methods as in our guide on how to back up using shell scripts.\\n\\nAs discussed in the introduction, a copy of the backup archives and/or media can then be transferred off-site.\\n## **Backing up to tape drives**\\n\\nA tape drive attached to the server can be used instead of an NFS share.',\n",
       " 'Using a tape drive simplifies archive rotation,\\nand makes taking the media off-site easier as well.\\n\\nWhen using a tape drive, the filename portions of the script aren’t needed because the data is sent directly to the tape\\ndevice. Some commands to manipulate the tape *are* needed, however. This is accomplished using mt, a magnetic tape\\ncontrol utility – part of the cpio package.\\n\\nHere is the shell script modified to use a tape drive:\\n\\n#!/bin/bash\\n\\n####################################\\n\\n#\\n\\n# Backup to tape drive script.\\n\\n444\\n\\n\\n-----\\n\\n#\\n\\n####################################\\n\\n# What to backup.\\n\\nbackup_files=\"/home /var/spool/mail /etc /root /boot /opt\"\\n\\n# Where to backup to.\\n\\ndest=\"/dev/st0\"\\n\\n# Print start status message.\\n\\necho \"Backing up $backup_files to $dest\"\\n\\ndate\\n\\necho\\n\\n# Make sure the tape is rewound.\\n\\nmt -f $dest rewind\\n\\n# Backup the files using tar.\\n\\ntar czf $dest $backup_files\\n\\n# Rewind and eject the tape.\\n\\nmt -f $dest rewoffl\\n\\n# Print end status message.\\n\\necho\\n\\necho \"Backup finished\"\\n\\ndate\\n\\n**Note** :\\n\\nThe default device name for a SCSI tape drive is /dev/st0 . Use the appropriate device path for your\\nsystem.\\n\\nRestoring from a tape drive is basically the same as restoring from a file. Simply rewind the tape and use the device\\npath instead of a file path. For example, to restore the /etc/hosts file to /tmp/etc/hosts :\\n\\nmt -f /dev/st0 rewind\\n\\ntar -xzf /dev/st0 -C /tmp etc/hosts\\n\\nThe primary mechanism for Ubuntu printing and print services is the **Common UNIX Printing System** (CUPS).\\nThis printing system is a freely available, portable printing layer which has become the new standard for printing in\\nmost Linux distributions.',\n",
       " 'CUPS manages print jobs and queues and provides network printing using the standard Internet Printing Protocol\\n(IPP), while offering support for a very large range of printers, from dot-matrix to laser and many in between. CUPS\\nalso supports PostScript Printer Description (PPD) and auto-detection of network printers, and features a simple\\nweb-based configuration and administration tool.\\n## **Installation**\\n\\nTo install CUPS on your Ubuntu computer, simply use sudo with the apt command and give the packages to install\\nas the first parameter. A complete CUPS install has many package dependencies, but they may all be specified on\\nthe same command line. Enter the following at a terminal prompt to install CUPS:\\n\\nsudo apt install cups\\n\\nUpon authenticating with your user password, the packages should be downloaded and installed without error. Upon\\nthe conclusion of installation, the CUPS server will be started automatically.\\n\\nFor troubleshooting purposes, you can access CUPS server errors via the error log file at: /var/log/cups/error_log .\\nIf the error log does not show enough information to troubleshoot any problems you encounter, the verbosity of the\\nCUPS log can be increased by changing the **LogLevel** directive in the configuration file (discussed below) to “debug”\\nor even “debug2”, which logs everything, from the default of “info”. If you make this change, remember to change it\\nback once you’ve solved your problem, to prevent the log file from becoming overly large.\\n\\n445\\n\\n\\n-----\\n\\n## **Configuration**\\n\\nThe Common UNIX Printing System server’s behavior is configured through the directives contained in the file\\n\\n/etc/cups/cupsd.conf .',\n",
       " 'The CUPS configuration file follows the same syntax as the primary configuration file for the\\nApache HTTP server, so users familiar with editing Apache’s configuration file should feel at ease when editing the\\nCUPS configuration file. Some examples of settings you may wish to change initially will be presented here.\\n\\n**Tip**\\n\\nPrior to editing the configuration file, you should make a copy of the original file and protect it from\\nwriting, so you will have the original settings as a reference, and to reuse as necessary.\\n\\nCopy the /etc/cups/cupsd.conf file and protect it from writing with the following commands, issued at a\\nterminal prompt:\\n\\nsudo cp /etc/cups/cupsd.conf /etc/cups/cupsd.conf.original\\n\\nsudo chmod a-w /etc/cups/cupsd.conf.original\\n\\n - **ServerAdmin** : To configure the email address of the designated administrator of the CUPS server, simply edit\\nthe /etc/cups/cupsd.conf configuration file with your preferred text editor, and add or modify the *ServerAdmin*\\nline accordingly. For example, if you are the Administrator for the CUPS server, and your e-mail address is\\n‘bjoy@somebigco.com’, then you would modify the ServerAdmin line to appear as such:\\n\\nServerAdmin bjoy@somebigco.com\\n\\n - **Listen** : By default on Ubuntu, the CUPS server installation listens only on the loopback interface at IP address\\n*127.0.0.1* . In order to instruct the CUPS server to listen on an actual network adapter’s IP address, you must\\nspecify either a hostname, the IP address, or optionally, an IP address/port pairing via the addition of a Listen\\ndirective. For example, if your CUPS server resides on a local network at the IP address *192.168.10.250* and',\n",
       " \"you’d like to make it accessible to the other systems on this subnetwork, you would edit the /etc/cups/cupsd.conf\\nand add a Listen directive, as such:\\n\\nListen 127.0.0.1:631 # existing loopback Listen\\n\\nListen /var/run/cups/cups.sock # existing socket Listen\\n\\nListen 192.168.10.250:631 # Listen on the LAN interface, Port 631 (IPP)\\n\\nIn the example above, you may comment out or remove the reference to the Loopback address (127.0.0.1) if you\\ndo not wish cupsd to listen on that interface, but would rather have it only listen on the Ethernet interfaces of\\nthe Local Area Network (LAN). To enable listening for all network interfaces for which a certain hostname is\\nbound, including the Loopback, you could create a Listen entry for the hostname *socrates* as such:\\n\\nListen socrates:631 # Listen on all interfaces for the hostname 'socrates'\\n\\nor by omitting the Listen directive and using *Port* instead, as in:\\n\\nPort 631 # Listen on port 631 on all interfaces\\n\\nFor more examples of configuration directives in the CUPS server configuration file, view the associated system manual\\npage by entering the following command at a terminal prompt:\\n\\nman cupsd.conf\\n\\n**Note**\\n\\nWhenever you make changes to the /etc/cups/cupsd.conf configuration file, you’ll need to restart the\\nCUPS server by typing the following command at a terminal prompt:\\n\\nsudo systemctl restart cups.service\\n## **Web Interface**\\n\\n**Tip**\\n\\nCUPS can be configured and monitored using a web interface, which by default is available at\\n\\nhttp://localhost:631/admin . The web interface can be used to perform all printer management tasks.\",\n",
       " 'In order to perform administrative tasks via the web interface, you must either have the root account enabled on your\\nserver, or authenticate as a user in the *lpadmin* group. For security reasons, CUPS won’t authenticate a user that\\ndoesn’t have a password.\\n\\nTo add a user to the *lpadmin* group, run at the terminal prompt:\\n\\nsudo usermod -aG lpadmin username\\n\\nFurther documentation is available in the *Documentation/Help* tab of the web interface.\\n\\n446\\n\\n\\n-----\\n\\n## **References**\\n\\n[CUPS Website](http://www.cups.org/)\\n\\n[Debian Open-iSCSI page](http://wiki.debian.org/SAN/iSCSI/open-iscsi)\\n\\nThis section describes what debuginfod is and how users can benefit from Ubuntu’s debuginfod service.\\n## **What is debuginfod?**\\n\\n[From the project’s official page:](https://sourceware.org/elfutils/Debuginfod.html)\\n\\ndebuginfod is a client/server […] that automatically distributes ELF/DWARF/source-code from servers to\\nclients such as debuggers across HTTP.\\n\\n[This means that users will be able to debug programs shipped with Ubuntu without the need to manually install the](https://wiki.ubuntu.com/Debug%20Symbol%20Packages)\\ndistribution’s debuginfo packages.\\n\\nUbuntu maintains its own debuginfod service, which regularly indexes the debug symbols present in ddebs and other\\npackages and serve this information over HTTPS.\\n\\nCurrently, the service only provides DWARF information. There are plans to make it also index and serve source-code\\nin the future.\\n## **Using the service**\\n\\nFrom Kinetic onwards, when you install GDB your system will be automatically configured to use Ubuntu’s debuginfod\\nservice.',\n",
       " 'For previous Ubuntu releases, you can manually enable the service by setting the DEBUGINFOD_URLS environment\\nvariable in your shell. If you use Bash, you can do that by adding the following snippet to your ~/.bashrc :\\n\\nexport DEBUGINFOD_URLS=\"https://debuginfod.ubuntu.com\"\\n\\nWhen you run GDB, and if you have the DEBUGINFOD_URLS variable in your environment, you will be asked whether you\\nwould like to use the service. If you want to make sure that GDB always uses debuginfod, you can put the following\\nsnippet inside your ~/.gdbinit file:\\n\\nset debuginfod enabled on\\n\\nThe debug symbol files will be downloaded on-the-fly during the debugging session, and will be saved locally inside\\nthe ~/.cache/debuginfod_client/ directory. You can safely remove this directory or any files inside it; they will be\\ndownloaded again only if and when needed.\\n## **Example session with GDB**\\n\\nAssuming that you have enabled the use of debuginfod in your system, here is what happens when you invoke GDB\\nto debug a binary from an Ubuntu package:\\n\\n$ gdb -q program\\n\\nReading symbols from program...\\n\\nThis GDB supports auto-downloading debuginfo from the following URLs:\\n\\nhttps://debuginfod.ubuntu.com\\n\\nEnable debuginfod for this session? (y or [n])\\n\\nWhen you answer y to the question above, GDB will download the debug symbols for program :\\n\\nEnable debuginfod for this session? (y or [n]) y\\n\\nDebuginfod has been enabled.\\n\\nTo make this setting permanent, add \\'set debuginfod enabled on\\' to .gdbinit.\\n\\nDownloading 0.20 MB separate debug info for /home/ubuntu/program\\n\\nReading symbols from /home/ubuntu/.cache/debuginfod_client/c0fbda15a807f880e9d0b2dcc635eeeb1f0f728e/debuginfo...\\n\\n(gdb)\\n## **Opting out of the service**',\n",
       " 'If, for some reason, you prefer not to use the service, you can opt-out of it by unsetting the DEBUGINFOD_URLS environment\\nvariable. This can be done by putting the following snippet inside your shell’s configuration file:\\n\\nunset DEBUGINFOD_URLS\\n\\nYou can also disable GDB’s willingness to use debuginfod by putting the following snippet inside your ~/.gdbinit :\\n\\n447\\n\\n\\n-----\\n\\nset debuginfod enabled off\\n## **FAQ**\\n\\nIf you have more questions about the service, please refer to the FAQ page.\\n## **What releases of Ubuntu are supported by the service?**\\n\\ndebuginfod is currently indexing ddebs [packages from all supported Ubuntu releases. Once a release goes unsupported,](https://releases.ubuntu.com/)\\nwe stop indexing ddebs from it and eventually stop serving debug symbols for its packages.\\n## How does debuginfod know how to find the debug symbols for the binary I am debugging?\\n\\ndebuginfod relies on a unique hash that identifies binaries and shared libraries called **Build-ID** . This 160-bit SHA-1\\nhash is generated by the compiler, and can be consulted using tools like readelf :\\n\\n$ readelf -n /usr/bin/bash\\n\\nDisplaying notes found in: .note.gnu.property\\n\\nOwner Data size Description\\n\\nGNU 0x00000020 NT_GNU_PROPERTY_TYPE_0\\n\\nProperties: x86 feature: IBT, SHSTK\\n\\nx86 ISA needed: x86-64-baseline\\n\\nDisplaying notes found in: .note.gnu.build-id\\n\\nOwner Data size Description\\n\\nGNU 0x00000014 NT_GNU_BUILD_ID (unique build ID bitstring)\\n\\nBuild ID: 3e770d2cd0302c6ff2a184e8d2bf4ec98cfcded4\\n\\nDisplaying notes found in: .note.ABI-tag\\n\\nOwner Data size Description\\n\\nGNU 0x00000010 NT_GNU_ABI_TAG (ABI version tag)\\n\\nOS: Linux, ABI: 3.2.0',\n",
       " 'When you are debugging a program, GDB will send the program’s Build-ID to the debuginfod server, who will check\\nif it has the corresponding debug information for that binary/library. If it does, then it will send the debug symbols\\nover HTTPS back to GDB.\\n## Where does debuginfod store the debug information it downloads?\\n\\nThe debug symbols downloaded from the service are saved inside $XDG_CACHE_HOME/.debuginfod_client/ . If\\n\\n$XDG_CACHE_HOME is empty, then ~/.cache/ is used instead.\\n## **Can I remove files from the cache directory?**\\n\\nYes, you can. debuginfod will simply download them again if and when needed.\\n## Can ddebs packages co-exist with debuginfod ?\\n\\nYes. GDB will try to use local debug information if available. That means that if you have a ddeb package installed\\nthat provides the necessary debug symbols for the program being debugged (or if you have already downloaded that\\ninformation from the debuginfod service earlier), then GDB will use it in favour of performing the download.\\n## Can I use debuginfod with my own binary that links against system libraries?\\n\\nYes! debuginfod will obviously not be able to provide any debug symbols for your own program, but it will happily\\nserve debug information for any system libraries that your program links against.\\n\\nDomain Name Service (DNS) is an Internet service that maps IP addresses and fully qualified domain names (FQDN)\\nto one another. In this way, DNS alleviates the need to remember IP addresses. Computers that run DNS are called\\n*name servers* . Ubuntu ships with BIND (Berkley Internet Naming Daemon), the most common program used for\\nmaintaining a name server on Linux.\\n\\n448\\n\\n\\n-----\\n\\n## **Installation**',\n",
       " 'At a terminal prompt, enter the following command to install dns:\\n\\nsudo apt install bind9\\n\\nA very useful package for testing and troubleshooting DNS issues is the dnsutils package. Very often these tools will\\nbe installed already, but to check and/or install dnsutils enter the following:\\n\\nsudo apt install dnsutils\\n## **Configuration**\\n\\nThere are many ways to configure BIND9. Some of the most common configurations are a caching nameserver, primary\\nserver, and secondary server.\\n\\n  - When configured as a caching nameserver BIND9 will find the answer to name queries and remember the answer\\nwhen the domain is queried again.\\n\\n  - As a primary server, BIND9 reads the data for a zone from a file on its host and is authoritative for that zone.\\n\\n  - As a secondary server, BIND9 gets the zone data from another nameserver that is authoritative for the zone.\\n## **Overview**\\n\\nThe DNS configuration files are stored in the /etc/bind directory. The primary configuration file is /etc/bind/named.conf,\\nwhich in the layout provided by the package just includes these files.\\n\\n - /etc/bind/named.conf.options : global DNS options\\n\\n - /etc/bind/named.conf.local : for your zones\\n\\n - /etc/bind/named.conf.default-zones : default zones such as localhost, its reverse, and the root hints\\n\\nThe root nameservers used to be described in the file /etc/bind/db.root . This is now provided instead by the\\n\\n/usr/share/dns/root.hints file shipped with the dns-root-data package, and is referenced in the named.conf.default\\nzones configuration file above.\\n\\nIt is possible to configure the same server to be a caching name server, primary, and secondary: it all depends on the\\nzones it is serving.',\n",
       " 'A server can be the Start of Authority (SOA) for one zone, while providing secondary service for\\nanother zone. All the while providing caching services for hosts on the local LAN.\\n## **Caching Nameserver**\\n\\nThe default configuration acts as a caching server. Simply uncomment and edit /etc/bind/named.conf.options to set\\nthe IP addresses of your ISP’s DNS servers:\\n\\nforwarders {\\n\\n1.2.3.4;\\n\\n5.6.7.8;\\n\\n};\\n\\n**Note**\\n\\nReplace 1.2.3.4 and 5.6.7.8 with the IP Addresses of actual nameservers.\\n\\nTo enable the new configuration, restart the DNS server. From a terminal prompt:\\n\\nsudo systemctl restart bind9.service\\n\\nSee dig for information on testing a caching DNS server.\\n## **Primary Server**\\n\\nIn this section BIND9 will be configured as the Primary server for the domain example.com . Simply replace example.com\\nwith your FQDN (Fully Qualified Domain Name).\\n\\n**Forward Zone File**\\n\\nTo add a DNS zone to BIND9, turning BIND9 into a Primary server, first edit /etc/bind/named.conf.local :\\n\\nzone \"example.com\" {\\n\\ntype master;\\n\\nfile \"/etc/bind/db.example.com\";\\n\\n};\\n\\n449\\n\\n\\n-----\\n\\n**Note**\\n\\nIf bind will be receiving automatic updates to the file as with DDNS, then use /var/lib/bind/db.example.com\\nrather than /etc/bind/db.example.com both here and in the copy command below.\\n\\nNow use an existing zone file as a template to create the /etc/bind/db.example.com file:\\n\\nsudo cp /etc/bind/db.local /etc/bind/db.example.com\\n\\nEdit the new zone file /etc/bind/db.example.com and change localhost. to the FQDN of your server, leaving the\\nadditional . at the end. Change 127.0.0.1 to the nameserver’s IP Address and root.localhost to a valid email',\n",
       " 'address, but with a . instead of the usual @ symbol, again leaving the . at the end. Change the comment to indicate\\nthe domain that this file is for.\\n\\nCreate an *A record* for the base domain, example.com . Also, create an *A record* for ns.example.com, the name server\\nin this example:\\n\\n;\\n\\n; BIND data file for example.com\\n\\n;\\n\\n$TTL 604800\\n\\n@ IN SOA example.com. root.example.com. (\\n\\n2 ; Serial\\n\\n604800 ; Refresh\\n\\n86400 ; Retry\\n\\n2419200 ; Expire\\n\\n604800 ) ; Negative Cache TTL\\n\\n@ IN NS ns.example.com.\\n\\n@ IN A 192.168.1.10\\n\\n@ IN AAAA ::1\\n\\nns IN A 192.168.1.10\\n\\nYou must increment the *Serial Number* every time you make changes to the zone file. If you make multiple changes\\nbefore restarting BIND9, simply increment the Serial once.\\n\\nNow, you can add DNS records to the bottom of the zone file. See Common Record Types for details.\\n\\n**Note**\\n\\nMany admins like to use the last date edited as the serial of a zone, such as *2020012100* which is *yyyymmddss*\\n(where *ss* is the Serial Number)\\n\\nOnce you have made changes to the zone file BIND9 needs to be restarted for the changes to take effect:\\n\\nsudo systemctl restart bind9.service\\n\\n**Reverse Zone File**\\n\\nNow that the zone is setup and resolving names to IP Addresses, a *Reverse zone* needs to be added to allows DNS to\\nresolve an address to a name.\\n\\nEdit /etc/bind/named.conf.local and add the following:\\n\\nzone \"1.168.192.in-addr.arpa\" {\\n\\ntype master;\\n\\nfile \"/etc/bind/db.192\";\\n\\n};\\n\\n**Note**\\n\\nReplace 1.168.192 with the first three octets of whatever network you are using. Also, name the zone file\\n\\n/etc/bind/db.192 appropriately. It should match the first octet of your network.\\n\\nNow create the /etc/bind/db.192 file:',\n",
       " 'sudo cp /etc/bind/db.127 /etc/bind/db.192\\n\\nNext edit /etc/bind/db.192 changing the same options as /etc/bind/db.example.com :\\n\\n;\\n\\n; BIND reverse data file for local 192.168.1.XXX net\\n\\n;\\n\\n450\\n\\n\\n-----\\n\\n$TTL 604800\\n\\n@ IN SOA ns.example.com. root.example.com. (\\n\\n2 ; Serial\\n\\n604800 ; Refresh\\n\\n86400 ; Retry\\n\\n2419200 ; Expire\\n\\n604800 ) ; Negative Cache TTL\\n\\n;\\n\\n@ IN NS ns.\\n\\n10 IN PTR ns.example.com.\\n\\nThe *Serial Number* in the Reverse zone needs to be incremented on each change as well. For each *A record* you configure\\nin /etc/bind/db.example.com, that is for a different address, you need to create a *PTR record* in /etc/bind/db.192 .\\n\\nAfter creating the reverse zone file restart BIND9:\\n\\nsudo systemctl restart bind9.service\\n## **Secondary Server**\\n\\nOnce a *Primary Server* has been configured a *Secondary Server* is highly recommended in order to maintain the\\navailability of the domain should the Primary become unavailable.\\n\\nFirst, on the Primary server, the zone transfer needs to be allowed. Add the allow-transfer option to the example\\nForward and Reverse zone definitions in /etc/bind/named.conf.local :\\n\\nzone \"example.com\" {\\n\\ntype master;\\n\\nfile \"/etc/bind/db.example.com\";\\n\\nallow-transfer { 192.168.1.11; };\\n\\n};\\n\\nzone \"1.168.192.in-addr.arpa\" {\\n\\ntype master;\\n\\nfile \"/etc/bind/db.192\";\\n\\nallow-transfer { 192.168.1.11; };\\n\\n};\\n\\n**Note**\\n\\nReplace 192.168.1.11 with the IP Address of your Secondary nameserver.\\n\\nRestart BIND9 on the Primary server:\\n\\nsudo systemctl restart bind9.service\\n\\nNext, on the Secondary server, install the bind9 package the same way as on the Primary. Then edit the\\n\\n/etc/bind/named.conf.local and add the following declarations for the Forward and Reverse zones:',\n",
       " 'zone \"example.com\" {\\n\\ntype secondary;\\n\\nfile \"db.example.com\";\\n\\nmasters { 192.168.1.10; };\\n\\n};\\n\\nzone \"1.168.192.in-addr.arpa\" {\\n\\ntype secondary;\\n\\nfile \"db.192\";\\n\\nmasters { 192.168.1.10; };\\n\\n};\\n\\n**Note**\\n\\nReplace 192.168.1.10 with the IP Address of your Primary nameserver.\\n\\nRestart BIND9 on the Secondary server:\\n\\nsudo systemctl restart bind9.service\\n\\nIn /var/log/syslog you should see something similar to the following (some lines have been split to fit the format of\\nthis document):\\n\\n451\\n\\n\\n-----\\n\\nclient 192.168.1.10#39448: received notify for zone \\'1.168.192.in-addr.arpa\\'\\n\\nzone 1.168.192.in-addr.arpa/IN: Transfer started.\\n\\ntransfer of \\'100.18.172.in-addr.arpa/IN\\' from 192.168.1.10#53:\\n\\nconnected using 192.168.1.11#37531\\n\\nzone 1.168.192.in-addr.arpa/IN: transferred serial 5\\n\\ntransfer of \\'100.18.172.in-addr.arpa/IN\\' from 192.168.1.10#53:\\n\\nTransfer completed: 1 messages,\\n\\n6 records, 212 bytes, 0.002 secs (106000 bytes/sec)\\n\\nzone 1.168.192.in-addr.arpa/IN: sending notifies (serial 5)\\n\\nclient 192.168.1.10#20329: received notify for zone \\'example.com\\'\\n\\nzone example.com/IN: Transfer started.\\n\\ntransfer of \\'example.com/IN\\' from 192.168.1.10#53: connected using 192.168.1.11#38577\\n\\nzone example.com/IN: transferred serial 5\\n\\ntransfer of \\'example.com/IN\\' from 192.168.1.10#53: Transfer completed: 1 messages,\\n\\n8 records, 225 bytes, 0.002 secs (112500 bytes/sec)\\n\\n**Note**\\n\\nNote: A zone is only transferred if the *Serial Number* on the Primary is larger than the one on the\\nSecondary. If you want to have your Primary DNS notifying other Secondary DNS Servers of zone changes,\\nyou can add also-notify { ipaddress; }; to /etc/bind/named.conf.local as shown in the example below:',\n",
       " 'zone \"example.com\" {\\n\\ntype master;\\n\\nfile \"/etc/bind/db.example.com\";\\n\\nallow-transfer { 192.168.1.11; };\\n\\nalso-notify { 192.168.1.11; };\\n\\n};\\n\\nzone \"1.168.192.in-addr.arpa\" {\\n\\ntype master;\\n\\nfile \"/etc/bind/db.192\";\\n\\nallow-transfer { 192.168.1.11; };\\n\\nalso-notify { 192.168.1.11; };\\n\\n};\\n\\n**Note**\\n\\nThe default directory for non-authoritative zone files is /var/cache/bind/ . This directory is also configured\\nin AppArmor to allow the named daemon to write to it. For more information on AppArmor see Security\\n\\n   - AppArmor.\\n## **Troubleshooting**\\n\\nThis section covers diagnosing problems with DNS and BIND9 configurations.\\n## **Testing**\\n\\n**resolv.conf**\\n\\nThe first step in testing BIND9 is to add the nameserver’s IP Address to a hosts resolver. The Primary nameserver\\nshould be configured as well as another host to double check things. Refer to DNS client configuration for details on\\nadding nameserver addresses to your network clients. In the end your nameserver line in /etc/resolv.conf should be\\npointing at 127.0.0.53 and you should have a search parameter for your domain. Something like this:\\n\\nnameserver 127.0.0.53\\n\\nsearch example.com\\n\\nTo check which DNS server your local resolver is using, run:\\n\\nresolvectl status\\n\\n**Note**\\n\\nYou should also add the IP Address of the Secondary nameserver to your client configuration in case the\\nPrimary becomes unavailable.\\n\\n452\\n\\n\\n-----\\n\\n**dig**\\n\\nIf you installed the dnsutils package you can test your setup using the DNS lookup utility dig:\\n\\n  - After installing BIND9 use dig against the loopback interface to make sure it is listening on port 53. From a\\nterminal prompt:\\n\\ndig -x 127.0.0.1',\n",
       " 'You should see lines similar to the following in the command output:\\n\\n;; Query time: 1 msec\\n\\n;; SERVER: 192.168.1.10#53(192.168.1.10)\\n\\n  - If you have configured BIND9 as a *Caching* nameserver “dig” an outside domain to check the query time:\\n\\ndig ubuntu.com\\n\\nNote the query time toward the end of the command output:\\n\\n;; Query time: 49 msec\\n\\nAfter a second dig there should be improvement:\\n\\n;; Query time: 1 msec\\n\\n**ping**\\n\\nNow to demonstrate how applications make use of DNS to resolve a host name use the ping utility to send an ICMP\\necho request:\\n\\nping example.com\\n\\nThis tests if the nameserver can resolve the name ns.example.com to an IP Address. The command output should\\nresemble:\\n\\nPING ns.example.com (192.168.1.10) 56(84) bytes of data.\\n\\n64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.800 ms\\n\\n64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.813 ms\\n\\n**named-checkzone**\\n\\nA great way to test your zone files is by using the named-checkzone utility installed with the bind9 package. This utility\\nallows you to make sure the configuration is correct before restarting BIND9 and making the changes live.\\n\\n  - To test our example Forward zone file enter the following from a command prompt:\\n\\nnamed-checkzone example.com /etc/bind/db.example.com\\n\\nIf everything is configured correctly you should see output similar to:\\n\\nzone example.com/IN: loaded serial 6\\n\\nOK\\n\\n  - Similarly, to test the Reverse zone file enter the following:\\n\\nnamed-checkzone 1.168.192.in-addr.arpa /etc/bind/db.192\\n\\nThe output should be similar to:\\n\\nzone 1.168.192.in-addr.arpa/IN: loaded serial 3\\n\\nOK\\n\\n**Note**\\n\\nThe *Serial Number* of your zone file will probably be different.\\n\\n**Quick temporary query logging**',\n",
       " 'With the rndc tool, you can quickly turn query logging on and off, without restarting the service or changing the\\nconfiguration file.\\n\\nTo turn query logging *on*, run:\\n\\nsudo rndc querylog on\\n\\nLikewise, to turn it off, run:\\n\\n453\\n\\n\\n-----\\n\\nsudo rndc querylog off\\n\\nThe logs will be sent to syslog and will show up in /var/log/syslog by default:\\n\\nJan 20 19:40:50 new-n1 named[816]: received control channel command \\'querylog on\\'\\n\\nJan 20 19:40:50 new-n1 named[816]: query logging is now on\\n\\nJan 20 19:40:57 new-n1 named[816]: client @0x7f48ec101480 192.168.1.10#36139 (ubuntu.com): query: ubuntu.com IN A +E(0)K (\\n\\n**Note**\\n\\nThe amount of logs generated by enabling querylog could be huge!\\n## **Logging**\\n\\nBIND9 has a wide variety of logging configuration options available, but the two main ones are *channel* and *category*,\\nwhich configure where logs go, and what information gets logged, respectively.\\n\\nIf no logging options are configured the default configuration is:\\n\\nlogging {\\n\\ncategory default { default_syslog; default_debug; };\\n\\ncategory unmatched { null; };\\n\\n};\\n\\nLet’s instead configure BIND9 to send *debug* messages related to DNS queries to a separate file.\\n\\nWe need to configure a *channel* to specify which file to send the messages to, and a *category* . In this example, the\\n*category* will log all queries. Edit /etc/bind/named.conf.local and add the following:\\n\\nlogging {\\n\\nchannel query.log {\\n\\nfile \"/var/log/named/query.log\";\\n\\nseverity debug 3;\\n\\n};\\n\\ncategory queries { query.log; };\\n\\n};\\n\\n**Note**\\n\\nThe *debug* option can be set from 1 to 3. If a level isn’t specified, level 1 is the default.',\n",
       " '  - Since the *named daemon* runs as the *bind* user the /var/log/named directory must be created and the ownership\\nchanged:\\n\\nsudo mkdir /var/log/named\\n\\nsudo chown bind:bind /var/log/named\\n\\n  - Now restart BIND9 for the changes to take effect:\\n\\nsudo systemctl restart bind9.service\\n\\nYou should see the file /var/log/named/query.log fill with query information. This is a simple example of the BIND9\\nlogging options. For coverage of advanced options see More Information.\\n## **References** **Common Record Types**\\n\\nThis section covers some of the most common DNS record types.\\n\\n - A record: This record maps an IP Address to a hostname.\\n\\nwww IN A 192.168.1.12\\n\\n - CNAME record: Used to create an alias to an existing A record. You cannot create a CNAME record pointing to\\nanother CNAME record.\\n\\nweb IN CNAME www\\n\\n - MX record: Used to define where email should be sent to. Must point to an A record, not a CNAME .\\n\\n@ IN MX 1 mail.example.com.\\n\\nmail IN A 192.168.1.13\\n\\n454\\n\\n\\n-----\\n\\n - NS record: Used to define which servers serve copies of a zone. It must point to an A record, not a CNAME . This is\\nwhere Primary and Secondary servers are defined.\\n\\n@ IN NS ns.example.com.\\n\\n@ IN NS ns2.example.com.\\n\\nns IN A 192.168.1.10\\n\\nns2 IN A 192.168.1.11\\n## **More Information**\\n\\n[• Upstream BIND9 Documentation](https://bind9.readthedocs.io/en/latest/)\\n\\n[• DNS and BIND is a popular book now in it’s fifth edition. There is now also a DNS and BIND on IPv6 book.](http://shop.oreilly.com/product/9780596100575.do)\\n\\n  - A great place to ask for BIND9 assistance, and get involved with the Ubuntu Server community, is the *#ubuntu-*\\n*server* [IRC channel on Libera Chat.](https://libera.chat/)',\n",
       " 'File Transfer Protocol (FTP) is a TCP protocol for downloading files between computers. In the past, it has also\\nbeen used for uploading but, as that method does not use encryption, user credentials as well as data transferred in\\nthe clear and are easily intercepted. So if you are here looking for a way to upload and download files securely, see\\nthe OpenSSH documentation instead.\\n\\nFTP works on a client/server model. The server component is called an *FTP daemon* . It continuously listens for FTP\\nrequests from remote clients. When a request is received, it manages the login and sets up the connection. For the\\nduration of the session it executes any of commands sent by the FTP client.\\n\\nAccess to an FTP server can be managed in two ways:\\n\\n - Anonymous\\n\\n  - Authenticated\\n\\nIn the Anonymous mode, remote clients can access the FTP server by using the default user account called “anonymous”\\nor “ftp” and sending an email address as the password. In the Authenticated mode a user must have an account and a\\npassword. This latter choice is very insecure and should not be used except in special circumstances. If you are looking\\nto transfer files securely see SFTP in the section on OpenSSH-Server. User access to the FTP server directories and\\nfiles is dependent on the permissions defined for the account used at login. As a general rule, the FTP daemon will\\nhide the root directory of the FTP server and change it to the FTP Home directory. This hides the rest of the file\\nsystem from remote sessions.\\n## **vsftpd - FTP Server Installation**\\n\\nvsftpd is an FTP daemon available in Ubuntu. It is easy to install, set up, and maintain. To install vsftpd you can\\nrun the following command:',\n",
       " 'sudo apt install vsftpd\\n## **Anonymous FTP Configuration**\\n\\nBy default vsftpd is *not* configured to allow anonymous download. If you wish to enable anonymous download edit\\n\\n/etc/vsftpd.conf by changing:\\n\\nanonymous_enable=YES\\n\\nDuring installation a *ftp* user is created with a home directory of /srv/ftp . This is the default FTP directory.\\n\\nIf you wish to change this location, to /srv/files/ftp for example, simply create a directory in another location and\\nchange the *ftp* user’s home directory:\\n\\nsudo mkdir -p /srv/files/ftp\\n\\nsudo usermod -d /srv/files/ftp ftp\\n\\nAfter making the change restart vsftpd:\\n\\nsudo systemctl restart vsftpd.service\\n\\nFinally, copy any files and directories you would like to make available through anonymous FTP to /srv/files/ftp,\\nor /srv/ftp if you wish to use the default.\\n\\n455\\n\\n\\n-----\\n\\n## **User Authenticated FTP Configuration**\\n\\nBy default vsftpd is configured to authenticate system users and allow them to download files. If you want users to\\nbe able to upload files, edit /etc/vsftpd.conf :\\n\\nwrite_enable=YES\\n\\nNow restart vsftpd:\\n\\nsudo systemctl restart vsftpd.service\\n\\nNow when system users login to FTP they will start in their *home* directories where they can download, upload, create\\ndirectories, etc.\\n\\nSimilarly, by default, anonymous users are not allowed to upload files to FTP server. To change this setting, you\\nshould uncomment the following line, and restart vsftpd:\\n\\nanon_upload_enable=YES\\n\\n**Warning**\\n\\nEnabling anonymous FTP upload can be an extreme security risk. It is best to not enable anonymous\\nupload on servers accessed directly from the Internet.\\n\\nThe configuration file consists of many configuration parameters.',\n",
       " 'The information about each parameter is available\\nin the configuration file. Alternatively, you can refer to the man page, man 5 vsftpd.conf for details of each parameter.\\n## **Securing FTP**\\n\\nThere are options in /etc/vsftpd.conf to help make vsftpd more secure. For example users can be limited to their\\nhome directories by uncommenting:\\n\\nchroot_local_user=YES\\n\\nYou can also limit a specific list of users to just their home directories:\\n\\nchroot_list_enable=YES\\n\\nchroot_list_file=/etc/vsftpd.chroot_list\\n\\nAfter uncommenting the above options, create a /etc/vsftpd.chroot_list containing a list of users one per line. Then\\nrestart vsftpd:\\n\\nsudo systemctl restart vsftpd.service\\n\\nAlso, the /etc/ftpusers file is a list of users that are *disallowed* FTP access. The default list includes root, daemon,\\nnobody, etc. To disable FTP access for additional users simply add them to the list.\\n\\nFTP can also be encrypted using *FTPS* . Different from *SFTP*, *FTPS* is FTP over Secure Socket Layer (SSL). *SFTP*\\nis a FTP like session over an encrypted *SSH* connection. A major difference is that users of SFTP need to have a\\n*shell* account on the system, instead of a *nologin* shell. Providing all users with a shell may not be ideal for some\\nenvironments, such as a shared web host. However, it is possible to restrict such accounts to only SFTP and disable\\nshell interaction.\\n\\nTo configure *FTPS*, edit /etc/vsftpd.conf and at the bottom add:\\n\\nssl_enable=YES\\n\\nAlso, notice the certificate and key related options:\\n\\nrsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem\\n\\nrsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key',\n",
       " 'By default these options are set to the certificate and key provided by the ssl-cert package. In a production environment\\nthese should be replaced with a certificate and key generated for the specific host. For more information on certificates\\nsee Security - Certificates.\\n\\nNow restart vsftpd, and non-anonymous users will be forced to use *FTPS* :\\n\\nsudo systemctl restart vsftpd.service\\n\\nTo allow users with a shell of /usr/sbin/nologin access to FTP, but have no shell access, edit /etc/shells adding the\\n*nologin* shell:\\n\\n# /etc/shells: valid login shells\\n\\n/bin/csh\\n\\n/bin/sh\\n\\n456\\n\\n\\n-----\\n\\n/usr/bin/es\\n\\n/usr/bin/ksh\\n\\n/bin/ksh\\n\\n/usr/bin/rc\\n\\n/usr/bin/tcsh\\n\\n/bin/tcsh\\n\\n/usr/bin/esh\\n\\n/bin/dash\\n\\n/bin/bash\\n\\n/bin/rbash\\n\\n/usr/bin/screen\\n\\n/usr/sbin/nologin\\n\\nThis is necessary because, by default vsftpd uses PAM for authentication, and the /etc/pam.d/vsftpd configuration\\nfile contains:\\n\\nauth required pam_shells.so\\n\\nThe *shells* PAM module restricts access to shells listed in the /etc/shells file.\\n\\nMost popular FTP clients can be configured to connect using FTPS. The lftp command line FTP client has the ability\\nto use FTPS as well.\\n## **References**\\n\\n[• See the vsftpd website for more information.](http://vsftpd.beasts.org/vsftpd_conf.html)\\n\\n[Wikipedia iSCSI Definition:](https://en.wikipedia.org/wiki/ISCSI)\\n\\niSCSI an acronym for **Internet Small Computer Systems Interface** [, an Internet Protocol (IP)-based](https://en.wikipedia.org/wiki/Internet_Protocol)\\n[storage networking standard for linking data storage facilities. It provides block-level access to storage](https://en.wikipedia.org/wiki/Block-level_storage)',\n",
       " '[devices by carrying SCSI commands over a TCP/IP network.](https://en.wikipedia.org/wiki/Computer_data_storage)\\n\\n[iSCSI is used to facilitate data transfers over intranets and to manage storage over long distances. It can](https://en.wikipedia.org/wiki/Intranet)\\n[be used to transmit data over local area networks (LANs), wide area networks (WANs), or the Internet](https://en.wikipedia.org/wiki/Local_area_network)\\nand can enable location-independent data storage and retrieval.\\n\\n[The protocol allows clients (called](https://en.wikipedia.org/wiki/Protocol_(computing)) *initiators* ) to send SCSI commands ( *[CDBs](https://en.wikipedia.org/wiki/SCSI_CDB)* ) to storage devices ( *targets* )\\non remote servers. [It is a storage area network (SAN) protocol, allowing organizations to consolidate](https://en.wikipedia.org/wiki/Storage_area_network)\\n[storage into storage arrays while providing clients (such as database and web servers) with the illusion of](https://en.wikipedia.org/wiki/Storage_array)\\nlocally attached SCSI disks.\\n\\n[It mainly competes with Fibre Channel, but unlike traditional Fibre Channel, which usually requires](https://en.wikipedia.org/wiki/Fibre_Channel)\\ndedicated cabling, iSCSI can be run over long distances using existing network infrastructure.\\n\\nUbuntu Server can be configured as both: **iSCSI initiator** and **iSCSI target** . This guide provides commands and\\nconfiguration options to setup an **iSCSI initiator** (or Client).\\n\\n*Note: It is assumed that* ***you already have an iSCSI target on your local network*** *and have the appropriate*\\n*rights to connect to it. The instructions for setting up a target vary greatly between hardware providers, so consult*',\n",
       " '*your vendor documentation to configure your specific iSCSI target.*\\n## **Network Interfaces Configuration**\\n\\nBefore start configuring iSCSI, make sure to have the network interfaces correctly set and configured in order to have\\nopen-iscsi package to behave appropriately, specially during boot time. In Ubuntu 20.04 LTS, the default network\\n[configuration tool is netplan.io.](https://netplan.readthedocs.io/en/latest/examples/)\\n\\nFor all the iSCSI examples bellow please consider the following netplan configuration for my iSCSI initiator:\\n\\n*/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg*\\n\\n{config: disabled}\\n\\n*/etc/netplan/50-cloud-init.yaml*\\n\\nnetwork:\\n\\nethernets:\\n\\nenp5s0:\\n\\nmatch:\\n\\nmacaddress: 00:16:3e:af:c4:d6\\n\\n457\\n\\n\\n-----\\n\\nset-name: eth0\\n\\ndhcp4: true\\n\\ndhcp-identifier: mac\\n\\nenp6s0:\\n\\nmatch:\\n\\nmacaddress: 00:16:3e:50:11:9c\\n\\nset-name: iscsi01\\n\\ndhcp4: true\\n\\ndhcp-identifier: mac\\n\\ndhcp4-overrides:\\n\\nroute-metric: 300\\n\\nenp7s0:\\n\\nmatch:\\n\\nmacaddress: 00:16:3e:b3:cc:50\\n\\nset-name: iscsi02\\n\\ndhcp4: true\\n\\ndhcp-identifier: mac\\n\\ndhcp4-overrides:\\n\\nroute-metric: 300\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\nWith this configuration, the interfaces names change by matching their mac addresses. This makes it easier to manage\\nthem in a server containing multiple interfaces.\\n\\nFrom this point and beyond, 2 interfaces are going to be mentioned: **iscsi01** and **iscsi02** . This helps to demonstrate\\nhow to configure iSCSI in a multipath environment as well (check the Device Mapper Multipath session in this same\\nServer Guide).\\n\\nIf you have only a single interface for the iSCSI network, make sure to follow the same instructions, but\\nonly consider the **iscsi01** interface command line examples.',\n",
       " '## **iSCSI Initiator Install**\\n\\nTo configure Ubuntu Server as an iSCSI initiator install the open-iscsi package. In a terminal enter:\\n\\n$ sudo apt install open-iscsi\\n\\nOnce the package is installed you will find the following files:\\n\\n  - /etc/iscsi/iscsid.conf\\n\\n  - /etc/iscsi/initiatorname.iscsi\\n## **iSCSI Initiator Configuration**\\n\\nConfigure the main configuration file like the example bellow:\\n\\n/etc/iscsi/iscsid.conf\\n\\n### startup settings\\n\\n## will be controlled by systemd, leave as is\\n\\niscsid.startup = /usr/sbin/iscsidnode.startup = manual\\n\\n### chap settings\\n\\n# node.session.auth.authmethod = CHAP\\n\\n## authentication of initiator by target (session)\\n\\n# node.session.auth.username = username\\n\\n# node.session.auth.password = password\\n\\n# discovery.sendtargets.auth.authmethod = CHAP\\n\\n## authentication of initiator by target (discovery)\\n\\n# discovery.sendtargets.auth.username = username\\n\\n# discovery.sendtargets.auth.password = password\\n\\n458\\n\\n\\n-----\\n\\n### timeouts\\n\\n## control how much time iscsi takes to propagate an error to the\\n\\n## upper layer. if using multipath, having 0 here is desirable\\n\\n## so multipath can handle path errors as quickly as possible\\n\\n## (and decide to queue or not if missing all paths)\\n\\nnode.session.timeo.replacement_timeout = 0\\n\\nnode.conn[0].timeo.login_timeout = 15\\n\\nnode.conn[0].timeo.logout_timeout = 15\\n\\n## interval for a NOP-Out request (a ping to the target)\\n\\nnode.conn[0].timeo.noop_out_interval = 5\\n\\n## and how much time to wait before declaring a timeout\\n\\nnode.conn[0].timeo.noop_out_timeout = 5\\n\\n## default timeouts for error recovery logics (lu & tgt resets)\\n\\nnode.session.err_timeo.abort_timeout = 15\\n\\nnode.session.err_timeo.lu_reset_timeout = 30',\n",
       " \"node.session.err_timeo.tgt_reset_timeout = 30\\n\\n### retry\\n\\nnode.session.initial_login_retry_max = 8\\n\\n### session and device queue depth\\n\\nnode.session.cmds_max = 128\\n\\nnode.session.queue_depth = 32\\n\\n### performance\\n\\nnode.session.xmit_thread_priority = -20\\n\\nand re-start the iSCSI daemon:\\n\\n$ systemctl restart iscsid.service\\n\\nThis will set basic things up for the rest of configuration.\\n\\nThe other file mentioned:\\n\\n/etc/iscsi/initiatorname.iscsi\\n\\nInitiatorName=iqn.2004-10.com.ubuntu:01:60f3517884c3\\n\\ncontains this node’s initiator name and is generated during open-iscsi package installation. If you modify this setting,\\nmake sure that you don’t have duplicates in the same iSCSI SAN (Storage Area Network).\\n## **iSCSI Network Configuration**\\n\\nBefore configuring the Logical Units that are going to be accessed by the initiator, it is important to inform the iSCSI\\nservice what are the interfaces acting as paths.\\n\\nA straightforward way to do that is by:\\n\\n  - configuring the following environment variables\\n\\n$ iscsi01_ip=$(ip -4 -o addr show iscsi01 | sed -r 's:.* (([0-9]{1,3}\\\\.){3}[0-9]{1,3})/.*:\\\\1:')\\n\\n$ iscsi02_ip=$(ip -4 -o addr show iscsi02 | sed -r 's:.* (([0-9]{1,3}\\\\.){3}[0-9]{1,3})/.*:\\\\1:')\\n\\n$ iscsi01_mac=$(ip -o link show iscsi01 | sed -r 's:.*\\\\s+link/ether (([0-f]{2}(\\\\:|)){6}).*:\\\\1:g')\\n\\n$ iscsi02_mac=$(ip -o link show iscsi02 | sed -r 's:.*\\\\s+link/ether (([0-f]{2}(\\\\:|)){6}).*:\\\\1:g')\\n\\n  - configuring **iscsi01** interface\\n\\n$ sudo iscsiadm -m iface -I iscsi01 --op=new\\n\\n459\\n\\n\\n-----\\n\\nNew interface iscsi01 added\\n\\n$ sudo iscsiadm -m iface -I iscsi01 --op=update -n iface.hwaddress -v $iscsi01_mac\\n\\niscsi01 updated.\",\n",
       " '$ sudo iscsiadm -m iface -I iscsi01 --op=update -n iface.ipaddress -v $iscsi01_ip\\n\\niscsi01 updated.\\n\\n  - configuring **iscsi02** interface\\n\\n$ sudo iscsiadm -m iface -I iscsi02 --op=new\\n\\nNew interface iscsi02 added\\n\\n$ sudo iscsiadm -m iface -I iscsi02 --op=update -n iface.hwaddress -v $iscsi02_mac\\n\\niscsi02 updated.\\n\\n$ sudo iscsiadm -m iface -I iscsi02 --op=update -n iface.ipaddress -v $iscsi02_ip\\n\\niscsi02 updated.\\n\\n  - discovering the **targets**\\n\\n$ sudo iscsiadm -m discovery -I iscsi01 --op=new --op=del --type sendtargets --portal storage.iscsi01\\n\\n10.250.94.99:3260,1 iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca\\n\\n$ sudo iscsiadm -m discovery -I iscsi02 --op=new --op=del --type sendtargets --portal storage.iscsi02\\n\\n10.250.93.99:3260,1 iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca\\n\\n  - configuring **automatic login**\\n\\n$ sudo iscsiadm -m node --op=update -n node.conn[0].startup -v automatic\\n\\n$ sudo iscsiadm -m node --op=update -n node.startup -v automatic\\n\\n - make sure needed **services** are enabled during OS initialization:\\n\\n$ systemctl enable open-iscsi\\n\\nSynchronizing state of open-iscsi.service with SysV service script with /lib/systemd/systemd-sysv-install.\\n\\nExecuting: /lib/systemd/systemd-sysv-install enable open-iscsi\\n\\nCreated symlink /etc/systemd/system/iscsi.service →/lib/systemd/system/open-iscsi.service.\\n\\nCreated symlink /etc/systemd/system/sysinit.target.wants/open-iscsi.service → /lib/systemd/system/open\\niscsi.service.\\n\\n$ systemctl enable iscsid\\n\\nSynchronizing state of iscsid.service with SysV service script with /lib/systemd/systemd-sysv-install.\\n\\nExecuting: /lib/systemd/systemd-sysv-install enable iscsid',\n",
       " 'Created symlink /etc/systemd/system/sysinit.target.wants/iscsid.service →/lib/systemd/system/iscsid.service.\\n\\n  - restarting **iscsid** service\\n\\n$ systemctl restart iscsid.service\\n\\n  - and, finally, **login in** discovered logical units\\n\\n$ sudo iscsiadm -m node --loginall=automatic\\n\\nLogging in to [iface: iscsi02, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.93.99,3\\n\\nLogging in to [iface: iscsi01, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.94.99,3\\n\\nLogin to [iface: iscsi02, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.93.99,3260]\\n\\nLogin to [iface: iscsi01, target: iqn.2003-01.org.linux-iscsi.storage.x8664:sn.2c084c8320ca, portal: 10.250.94.99,3260]\\n## **Accessing the Logical Units (or LUNs)**\\n\\nCheck dmesg to make sure that the new disks have been detected:\\n\\ndmesg\\n\\n[ 166.840694] scsi 7:0:0:4: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.840892] scsi 8:0:0:4: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.841741] sd 7:0:0:4: Attached scsi generic sg2 type 0\\n\\n[ 166.841808] sd 8:0:0:4: Attached scsi generic sg3 type 0\\n\\n[ 166.842278] scsi 7:0:0:3: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.842571] scsi 8:0:0:3: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.843482] sd 8:0:0:3: Attached scsi generic sg4 type 0\\n\\n[ 166.843681] sd 7:0:0:3: Attached scsi generic sg5 type 0\\n\\n[ 166.843706] sd 8:0:0:4: [sdd] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.843884] scsi 8:0:0:2: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n460\\n\\n\\n-----\\n\\n[ 166.843971] sd 8:0:0:4: [sdd] Write Protect is off',\n",
       " \"[ 166.843972] sd 8:0:0:4: [sdd] Mode Sense: 2f 00 00 00\\n\\n[ 166.844127] scsi 7:0:0:2: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.844232] sd 7:0:0:4: [sdc] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.844421] sd 8:0:0:4: [sdd] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.844566] sd 7:0:0:4: [sdc] Write Protect is off\\n\\n[ 166.844568] sd 7:0:0:4: [sdc] Mode Sense: 2f 00 00 00\\n\\n[ 166.844846] sd 8:0:0:2: Attached scsi generic sg6 type 0\\n\\n[ 166.845147] sd 7:0:0:4: [sdc] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.845188] sd 8:0:0:4: [sdd] Optimal transfer size 65536 bytes\\n\\n[ 166.845527] sd 7:0:0:2: Attached scsi generic sg7 type 0\\n\\n[ 166.845678] sd 8:0:0:3: [sde] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.845785] scsi 8:0:0:1: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.845799] sd 7:0:0:4: [sdc] Optimal transfer size 65536 bytes\\n\\n[ 166.845931] sd 8:0:0:3: [sde] Write Protect is off\\n\\n[ 166.845933] sd 8:0:0:3: [sde] Mode Sense: 2f 00 00 00\\n\\n[ 166.846424] scsi 7:0:0:1: Direct-Access LIO-ORG TCMU device > 0002 PQ: 0 ANSI: 5\\n\\n[ 166.846552] sd 8:0:0:3: [sde] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.846708] sd 7:0:0:3: [sdf] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.847024] sd 8:0:0:1: Attached scsi generic sg8 type 0\\n\\n[ 166.847029] sd 7:0:0:3: [sdf] Write Protect is off\\n\\n[ 166.847031] sd 7:0:0:3: [sdf] Mode Sense: 2f 00 00 00\\n\\n[ 166.847043] sd 8:0:0:3: [sde] Optimal transfer size 65536 bytes\\n\\n[ 166.847133] sd 8:0:0:2: [sdg] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\",\n",
       " \"[ 166.849212] sd 8:0:0:2: [sdg] Write Protect is off\\n\\n[ 166.849214] sd 8:0:0:2: [sdg] Mode Sense: 2f 00 00 00\\n\\n[ 166.849711] sd 7:0:0:3: [sdf] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.849718] sd 7:0:0:1: Attached scsi generic sg9 type 0\\n\\n[ 166.849721] sd 7:0:0:2: [sdh] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.853296] sd 8:0:0:2: [sdg] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.853721] sd 8:0:0:2: [sdg] Optimal transfer size 65536 bytes\\n\\n[ 166.853810] sd 7:0:0:2: [sdh] Write Protect is off\\n\\n[ 166.853812] sd 7:0:0:2: [sdh] Mode Sense: 2f 00 00 00\\n\\n[ 166.854026] sd 7:0:0:3: [sdf] Optimal transfer size 65536 bytes\\n\\n[ 166.854431] sd 7:0:0:2: [sdh] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.854625] sd 8:0:0:1: [sdi] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.854898] sd 8:0:0:1: [sdi] Write Protect is off\\n\\n[ 166.854900] sd 8:0:0:1: [sdi] Mode Sense: 2f 00 00 00\\n\\n[ 166.855022] sd 7:0:0:2: [sdh] Optimal transfer size 65536 bytes\\n\\n[ 166.855465] sd 8:0:0:1: [sdi] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.855578] sd 7:0:0:1: [sdj] 2097152 512-byte logical blocks: > (1.07 GB/1.00 GiB)\\n\\n[ 166.855845] sd 7:0:0:1: [sdj] Write Protect is off\\n\\n[ 166.855847] sd 7:0:0:1: [sdj] Mode Sense: 2f 00 00 00\\n\\n[ 166.855978] sd 8:0:0:1: [sdi] Optimal transfer size 65536 bytes\\n\\n[ 166.856305] sd 7:0:0:1: [sdj] Write cache: enabled, read cache: > enabled, doesn't support DPO or FUA\\n\\n[ 166.856701] sd 7:0:0:1: [sdj] Optimal transfer size 65536 bytes\\n\\n[ 166.859624] sd 8:0:0:4: [sdd] Attached SCSI disk\",\n",
       " \"[ 166.861304] sd 7:0:0:4: [sdc] Attached SCSI disk\\n\\n[ 166.864409] sd 8:0:0:3: [sde] Attached SCSI disk\\n\\n[ 166.864833] sd 7:0:0:3: [sdf] Attached SCSI disk\\n\\n[ 166.867906] sd 8:0:0:2: [sdg] Attached SCSI disk\\n\\n[ 166.868446] sd 8:0:0:1: [sdi] Attached SCSI disk\\n\\n[ 166.871588] sd 7:0:0:1: [sdj] Attached SCSI disk\\n\\n[ 166.871773] sd 7:0:0:2: [sdh] Attached SCSI disk\\n\\nIn the output above you will find **8 x SCSI disks** recognized. The storage server is mapping **4 x LUNs** to this node,\\nAND the node has **2 x PATHs** to each LUN. The OS recognizes each path to each device as 1 SCSI device.\\n\\nYou will find different output depending on the storage server your node is mapping the LUNs from, and\\nthe amount of LUNs being mapped as well.\\n\\nAlthough not the objective of this session, let’s find the 4 mapped LUNs using multipath-tools.\\n\\nYou will find further details about multipath in “Device Mapper Multipathing” session of this same guide.\\n\\n461\\n\\n\\n-----\\n\\n$ apt-get install multipath-tools\\n\\n$ sudo multipath -r\\n\\n$ sudo multipath -ll\\n\\nmpathd (360014051a042fb7c41c4249af9f2cfbc) dm-3 LIO-ORG,TCMU device\\n\\nsize=1.0G features='0' hwhandler='0' wp=rw\\n\\n|-+- policy='service-time 0' prio=1 status=active\\n\\n| `- 7:0:0:4 sde 8:64 active ready running\\n\\n`-+- policy='service-time 0' prio=1 status=enabled\\n\\n`- 8:0:0:4 sdc 8:32 active ready running\\n\\nmpathc (360014050d6871110232471d8bcd155a3) dm-2 LIO-ORG,TCMU device\\n\\nsize=1.0G features='0' hwhandler='0' wp=rw\\n\\n|-+- policy='service-time 0' prio=1 status=active\\n\\n| `- 7:0:0:3 sdf 8:80 active ready running\\n\\n`-+- policy='service-time 0' prio=1 status=enabled\\n\\n`- 8:0:0:3 sdd 8:48 active ready running\",\n",
       " \"mpathb (360014051f65c6cb11b74541b703ce1d4) dm-1 LIO-ORG,TCMU device\\n\\nsize=1.0G features='0' hwhandler='0' wp=rw\\n\\n|-+- policy='service-time 0' prio=1 status=active\\n\\n| `- 7:0:0:2 sdh 8:112 active ready running\\n\\n`-+- policy='service-time 0' prio=1 status=enabled\\n\\n`- 8:0:0:2 sdg 8:96 active ready running\\n\\nmpatha (36001405b816e24fcab64fb88332a3fc9) dm-0 LIO-ORG,TCMU device\\n\\nsize=1.0G features='0' hwhandler='0' wp=rw\\n\\n|-+- policy='service-time 0' prio=1 status=active\\n\\n| `- 7:0:0:1 sdj 8:144 active ready running\\n\\n`-+- policy='service-time 0' prio=1 status=enabled\\n\\n`- 8:0:0:1 sdi 8:128 active ready running\\n\\nNow it is much easier to understand each recognized SCSI device and common paths to same LUNs in the storage\\nserver. With the output above one can easily see that:\\n\\n - **mpatha device** (/dev/mapper/mpatha) is a multipath device for:\\n\\n**–**\\n/dev/sdj\\n\\n**–**\\n/dev/dsi\\n\\n - **mpathb device** (/dev/mapper/mpathb) is a multipath device for:\\n\\n**–**\\n/dev/sdh\\n\\n**–**\\n/dev/dsg\\n\\n - **mpathc device** (/dev/mapper/mpathc) is a multipath device for:\\n\\n**–**\\n/dev/sdf\\n\\n**–**\\n/dev/sdd\\n\\n - **mpathd device** (/dev/mapper/mpathd) is a multipath device for:\\n\\n**–**\\n/dev/sde\\n\\n**–**\\n/dev/sdc\\n\\n**Do not use this in production** without checking appropriate multipath configuration options in the\\n**Device Mapper Multipathing** session. The *default multipath configuration* is less than optimal for\\nregular usage.\\n\\nFinally, to access the LUN (or remote iSCSI disk) you will:\\n\\n  - If accessing through a single network interface:\\n\\n**–**\\naccess it through /dev/sdX where X is a letter given by the OS\\n\\n  - If accessing through multiple network interfaces:\\n\\n**–**\",\n",
       " \"configure multipath and access the device through /dev/mapper/X\\n\\nFor everything else, the created devices are block devices and all commands used with local disks should work the\\n\\nsame way:\\n\\n  - Creating a partition:\\n\\n$ sudo fdisk /dev/mapper/mpatha\\n\\nWelcome to fdisk (util-linux 2.34).\\n\\nChanges will remain in memory only, until you decide to write them.\\n\\n462\\n\\n\\n-----\\n\\nBe careful before using the write command.\\n\\nDevice does not contain a recognized partition table.\\n\\nCreated a new DOS disklabel with disk identifier 0x92c0322a.\\n\\nCommand (m for help): p\\n\\nDisk /dev/mapper/mpatha: 1 GiB, 1073741824 bytes, 2097152 sectors\\n\\nUnits: sectors of 1 * 512 = 512 bytes\\n\\nSector size (logical/physical): 512 bytes / 512 bytes\\n\\nI/O size (minimum/optimal): 512 bytes / 65536 bytes\\n\\nDisklabel type: dos\\n\\nDisk identifier: 0x92c0322a\\n\\nCommand (m for help): n\\n\\nPartition type\\n\\np primary (0 primary, 0 extended, 4 free)\\n\\ne extended (container for logical partitions)\\n\\nSelect (default p): p\\n\\nPartition number (1-4, default 1):\\n\\nFirst sector (2048-2097151, default 2048):\\n\\nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-2097151, default 2097151):\\n\\nCreated a new partition 1 of type 'Linux' and of size 1023 MiB.\\n\\nCommand (m for help): w\\n\\nThe partition table has been altered.\\n\\n  - Creating a filesystem:\\n\\n$ sudo mkfs.ext4 /dev/mapper/mpatha-part1\\n\\nmke2fs 1.45.5 (07-Jan-2020)\\n\\nCreating filesystem with 261888 4k blocks and 65536 inodes\\n\\nFilesystem UUID: cdb70b1e-c47c-47fd-9c4a-03db6f038988\\n\\nSuperblock backups stored on blocks:\\n\\n32768, 98304, 163840, 229376\\n\\nAllocating group tables: done\\n\\nWriting inode tables: done\\n\\nCreating journal (4096 blocks): done\",\n",
       " 'Writing superblocks and filesystem accounting information: done\\n\\n  - Mounting the block device:\\n\\n$ sudo mount /dev/mapper/mpatha-part1 /mnt\\n\\n  - Accessing the data:\\n\\n$ ls /mnt\\n\\nlost+found\\n\\nMake sure to read other important sessions in Ubuntu Server Guide to follow up with concepts explored in this one.\\n## **References**\\n\\n[1. iscsid](https://linux.die.net/man/8/iscsid)\\n\\n[2. iscsi.conf](https://linux.die.net/man/5/iscsi.conf)\\n\\n[3. iscsid.conf](https://github.com/open-iscsi/open-iscsi/blob/master/etc/iscsid.conf)\\n\\n[4. iscsi.service](https://github.com/open-iscsi/open-iscsi/blob/master/etc/systemd/iscsi.service.template)\\n\\n[5. iscsid.service](https://github.com/open-iscsi/open-iscsi/blob/master/etc/systemd/iscsid.service.template)\\n\\n[6. Open-iSCSI](http://www.open-iscsi.com/)\\n[7. Debian Open-iSCSI](http://wiki.debian.org/SAN/iSCSI/open-iscsi)\\n\\nNFS allows a system to share directories and files with others over a network. By using NFS, users and programs can\\naccess files on remote systems almost as if they were local files.\\n\\nSome of the most notable benefits that NFS can provide are:\\n\\n463\\n\\n\\n-----\\n\\n  - Local workstations use less disk space because commonly used data can be stored on a single machine and still\\nremain accessible to others over the network.\\n\\n  - There is no need for users to have separate home directories on every network machine. Home directories could\\nbe set up on the NFS server and made available throughout the network.\\n\\n  - Storage devices such as floppy disks, CDROM drives, and USB Thumb drives can be used by other machines on\\nthe network. This may reduce the number of removable media drives throughout the network.\\n## **Installation**',\n",
       " 'At a terminal prompt enter the following command to install the NFS Server:\\n\\nsudo apt install nfs-kernel-server\\n\\nTo start the NFS server, you can run the following command at a terminal prompt:\\n\\nsudo systemctl start nfs-kernel-server.service\\n## **Configuration**\\n\\nYou can configure the directories to be exported by adding them to the /etc/exports file. For example:\\n\\n/srv *(ro,sync,subtree_check)\\n\\n/home *.hostname.com(rw,sync,no_subtree_check)\\n\\n/scratch *(rw,async,no_subtree_check,no_root_squash)\\n\\nMake sure any custom mount points you’re adding have been created (/srv and /home will already exist):\\n\\nsudo mkdir /scratch\\n\\nApply the new config via:\\n\\nsudo exportfs -a\\n\\nYou can replace * with one of the hostname formats. Make the hostname declaration as specific as possible so\\nunwanted systems cannot access the NFS mount. Be aware that *.hostname.com will match foo.hostname.com but not\\n\\nfoo.bar.my-domain.com .\\n\\nThe *sync* / *async* options control whether changes are gauranteed to be committed to stable storage before replying to\\nrequests. *async* thus gives a performance benefit but risks data loss or corruption. Even though *sync* is the default,\\nit’s worth setting since exportfs will issue a warning if it’s left unspecified.\\n\\n*subtree_check* and *no_subtree_check* enables or disables a security verification that subdirectories a client attempts\\nto mount for an exported filesystem are ones they’re permitted to do so. This verification step has some performance\\nimplications for some use cases, such as home directories with frequent file renames. Read-only filesystems are more\\nsuitable to enable *subtree_check* on. Like with sync, exportfs will warn if it’s left unspecified.',\n",
       " 'There are a number of optional settings for NFS mounts for tuning performance, tightening security, or providing\\nconveniences. These settings each have their own trade-offs so it is important to use them with care, only as needed\\nfor the particular use case. *no_root_squash*, for example, adds a convenience to allow root-owned files to be modified\\nby any client system’s root user; in a multi-user environment where executables are allowed on a shared mount point,\\nthis could lead to security problems.\\n## **NFS Client Configuration**\\n\\nTo enable NFS support on a client system, enter the following command at the terminal prompt:\\n\\nsudo apt install nfs-common\\n\\nUse the mount command to mount a shared NFS directory from another machine, by typing a command line similar\\nto the following at a terminal prompt:\\n\\nsudo mkdir /opt/example\\n\\nsudo mount example.hostname.com:/srv /opt/example\\n\\n**Warning**\\n\\nThe mount point directory /opt/example must exist. There should be no files or subdirectories in the\\n\\n/opt/example directory, else they will become inaccessible until the nfs filesystem is unmounted.\\n\\n464\\n\\n\\n-----\\n\\nAn alternate way to mount an NFS share from another machine is to add a line to the /etc/fstab file. The line\\nmust state the hostname of the NFS server, the directory on the server being exported, and the directory on the local\\nmachine where the NFS share is to be mounted.\\n\\nThe general syntax for the line in /etc/fstab file is as follows:\\n\\nexample.hostname.com:/srv /opt/example nfs rsize=8192,wsize=8192,timeo=14,intr\\n## **Advanced Configuration**\\n\\nNFS is comprised of several services, both on the server and the client. Each one of these services can have its own',\n",
       " 'default configuration, and depending on the Ubuntu Server release you have installed, this configuration is done in\\ndifferent files, and with a different syntax.\\n\\n**Ubuntu Server 22.04 LTS (“jammy”)**\\n\\nAll NFS related services read a single configuration file: /etc/nfs.conf . This is a INI-style config file, see the\\n\\n[nfs.conf(5)](http://manpages.ubuntu.com/manpages/jammy/man5/nfs.conf.5.html) manpage for details. Furthermore, there is a /etc/nfs.conf.d directory which can hold *.conf snippets\\nthat can override settings from previous snippets or from the nfs.conf main config file itself.\\n\\nThere is a new command-line tool called [nfsconf(8)](http://manpages.ubuntu.com/manpages/jammy/man8/nfsconf.8.html) which can be used to query or even set configuration parameters\\nin nfs.conf . In particular, it has a --dump parameter which will show the effective configuration including all changes\\ndone by /etc/nfs.conf.d/*.conf snippets.\\n\\n**For Ubuntu Server 20.04 LTS (“focal”) and earlier**\\n\\nEarlier Ubuntu releases use the traditional configuration mechanism for the NFS services via /etc/defaults/ configuration files. These are /etc/default/nfs-common and /etc/default/nfs/kernel-server, and are used basically to adjust\\nthe command-line options given to each daemon.\\n\\nEach file has a small explanation about the available settings.\\n\\n**Warning**\\n\\nThe NEED_* parameters have no effect on systemd-based installations, like Ubuntu 20.04 LTS (“focal”) and\\nUbuntu 18.04 LTS (“bionic”).\\nIn those systems, to control whether a service should be running or not, use systemctl enable or systemctl\\n\\ndisable, respectively.\\n## **Upgrading to Ubuntu 22.04 LTS (“jammy”)**',\n",
       " 'The main change to the NFS packages in Ubuntu 22.04 LTS (“jammy”) is the configuration file. Instead of multiple\\nfiles sourced by startup scripts from /etc/default/nfs-*, now there is one main configuration file in /etc/nfs.conf,\\nwith an INI-style syntax.\\nWhen upgrading to Ubuntu 22.04 LTS (“jammy”) from a release that still uses the /etc/defaults/nfs-* configuration\\nfiles, the following will happen:\\n\\n  - a default /etc/nfs.conf configuration file will be installed\\n\\n  - if the /etc/default/nfs-* files have been modified, a conversion script will be run and it will create\\n\\n/etc/nfs.conf.d/local.conf with the local modifications.\\n\\nIf this conversion script fails, then the package installation will fail. This can happen if the /etc/default/nfs-* files\\nhave an option that the conversion script wasn’t prepared to handle, or a syntax error for example. In such cases,\\n[please file a bug using this link: OpenID transaction in progress](https://bugs.launchpad.net/ubuntu/+source/nfs-utils/+filebug)\\n\\nYou can run the conversion tool manually to gather more information about the error: it’s in /usr/share/nfs\\ncommon/nfsconvert.py and must be run as root .\\n\\nIf all goes well, as it should in most cases, the system will have /etc/nfs.conf with the defaults, and\\n\\n/etc/nfs.conf.d/local.conf with the changes. You can merge these two together manually, and then delete\\n\\nlocal.conf, or leave it as is. Just keep in mind that /etc/nfs.conf is not the whole story: always inspect\\n\\n/etc/nfs.conf.d as well, as it may contain files overriding the defaults.\\n\\nYou can always run nfsconf --dump to check the final settings, as it merges together all configuration files and shows',\n",
       " 'the resulting non-default settings.\\n\\n465\\n\\n\\n-----\\n\\n## **Restarting NFS services**\\n\\nSince NFS is comprised of several individual services, it can be difficult to determine what to restart after a certain\\nconfiguration change.\\n\\nThe tables below summarize all available services, which “meta” service they are linked to, and which configuration\\nfile each service uses.\\n\\nService nfs-utils.service nfs-server.service config file (22.04) config file (< 22.04) /etc/default/nfs-*\\n\\nnfs-blkmap PartOf nfs.conf\\nnfs-mountd BindsTo nfs.conf nfs-kernel-server\\n\\nnfsdcld\\n\\nnfs-idmapd BindsTo nfs.conf, idmapd.conf idmapd.conf\\nrpc-gssd PartOf nfs.conf\\nrpc-statd PartOf nfs.conf nfs-common\\nrpc-svcgssd PartOf BindsTo nfs.conf nfs-kernel-server\\n\\nFor example, systemctl restart nfs-server.service will restart nfs-mountd, nfs-idmapd and rpc-svcgssd (if running).\\nOn the other hand, restarting nfs-utils.service will restart nfs-blkmap, rpc-gssd, rpc-statd and rpc-svcgssd .\\n\\nOf course, each service can still be individually restarted with the usual systemctl restart <service> .\\n\\nThe [nfs.systemd(7)](http://manpages.ubuntu.com/manpages/jammy/man7/nfs.systemd.7.html) manpage has more details on the several systemd units available with the NFS packages.\\n## **NFS with Kerberos**\\n\\nKerberos with NFS adds an extra layer of security on top of NFS. It can be just a stronger authentication mechanism,\\nor it can also be used to sign and encrypt the NFS traffic.\\n\\nThis section will assume you already have setup a Kerberos server, with a running KDC and admin services. Setting\\nthat up is explained elsewhere in the Ubuntu Server Guide.\\n\\n**NFS server (using kerberos)**',\n",
       " 'The NFS server will have the usual nfs-kernel-server package and its dependencies, but we will also have to install\\nkerberos packages. The kerberos packages are not strictly necessary, as the necessary keys can be copied over from\\nthe KDC, but it makes things much easier.\\n\\nFor this example, we will use:\\n\\n - .vms DNS domain\\n\\n - VMS Kerberos realm\\n\\n - j-nfs-server.vms for the NFS server\\n\\n - j-nfs-client.vms for the NFS client\\n\\n - ubuntu/admin principal has admin privileges on the KDC\\n\\nAdjust these names according to your setup.\\n\\nFirst, install the krb5-user package:\\n\\nsudo apt install krb5-user\\n\\nThen, with an admin principal, let’s create a key for the NFS server:\\n\\n$ sudo kadmin -p ubuntu/admin -q \"addprinc -randkey nfs/j-nfs-server.vms\"\\n\\nAnd extract the key into the local keytab:\\n\\n$ sudo kadmin -p ubuntu/admin -q \"ktadd nfs/j-nfs-server.vms\"\\n\\nAuthenticating as principal ubuntu/admin with password.\\n\\nPassword for ubuntu/admin@VMS:\\n\\nEntry for principal nfs/j-nfs-server.vms with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab FILE:/etc/k\\n\\nEntry for principal nfs/j-nfs-server.vms with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab FILE:/etc/k\\n\\nConfirm the key is available:\\n\\n$ sudo klist -k\\n\\nKeytab name: FILE:/etc/krb5.keytab\\n\\nKVNO Principal\\n\\n---- -------------------------------------------------------------------------\\n466\\n\\n\\n-----\\n\\n2 nfs/j-nfs-server.vms@VMS\\n\\n2 nfs/j-nfs-server.vms@VMS\\n\\nNow install the NFS server:\\n\\n$ sudo apt install nfs-kernel-server\\n\\nThis will already automatically start the kerberos-related nfs services, because of the presence of /etc/krb5.keytab .\\n\\nNow populate /etc/exports, restricting the exports to krb5 authentication.',\n",
       " 'For example, exporting /storage using\\n\\nkrb5p :\\n\\n/storage *(rw,sync,no_subtree_check,sec=krb5p)\\n\\nRefresh the exports:\\n\\n$ sudo exportfs -rav\\n\\nexporting *:/storage\\n\\nThe security options are explained in the [exports(5)](http://manpages.ubuntu.com/manpages/jammy/man5/exports.5.html) manpage, but generally they are:\\n\\n - krb5 : use kerberos for authentication only (non-auth traffic is in clear text)\\n\\n - krb5i : use kerberos for authentication and integrity checks (non-auth traffic is in clear text)\\n\\n - krb5p : use kerberos for authentication, integrity and privacy protection (non-auth traffic is encrypted)\\n\\n**NFS client (using kerberos)**\\n\\nThe NFS client has a similar set of steps. First we will prepare the client’s keytab, so that when we install the NFS\\nclient package it will start the extra kerberos services automatically just by detecting the presence of the keytab:\\n\\nsudo apt install krb5-user\\n\\nTo allow the root user to mount NFS shares via kerberos without a password, we have to create a host key for the\\nNFS client:\\n\\nsudo kadmin -p ubuntu/admin -q \"addprinc -randkey host/j-nfs-client.vms\"\\n\\nAnd extract it:\\n\\n$ sudo kadmin -p ubuntu/admin -q \"ktadd host/j-nfs-client.vms\"\\n\\nNow install the NFS client package:\\n\\n$ sudo apt install nfs-common\\n\\nAnd you should be able to do your first NFS kerberos mount:\\n\\n$ sudo mount j-nfs-server:/storage /mnt\\n\\nIf you are using a machine credential, then the above mount will work without having a kerberos ticket, i.e., klist\\nwill show no tickets:\\n\\n# mount j-nfs-server:/storage /mnt\\n\\n# ls -l /mnt/*\\n\\n-rw-r--r-- 1 root root 0 Apr 5 14:50 /mnt/hello-from-nfs-server.txt\\n\\n# klist\\n\\nklist: No credentials cache found (filename: /tmp/krb5cc_0)',\n",
       " \"Notice the above was done with root . Let’s try accessing that existing mount with the ubuntu user, without acquiring\\na kerberos ticket:\\n\\n# sudo -u ubuntu -i\\n\\n$ ls -l /mnt/*\\n\\nls: cannot access '/mnt/*': Permission denied\\n\\nThe ubuntu user will only be able to access that mount if they have a kerberos ticket:\\n\\n$ kinit\\n\\nPassword for ubuntu@VMS:\\n\\n$ ls -l /mnt/*\\n\\n-rw-r--r-- 1 root root 0 Apr 5 14:50 /mnt/hello-from-nfs-server.txt\\n\\nAnd now we have not only the TGT, but also a ticket for the NFS service:\\n\\n467\\n\\n\\n-----\\n\\n$ klist\\n\\nTicket cache: FILE:/tmp/krb5cc_1000\\n\\nDefault principal: ubuntu@VMS\\n\\nValid starting Expires Service principal\\n\\n04/05/22 17:48:50 04/06/22 03:48:50 krbtgt/VMS@VMS\\n\\nrenew until 04/06/22 17:48:48\\n\\n04/05/22 17:48:52 04/06/22 03:48:50 nfs/j-nfs-server.vms@\\n\\nrenew until 04/06/22 17:48:48\\n\\nTicket server: nfs/j-nfs-server.vms@VMS\\n\\nOne drawback of using a machine credential for mounts done by the root user is that you need a persistent secret\\n(the /etc/krb5.keytab file) in the filesystem. Some sites may not allow such a persistent secret to be stored in the\\nfilesystem. An alternative is to use rpc.gssd s -n option. From rpc.gssd(8) :\\n\\n - -n : when specified, UID 0 is forced to obtain user credentials which are used instead of the local system’s machine\\ncredentials.\\n\\nWhen this option is enabled and rpc.gssd restarted, then even the root user will need to obtain a kerberos ticket to\\nperform an NFS kerberos mount.\\n\\nWarning\\n\\nNote that this prevents automatic NFS mounts via /etc/fstab, unless a kerberos ticket is obtained before.\\n\\nIn Ubuntu 22.04 LTS (“jammy”), this option is controlled in /etc/nfs.conf in the [gssd] section:\\n\\n[gssd]\\n\\nuse-machine-creds=0\",\n",
       " 'In older Ubuntu releases, the command line options for the rpc.gssd daemon are not exposed in /etc/default/nfs\\ncommon, therefore a systemd override file needs to be created. You can either run:\\n\\n$ sudo systemctl edit rpc-gssd.service\\n\\nAnd paste the following into the editor that will open:\\n\\n[Service]\\n\\nExecStart=\\n\\nExecStart=/usr/sbin/rpc.gssd $GSSDARGS -n\\n\\nOr manually create the file /etc/systemd/system/rpc-gssd.service.d/override.conf and any needed directories up to\\nit, with the contents above.\\n\\nAfter you restart the service with systemctl restart rpc-gssd.service, the root user won’t be able to mount the NFS\\nkerberos share without obtaining a ticket first.\\n## **References**\\n\\n[Linux NFS wiki](http://linux-nfs.org/wiki/)\\n\\n[Linux NFS faq](http://nfs.sourceforge.net/)\\n\\n[Ubuntu Wiki NFS Howto](https://help.ubuntu.com/community/SettingUpNFSHowTo)\\n\\n[Ubuntu Wiki NFSv4 Howto](https://help.ubuntu.com/community/NFSv4Howto)\\n## **Introduction**\\n\\nOpenSSH is a powerful collection of tools for the remote control of, and transfer of data between, networked computers.\\nYou will also learn about some of the configuration settings possible with the OpenSSH server application and how to\\nchange them on your Ubuntu system.\\n\\nOpenSSH is a freely available version of the Secure Shell (SSH) protocol family of tools for remotely controlling, or\\ntransferring files between, computers. Traditional tools used to accomplish these functions, such as telnet or rcp, are\\ninsecure and transmit the user’s password in cleartext when used. OpenSSH provides a server daemon and client tools\\nto facilitate secure, encrypted remote control and file transfer operations, effectively replacing the legacy tools.',\n",
       " 'The OpenSSH server component, sshd, listens continuously for client connections from any of the client tools. When\\na connection request occurs, sshd sets up the correct connection depending on the type of client tool connecting. For\\nexample, if the remote computer is connecting with the ssh client application, the OpenSSH server sets up a remote\\ncontrol session after authentication. If a remote user connects to an OpenSSH server with scp, the OpenSSH server\\n\\n468\\n\\n\\n-----\\n\\ndaemon initiates a secure copy of files between the server and client after authentication. OpenSSH can use many\\nauthentication methods, including plain password, public key, and Kerberos tickets.\\n## **Installation**\\n\\nInstallation of the OpenSSH client and server applications is simple. To install the OpenSSH client applications on\\nyour Ubuntu system, use this command at a terminal prompt:\\n\\nsudo apt install openssh-client\\n\\nTo install the OpenSSH server application, and related support files, use this command at a terminal prompt:\\n\\nsudo apt install openssh-server\\n## **Configuration**\\n\\nYou may configure the default behavior of the OpenSSH server application, sshd, by editing the file /etc/ssh/sshd_config .\\nFor information about the configuration directives used in this file, you may view the appropriate manual page with\\nthe following command, issued at a terminal prompt:\\n\\nman sshd_config\\n\\nThere are many directives in the sshd configuration file controlling such things as communication settings, and\\nauthentication modes. The following are examples of configuration directives that can be changed by editing the\\n\\n/etc/ssh/sshd_config file.\\n\\n**Tip**',\n",
       " 'Prior to editing the configuration file, you should make a copy of the original file and protect it from writing\\nso you will have the original settings as a reference and to reuse as necessary.\\n\\nCopy the /etc/ssh/sshd_config file and protect it from writing with the following commands, issued at a\\nterminal prompt:\\n\\nsudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.original\\n\\nsudo chmod a-w /etc/ssh/sshd_config.original\\n\\nFurthermore since losing an ssh server might mean losing your way to reach a server, check the configuration after\\nchanging it and before restarting the server:\\n\\nsudo sshd -t -f /etc/ssh/sshd_config\\n\\nThe following is an *example* of a configuration directive you may change:\\n\\n  - To make your OpenSSH server display the contents of the /etc/issue.net file as a pre-login banner, simply add\\nor modify this line in the /etc/ssh/sshd_config file:\\n\\nBanner /etc/issue.net\\n\\nAfter making changes to the /etc/ssh/sshd_config file, save the file, and restart the sshd server application to effect\\nthe changes using the following command at a terminal prompt:\\n\\nsudo systemctl restart sshd.service\\n\\n**Warning**\\n\\nMany other configuration directives for sshd are available to change the server application’s behavior to fit\\nyour needs. Be advised, however, if your only method of access to a server is ssh, and you make a mistake\\nin configuring sshd via the /etc/ssh/sshd_config file, you may find you are locked out of the server upon\\nrestarting it. Additionally, if an incorrect configuration directive is supplied, the sshd server may refuse to\\nstart, so be extra careful when editing this file on a remote server.\\n## **SSH Keys**',\n",
       " 'SSH allow authentication between two hosts without the need of a password. SSH key authentication uses a *private*\\n*key* and a *public key* .\\n\\nTo generate the keys, from a terminal prompt enter:\\n\\nssh-keygen -t rsa\\n\\nThis will generate the keys using the *RSA Algorithm* . At the time of this writing, the generated keys will have 3072\\nbits. You can modify the number of bits by using the -b option. For example, to generate keys with 4096 bits, you\\ncan do:\\n\\n469\\n\\n\\n-----\\n\\nssh-keygen -t rsa -b 4096\\n\\nDuring the process you will be prompted for a password. Simply hit *Enter* when prompted to create the key.\\n\\nBy default the *public* key is saved in the file ~/.ssh/id_rsa.pub, while ~/.ssh/id_rsa is the *private* key. Now copy the\\n\\nid_rsa.pub file to the remote host and append it to ~/.ssh/authorized_keys by entering:\\n\\nssh-copy-id username@remotehost\\n\\nFinally, double check the permissions on the authorized_keys file, only the authenticated user should have read and\\nwrite permissions. If the permissions are not correct change them by:\\n\\nchmod 600 .ssh/authorized_keys\\n\\nYou should now be able to SSH to the host without being prompted for a password.\\n## **Import keys from public keyservers**\\n\\nThese days many users have already ssh keys registered with services like launchpad or github. Those can be easily\\nimported with:\\n\\nssh-import-id <username-on-remote-service>\\n\\nThe prefix lp: is implied and means fetching from launchpad, the alternative gh: will make the tool fetch from github\\ninstead.\\n## **Two factor authentication with U2F/FIDO**\\n\\n[OpenSSH 8.2 added support for U2F/FIDO hardware authentication devices. These devices are used to provide an](https://www.openssh.com/txt/release-8.2)',\n",
       " 'extra layer of security on top of the existing key-based authentication, as the hardware token needs to be present to\\nfinish the authentication.\\n\\nIt’s very simple to use and setup. The only extra step is generate a new keypair that can be used with the hardware\\ndevice. For that, there are two key types that can be used: ecdsa-sk and ed25519-sk . The former has broader hardware\\nsupport, while the latter might need a more recent device.\\n\\nOnce the keypair is generated, it can be used as you would normally use any other type of key in openssh. The only\\nrequirement is that in order to use the private key, the U2F device has to be present on the host.\\n\\nFor example, plug the U2F device in and generate a keypair to use with it:\\n\\n$ ssh-keygen -t ecdsa-sk\\n\\nGenerating public/private ecdsa-sk key pair.\\n\\nYou may need to touch your authenticator to authorize key generation. <-- touch device\\n\\nEnter file in which to save the key (/home/ubuntu/.ssh/id_ecdsa_sk):\\n\\nEnter passphrase (empty for no passphrase):\\n\\nEnter same passphrase again:\\n\\nYour identification has been saved in /home/ubuntu/.ssh/id_ecdsa_sk\\n\\nYour public key has been saved in /home/ubuntu/.ssh/id_ecdsa_sk.pub\\n\\nThe key fingerprint is:\\n\\nSHA256:V9PQ1MqaU8FODXdHqDiH9Mxb8XK3o5aVYDQLVl9IFRo ubuntu@focal\\n\\nNow just transfer the public part to the server to ~/.ssh/authorized_keys and you are ready to go:\\n\\n$ ssh -i .ssh/id_ecdsa_sk ubuntu@focal.server\\n\\nConfirm user presence for key ECDSA-SK SHA256:V9PQ1MqaU8FODXdHqDiH9Mxb8XK3o5aVYDQLVl9IFRo <-- touch device\\n\\nWelcome to Ubuntu Focal Fossa (GNU/Linux 5.4.0-21-generic x86_64)\\n\\n(...)\\n\\nubuntu@focal.server:~$\\n\\n**FIDO2 resident keys**',\n",
       " 'FIDO2 private keys consist of two parts: a “key handle” part stored in the private key file on disk, and a per-device\\nkey that is unique to each FIDO2 token and that cannot be exported from the token hardware. These are combined\\nby the hardware at authentication time to derive the real key that is used to sign authentication challenges.\\n\\nFor tokens that are required to move between computers, it can be cumbersome to have to move the private key file\\nfirst. To avoid this, tokens implementing the newer FIDO2 standard support *resident keys*, where it is possible to\\nretrieve the key handle part of the key from the hardware.\\n\\n470\\n\\n\\n-----\\n\\nUsing resident keys increases the likelihood of an attacker being able to use a stolen token device. For this reason,\\ntokens normally enforce PIN authentication before allowing download of keys, and users should set a PIN on their\\ntokens before creating any resident keys. This is done via the hardware token management software.\\n\\nOpenSSH allows resident keys to be generated using the ssh-keygen -O resident flag at key generation time:\\n\\n$ ssh-keygen -t ecdsa-sk -O resident -O application=ssh:mykeyname\\n\\nGenerating public/private ecdsa-sk key pair.\\n\\nYou may need to touch your authenticator to authorize key generation.\\n\\nEnter PIN for authenticator:\\n\\nEnter file in which to save the key (/home/ubuntu/.ssh/id_ecdsa_sk): mytoken\\n\\nEnter passphrase (empty for no passphrase):\\n\\nEnter same passphrase again:\\n\\nYour identification has been saved in mytoken\\n\\n(...)\\n\\nThis will produce a public/private key pair as usual, but it will be possible to retrieve the private key part (the key\\nhandle) from the token later. This is done by running:\\n\\n$ ssh-keygen -K',\n",
       " \"Enter PIN for authenticator:\\n\\nYou may need to touch your authenticator to authorize key download.\\n\\nEnter passphrase (empty for no passphrase):\\n\\nEnter same passphrase again:\\n\\nSaved ECDSA-SK key ssh:mytoken to id_ecdsa_sk_rk_mytoken\\n\\nIt will use the part after ssh: from the *application* parameter from before as part of the key filenames:\\n\\n$ l id_ecdsa_sk_rk_mytoken*\\n\\n-rw------- 1 ubuntu ubuntu 598 out 4 18:49 id_ecdsa_sk_rk_mytoken\\n\\n-rw-r--r-- 1 ubuntu ubuntu 228 out 4 18:49 id_ecdsa_sk_rk_mytoken.pub\\n\\nIf you set a passphrase when extracting the keys from the hardware token, and later use these keys, you will be\\nprompted for both the key passphrase, and the hardware key PIN, and you will also have to touch the token:\\n\\n$ ssh -i ./id_ecdsa_sk_rk_mytoken ubuntu@focal.server\\n\\nEnter passphrase for key './id_ecdsa_sk_rk_mytoken':\\n\\nConfirm user presence for key ECDSA-SK\\n\\nSHA256:t+l26IgTXeURY6e36wtrq7wVYJtDVZrO+iuobs1CvVQ\\n\\nUser presence confirmed\\n\\n(...)\\n\\nIt is also possible to download and add resident keys directly to ssh-agent by running\\n\\n$ ssh-add -K\\n\\nIn this case no file is written, and the public key can be printed by running ssh-add -L .\\n\\n**NOTE**\\n\\nIf you used the -O verify-required option when generating the keys, or if that option is set on the SSH\\nserver via /etc/ssh/sshd_config ’s PubkeyAuthOptions verify-required, then using the agent currently in\\nUbuntu 22.04 LTS won’t work.\\n## **Two factor authentication with TOTP/HOTP**\\n\\nFor the best two factor authentication (2FA) security, we recommend using hardware authentication devices that\\nsupport U2F/FIDO. See the previous section for details. However, if this is not possible or practical to implement in\",\n",
       " 'your case, TOTP/HOTP based 2FA is an improvement over no two factor at all. Smartphone apps to support this\\ntype of 2FA are common, such as Google Authenticator.\\n\\n**Background**\\n\\nThe configuration presented here makes public key authentication the first factor, the TOTP/HOTP code the second\\nfactor, and makes password authentication unavailable. Apart from the usual setup steps required for public key\\nauthentication, all configuration and setup takes place on the server. No changes are required at the client end; the\\n2FA prompt appears in place of the password prompt.\\n\\nThe two supported methods are HOTP and TOTP. Generally, TOTP is preferable if the 2FA device supports it.\\n\\n[HOTP is based on a sequence predictable only to those who share a secret. The user must take an action to cause the](https://en.wikipedia.org/wiki/HMAC-based_one-time_password)\\nclient to generate the next code in the sequence, and this response is sent to the server. The server also generates the\\n\\n471\\n\\n\\n-----\\n\\nnext code, and if it matches the one supplied by the user, then the user has proven to the server that they share the\\nsecret. A downside of this approach is that if the user generates codes without the server following along, such as in\\nthe case of a typo, then the sequence generators can fall “out of sync”. Servers compensate by allowing a gap in the\\nsequence and considering a few subsequent codes to also be valid; if this mechanism is used, then the server “skips\\nahead” to sync back up. But to remain secure, this can only go so far before the server must refuse. When HOTP falls\\nout of sync like this, it must be reset using some out of band method, such as authenticating using a second backup',\n",
       " 'key in order to reset the secret for the first one.\\n\\n[TOTP avoids this downside of HOTP by using the current timezone independent date and time to determine the](https://en.wikipedia.org/wiki/Time-based_one-time_password)\\nappropriate position in the sequence. However, this results in additional requirements and a different failure mode.\\nBoth devices must have the ability to tell the time, which is not practical for a USB 2FA token with no battery, for\\nexample. And both the server and client must agree on the correct time. If their clocks are skewed, then they will\\ndisagree on their current position in the sequence. Servers compensate for clock skew by allowing a few codes either\\nside to also be valid. But like HOTP, they can only go so far before the server must refuse. One advantage of TOTP\\nover HOTP is that correcting for this condition involves ensuring the clocks are correct at both ends; an out-of-band\\nauthentication to reset unfortunate users’ secrets is not required. When using a modern smartphone app, for example,\\nthe requirement to keep the clock correct isn’t usually a problem since this is typically done automatically at both\\nends by default.\\n\\nNote\\n\\nIt is not recommended to configure U2F/FIDO at the same time as TOTP/HOTP. This combination has\\nnot been tested, and using the configuration presented here, TOTP/HOTP would become mandatory for\\neveryone, whether or not they are also using U2F/FIDO.\\n\\n**Install software**\\n\\nFrom a terminal prompt, install the google-authenticator PAM module:\\n\\nsudo apt update\\n\\nsudo apt install libpam-google-authenticator\\n\\nNote',\n",
       " 'The libpam-google-authenticator package is in Ubuntu’s universe archive component, which receives besteffort community support only.\\n\\n**Configure users**\\n\\nSince public key authentication with TOTP/HOTP 2FA is about to be configured to be mandatory for users, each\\nuser who wishes to continue using ssh must first set up public key authentication and then configure their 2FA keys\\nby running the user setup tool. If this isn’t done first, users will not be able to do it later over ssh, since at that point\\nthey won’t have public key authentication and/or 2FA configured to authenticate with.\\n\\n**Configure users’ key-based authentication**\\n\\nTo set up key-based authentication, see “SSH Keys” above. Once this is done, it can be tested independently of\\nsubsequent 2FA configuration. At this stage, user authentication should work with keys only, requiring the supply of\\nthe private key passphrase only if it was configured. If configured correctly, the user should not be prompted for their\\npassword.\\n\\n**Configure users’ TOTP/HOTP 2FA secrets**\\n\\nEach user needs to run the setup tool to configure 2FA. This will ask some questions, generate a key, and display a QR\\ncode for the user to import the secret into their smartphone app, such as the Google Authenticator app on Android.\\nThe tool creates the file ~/.google-authenticator, which contains a shared secret, emergency passcodes and per-user\\nconfiguration.\\n\\nAs a user that needs 2FA configured, from a terminal prompt run the following command:\\n\\ngoogle-authenticator\\n\\nFollow the prompts, scanning the QR code into your 2FA app as directed.\\n\\nIt’s important to plan for the eventuality that the 2FA device gets lost or damaged.',\n",
       " 'Will this lock the user out of\\ntheir account? In mitigation, it’s worth each user considering doing one or more of the following:\\n\\n  - Use the 2FA device’s backup or cloud sync facility if it has one.\\n\\n  - Write down the backup codes printed by the setup tool.\\n\\n  - Take a photo of the QR code.\\n\\n472\\n\\n\\n-----\\n\\n  - (TOTP only) Scan the QR code on multiple 2FA devices. This only works for TOTP, since multiple HOTP 2FA\\ndevices will not be able to stay in sync.\\n\\n  - Ensure that the user has a different authentication path to be able to rerun the setup tool if required.\\n\\nOf course, any of these backup steps also negate any benefit of 2FA should someone else get access to the backup, so\\nthe steps taken to protect any backup should be considered carefully.\\n\\n**Configure the ssh server**\\n\\nOnce all users are configured, configure sshd itself by editing /etc/ssh/sshd_config . Depending on your installation,\\nsome of these settings may be configured already, but not necessarily with the values required for this configuration.\\nCheck for and adjust existing occurences of these configuration directives, or add new ones, as required:\\n\\nKbdInteractiveAuthentication yes\\n\\nPasswordAuthentication no\\n\\nAuthenticationMethods publickey,keyboard-interactive\\n\\nNote\\n\\nOn Ubuntu 20.04 “Focal Fossa” and earlier, use ChallengeResponseAuthentication yes instead of KbdIn\\nteractiveAUthentication yes .\\n\\nRestart the ssh service to pick up configuration changes:\\n\\nsudo systemctl try-reload-or-restart ssh\\n\\nEdit /etc/pam.d/sshd and replace the line:\\n\\n@include common-auth\\n\\nwith:\\n\\nauth required pam_google_authenticator.so\\n\\nChanges to PAM configuration have immediate effect, and no separate reloading command is required.',\n",
       " \"**Log in using 2FA**\\n\\nNow when you log in using ssh, in addition to the normal public key authentication, you will be prompted for your\\nTOTP or HOTP code:\\n\\n$ ssh jammy.server\\n\\nEnter passphrase for key 'id_rsa':\\n\\n(ubuntu@jammy.server) Verification code:\\n\\nWelcome to Ubuntu Jammy Jellyfish...\\n\\n(...)\\n\\nubuntu@jammy.server:~$\\n\\n**Special cases**\\n\\nOn Ubuntu, the following settings are default in /etc/ssh/sshd_config, but if you have overridden them, note that\\nthey are required for this configuration to work correctly and must be restored as follows:\\n\\nUsePAM yes\\n\\nPubkeyAuthentication yes\\n\\nRemember to run sudo systemctl try-reload-or-restart ssh for any changes make to sshd configuration to take\\neffect.\\n## **References**\\n\\n[• Ubuntu Wiki SSH page.](https://help.ubuntu.com/community/SSH)\\n\\n[• OpenSSH Website](http://www.openssh.org/)\\n\\n[• OpenSSH 8.2 release notes](https://www.openssh.com/txt/release-8.2)\\n\\n[• Advanced OpenSSH Wiki Page](https://wiki.ubuntu.com/AdvancedOpenSSH)\\n\\n [Yubikey documentation for OpenSSH FIDO/FIDO2 usage](https://developers.yubico.com/SSH/Securing_SSH_with_FIDO2.html)\\n\\n[• Wikipedia on TOTP](https://en.wikipedia.org/wiki/Time-based_one-time_password)\\n\\n[• Wikipedia on HOTP](https://en.wikipedia.org/wiki/HMAC-based_one-time_password)\\n\\n473\\n\\n\\n-----\\n\\nOpenVPN is a Virtual Private Networking (VPN) solution provided in the Ubuntu Repositories. It is flexible, reliable\\nand secure. It belongs to the family of SSL/TLS VPN stacks (different from IPSec VPNs). This chapter will cover\\ninstalling and configuring OpenVPN to create a VPN.\\n\\nIf you want more than just pre-shared keys OpenVPN makes it easy to set up a Public Key Infrastructure (PKI) to use\",\n",
       " 'SSL/TLS certificates for authentication and key exchange between the VPN server and clients. OpenVPN can be used\\nin a routed or bridged VPN mode and can be configured to use either UDP or TCP. The port number can be configured\\nas well, but port 1194 is the official one; this single port is used for all communication. VPN client implementations\\nare available for almost anything including all Linux distributions, macOS, Windows and OpenWRT-based WLAN\\n\\nrouters.\\n## **Server Installation**\\n\\nTo install openvpn in a terminal enter:\\n\\nsudo apt install openvpn easy-rsa\\n## **Public Key Infrastructure Setup**\\n\\nThe first step in building an OpenVPN configuration is to establish a PKI (public key infrastructure). The PKI\\nconsists of:\\n\\n  - a separate certificate (also known as a public key) and private key for the server and each client.\\n\\n  - a master Certificate Authority (CA) certificate and key, used to sign the server and client certificates.\\n\\nOpenVPN supports bidirectional authentication based on certificates, meaning that the client must authenticate the\\nserver certificate and the server must authenticate the client certificate before mutual trust is established.\\n\\nBoth server and client will authenticate the other by first verifying that the presented certificate was signed by the\\nmaster certificate authority (CA), and then by testing information in the now-authenticated certificate header, such\\nas the certificate common name or certificate type (client or server).\\n\\n**Certificate Authority Setup**\\n\\nTo setup your own Certificate Authority (CA) and generate certificates and keys for an OpenVPN server and multiple\\nclients first copy the easy-rsa directory to /etc/openvpn .',\n",
       " 'This will ensure that any changes to the scripts will not be\\nlost when the package is updated. From a terminal, run:\\n\\nsudo make-cadir /etc/openvpn/easy-rsa\\n\\nNote: If desired, you can alternatively edit /etc/openvpn/easy-rsa/vars directly, adjusting it to your needs.\\n\\nAs root user change to the newly created directory /etc/openvpn/easy-rsa and run:\\n\\n./easyrsa init-pki\\n\\n./easyrsa build-ca\\n\\nThe PEM passphrase set when creating the CA will be asked every time you need to encrypt the output of a command\\nsuch as a private key. The encryption here is important to avoid printing any private key in plain text.\\n\\n**Server Keys and Certificates**\\n\\nNext, we will generate a key pair for the server:\\n\\n./easyrsa gen-req myservername nopass\\n\\nDiffie Hellman parameters must be generated for the OpenVPN server. The following will place them in pki/dh.pem .\\n\\n./easyrsa gen-dh\\n\\nAnd finally a certificate for the server:\\n\\n./easyrsa sign-req server myservername\\n\\nAll certificates and keys have been generated in subdirectories. Common practice is to copy them to /etc/openvpn/:\\n\\ncp pki/dh.pem pki/ca.crt pki/issued/myservername.crt pki/private/myservername.key /etc/openvpn/\\n\\n474\\n\\n\\n-----\\n\\n**Client Certificates**\\n\\nThe VPN client will also need a certificate to authenticate itself to the server. Usually you create a different certificate\\nfor each client.\\n\\nThis can either be done on the server (as the keys and certificates above) and then securely distributed to the client.\\nOr vice versa: the client can generate and submit a request that is sent and signed by the server.\\n\\nTo create the certificate, enter the following in a terminal while being user root:\\n\\n./easyrsa gen-req myclient1 nopass',\n",
       " './easyrsa sign-req client myclient1\\n\\nIf the first command above was done on a remote system, then copy the .req file to the CA server. There you can then\\nimport it via easyrsa import-req /incoming/myclient1.req myclient1 . Then you can go on with the second sign-eq\\ncommand.\\n\\nIn both cases, afterwards copy the following files to the client using a secure method:\\n\\n - pki/ca.crt\\n\\n - pki/issued/myclient1.crt\\n\\nAs the client certificates and keys are only required on the client machine, you can remove them from the server.\\n## **Simple Server Configuration**\\n\\nAlong with your OpenVPN installation you got these sample config files (and many more if you check):\\n\\nroot@server:/# ls -l /usr/share/doc/openvpn/examples/sample-config-files/\\n\\ntotal 68\\n\\n-rw-r--r-- 1 root root 3427 2011-07-04 15:09 client.conf\\n\\n-rw-r--r-- 1 root root 4141 2011-07-04 15:09 server.conf.gz\\n\\nStart with copying and unpacking server.conf.gz to /etc/openvpn/server.conf.\\n\\nsudo cp /usr/share/doc/openvpn/examples/sample-config-files/server.conf.gz /etc/openvpn/myserver.conf.gz\\n\\nsudo gzip -d /etc/openvpn/myserver.conf.gz\\n\\nEdit /etc/openvpn/myserver.conf to make sure the following lines are pointing to the certificates and keys you created\\nin the section above.\\n\\nca ca.crt\\n\\ncert myservername.crt\\n\\nkey myservername.key\\n\\ndh dh2048.pem\\n\\nComplete this set with a ta key in etc/openvpn for tls-auth like:\\n\\nsudo openvpn --genkey --secret ta.key\\n\\nEdit /etc/sysctl.conf and uncomment the following line to enable IP forwarding.\\n\\n#net.ipv4.ip_forward=1\\n\\nThen reload sysctl.\\n\\nsudo sysctl -p /etc/sysctl.conf\\n\\nThat is the minimum you have to configure to get a working OpenVPN server. You can use all the default settings in',\n",
       " 'the sample server.conf file. Now start the server.\\n\\nBe aware that the *“systemctl start openvpn”* is **not** starting your openvpn you just defined.\\nOpenvpn uses templatized systemd jobs, openvpn@CONFIGFILENAME. So if for example your configuration file is\\n\\nmyserver.conf your service is called openvpn@myserver. You can run all kinds of service and systemctl commands\\nlike start/stop/enable/disable/preset against a templatized service like openvpn@server.\\n\\n$ sudo systemctl start openvpn@myserver\\n\\nYou will find logging and error messages in the journal. [For example, if you started a templatized service open-](https://www.freedesktop.org/software/systemd/man/systemd.unit.html)\\nvpn@server you can filter for this particular message source with:\\n\\nsudo journalctl -u openvpn@myserver -xe\\n\\nThe same templatized approach works for all of systemctl:\\n\\n475\\n\\n\\n-----\\n\\n$ sudo systemctl status openvpn@myserver\\n\\nopenvpn@myserver.service - OpenVPN connection to myserver\\n\\nLoaded: loaded (/lib/systemd/system/openvpn@.service; disabled; vendor preset: enabled)\\n\\nActive: active (running) since Thu 2019-10-24 10:59:25 UTC; 10s ago\\n\\nDocs: man:openvpn(8)\\n\\nhttps://community.openvpn.net/openvpn/wiki/Openvpn24ManPage\\n\\nhttps://community.openvpn.net/openvpn/wiki/HOWTO\\n\\nMain PID: 4138 (openvpn)\\n\\nStatus: \"Initialization Sequence Completed\"\\n\\nTasks: 1 (limit: 533)\\n\\nMemory: 1.0M\\n\\nCGroup: /system.slice/system-openvpn.slice/openvpn@myserver.service\\n\\n└─4138 /usr/sbin/openvpn --daemon ovpn-myserver --status /run/openvpn/myserver.status 10 -\\ncd /etc/openvpn --script-security 2 --config /etc/openvpn/myserver.conf --writepid /run/',\n",
       " 'Oct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: /sbin/ip addr add dev tun0 local 10.8.0.1 peer 10.8.0.2\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: /sbin/ip route add 10.8.0.0/24 via 10.8.0.2\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: Could not determine IPv4/IPv6 protocol. Using AF_INET\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: Socket Buffers: R=[212992->212992] S=[212992->212992]\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: UDPv4 link local (bound): [AF_INET][undef]:1194\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: UDPv4 link remote: [AF_UNSPEC]\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: MULTI: multi_init called, r=256 v=256\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: IFCONFIG POOL: base=10.8.0.4 size=62, ipv6=0\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: IFCONFIG POOL LIST\\n\\nOct 24 10:59:26 eoan-vpn-server ovpn-myserver[4138]: Initialization Sequence Completed\\n\\nYou can enable/disable various openvpn services on one system, but you could also let Ubuntu do it for you. There\\nis config for AUTOSTART in /etc/default/openvpn . Allowed values are “all”, “none” or space separated list of names of\\nthe VPNs. If empty, “all” is assumed. The VPN name refers to the VPN configutation file name. i.e. home would be\\n\\n/etc/openvpn/home.conf If you’re running systemd, changing this variable will require running systemctl daemon-reload\\nfollowed by a *restart* of the openvpn service (if you removed entries you may have to stop those manually).\\n\\nAfter “systemctl daemon-reload” a restart of the “generic” openvpn will restart all dependent services that the generator',\n",
       " 'in /lib/systemd/system-generators/openvpn-generator created for your conf files when you called daemon-reload.\\n\\nNow check if OpenVPN created a tun0 interface:\\n\\nroot@server:/etc/openvpn# ip addr show dev tun0\\n\\n5: tun0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 100\\n\\nlink/none\\n\\ninet 10.8.0.1 peer 10.8.0.2/32 scope global tun0\\n\\nvalid_lft forever preferred_lft forever\\n\\ninet6 fe80::b5ac:7829:f31e:32c5/64 scope link stable-privacy\\n\\nvalid_lft forever preferred_lft forever\\n## **Simple Client Configuration**\\n\\nThere are various different OpenVPN client implementations with and without GUIs. You can read more about clients\\nin a later section on VPN Clients. For now we use commandline/service based OpenVPN client for Ubuntu which is\\npart of the very same package as the server. So you have to install the openvpn package again on the client machine:\\n\\nsudo apt install openvpn\\n\\nThis time copy the client.conf sample config file to /etc/openvpn/:\\n\\nsudo cp /usr/share/doc/openvpn/examples/sample-config-files/client.conf /etc/openvpn/\\n\\nCopy the following client keys and certificate files you created in the section above to e.g. /etc/openvpn/ and\\nedit /etc/openvpn/client.conf to make sure the following lines are pointing to those files. If you have the files in\\n/etc/openvpn/ you can omit the path.\\n\\nca ca.crt\\n\\ncert myclient1.crt\\n\\nkey myclient1.key\\n\\ntls-auth ta.key 1\\n\\nAnd you have to specify the OpenVPN server name or address. Make sure the keyword client is in the config. That’s\\nwhat enables client mode.\\n\\n476\\n\\n\\n-----\\n\\nclient\\n\\nremote vpnserver.example.com 1194\\n\\nNow start the OpenVPN client with the same templatized mechanism:',\n",
       " '$ sudo systemctl start openvpn@client\\n\\nYou can check status as you did on the server:\\n\\n$ sudo systemctl status openvpn@client\\n\\nopenvpn@client.service - OpenVPN connection to client\\n\\nLoaded: loaded (/lib/systemd/system/openvpn@.service; disabled; vendor preset: enabled)\\n\\nActive: active (running) since Thu 2019-10-24 11:42:35 UTC; 6s ago\\n\\nDocs: man:openvpn(8)\\n\\nhttps://community.openvpn.net/openvpn/wiki/Openvpn24ManPage\\n\\nhttps://community.openvpn.net/openvpn/wiki/HOWTO\\n\\nMain PID: 3616 (openvpn)\\n\\nStatus: \"Initialization Sequence Completed\"\\n\\nTasks: 1 (limit: 533)\\n\\nMemory: 1.3M\\n\\nCGroup: /system.slice/system-openvpn.slice/openvpn@client.service\\n\\n└─3616 /usr/sbin/openvpn --daemon ovpn-client --status /run/openvpn/client.status 10 -\\ncd /etc/openvpn --script-security 2 --config /etc/openvpn/client.conf --writepid /run/openvp\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: Outgoing Data Channel: Cipher \\'AES-256-GCM\\' initialized with 256 bit ke\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: Incoming Data Channel: Cipher \\'AES-256-GCM\\' initialized with 256 bit ke\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: ROUTE_GATEWAY 192.168.122.1/255.255.255.0 IFACE=ens3 HWADDR=52:54:00:\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: TUN/TAP device tun0 opened\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: TUN/TAP TX queue length set to 100\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: /sbin/ip link set dev tun0 up mtu 1500\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: /sbin/ip addr add dev tun0 local 10.8.0.6 peer 10.8.0.5\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: /sbin/ip route add 10.8.0.1/32 via 10.8.0.5',\n",
       " 'Oct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: WARNING: this configuration may cache passwords in memory \\n- use the auth-nocache option to prevent this\\n\\nOct 24 11:42:36 eoan-vpn-client ovpn-client[3616]: Initialization Sequence Completed\\n\\nOn the server log an incoming connection looks like the following.\\nYou can see client name and source address as well as success/failure messages.\\n\\novpn-myserver[4818]: 192.168.122.114:55738 TLS: Initial packet from [AF_INET]192.168.122.114:55738, sid=5e943ab8 40ab9fe\\n\\novpn-myserver[4818]: 192.168.122.114:55738 VERIFY OK: depth=1, CN=Easy-RSA CA\\n\\novpn-myserver[4818]: 192.168.122.114:55738 VERIFY OK: depth=0, CN=myclient1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_VER=2.4.7\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_PLAT=linux\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_PROTO=2\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_NCP=2\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_LZ4=1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_LZ4v2=1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_LZO=1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_COMP_STUB=1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_COMP_STUBv2=1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 peer info: IV_TCPNL=1\\n\\novpn-myserver[4818]: 192.168.122.114:55738 Control Channel: TLSv1.3, cipher TLSv1.3 TLS_AES_256_GCM_SHA384, 2048 bit RSA\\n\\novpn-myserver[4818]: 192.168.122.114:55738 [myclient1] Peer Connection Initiated with [AF_INET]192.168.122.114:55738\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 MULTI_sva: pool returned IPv4=10.8.0.6, IPv6=(Not enabled)',\n",
       " \"ovpn-myserver[4818]: myclient1/192.168.122.114:55738 MULTI: Learn: 10.8.0.6 -> myclient1/192.168.122.114:55738\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 MULTI: primary virtual IP for myclient1/192.168.122.114:55738: 10.8\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 PUSH: Received control message: 'PUSH_REQUEST'\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 SENT CONTROL [myclient1]: 'PUSH_REPLY,route 10.8.0.1,topology net30\\n\\nrestart 120,ifconfig 10.8.0.6 10.8.0.5,peer-id 0,cipher AES-256-GCM' (status=1)\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 Data Channel: using negotiated cipher 'AES-256-GCM'\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 Outgoing Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit\\n\\novpn-myserver[4818]: myclient1/192.168.122.114:55738 Incoming Data Channel: Cipher 'AES-256-GCM' initialized with 256 bit\\n\\nAnd you can check on the client if it created a tun0 interface:\\n\\n$ ip addr show dev tun0\\n\\n477\\n\\n\\n-----\\n\\n4: tun0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 100\\n\\nlink/none\\n\\ninet 10.8.0.6 peer 10.8.0.5/32 scope global tun0\\n\\nvalid_lft forever preferred_lft forever\\n\\ninet6 fe80::5a94:ae12:8901:5a75/64 scope link stable-privacy\\n\\nvalid_lft forever preferred_lft forever\\n\\nCheck if you can ping the OpenVPN server:\\n\\nroot@client:/etc/openvpn# ping 10.8.0.1\\n\\nPING 10.8.0.1 (10.8.0.1) 56(84) bytes of data.\\n\\n64 bytes from 10.8.0.1: icmp_req=1 ttl=64 time=0.920 ms\\n\\n**Note**\\n\\nThe OpenVPN server always uses the first usable IP address in the client network and only that IP is\\npingable.\",\n",
       " 'E.g. if you configured a /24 for the client network mask, the .1 address will be used. The P-t-P\\naddress you see in the ip addr output above is usually not answering ping requests.\\n\\nCheck out your routes:\\n\\n$ ip route\\n\\ndefault via 192.168.122.1 dev ens3 proto dhcp src 192.168.122.114 metric 100\\n\\n10.8.0.1 via 10.8.0.5 dev tun0\\n\\n10.8.0.5 dev tun0 proto kernel scope link src 10.8.0.6\\n\\n192.168.122.0/24 dev ens3 proto kernel scope link src 192.168.122.114\\n\\n192.168.122.1 dev ens3 proto dhcp scope link src 192.168.122.114 metric 100\\n## **First trouble shooting**\\n\\nIf the above didn’t work for you, check this:\\n\\n - Check your journal -xe\\n\\n  - Check that you have specified the keyfile names correctly in client and server conf files\\n\\n  - Can the client connect to the server machine? Maybe a firewall is blocking access? Check journal on server.\\n\\n  - Client and server must use same protocol and port, e.g. UDP port 1194, see port and proto config option\\n\\n  - Client and server must use same config regarding compression, see comp-lzo config option\\n\\n  - Client and server must use same config regarding bridged vs routed mode, see server vs server-bridge config\\noption\\n## **Advanced configuration**\\n\\n**Advanced routed VPN configuration on server**\\n\\nThe above is a very simple working VPN. The client can access services on the VPN server machine through an\\nencrypted tunnel. If you want to reach more servers or anything in other networks, push some routes to the clients.\\nE.g. if your company’s network can be summarized to the network 192.168.0.0/16, you could push this route to the\\nclients. But you will also have to change the routing for the way back - your servers need to know a route to the VPN',\n",
       " 'client-network.\\n\\nThe example config files that we have been using in this guide are full of all these advanced options in the form of a\\ncomment and a disabled configuration line as an example.\\n\\n**Note**\\n\\n[Please read the OpenVPN hardening security guide for further security advice.](http://openvpn.net/index.php/open-source/documentation/howto.html#security)\\n\\n**Advanced bridged VPN configuration on server**\\n\\nOpenVPN can be setup for either a routed or a bridged VPN mode. Sometimes this is also referred to as OSI layer-2\\nversus layer-3 VPN. In a bridged VPN all layer-2 frames - e.g. all ethernet frames - are sent to the VPN partners and\\nin a routed VPN only layer-3 packets are sent to VPN partners. In bridged mode all traffic including traffic which was\\ntraditionally LAN-local like local network broadcasts, DHCP requests, ARP requests etc. are sent to VPN partners\\nwhereas in routed mode this would be filtered.\\n\\n**Prepare interface config for bridging on server**\\n\\nFirst, use netplan to configure a bridge device using the desired ethernet device.\\n\\n478\\n\\n\\n-----\\n\\n$ cat /etc/netplan/01-netcfg.yaml\\n\\nnetwork:\\n\\nversion: 2\\n\\nrenderer: networkd\\n\\nethernets:\\n\\nenp0s31f6:\\n\\ndhcp4: no\\n\\nbridges:\\n\\nbr0:\\n\\ninterfaces: [enp0s31f6]\\n\\ndhcp4: no\\n\\naddresses: [10.0.1.100/24]\\n\\ngateway4: 10.0.1.1\\n\\nnameservers:\\n\\naddresses: [10.0.1.1]\\n\\nStatic IP addressing is highly suggested. DHCP addressing can also work, but you will still have to encode a static\\naddress in the OpenVPN configuration file.\\n\\nThe next step on the server is to configure the ethernet device for promiscuous mode on boot. To do this, ensure the\\nnetworkd-dispatcher package is installed and create the following configuration script.',\n",
       " 'sudo apt update\\n\\nsudo apt install networkd-dispatcher\\n\\nsudo touch /usr/lib/networkd-dispatcher/dormant.d/promisc_bridge\\n\\nsudo chmod +x /usr/lib/networkd-dispatcher/dormant.d/promisc_bridge\\n\\nThen add the following contents.\\n\\n#!/bin/sh\\n\\nset -e\\n\\nif [ \"$IFACE\" = br0 ]; then\\n\\n# no networkd-dispatcher event for \\'carrier\\' on the physical interface\\n\\nip link set enp0s31f6 up promisc on\\n\\nfi\\n\\n**Prepare server config for bridging**\\n\\nEdit /etc/openvpn/server.conf to use tap rather than tun and set the server to use the server-bridge directive:\\n\\n;dev tun\\n\\ndev tap\\n\\n;server 10.8.0.0 255.255.255.0\\n\\nserver-bridge 10.0.0.4 255.255.255.0 10.0.0.128 10.0.0.254\\n\\nAfter configuring the server, restart openvpn by entering:\\n\\nsudo systemctl restart openvpn@myserver\\n\\n**Prepare client config for bridging**\\n\\nThe only difference on the client side for bridged mode to what was outlined above is that you need to edit\\n\\n/etc/openvpn/client.conf and set tap mode:\\n\\ndev tap\\n\\n;dev tun\\n\\nFinally, restart openvpn:\\n\\nsudo systemctl restart openvpn@client\\n\\nYou should now be able to connect to the full remote LAN through the VPN.\\n## **References**\\n\\n[• EasyRSA](https://github.com/OpenVPN/easy-rsa/blob/master/README.quickstart.md)\\n\\n[• OpenVPN quick start guide](https://openvpn.net/quick-start-guide/)\\n\\n[• Snap’ed version of openvpn easy-openvpn](https://snapcraft.io/)\\n\\n[• Debian’s OpenVPN Guide](https://wiki.debian.org/OpenVPN)\\n\\n479\\n\\n\\n-----\\n\\n## **Installing a gitolite server**\\n\\nGitolite provides a traditional source control management server for git, with multiple users and access rights management. gitolite can be installed with the following command:\\n\\nsudo apt install gitolite3\\n## **Gitolite configuration**',\n",
       " 'Configuration of the gitolite server is a little different that most other servers on Unix-like systems, in that gitolite\\nstores its configuration in a git repository rather than in files in /etc/ . The first step to configuring a new installation\\nis therefore to allow access to the configuration repository.\\n\\nFirst of all, let’s create a user for gitolite to use for the service:\\n\\nsudo adduser --system --shell /bin/bash --group --disabled-password --home /home/git git\\n\\nNow we want to let gitolite know about the repository administrator’s public SSH key. This assumes that the current\\nuser is the repository administrator. If you have not yet configured an SSH key, refer to openssh-keys in this manual.\\n\\ncp ~/.ssh/id_rsa.pub /tmp/$(whoami).pub\\n\\nLet’s switch to the git user and import the administrator’s key into gitolite.\\n\\nsudo su - git\\n\\ngl-setup /tmp/*.pub\\n\\nGitolite will allow you to make initial changes to its configuration file during the setup process. You can now clone\\nand modify the gitolite configuration repository from your administrator user (the user whose public SSH key you\\nimported). Switch back to that user, then clone the configuration repository:\\n\\nexit\\n\\ngit clone git@$IP_ADDRESS:gitolite-admin.git\\n\\ncd gitolite-admin\\n\\nThe gitolite-admin contains two subdirectories, “conf” and “keydir”. The configuration files are in the conf dir, and\\nthe keydir directory contains the list of user’s public SSH keys.\\n## **Managing gitolite users and repositories**\\n\\nAdding a new user to gitolite is simple: just obtain their public SSH key and add it to the keydir directory as\\n\\n$DESIRED_USER_NAME.pub .',\n",
       " 'Note that the gitolite usernames don’t have to match the system usernames - they are only\\nused in the gitolite configuration file to manage access control. Similarly, users are deleted by deleting their public key\\nfiles. After each change, do not forget to commit the changes to git, and push the changes back to the server with\\n\\ngit commit -a\\n\\ngit push origin master\\n\\nRepositories are managed by editing the conf/gitolite.conf file. The syntax is space separated, and simply specifies\\nthe list of repositories followed by some access rules. The following is a default example\\n\\nrepo gitolite-admin\\n\\nRW+ = admin\\n\\nR = alice\\n\\nrepo project1\\n\\nRW+ = alice\\n\\nRW = bob\\n\\nR = denise\\n## **Using your server**\\n\\nOnce a user’s public key has been imported by the gitolite admin and authorization granted to the user to one or more\\nrepositories, the user can access repositories with the following command:\\n\\ngit clone git@$SERVER_IP:$PROJECT_NAME.git\\n\\nTo add the server as a new remote for an existing git repository:\\n\\ngit remote add gitolite git@$SERVER_IP:$PROJECT_NAME.git\\n\\n480\\n\\n\\n-----\\n\\n## **References**\\n\\n[• Gitolite’s code repository provides access to source code.](https://github.com/sitaramc/gitolite)\\n\\n[• Gitolite’s documentation includes a “fool proof setup” guide and a cookbook with recipes for common tasks.](https://gitolite.com/gitolite/)\\n\\n  - Gitolite’s maintainer has written a book, *[Gitolite Essentials](https://www.packtpub.com/hardware-and-creative/gitolite-essentials)*, for more in-depth information about the software.\\n\\n[• General information about git itself can be found at the Git homepage.](http://git-scm.com)\\n## **Linux Network-Manager GUI for OpenVPN**',\n",
       " 'Many Linux distributions including Ubuntu desktop variants come with Network Manager, a nice GUI to configure\\nyour network settings. It also can manage your VPN connections. It is the default, but if in doubt make sure you\\nhave package network-manager-openvpn installed.\\n\\nOpen the Network Manager GUI, select the VPN tab and then the ‘Add’ button. Select OpenVPN as the VPN type\\nin the opening requester and press ‘Create’. In the next window add the OpenVPN’s server name as the ‘Gateway’, set\\n‘Type’ to ‘Certificates (TLS)’, point ‘User Certificate’ to your user certificate, ‘CA Certificate’ to your CA certificate\\nand ‘Private Key’ to your private key file. Use the advanced button to enable compression (e.g. comp-lzo), dev tap,\\nor other special settings you set on the server. Now try to establish your VPN.\\n## **OpenVPN with GUI for Mac OS X**\\n\\n[• Tunnelblick is an excellent free, open source implementation of a GUI for OpenVPN for OS X. Download the](https://tunnelblick.net)\\n[latest OS X installer from there and install it. It also is recommended by upstream which would have a alternative](https://openvpn.net/vpn-server-resources/connecting-to-access-server-with-macos/#alternative-openvpn-open-source-tunnelblick-program)\\non their own\\n\\nThen put your client.ovpn config file together with the certificates and keys in /Users/username/Library/Application\\nSupport/Tunnelblick/Configurations/ and lauch Tunnelblick from your Application folder.\\n\\nInstead of downloading manually, if you have brew set up on MacOS this is as easy as:\\n\\nbrew cask install tunnelblick\\n## **OpenVPN with GUI for Win**\\n\\n[First download and install the latest OpenVPN Windows Installer.',\n",
       " 'As of this writing, the management GUI is included](https://openvpn.net/community-downloads/)\\nwith the Windows binary installer.\\n\\nYou need to start the OpenVPN service. Goto Start > Computer > Manage > Services and Applications > Services.\\nFind the OpenVPN service and start it. Set it’s startup type to automatic.\\n\\nWhen you start the OpenVPN MI GUI the first time you need to run it as an administrator. You have to right click\\non it and you will see that option.\\n\\n[There is an updated guide by the upstream project for the client on Windows.](https://community.openvpn.net/openvpn/wiki/Easy_Windows_Guide)\\n## **References**\\n\\n[• See the OpenVPN website for additional information.](http://openvpn.net/)\\n\\n[• OpenVPN hardening security guide](http://openvpn.net/index.php/open-source/documentation/howto.html#security)\\n\\n[• Also, Pakt’s OpenVPN: Building and Integrating Virtual Private Networks is a good resource.](http://www.packtpub.com/openvpn/book)\\n\\nOne of the most useful applications for any system administrator is an xterm multiplexer such as screen or tmux. It\\nallows for the execution of multiple shells in one terminal. To make some of the advanced multiplexer features more\\nuser-friendly and provide some useful information about the system, the byobu package was created. It acts as a\\nwrapper to these programs. By default Byobu is installed in Ubuntu server and it uses tmux (if installed) but this\\ncan be changed by the user.\\n\\nInvoke it simply with:\\n\\nbyobu\\n\\nNow bring up the configuration menu. By default this is done by pressing the *F9* key. This will allow you to:\\n\\n  - Help – Quick Start Guide\\n\\n  - Toggle status notifications\\n\\n  - Change the escape sequence',\n",
       " '  - Byobu currently does not launch at login (toggle on)\\n\\n481\\n\\n\\n-----\\n\\nbyobu provides a menu which displays the Ubuntu release, processor information, memory information, and the time\\nand date. The effect is similar to a desktop menu.\\n\\nUsing the *“Byobu currently does not launch at login (toggle on)”* option will cause byobu to be executed any time a\\nterminal is opened. Changes made to byobu are on a per user basis, and will not affect other users on the system.\\n\\nOne difference when using byobu is the *scrollback* mode. Press the *F7* key to enter scrollback mode. Scrollback mode\\nallows you to navigate past output using *vi* like commands. Here is a quick list of movement commands:\\n\\n - *h*  - Move the cursor left by one character\\n\\n - *j*  - Move the cursor down by one line\\n\\n - *k*  - Move the cursor up by one line\\n\\n - *l*  - Move the cursor right by one character\\n\\n - *0*  - Move to the beginning of the current line\\n\\n - *$*  - Move to the end of the current line\\n\\n - *G*  - Moves to the specified line (defaults to the end of the buffer)\\n\\n - */*  - Search forward\\n\\n - *?*  - Search backward\\n\\n - *n*  - Moves to the next match, either forward or backward\\n## **Resources**\\n\\n[• For more information on screen see the screen web site.](http://www.gnu.org/software/screen/)\\n\\n[• And the Ubuntu Wiki screen page.](https://help.ubuntu.com/community/Screen)\\n\\n[• Also, see the byobu project page for more information.](https://launchpad.net/byobu)\\n\\netckeeper allows the contents of /etc to be stored in a Version Control System (VCS) repository. It integrates with\\nAPT and automatically commits changes to /etc when packages are installed or upgraded. Placing /etc under version',\n",
       " 'control is considered an industry best practice, and the goal of etckeeper is to make this process as painless as possible.\\n\\nInstall etckeeper by entering the following in a terminal:\\n\\nsudo apt install etckeeper\\n\\nThe main configuration file, /etc/etckeeper/etckeeper.conf, is fairly simple. The main option is which VCS to use\\nand by default etckeeper is configured to use git. The repository is automatically initialized (and committed for the\\nfirst time) during package installation. It is possible to undo this by entering the following command:\\n\\nsudo etckeeper uninit\\n\\nBy default, etckeeper will commit uncommitted changes made to /etc daily. This can be disabled using the\\nAVOID_DAILY_AUTOCOMMITS configuration option. It will also automatically commit changes before and after\\npackage installation. For a more precise tracking of changes, it is recommended to commit your changes manually,\\ntogether with a commit message, using:\\n\\nsudo etckeeper commit \"Reason for configuration change\"\\n\\nThe vcs etckeeper command allows to run any subcommand of the VCS that etckeeper is configured to run. t will be\\nrun in /etc . For example, in the case of git:\\n\\nsudo etckeeper vcs log /etc/passwd\\n\\nTo demonstrate the integration with the package management system (APT), install postfix:\\n\\nsudo apt install postfix\\n\\nWhen the installation is finished, all the postfix configuration files should be committed to the repository:\\n\\n[master 5a16a0d] committing changes in /etc made by \"apt install postfix\"\\n\\nAuthor: Your Name <xyz@example.com>\\n\\n36 files changed, 2987 insertions(+), 4 deletions(-)\\n\\ncreate mode 100755 init.d/postfix\\n\\ncreate mode 100644 insserv.conf.d/postfix',\n",
       " 'create mode 100755 network/if-down.d/postfix\\n\\ncreate mode 100755 network/if-up.d/postfix\\n\\ncreate mode 100644 postfix/dynamicmaps.cf\\n\\n482\\n\\n\\n-----\\n\\ncreate mode 100644 postfix/main.cf\\n\\ncreate mode 100644 postfix/main.cf.proto\\n\\ncreate mode 120000 postfix/makedefs.out\\n\\ncreate mode 100644 postfix/master.cf\\n\\ncreate mode 100644 postfix/master.cf.proto\\n\\ncreate mode 100755 postfix/post-install\\n\\ncreate mode 100644 postfix/postfix-files\\n\\ncreate mode 100755 postfix/postfix-script\\n\\ncreate mode 100755 ppp/ip-down.d/postfix\\n\\ncreate mode 100755 ppp/ip-up.d/postfix\\n\\ncreate mode 120000 rc0.d/K01postfix\\n\\ncreate mode 120000 rc1.d/K01postfix\\n\\ncreate mode 120000 rc2.d/S01postfix\\n\\ncreate mode 120000 rc3.d/S01postfix\\n\\ncreate mode 120000 rc4.d/S01postfix\\n\\ncreate mode 120000 rc5.d/S01postfix\\n\\ncreate mode 120000 rc6.d/K01postfix\\n\\ncreate mode 100755 resolvconf/update-libc.d/postfix\\n\\ncreate mode 100644 rsyslog.d/postfix.conf\\n\\ncreate mode 120000 systemd/system/multi-user.target.wants/postfix.service\\n\\ncreate mode 100644 ufw/applications.d/postfix\\n\\nFor an example of how etckeeper tracks manual changes, add new a host to /etc/hosts . Using git you can see which\\nfiles have been modified:\\n\\nsudo etckeeper vcs status\\n\\nand how:\\n\\nsudo etckeeper vcs diff\\n\\nIf you are happy with the changes you can now commit them:\\n\\nsudo etckeeper commit \"added new host\"\\n## **Resources**\\n\\n[• See the etckeeper site for more details on using etckeeper.](https://etckeeper.branchable.com/)\\n\\n[• For documentation on the git VCS tool see the Git website.](https://git-scm.com/)\\n\\nWhen logging into an Ubuntu server you may have noticed the informative Message Of The Day (MOTD). This',\n",
       " 'information is obtained and displayed using a couple of packages:\\n\\n - *landscape-common:* provides the core libraries of landscape-client, which is needed to manage systems with\\n\\n[Landscape (proprietary). Yet the package also includes the landscape-sysinfo utility which is responsible for](http://landscape.canonical.com/)\\ndisplaying core system data involving cpu, memory, disk space, etc. For instance:\\n\\nSystem load: 0.0 Processes: 76\\n\\nUsage of /: 30.2% of 3.11GB Users logged in: 1\\n\\nMemory usage: 20% IP address for eth0: 10.153.107.115\\n\\nSwap usage: 0%\\n\\nGraph this data and manage this system at https://landscape.canonical.com/\\n\\n**Note**\\n\\nYou can run landscape-sysinfo manually at any time.\\n\\n - *update-notifier-common:* provides information on available package updates, impending filesystem checks (fsck),\\nand required reboots (e.g.: after a kernel upgrade).\\n\\npam_motd executes the scripts in /etc/update-motd.d in order based on the number prepended to the script. The\\noutput of the scripts is written to /var/run/motd, keeping the numerical order, then concatenated with /etc/motd.tail .\\n\\nYou can add your own dynamic information to the MOTD. For example, to add local weather information:\\n\\n  - First, install the weather-util package:\\n\\nsudo apt install weather-util\\n\\n483\\n\\n\\n-----\\n\\n  - The weather utility uses METAR data from the National Oceanic and Atmospheric Administration and forecasts\\nfrom the National Weather Service. In order to find local information you will need the 4-character ICAO location\\n[indicator. This can be determined by browsing to the National Weather Service site.](https://www.weather.gov/tg/siteloc)',\n",
       " 'Although the National Weather Service is a United States government agency there are weather stations available\\nworld wide. However, local weather information for all locations outside the U.S. may not be available.\\n\\n - Create /usr/local/bin/local-weather, a simple shell script to use weather with your local ICAO indicator:\\n\\n#!/bin/sh\\n\\n#\\n\\n#\\n\\n# Prints the local weather information for the MOTD.\\n\\n#\\n\\n#\\n\\n# Replace KINT with your local weather station.\\n\\n# Local stations can be found here: http://www.weather.gov/tg/siteloc.shtml\\n\\necho\\n\\nweather KINT\\n\\necho\\n\\n  - Make the script executable:\\n\\nsudo chmod 755 /usr/local/bin/local-weather\\n\\n  - Next, create a symlink to /etc/update-motd.d/98-local-weather :\\n\\nsudo ln -s /usr/local/bin/local-weather /etc/update-motd.d/98-local-weather\\n\\n  - Finally, exit the server and re-login to view the new MOTD.\\n\\nYou should now be greeted with some useful information, and some information about the local weather that may not\\nbe quite so useful. Hopefully the local-weather example demonstrates the flexibility of pam_motd.\\n## **Resources**\\n\\n[• See the update-motd man page for more options available to update-motd.](http://manpages.ubuntu.com/manpages/jammy/en/man5/update-motd.5.html)\\n\\n[• The Debian Package of the Day weather article has more details about using the weatherutility.](http://debaday.debian.net/2007/10/04/weather-check-weather-conditions-and-forecasts-on-the-command-line/)\\n\\nPuppet is a cross platform framework enabling system administrators to perform common tasks using code. The code\\ncan do a variety of tasks from installing new software, to checking file permissions, or updating user accounts. Puppet',\n",
       " \"is great not only during the initial installation of a system, but also throughout the system’s entire life cycle. In most\\ncircumstances puppet will be used in a client/server configuration.\\n\\nThis section will cover installing and configuring Puppet in a client/server configuration. This simple example will\\ndemonstrate how to install Apache using Puppet.\\n## **Preconfiguration**\\n\\nPrior to configuring puppet you may want to add a DNS *CNAME* record for puppet.example.com, where example.com\\nis your domain. By default Puppet clients check DNS for puppet.example.com as the puppet server name, or *Puppet*\\n*Master* . See Domain Name Server for more details.\\n\\nIf you do not wish to use DNS, you can add entries to the server and client /etc/hosts file. For example, in the Puppet\\nserver’s /etc/hosts file add:\\n\\n127.0.0.1 localhost.localdomain localhost puppet\\n\\n192.168.1.17 puppetclient.example.com puppetclient\\n\\nOn each Puppet client, add an entry for the server:\\n\\n192.168.1.16 puppetmaster.example.com puppetmaster puppet\\n\\n**Note**\\n\\nReplace the example IP addresses and domain names above with your actual server and client addresses\\nand domain names.\\n\\n484\\n\\n\\n-----\\n\\n## **Installation**\\n\\nTo install Puppet, in a terminal on the *server* enter:\\n\\nsudo apt install puppetmaster\\n\\nOn the *client* machine, or machines, enter:\\n\\nsudo apt install puppet\\n## **Configuration**\\n\\nCreate a folder path for the apache2 class:\\n\\nsudo mkdir -p /etc/puppet/modules/apache2/manifests\\n\\nNow setup some resources for apache2. Create a file /etc/puppet/modules/apache2/manifests/init.pp containing the\\nfollowing:\\n\\nclass apache2 {\\n\\npackage { 'apache2':\\n\\nensure => installed,\\n\\n}\\n\\nservice { 'apache2':\\n\\nensure => true,\",\n",
       " \"enable => true,\\n\\nrequire => Package['apache2'],\\n\\n}\\n\\n}\\n\\nNext, create a node file /etc/puppet/code/environments/production/manifests/site.pp with:\\n\\nnode 'puppetclient.example.com' {\\n\\ninclude apache2\\n\\n}\\n\\n**Note**\\n\\nReplace puppetclient.example.com with your actual Puppet client’s host name.\\n\\nThe final step for this simple Puppet server is to restart the daemon:\\n\\nsudo systemctl restart puppetmaster.service\\n\\nNow everything is configured on the Puppet server, it is time to configure the client.\\n\\nFirst, configure the Puppet agent daemon to start. Edit /etc/default/puppet, changing *START* to yes:\\n\\nSTART=yes\\n\\nThen start the service:\\n\\nsudo systemctl start puppet.service\\n\\nView the client cert fingerprint\\n\\nsudo puppet agent --fingerprint\\n\\nBack on the Puppet server, view pending certificate signing requests:\\n\\nsudo puppet cert list\\n\\nOn the Puppet server, verify the fingerprint of the client and sign puppetclient’s cert:\\n\\nsudo puppet cert sign puppetclient.example.com\\n\\nOn the Puppet client, run the puppet agent manually in the foreground. This step isn’t strictly speaking necessary,\\nbut it is the best way to test and debug the puppet service.\\n\\nsudo puppet agent --test\\n\\nCheck /var/log/syslog on both hosts for any errors with the configuration. If all goes well the apache2 package and\\nit’s dependencies will be installed on the Puppet client.\\n\\n485\\n\\n\\n-----\\n\\n**Note**\\n\\nThis example is *very* simple, and does not highlight many of Puppet’s features and benefits. For more\\ninformation see Resources.\\n## **Resources**\\n\\n[• See the Official Puppet Documentation web site.](http://docs.puppetlabs.com/)\\n\\n[• See the Puppet forge, online repository of puppet modules.](http://forge.puppetlabs.com/)\",\n",
       " '[• Also see Pro Puppet.](http://www.apress.com/9781430230571)\\n\\n[Generated from the online docs at https://ubuntu.com/server/docs on Mon Jan 22 09:06:37 UTC 2024](https://ubuntu.com/server/docs)\\n\\n486\\n\\n\\n-----']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:33:51.892030Z",
     "start_time": "2025-04-24T07:33:46.125294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ManualHFEmbedder:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
    "        inputs = self.tokenizer(input, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return embeddings.cpu().tolist()\n",
    "\n",
    "\n",
    "# 2. Define the four embedding functions\n",
    "models = {\n",
    "    \"chromadb_default\": embedding_functions.DefaultEmbeddingFunction(),  # ONNX MiniLM\n",
    "    \"e5_large\": ManualHFEmbedder(\"intfloat/multilingual-e5-large\"),\n",
    "    \"e5_base\": ManualHFEmbedder(\"intfloat/multilingual-e5-base\"),\n",
    "    \"e5_small\": ManualHFEmbedder(\"intfloat/multilingual-e5-small\"),\n",
    "}\n"
   ],
   "id": "b04161c7f393896f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:12:55.161333Z",
     "start_time": "2025-04-23T14:06:00.860883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Initialize persistent client (once)\n",
    "client = chromadb.PersistentClient(path=\"./data/chromadb\")\n",
    "\n",
    "results_times = {}\n",
    "BATCH_SIZE = 512\n",
    "MAX_DOCS = 5461\n",
    "\n",
    "for name, ef in models.items():\n",
    "    print(f\"\\n--- Benchmarking {name} ---\")\n",
    "\n",
    "    col_name = f\"chunks_{name}\"\n",
    "    # Remove any previous run of this collection\n",
    "    try:\n",
    "        client.delete_collection(name=col_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Create (or recreate) the collection\n",
    "    col = client.create_collection(\n",
    "        name=col_name,\n",
    "        embedding_function=ef\n",
    "    )\n",
    "\n",
    "    # Prepare your capped set of chunks\n",
    "    texts = all_texts[:MAX_DOCS]\n",
    "    ids    = all_ids[:MAX_DOCS]\n",
    "\n",
    "    # Batch‐wise embedding + insert\n",
    "    num_batches = ceil(len(texts) / BATCH_SIZE)\n",
    "    t0 = time.time()\n",
    "    for i in range(num_batches):\n",
    "        start = i * BATCH_SIZE\n",
    "        end   = start + BATCH_SIZE\n",
    "        batch_texts = texts[start:end]\n",
    "        batch_ids   = ids[start:end]\n",
    "        col.add(documents=batch_texts, ids=batch_ids)\n",
    "        if name != \"chromadb_default\":\n",
    "            torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(f\"{name} embed+insert time: {elapsed:.2f}s\")\n",
    "    results_times[name] = elapsed\n",
    "\n",
    "# 4. Summary\n",
    "print(\"\\n=== Embedding & Insertion Times ===\")\n",
    "print(f\"{'Model':<20} {'Time (s)':>10}\")\n",
    "for name, t in results_times.items():\n",
    "    print(f\"{name:<20} {t:10.2f}\")\n",
    "\n"
   ],
   "id": "36ddb2f770315f1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking chromadb_default ---\n",
      "chromadb_default embed+insert time: 120.04s\n",
      "\n",
      "--- Benchmarking e5_large ---\n",
      "e5_large embed+insert time: 201.67s\n",
      "\n",
      "--- Benchmarking e5_base ---\n",
      "e5_base embed+insert time: 64.18s\n",
      "\n",
      "--- Benchmarking e5_small ---\n",
      "e5_small embed+insert time: 28.24s\n",
      "\n",
      "=== Embedding & Insertion Times ===\n",
      "Model                  Time (s)\n",
      "chromadb_default         120.04\n",
      "e5_large                 201.67\n",
      "e5_base                   64.18\n",
      "e5_small                  28.24\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:33:56.663185Z",
     "start_time": "2025-04-24T07:33:56.659277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"Wie starte ich den PostgreSQL-Server unter Ubuntu?\",\n",
    "    \"How do I start the PostgreSQL server on Ubuntu?\",\n",
    "    \"Wie überprüfe ich die Speichernutzung von PostgreSQL mit Standard-Tools?\",\n",
    "    \"How do I check PostgreSQL memory usage with standard tools?\",\n",
    "    \"Wie stoppe ich den PostgreSQL-Server unter Ubuntu?\",\n",
    "    \"How do I stop the PostgreSQL server on Ubuntu?\",\n",
    "    \"Wie erstelle ich einen neuen Benutzer in PostgreSQL?\",\n",
    "    \"How do I create a new user in PostgreSQL?\",\n",
    "    \"Wie führe ich ein Backup einer PostgreSQL-Datenbank durch?\",\n",
    "    \"How do I perform a backup of a PostgreSQL database?\",\n",
    "    \"Wie installiere ich PostgreSQL auf Ubuntu über die Kommandozeile?\",\n",
    "    \"How do I install PostgreSQL on Ubuntu via command line?\",\n",
    "    \"Wie finde ich das Datenverzeichnis eines PostgreSQL-Servers?\",\n",
    "    \"How do I find the data directory of a PostgreSQL server?\",\n",
    "    \"Wie konfiguriere ich eine Master-Slave-Replikation in PostgreSQL?\",\n",
    "    \"How do I configure a master-slave replication in PostgreSQL?\",\n",
    "    \"Wie erstelle ich einen Foreign Key mit Cascading Delete in PostgreSQL?\",\n",
    "    \"How do I create a foreign key with cascading delete in PostgreSQL?\",\n",
    "    \"Wie plane ich regelmäßige Wartungsjobs mit PostgreSQL-internen Mitteln (z.B. autovacuum)?\",\n",
    "    \"How do I schedule regular maintenance jobs with PostgreSQL internal tools (e.g., autovacuum)?\",\n",
    "    \"Wie optimiere ich eine langsame PostgreSQL-Abfrage mit EXPLAIN ANALYZE?\",\n",
    "    \"How do I optimize a slow PostgreSQL query with EXPLAIN ANALYZE?\",\n",
    "    \"Wie konfiguriere ich PostgreSQL über Umgebungsvariablen oder Konfigurationsdateien dauerhaft?\",\n",
    "    \"How do I permanently configure PostgreSQL using environment variables or configuration files?\",\n",
    "    \"Wie erstelle ich eine materialisierte Sicht in PostgreSQL und aktualisiere sie regelmäßig?\",\n",
    "    \"How do I create a materialized view in PostgreSQL and update it regularly?\",\n",
    "    \"Wie authentifiziere ich PostgreSQL-Zugriffe über Zertifikate oder SSH-Tunnel?\",\n",
    "    \"How do I authenticate PostgreSQL access via certificates or SSH tunnels?\",\n",
    "    \"Wie verwende ich Transaktionen mit COMMIT und ROLLBACK in PostgreSQL?\",\n",
    "    \"How do I use transactions with COMMIT and ROLLBACK in PostgreSQL?\",\n",
    "    \"Wie erstelle ich eine PostgreSQL Datenbank?\",\n",
    "    \"How do I create a PostgreSQL database?\"\n",
    "]\n",
    "\n",
    "hard_questions = [\n",
    "    # Advanced Questions\n",
    "    \"Wie implementiere ich eine partitionierte Tabelle in PostgreSQL mit Bereichspartitionierung für Zeitstempel?\",\n",
    "    \"Wie konfiguriere ich ein RAID 10 System unter Ubuntu Server mit mdadm?\",\n",
    "    \"Wie optimiere ich den PostgreSQL-Arbeitsspeicherverbrauch für einen Server mit 64GB RAM?\",\n",
    "    \"Wie erstelle ich eine benutzerdefinierte Aggregatfunktion in PostgreSQL mit PL/pgSQL?\",\n",
    "    \"Wie implementiere ich ein automatisches Failover-System für PostgreSQL mit repmgr unter Ubuntu?\",\n",
    "    \"Wie konfiguriere ich SELinux-Richtlinien für eine sichere PostgreSQL-Installation?\",\n",
    "    \"Wie erstelle ich einen benutzerdefinierten Index-Typ in PostgreSQL für Volltextsuche in deutschen Texten?\",\n",
    "    \"Wie setze ich Kernel-Parameter für optimale PostgreSQL-Leistung unter hoher Last in Ubuntu?\",\n",
    "\n",
    "    # Specific Technical Questions\n",
    "    \"Wie führe ich ein VACUUM FULL auf eine PostgreSQL-Tabelle durch, ohne den Betrieb zu unterbrechen?\",\n",
    "    \"Wie konfiguriere ich die pg_hba.conf Datei für SCRAM-SHA-256 Authentifizierung?\",\n",
    "    \"Wie nutze ich systemd, um PostgreSQL bei Absturz automatisch neu zu starten?\",\n",
    "    \"Wie erstelle ich eine PostgreSQL-Erweiterung mit C in Ubuntu?\",\n",
    "    \"Wie implementiere ich Row-Level Security in PostgreSQL für mandantenfähige Anwendungen?\",\n",
    "    \"Wie konfiguriere ich AppArmor-Profile für PostgreSQL in Ubuntu?\",\n",
    "    \"Wie benutze ich die Ubuntu Live Migration für laufende virtuelle Maschinen mit PostgreSQL?\",\n",
    "    \"Wie löse ich das Problem 'too many connections' in PostgreSQL ohne Neustart des Servers?\",\n",
    "\n",
    "    # Combination/Scenario Questions\n",
    "    \"Wie konfiguriere ich einen Ubuntu-Server als PostgreSQL-Datenbank mit automatischem Backup auf S3?\",\n",
    "    \"Wie migriere ich eine große PostgreSQL-Datenbank von Ubuntu 20.04 auf 22.04 mit minimaler Ausfallzeit?\",\n",
    "    \"Wie überwache ich PostgreSQL-Performance-Metriken in Ubuntu mit Prometheus und Grafana?\",\n",
    "    \"Wie richte ich eine hochverfügbare PostgreSQL-Umgebung mit Patroni und etcd in Ubuntu ein?\",\n",
    "    \"Wie implementiere ich eine sichere PostgreSQL-Installation gemäß den DSGVO-Anforderungen unter Ubuntu?\",\n",
    "    \"Wie optimiere ich sowohl das Ubuntu-Dateisystem als auch PostgreSQL für SSD-Speicher?\",\n",
    "    \"Wie konfiguriere ich TLS/SSL für die PostgreSQL-Verbindungen auf einem Ubuntu-Server?\",\n",
    "    \"Wie implementiere ich logische Replikation zwischen verschiedenen PostgreSQL-Versionen in Ubuntu?\",\n",
    "]"
   ],
   "id": "b1f10b397b2d1914",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:34:05.130546Z",
     "start_time": "2025-04-24T07:33:59.151586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Configurable parameters\n",
    "PERSIST_PATH = \"project_root/data/chromadb\"\n",
    "top_k       = 1     # <-- change this if you want more than 1 result per query\n",
    "\n",
    "# 2) Connect to your persistent DB and load collections\n",
    "client = chromadb.PersistentClient(path=\"project_root/data/chromadb\")\n",
    "cols = {\n",
    "    \"default\": client.get_collection(\"chunks_chromadb_default\", embedding_function=models[\"chromadb_default\"]),\n",
    "    \"small\":   client.get_collection(\"chunks_e5_small\", embedding_function=models[\"e5_small\"]),\n",
    "    \"base\":    client.get_collection(\"chunks_e5_base\", embedding_function=models[\"e5_base\"]),\n",
    "    \"large\":   client.get_collection(\"chunks_e5_large\", embedding_function=models[\"e5_large\"]),\n",
    "}\n",
    "\n",
    "\n",
    "# 3) Prepare the results dict\n",
    "results = {\n",
    "    \"question\":       [],\n",
    "    \"time_default\":   [],\n",
    "    \"time_small\":     [],\n",
    "    \"time_base\":      [],\n",
    "    \"time_large\":     [],\n",
    "    \"texts_default\":  [],\n",
    "    \"texts_small\":    [],\n",
    "    \"texts_base\":     [],\n",
    "    \"texts_large\":    [],\n",
    "}\n",
    "\n",
    "# 4) For each question, query all four collections\n",
    "for q in questions:\n",
    "    results[\"question\"].append(q)\n",
    "    # default\n",
    "    t0 = time.perf_counter()\n",
    "    qr = cols[\"default\"].query(query_texts=[q], n_results=top_k)\n",
    "    t1 = time.perf_counter()\n",
    "    results[\"time_default\"].append(t1 - t0)\n",
    "    results[\"texts_default\"].append(qr[\"documents\"][0])\n",
    "    # small\n",
    "    t0 = time.perf_counter()\n",
    "    qr = cols[\"small\"].query(query_texts=[q], n_results=top_k)\n",
    "    t1 = time.perf_counter()\n",
    "    results[\"time_small\"].append(t1 - t0)\n",
    "    results[\"texts_small\"].append(qr[\"documents\"][0])\n",
    "    # base\n",
    "    t0 = time.perf_counter()\n",
    "    qr = cols[\"base\"].query(query_texts=[q], n_results=top_k)\n",
    "    t1 = time.perf_counter()\n",
    "    results[\"time_base\"].append(t1 - t0)\n",
    "    results[\"texts_base\"].append(qr[\"documents\"][0])\n",
    "    # large\n",
    "    t0 = time.perf_counter()\n",
    "    qr = cols[\"large\"].query(query_texts=[q], n_results=top_k)\n",
    "    t1 = time.perf_counter()\n",
    "    results[\"time_large\"].append(t1 - t0)\n",
    "    results[\"texts_large\"].append(qr[\"documents\"][0])\n",
    "\n",
    "# 5) Build DataFrame and export\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df.to_excel(\"./data/retrieval_results.xlsx\", index=False)\n",
    "print(\"Wrote retrieval_results.xlsx\")\n"
   ],
   "id": "d125c5768d7b66f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote retrieval_results.xlsx\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33fce287dfe6dd72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
